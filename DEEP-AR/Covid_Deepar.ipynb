{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = date(2020, 2, 29)\n",
    "end_date = date(2021, 8, 25)\n",
    "delta = timedelta(days=1)\n",
    "start_date_use = []\n",
    "while start_date <= end_date:\n",
    "    start_date_use.append(start_date.strftime(\"%Y-%m-%d\"))\n",
    "    start_date += delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2020-02-29'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_date_use[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The method get_image_uri has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "Defaulting to the only supported framework/algorithm version: 1. Ignoring framework/algorithm version: latest.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import zipfile\n",
    "from dateutil.parser import parse\n",
    "import json\n",
    "from random import shuffle\n",
    "import random\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "import boto3\n",
    "import s3fs\n",
    "import sagemaker\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import timedelta\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import IntSlider, FloatSlider, Checkbox\n",
    "\n",
    "from sagemaker.serializers import IdentitySerializer\n",
    "\n",
    "# set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "s3_bucket = sagemaker.Session().default_bucket()  # replace with an existing bucket if needed\n",
    "s3_prefix = \"deepar-covid-demo-notebook\"  # prefix used for all data stored within the bucket\n",
    "\n",
    "region = sagemaker_session.boto_region_name\n",
    "role = sagemaker.get_execution_role() \n",
    "\n",
    "s3_data_path = \"s3://{}/{}/data\".format(s3_bucket, s3_prefix)\n",
    "s3_output_path = \"s3://{}/{}/output\".format(s3_bucket, s3_prefix)\n",
    "image_name = sagemaker.amazon.amazon_estimator.get_image_uri(region, \"forecasting-deepar\", \"latest\")\n",
    "\n",
    "#make the file direction\n",
    "bucket = 'bingnan-covid-test/covid'\n",
    "data_key = 'after228.csv'\n",
    "\n",
    "data_location = 's3://{}/{}'.format(bucket, data_key)\n",
    "\n",
    "#re-format the data\n",
    "data = pd.read_csv(data_location, index_col=0, parse_dates=True , decimal=\",\")\n",
    "data_new = data.iloc[:, :]\n",
    "\n",
    "#reset the time index\n",
    "data_new.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#set the ts.timeframe date time\n",
    "data_new['date'] = data_new['date'].apply(lambda x: pd.Timestamp(x).strftime('%Y-%m-%d'))\n",
    "data_new = data_new.set_index(pd.to_datetime(data_new['date']))\n",
    "\n",
    "data_new_use = data_new.iloc[: , :]\n",
    "\n",
    "data_new_use = data_new_use.resample(\"D\").sum()\n",
    "\n",
    "num_timeseries = data_new_use.shape[1]\n",
    "\n",
    "# we use 24 hour frequency for the time series\n",
    "freq = \"D\"\n",
    "\n",
    "# we predict for 14 days\n",
    "prediction_length = 7 * 2\n",
    "\n",
    "# we also use 14 days as context length, this is the number of state updates accomplished before making predictions\n",
    "context_length = 7 * 2 \n",
    "\n",
    "\n",
    "def write_dicts_to_file(path, data):\n",
    "    with open(path, \"wb\") as fp:\n",
    "        for d in data:\n",
    "            fp.write(json.dumps(d).encode(\"utf-8\"))\n",
    "            fp.write(\"\\n\".encode(\"utf-8\"))\n",
    "\n",
    "s3 = boto3.resource(\"s3\")\n",
    "\n",
    "\n",
    "def copy_to_s3(local_file, s3_path, override=True):\n",
    "    assert s3_path.startswith(\"s3://\")\n",
    "    split = s3_path.split(\"/\")\n",
    "    bucket = split[2]\n",
    "    path = \"/\".join(split[3:])\n",
    "    buk = s3.Bucket(bucket)\n",
    "\n",
    "    if len(list(buk.objects.filter(Prefix=path))) > 0:\n",
    "        if not override:\n",
    "            print(\n",
    "                \"File s3://{}/{} already exists.\\nSet override to upload anyway.\\n\".format(\n",
    "                    s3_bucket, s3_path\n",
    "                )\n",
    "            )\n",
    "            return\n",
    "        else:\n",
    "            print(\"Overwriting existing file\")\n",
    "    with open(local_file, \"rb\") as data:\n",
    "        print(\"Uploading file to {}\".format(s3_path))\n",
    "        buk.put_object(Key=path, Body=data)\n",
    "\n",
    "estimator = sagemaker.estimator.Estimator(\n",
    "    image_uri=image_name,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.c4.2xlarge\",\n",
    "    base_job_name=\"deepar-covid-demo\",\n",
    "    output_path=s3_output_path,\n",
    ")\n",
    "\n",
    "#default number of layers is 2\n",
    "hyperparameters = {\n",
    "    \"time_freq\": freq,\n",
    "    \"epochs\": \"200\",\n",
    "    \"early_stopping_patience\": \"40\",\n",
    "    \"mini_batch_size\": \"64\",\n",
    "    \"learning_rate\": \"5E-4\",\n",
    "    \"context_length\": str(context_length),\n",
    "    \"prediction_length\": str(prediction_length),\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DeepARPredictor(sagemaker.predictor.Predictor):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(\n",
    "            *args,\n",
    "            # serializer=JSONSerializer(),\n",
    "            serializer=IdentitySerializer(content_type=\"application/json\"),\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        ts,\n",
    "        cat=None,\n",
    "        dynamic_feat=None,\n",
    "        num_samples=10,\n",
    "        return_samples=False,\n",
    "        quantiles=[\"0.1\", \"0.5\", \"0.9\"],\n",
    "    ):\n",
    "        \"\"\"Requests the prediction of for the time series listed in `ts`, each with the (optional)\n",
    "        corresponding category listed in `cat`.\n",
    "\n",
    "        ts -- `pandas.Series` object, the time series to predict\n",
    "        cat -- integer, the group associated to the time series (default: None)\n",
    "        num_samples -- integer, number of samples to compute at prediction time (default: 100)\n",
    "        return_samples -- boolean indicating whether to include samples in the response (default: False)\n",
    "        quantiles -- list of strings specifying the quantiles to compute (default: [\"0.1\", \"0.5\", \"0.9\"])\n",
    "\n",
    "        Return value: list of `pandas.DataFrame` objects, each containing the predictions\n",
    "        \"\"\"\n",
    "        prediction_time = ts.index[-1] + ts.index.freq\n",
    "        quantiles = [str(q) for q in quantiles]\n",
    "        req = self.__encode_request(ts, cat, dynamic_feat, num_samples, return_samples, quantiles)\n",
    "        res = super(DeepARPredictor, self).predict(req)\n",
    "        return self.__decode_response(res, ts.index.freq, prediction_time, return_samples)\n",
    "\n",
    "    def __encode_request(self, ts, cat, dynamic_feat, num_samples, return_samples, quantiles):\n",
    "        instance = series_to_dict(\n",
    "            ts, cat if cat is not None else None, dynamic_feat if dynamic_feat else None\n",
    "        )\n",
    "\n",
    "        configuration = {\n",
    "            \"num_samples\": num_samples,\n",
    "            \"output_types\": [\"quantiles\", \"samples\"] if return_samples else [\"quantiles\"],\n",
    "            \"quantiles\": quantiles,\n",
    "        }\n",
    "\n",
    "        http_request_data = {\"instances\": [instance], \"configuration\": configuration}\n",
    "\n",
    "        return json.dumps(http_request_data).encode(\"utf-8\")\n",
    "\n",
    "    def __decode_response(self, response, freq, prediction_time, return_samples):\n",
    "        # we only sent one time series so we only receive one in return\n",
    "        # however, if possible one will pass multiple time series as predictions will then be faster\n",
    "        predictions = json.loads(response.decode(\"utf-8\"))[\"predictions\"][0]\n",
    "        prediction_length = len(next(iter(predictions[\"quantiles\"].values())))\n",
    "        prediction_index = pd.date_range(\n",
    "            start=prediction_time, freq=freq, periods=prediction_length\n",
    "        )\n",
    "        if return_samples:\n",
    "            dict_of_samples = {\"sample_\" + str(i): s for i, s in enumerate(predictions[\"samples\"])}\n",
    "        else:\n",
    "            dict_of_samples = {}\n",
    "        return pd.DataFrame(\n",
    "            data={**predictions[\"quantiles\"], **dict_of_samples}, index=prediction_index\n",
    "        )\n",
    "\n",
    "    def set_frequency(self, freq):\n",
    "        self.freq = freq\n",
    "\n",
    "\n",
    "def encode_target(ts):\n",
    "    return [x if np.isfinite(x) else \"NaN\" for x in ts]\n",
    "\n",
    "\n",
    "def series_to_dict(ts, cat=None, dynamic_feat=None):\n",
    "    \"\"\"Given a pandas.Series object, returns a dictionary encoding the time series.\n",
    "\n",
    "    ts -- a pands.Series object with the target time series\n",
    "    cat -- an integer indicating the time series category\n",
    "\n",
    "    Return value: a dictionary\n",
    "    \"\"\"\n",
    "    obj = {\"start\": str(ts.index[0]), \"target\": encode_target(ts)}\n",
    "    if cat is not None:\n",
    "        obj[\"cat\"] = cat\n",
    "    if dynamic_feat is not None:\n",
    "        obj[\"dynamic_feat\"] = dynamic_feat\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting existing file\n",
      "Uploading file to s3://sagemaker-us-east-2-222195595958/deepar-covid-demo-notebook/data/train/train.json\n",
      "2021-09-12 14:35:56 Starting - Starting the training job...\n",
      "2021-09-12 14:36:19 Starting - Launching requested ML instancesProfilerReport-1631457356: InProgress\n",
      "...\n",
      "2021-09-12 14:36:46 Starting - Preparing the instances for training......\n",
      "2021-09-12 14:37:56 Downloading - Downloading input data\n",
      "2021-09-12 14:37:56 Training - Downloading the training image........\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:06 INFO 140186635052416] Reading default configuration from /opt/amazon/lib/python3.6/site-packages/algorithm/resources/default-input.json: {'_kvstore': 'auto', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_tuning_objective_metric': '', 'cardinality': 'auto', 'dropout_rate': '0.10', 'early_stopping_patience': '', 'embedding_dimension': '10', 'learning_rate': '0.001', 'likelihood': 'student-t', 'mini_batch_size': '128', 'num_cells': '40', 'num_dynamic_feat': 'auto', 'num_eval_samples': '100', 'num_layers': '2', 'test_quantiles': '[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]'}\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:06 INFO 140186635052416] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {'prediction_length': '14', 'time_freq': 'D', 'context_length': '14', 'epochs': '200', 'learning_rate': '5E-4', 'early_stopping_patience': '40', 'mini_batch_size': '64'}\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:06 INFO 140186635052416] Final configuration: {'_kvstore': 'auto', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_tuning_objective_metric': '', 'cardinality': 'auto', 'dropout_rate': '0.10', 'early_stopping_patience': '40', 'embedding_dimension': '10', 'learning_rate': '5E-4', 'likelihood': 'student-t', 'mini_batch_size': '64', 'num_cells': '40', 'num_dynamic_feat': 'auto', 'num_eval_samples': '100', 'num_layers': '2', 'test_quantiles': '[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', 'prediction_length': '14', 'time_freq': 'D', 'context_length': '14', 'epochs': '200'}\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:06 INFO 140186635052416] Detected entry point for worker worker\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:06 INFO 140186635052416] Using early stopping with patience 40\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:06 INFO 140186635052416] random_seed is None\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:06 INFO 140186635052416] [cardinality=auto] `cat` field was NOT found in the file `/opt/ml/input/data/train/train.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:06 INFO 140186635052416] [num_dynamic_feat=auto] `dynamic_feat` field was NOT found in the file `/opt/ml/input/data/train/train.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:06 INFO 140186635052416] Training set statistics:\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:06 INFO 140186635052416] Integer time series\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:06 INFO 140186635052416] number of time series: 41\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:06 INFO 140186635052416] number of observations: 13488\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:06 INFO 140186635052416] mean target length: 328.9756097560976\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:06 INFO 140186635052416] min/mean/max target: 0.0/15052094.753707/347377152.0\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:06 INFO 140186635052416] mean abs(target): 15052094.753707\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:06 INFO 140186635052416] contains missing values: no\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:06 INFO 140186635052416] Small number of time series. Doing 16 passes over dataset with prob 0.975609756097561 per epoch.\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:06 INFO 140186635052416] No test channel found not running evaluations\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:06 INFO 140186635052416] #memory_usage::<batchbuffer> = 2.2568130493164062 mb\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:06 INFO 140186635052416] nvidia-smi took: 0.025211572647094727 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:06 INFO 140186635052416] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:06 INFO 140186635052416] Create Store: local\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457546.451822, \"EndTime\": 1631457546.5370116, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"get_graph.time\": {\"sum\": 83.27054977416992, \"count\": 1, \"min\": 83.27054977416992, \"max\": 83.27054977416992}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:06 INFO 140186635052416] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:06 INFO 140186635052416] #memory_usage::<model> = 12 mb\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457546.5370834, \"EndTime\": 1631457546.6503918, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"initialize.time\": {\"sum\": 198.4539031982422, \"count\": 1, \"min\": 198.4539031982422, \"max\": 198.4539031982422}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:07 INFO 140186635052416] Epoch[0] Batch[0] avg_epoch_loss=14.567205\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:07 INFO 140186635052416] #quality_metric: host=algo-1, epoch=0, batch=0 train loss <loss>=14.567205429077148\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:07 INFO 140186635052416] Epoch[0] Batch[5] avg_epoch_loss=14.210937\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:07 INFO 140186635052416] #quality_metric: host=algo-1, epoch=0, batch=5 train loss <loss>=14.210937023162842\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:07 INFO 140186635052416] Epoch[0] Batch [5]#011Speed: 1865.75 samples/sec#011loss=14.210937\u001b[0m\n",
      "\u001b[34m/opt/amazon/python3.6/lib/python3.6/contextlib.py:99: DeprecationWarning: generator 'local_timer' raised StopIteration\n",
      "  self.gen.throw(type, value, traceback)\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:07 INFO 140186635052416] processed a total of 600 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457546.650482, \"EndTime\": 1631457547.3205016, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"epochs\": {\"sum\": 200.0, \"count\": 1, \"min\": 200, \"max\": 200}, \"update.time\": {\"sum\": 669.9128150939941, \"count\": 1, \"min\": 669.9128150939941, \"max\": 669.9128150939941}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:07 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=895.4423069738315 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:07 INFO 140186635052416] #progress_metric: host=algo-1, completed 0.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:07 INFO 140186635052416] #quality_metric: host=algo-1, epoch=0, train loss <loss>=13.552805614471435\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:07 INFO 140186635052416] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:07 INFO 140186635052416] Saved checkpoint to \"/opt/ml/model/state_53bdc4e6-c26e-4cc2-b7b2-d1b371344970-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457547.3205957, \"EndTime\": 1631457547.3353794, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 14.181137084960938, \"count\": 1, \"min\": 14.181137084960938, \"max\": 14.181137084960938}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:07 INFO 140186635052416] Epoch[1] Batch[0] avg_epoch_loss=12.944639\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:07 INFO 140186635052416] #quality_metric: host=algo-1, epoch=1, batch=0 train loss <loss>=12.944639205932617\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:07 INFO 140186635052416] Epoch[1] Batch[5] avg_epoch_loss=12.814784\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:07 INFO 140186635052416] #quality_metric: host=algo-1, epoch=1, batch=5 train loss <loss>=12.814783732096354\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:07 INFO 140186635052416] Epoch[1] Batch [5]#011Speed: 1574.46 samples/sec#011loss=12.814784\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:07 INFO 140186635052416] processed a total of 613 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457547.3354445, \"EndTime\": 1631457547.9598794, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 624.3667602539062, \"count\": 1, \"min\": 624.3667602539062, \"max\": 624.3667602539062}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:07 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=981.5649754504567 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:07 INFO 140186635052416] #progress_metric: host=algo-1, completed 1.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:07 INFO 140186635052416] #quality_metric: host=algo-1, epoch=1, train loss <loss>=12.603193855285644\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:07 INFO 140186635052416] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:07 INFO 140186635052416] Saved checkpoint to \"/opt/ml/model/state_4b06ca08-c1af-45b2-a348-e6bcb19a0782-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457547.9599786, \"EndTime\": 1631457547.9799206, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 19.306182861328125, \"count\": 1, \"min\": 19.306182861328125, \"max\": 19.306182861328125}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:08 INFO 140186635052416] Epoch[2] Batch[0] avg_epoch_loss=12.944263\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:08 INFO 140186635052416] #quality_metric: host=algo-1, epoch=2, batch=0 train loss <loss>=12.944262504577637\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:08 INFO 140186635052416] Epoch[2] Batch[5] avg_epoch_loss=12.313832\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:08 INFO 140186635052416] #quality_metric: host=algo-1, epoch=2, batch=5 train loss <loss>=12.313831806182861\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:08 INFO 140186635052416] Epoch[2] Batch [5]#011Speed: 1552.46 samples/sec#011loss=12.313832\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:08 INFO 140186635052416] Epoch[2] Batch[10] avg_epoch_loss=12.459608\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:08 INFO 140186635052416] #quality_metric: host=algo-1, epoch=2, batch=10 train loss <loss>=12.63453941345215\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:08 INFO 140186635052416] Epoch[2] Batch [10]#011Speed: 1634.44 samples/sec#011loss=12.634539\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:08 INFO 140186635052416] processed a total of 645 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457547.9800003, \"EndTime\": 1631457548.7433858, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 763.314962387085, \"count\": 1, \"min\": 763.314962387085, \"max\": 763.314962387085}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:08 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=844.762085277275 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:08 INFO 140186635052416] #progress_metric: host=algo-1, completed 1.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:08 INFO 140186635052416] #quality_metric: host=algo-1, epoch=2, train loss <loss>=12.459607991305264\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:08 INFO 140186635052416] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:08 INFO 140186635052416] Saved checkpoint to \"/opt/ml/model/state_5a001d85-6a77-4219-a2e2-ac45eceae400-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457548.743555, \"EndTime\": 1631457548.7634735, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 19.138097763061523, \"count\": 1, \"min\": 19.138097763061523, \"max\": 19.138097763061523}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:09 INFO 140186635052416] Epoch[3] Batch[0] avg_epoch_loss=12.156799\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:09 INFO 140186635052416] #quality_metric: host=algo-1, epoch=3, batch=0 train loss <loss>=12.15679931640625\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:09 INFO 140186635052416] Epoch[3] Batch[5] avg_epoch_loss=12.415330\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:09 INFO 140186635052416] #quality_metric: host=algo-1, epoch=3, batch=5 train loss <loss>=12.415330410003662\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:09 INFO 140186635052416] Epoch[3] Batch [5]#011Speed: 2100.13 samples/sec#011loss=12.415330\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:09 INFO 140186635052416] processed a total of 630 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457548.7635615, \"EndTime\": 1631457549.361437, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 597.7919101715088, \"count\": 1, \"min\": 597.7919101715088, \"max\": 597.7919101715088}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:09 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=1053.6489853728287 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:09 INFO 140186635052416] #progress_metric: host=algo-1, completed 2.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:09 INFO 140186635052416] #quality_metric: host=algo-1, epoch=3, train loss <loss>=12.295358657836914\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:09 INFO 140186635052416] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:09 INFO 140186635052416] Saved checkpoint to \"/opt/ml/model/state_2ad4482e-bbb2-4322-ba1d-b2489c796c2f-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457549.361517, \"EndTime\": 1631457549.373622, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 11.535406112670898, \"count\": 1, \"min\": 11.535406112670898, \"max\": 11.535406112670898}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:09 INFO 140186635052416] Epoch[4] Batch[0] avg_epoch_loss=11.065371\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:09 INFO 140186635052416] #quality_metric: host=algo-1, epoch=4, batch=0 train loss <loss>=11.065370559692383\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:09 INFO 140186635052416] Epoch[4] Batch[5] avg_epoch_loss=11.385210\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:09 INFO 140186635052416] #quality_metric: host=algo-1, epoch=4, batch=5 train loss <loss>=11.385209560394287\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:09 INFO 140186635052416] Epoch[4] Batch [5]#011Speed: 2013.61 samples/sec#011loss=11.385210\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:09 INFO 140186635052416] processed a total of 597 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457549.3736787, \"EndTime\": 1631457549.8989441, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 525.1963138580322, \"count\": 1, \"min\": 525.1963138580322, \"max\": 525.1963138580322}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:09 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=1136.4051775429227 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:09 INFO 140186635052416] #progress_metric: host=algo-1, completed 2.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:09 INFO 140186635052416] #quality_metric: host=algo-1, epoch=4, train loss <loss>=11.470409870147705\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:09 INFO 140186635052416] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:09 INFO 140186635052416] Saved checkpoint to \"/opt/ml/model/state_aa26fa49-833a-4574-9134-a8d5cae9475b-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457549.8990455, \"EndTime\": 1631457549.918345, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 18.726587295532227, \"count\": 1, \"min\": 18.726587295532227, \"max\": 18.726587295532227}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:10 INFO 140186635052416] Epoch[5] Batch[0] avg_epoch_loss=11.744137\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:10 INFO 140186635052416] #quality_metric: host=algo-1, epoch=5, batch=0 train loss <loss>=11.744136810302734\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:10 INFO 140186635052416] Epoch[5] Batch[5] avg_epoch_loss=11.451336\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:10 INFO 140186635052416] #quality_metric: host=algo-1, epoch=5, batch=5 train loss <loss>=11.45133606592814\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:10 INFO 140186635052416] Epoch[5] Batch [5]#011Speed: 1810.61 samples/sec#011loss=11.451336\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:10 INFO 140186635052416] processed a total of 583 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457549.9184098, \"EndTime\": 1631457550.4782794, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 559.8065853118896, \"count\": 1, \"min\": 559.8065853118896, \"max\": 559.8065853118896}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:10 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=1041.1845717898877 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:10 INFO 140186635052416] #progress_metric: host=algo-1, completed 3.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:10 INFO 140186635052416] #quality_metric: host=algo-1, epoch=5, train loss <loss>=11.905521011352539\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:10 INFO 140186635052416] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:10 INFO 140186635052416] Epoch[6] Batch[0] avg_epoch_loss=10.218751\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:10 INFO 140186635052416] #quality_metric: host=algo-1, epoch=6, batch=0 train loss <loss>=10.218750953674316\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:10 INFO 140186635052416] Epoch[6] Batch[5] avg_epoch_loss=10.783054\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:10 INFO 140186635052416] #quality_metric: host=algo-1, epoch=6, batch=5 train loss <loss>=10.783054192860922\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:10 INFO 140186635052416] Epoch[6] Batch [5]#011Speed: 1515.94 samples/sec#011loss=10.783054\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:11 INFO 140186635052416] processed a total of 611 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457550.478371, \"EndTime\": 1631457551.0855756, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 606.6844463348389, \"count\": 1, \"min\": 606.6844463348389, \"max\": 606.6844463348389}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:11 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=1006.8798845202889 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:11 INFO 140186635052416] #progress_metric: host=algo-1, completed 3.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:11 INFO 140186635052416] #quality_metric: host=algo-1, epoch=6, train loss <loss>=10.776082420349121\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:11 INFO 140186635052416] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:11 INFO 140186635052416] Saved checkpoint to \"/opt/ml/model/state_7a7e2978-2f85-4417-bb1a-06c30c8a74ec-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457551.0856686, \"EndTime\": 1631457551.0982068, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 11.946678161621094, \"count\": 1, \"min\": 11.946678161621094, \"max\": 11.946678161621094}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:11 INFO 140186635052416] Epoch[7] Batch[0] avg_epoch_loss=11.066595\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:11 INFO 140186635052416] #quality_metric: host=algo-1, epoch=7, batch=0 train loss <loss>=11.066595077514648\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:11 INFO 140186635052416] Epoch[7] Batch[5] avg_epoch_loss=10.762914\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:11 INFO 140186635052416] #quality_metric: host=algo-1, epoch=7, batch=5 train loss <loss>=10.762914180755615\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:11 INFO 140186635052416] Epoch[7] Batch [5]#011Speed: 1817.85 samples/sec#011loss=10.762914\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:11 INFO 140186635052416] Epoch[7] Batch[10] avg_epoch_loss=10.615700\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:11 INFO 140186635052416] #quality_metric: host=algo-1, epoch=7, batch=10 train loss <loss>=10.439042854309083\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:11 INFO 140186635052416] Epoch[7] Batch [10]#011Speed: 2008.28 samples/sec#011loss=10.439043\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:11 INFO 140186635052416] processed a total of 650 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457551.098276, \"EndTime\": 1631457551.6820257, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 583.6818218231201, \"count\": 1, \"min\": 583.6818218231201, \"max\": 583.6818218231201}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:11 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=1113.3766628891135 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:11 INFO 140186635052416] #progress_metric: host=algo-1, completed 4.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:11 INFO 140186635052416] #quality_metric: host=algo-1, epoch=7, train loss <loss>=10.615699941461736\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:11 INFO 140186635052416] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:11 INFO 140186635052416] Saved checkpoint to \"/opt/ml/model/state_ddae053c-619a-4af0-a44b-b4d07196d417-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457551.6821132, \"EndTime\": 1631457551.7014642, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 18.8138484954834, \"count\": 1, \"min\": 18.8138484954834, \"max\": 18.8138484954834}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:11 INFO 140186635052416] Epoch[8] Batch[0] avg_epoch_loss=11.405850\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:11 INFO 140186635052416] #quality_metric: host=algo-1, epoch=8, batch=0 train loss <loss>=11.405850410461426\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:12 INFO 140186635052416] Epoch[8] Batch[5] avg_epoch_loss=10.874860\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:12 INFO 140186635052416] #quality_metric: host=algo-1, epoch=8, batch=5 train loss <loss>=10.874859809875488\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:12 INFO 140186635052416] Epoch[8] Batch [5]#011Speed: 1621.58 samples/sec#011loss=10.874860\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:12 INFO 140186635052416] Epoch[8] Batch[10] avg_epoch_loss=11.229575\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:12 INFO 140186635052416] #quality_metric: host=algo-1, epoch=8, batch=10 train loss <loss>=11.6552339553833\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:12 INFO 140186635052416] Epoch[8] Batch [10]#011Speed: 2002.03 samples/sec#011loss=11.655234\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:12 INFO 140186635052416] processed a total of 656 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457551.701539, \"EndTime\": 1631457552.3199534, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 618.3481216430664, \"count\": 1, \"min\": 618.3481216430664, \"max\": 618.3481216430664}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:12 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=1060.6759519611298 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:12 INFO 140186635052416] #progress_metric: host=algo-1, completed 4.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:12 INFO 140186635052416] #quality_metric: host=algo-1, epoch=8, train loss <loss>=11.229575330560857\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:12 INFO 140186635052416] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:12 INFO 140186635052416] Epoch[9] Batch[0] avg_epoch_loss=10.369987\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:12 INFO 140186635052416] #quality_metric: host=algo-1, epoch=9, batch=0 train loss <loss>=10.369986534118652\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:12 INFO 140186635052416] Epoch[9] Batch[5] avg_epoch_loss=10.719751\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:12 INFO 140186635052416] #quality_metric: host=algo-1, epoch=9, batch=5 train loss <loss>=10.719750881195068\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:12 INFO 140186635052416] Epoch[9] Batch [5]#011Speed: 2024.92 samples/sec#011loss=10.719751\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:12 INFO 140186635052416] Epoch[9] Batch[10] avg_epoch_loss=10.547254\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:12 INFO 140186635052416] #quality_metric: host=algo-1, epoch=9, batch=10 train loss <loss>=10.340256881713866\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:12 INFO 140186635052416] Epoch[9] Batch [10]#011Speed: 1804.31 samples/sec#011loss=10.340257\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:12 INFO 140186635052416] processed a total of 671 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457552.3200393, \"EndTime\": 1631457552.9079666, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 587.4307155609131, \"count\": 1, \"min\": 587.4307155609131, \"max\": 587.4307155609131}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:12 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=1142.0329286572812 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:12 INFO 140186635052416] #progress_metric: host=algo-1, completed 5.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:12 INFO 140186635052416] #quality_metric: host=algo-1, epoch=9, train loss <loss>=10.547253608703613\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:12 INFO 140186635052416] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:12 INFO 140186635052416] Saved checkpoint to \"/opt/ml/model/state_c4524c3b-0319-4204-94f8-57d51d85ac84-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457552.9080493, \"EndTime\": 1631457552.9203272, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 11.733055114746094, \"count\": 1, \"min\": 11.733055114746094, \"max\": 11.733055114746094}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:13 INFO 140186635052416] Epoch[10] Batch[0] avg_epoch_loss=11.341955\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:13 INFO 140186635052416] #quality_metric: host=algo-1, epoch=10, batch=0 train loss <loss>=11.341955184936523\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:13 INFO 140186635052416] Epoch[10] Batch[5] avg_epoch_loss=10.934358\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:13 INFO 140186635052416] #quality_metric: host=algo-1, epoch=10, batch=5 train loss <loss>=10.934358278910318\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:13 INFO 140186635052416] Epoch[10] Batch [5]#011Speed: 2008.29 samples/sec#011loss=10.934358\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:13 INFO 140186635052416] Epoch[10] Batch[10] avg_epoch_loss=11.007501\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:13 INFO 140186635052416] #quality_metric: host=algo-1, epoch=10, batch=10 train loss <loss>=11.095272827148438\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:13 INFO 140186635052416] Epoch[10] Batch [10]#011Speed: 1714.92 samples/sec#011loss=11.095273\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:13 INFO 140186635052416] processed a total of 718 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457552.9203947, \"EndTime\": 1631457553.5451412, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 624.6614456176758, \"count\": 1, \"min\": 624.6614456176758, \"max\": 624.6614456176758}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:13 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=1149.1397305645705 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:13 INFO 140186635052416] #progress_metric: host=algo-1, completed 5.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:13 INFO 140186635052416] #quality_metric: host=algo-1, epoch=10, train loss <loss>=11.365058024724325\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:13 INFO 140186635052416] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:13 INFO 140186635052416] Epoch[11] Batch[0] avg_epoch_loss=10.865612\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:13 INFO 140186635052416] #quality_metric: host=algo-1, epoch=11, batch=0 train loss <loss>=10.865612030029297\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:14 INFO 140186635052416] Epoch[11] Batch[5] avg_epoch_loss=10.642384\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:14 INFO 140186635052416] #quality_metric: host=algo-1, epoch=11, batch=5 train loss <loss>=10.642383893330893\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:14 INFO 140186635052416] Epoch[11] Batch [5]#011Speed: 1656.70 samples/sec#011loss=10.642384\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:14 INFO 140186635052416] processed a total of 620 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457553.5452335, \"EndTime\": 1631457554.1652172, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 619.4288730621338, \"count\": 1, \"min\": 619.4288730621338, \"max\": 619.4288730621338}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:14 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=1000.6947726580938 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:14 INFO 140186635052416] #progress_metric: host=algo-1, completed 6.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:14 INFO 140186635052416] #quality_metric: host=algo-1, epoch=11, train loss <loss>=10.668338012695312\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:14 INFO 140186635052416] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:14 INFO 140186635052416] Epoch[12] Batch[0] avg_epoch_loss=10.335279\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:14 INFO 140186635052416] #quality_metric: host=algo-1, epoch=12, batch=0 train loss <loss>=10.335278511047363\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:14 INFO 140186635052416] Epoch[12] Batch[5] avg_epoch_loss=10.487843\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:14 INFO 140186635052416] #quality_metric: host=algo-1, epoch=12, batch=5 train loss <loss>=10.487842718760172\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:14 INFO 140186635052416] Epoch[12] Batch [5]#011Speed: 2085.61 samples/sec#011loss=10.487843\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:14 INFO 140186635052416] Epoch[12] Batch[10] avg_epoch_loss=10.493104\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:14 INFO 140186635052416] #quality_metric: host=algo-1, epoch=12, batch=10 train loss <loss>=10.499417877197265\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:14 INFO 140186635052416] Epoch[12] Batch [10]#011Speed: 1735.92 samples/sec#011loss=10.499418\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:14 INFO 140186635052416] processed a total of 693 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457554.1653125, \"EndTime\": 1631457554.7558424, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 589.9715423583984, \"count\": 1, \"min\": 589.9715423583984, \"max\": 589.9715423583984}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:14 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=1174.376650263024 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:14 INFO 140186635052416] #progress_metric: host=algo-1, completed 6.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:14 INFO 140186635052416] #quality_metric: host=algo-1, epoch=12, train loss <loss>=10.493104154413397\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:14 INFO 140186635052416] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:14 INFO 140186635052416] Saved checkpoint to \"/opt/ml/model/state_083021e1-7398-4f3a-b9d6-c1f5e4a895ef-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457554.7559292, \"EndTime\": 1631457554.7754643, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 19.00029182434082, \"count\": 1, \"min\": 19.00029182434082, \"max\": 19.00029182434082}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:15 INFO 140186635052416] Epoch[13] Batch[0] avg_epoch_loss=9.852833\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:15 INFO 140186635052416] #quality_metric: host=algo-1, epoch=13, batch=0 train loss <loss>=9.852832794189453\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:15 INFO 140186635052416] Epoch[13] Batch[5] avg_epoch_loss=10.036982\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:15 INFO 140186635052416] #quality_metric: host=algo-1, epoch=13, batch=5 train loss <loss>=10.036981900533041\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:15 INFO 140186635052416] Epoch[13] Batch [5]#011Speed: 1789.22 samples/sec#011loss=10.036982\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:15 INFO 140186635052416] Epoch[13] Batch[10] avg_epoch_loss=10.082416\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:15 INFO 140186635052416] #quality_metric: host=algo-1, epoch=13, batch=10 train loss <loss>=10.136936950683594\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:15 INFO 140186635052416] Epoch[13] Batch [10]#011Speed: 1979.47 samples/sec#011loss=10.136937\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:15 INFO 140186635052416] processed a total of 661 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457554.7755375, \"EndTime\": 1631457555.4117901, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 636.1813545227051, \"count\": 1, \"min\": 636.1813545227051, \"max\": 636.1813545227051}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:15 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=1038.8008992536195 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:15 INFO 140186635052416] #progress_metric: host=algo-1, completed 7.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:15 INFO 140186635052416] #quality_metric: host=algo-1, epoch=13, train loss <loss>=10.082416014237838\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:15 INFO 140186635052416] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:15 INFO 140186635052416] Saved checkpoint to \"/opt/ml/model/state_d11e6b8c-e373-4f9f-8015-848e2db37163-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457555.4118772, \"EndTime\": 1631457555.4253933, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 13.014793395996094, \"count\": 1, \"min\": 13.014793395996094, \"max\": 13.014793395996094}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:15 INFO 140186635052416] Epoch[14] Batch[0] avg_epoch_loss=10.179887\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:15 INFO 140186635052416] #quality_metric: host=algo-1, epoch=14, batch=0 train loss <loss>=10.179886817932129\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:15 INFO 140186635052416] Epoch[14] Batch[5] avg_epoch_loss=10.590348\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:15 INFO 140186635052416] #quality_metric: host=algo-1, epoch=14, batch=5 train loss <loss>=10.590348243713379\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:15 INFO 140186635052416] Epoch[14] Batch [5]#011Speed: 1926.40 samples/sec#011loss=10.590348\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:15 INFO 140186635052416] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457555.4254525, \"EndTime\": 1631457555.9913554, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 565.8416748046875, \"count\": 1, \"min\": 565.8416748046875, \"max\": 565.8416748046875}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:15 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=1118.4278250261284 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:15 INFO 140186635052416] #progress_metric: host=algo-1, completed 7.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:15 INFO 140186635052416] #quality_metric: host=algo-1, epoch=14, train loss <loss>=10.504817676544189\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:15 INFO 140186635052416] loss did not improve\u001b[0m\n",
      "\n",
      "2021-09-12 14:39:20 Training - Training image download completed. Training in progress.\u001b[34m[09/12/2021 14:39:16 INFO 140186635052416] Epoch[15] Batch[0] avg_epoch_loss=10.974829\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:16 INFO 140186635052416] #quality_metric: host=algo-1, epoch=15, batch=0 train loss <loss>=10.974828720092773\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:16 INFO 140186635052416] Epoch[15] Batch[5] avg_epoch_loss=10.755213\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:16 INFO 140186635052416] #quality_metric: host=algo-1, epoch=15, batch=5 train loss <loss>=10.755212942759195\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:16 INFO 140186635052416] Epoch[15] Batch [5]#011Speed: 1983.60 samples/sec#011loss=10.755213\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:16 INFO 140186635052416] Epoch[15] Batch[10] avg_epoch_loss=10.295781\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:16 INFO 140186635052416] #quality_metric: host=algo-1, epoch=15, batch=10 train loss <loss>=9.744463443756104\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:16 INFO 140186635052416] Epoch[15] Batch [10]#011Speed: 1844.61 samples/sec#011loss=9.744463\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:16 INFO 140186635052416] processed a total of 672 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457555.99145, \"EndTime\": 1631457556.5682256, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 576.1606693267822, \"count\": 1, \"min\": 576.1606693267822, \"max\": 576.1606693267822}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:16 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=1166.0855852184602 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:16 INFO 140186635052416] #progress_metric: host=algo-1, completed 8.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:16 INFO 140186635052416] #quality_metric: host=algo-1, epoch=15, train loss <loss>=10.295781352303244\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:16 INFO 140186635052416] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:16 INFO 140186635052416] Epoch[16] Batch[0] avg_epoch_loss=9.412075\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:16 INFO 140186635052416] #quality_metric: host=algo-1, epoch=16, batch=0 train loss <loss>=9.41207504272461\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:16 INFO 140186635052416] Epoch[16] Batch[5] avg_epoch_loss=10.269310\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:16 INFO 140186635052416] #quality_metric: host=algo-1, epoch=16, batch=5 train loss <loss>=10.269309520721436\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:16 INFO 140186635052416] Epoch[16] Batch [5]#011Speed: 2123.00 samples/sec#011loss=10.269310\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:17 INFO 140186635052416] processed a total of 627 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457556.5683076, \"EndTime\": 1631457557.0953617, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 526.5600681304932, \"count\": 1, \"min\": 526.5600681304932, \"max\": 526.5600681304932}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:17 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=1190.4363292336802 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:17 INFO 140186635052416] #progress_metric: host=algo-1, completed 8.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:17 INFO 140186635052416] #quality_metric: host=algo-1, epoch=16, train loss <loss>=10.140989303588867\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:17 INFO 140186635052416] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:17 INFO 140186635052416] Epoch[17] Batch[0] avg_epoch_loss=11.574153\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:17 INFO 140186635052416] #quality_metric: host=algo-1, epoch=17, batch=0 train loss <loss>=11.574152946472168\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:17 INFO 140186635052416] Epoch[17] Batch[5] avg_epoch_loss=10.396627\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:17 INFO 140186635052416] #quality_metric: host=algo-1, epoch=17, batch=5 train loss <loss>=10.396626631418863\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:17 INFO 140186635052416] Epoch[17] Batch [5]#011Speed: 2094.19 samples/sec#011loss=10.396627\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:17 INFO 140186635052416] processed a total of 636 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457557.0954537, \"EndTime\": 1631457557.6563833, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 560.3947639465332, \"count\": 1, \"min\": 560.3947639465332, \"max\": 560.3947639465332}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:17 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=1134.635227584001 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:17 INFO 140186635052416] #progress_metric: host=algo-1, completed 9.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:17 INFO 140186635052416] #quality_metric: host=algo-1, epoch=17, train loss <loss>=10.475728607177734\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:17 INFO 140186635052416] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:17 INFO 140186635052416] Epoch[18] Batch[0] avg_epoch_loss=10.713499\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:17 INFO 140186635052416] #quality_metric: host=algo-1, epoch=18, batch=0 train loss <loss>=10.713499069213867\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:18 INFO 140186635052416] Epoch[18] Batch[5] avg_epoch_loss=10.512068\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:18 INFO 140186635052416] #quality_metric: host=algo-1, epoch=18, batch=5 train loss <loss>=10.512067794799805\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:18 INFO 140186635052416] Epoch[18] Batch [5]#011Speed: 2064.63 samples/sec#011loss=10.512068\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:18 INFO 140186635052416] Epoch[18] Batch[10] avg_epoch_loss=10.254010\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:18 INFO 140186635052416] #quality_metric: host=algo-1, epoch=18, batch=10 train loss <loss>=9.944339942932128\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:18 INFO 140186635052416] Epoch[18] Batch [10]#011Speed: 1743.07 samples/sec#011loss=9.944340\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:18 INFO 140186635052416] processed a total of 685 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457557.6564748, \"EndTime\": 1631457558.2441976, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 587.1977806091309, \"count\": 1, \"min\": 587.1977806091309, \"max\": 587.1977806091309}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:18 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=1166.3022986289413 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:18 INFO 140186635052416] #progress_metric: host=algo-1, completed 9.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:18 INFO 140186635052416] #quality_metric: host=algo-1, epoch=18, train loss <loss>=10.254009680314498\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:18 INFO 140186635052416] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:18 INFO 140186635052416] Epoch[19] Batch[0] avg_epoch_loss=10.513011\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:18 INFO 140186635052416] #quality_metric: host=algo-1, epoch=19, batch=0 train loss <loss>=10.51301097869873\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:18 INFO 140186635052416] Epoch[19] Batch[5] avg_epoch_loss=10.286573\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:18 INFO 140186635052416] #quality_metric: host=algo-1, epoch=19, batch=5 train loss <loss>=10.286572615305582\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:18 INFO 140186635052416] Epoch[19] Batch [5]#011Speed: 2102.73 samples/sec#011loss=10.286573\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:18 INFO 140186635052416] processed a total of 636 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457558.2442837, \"EndTime\": 1631457558.7768033, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 532.0298671722412, \"count\": 1, \"min\": 532.0298671722412, \"max\": 532.0298671722412}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:18 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=1195.1115169120737 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:18 INFO 140186635052416] #progress_metric: host=algo-1, completed 10.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:18 INFO 140186635052416] #quality_metric: host=algo-1, epoch=19, train loss <loss>=10.269059371948241\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:18 INFO 140186635052416] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:19 INFO 140186635052416] Epoch[20] Batch[0] avg_epoch_loss=10.462418\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:19 INFO 140186635052416] #quality_metric: host=algo-1, epoch=20, batch=0 train loss <loss>=10.462417602539062\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:19 INFO 140186635052416] Epoch[20] Batch[5] avg_epoch_loss=10.264456\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:19 INFO 140186635052416] #quality_metric: host=algo-1, epoch=20, batch=5 train loss <loss>=10.264455795288086\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:19 INFO 140186635052416] Epoch[20] Batch [5]#011Speed: 2050.11 samples/sec#011loss=10.264456\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:19 INFO 140186635052416] processed a total of 618 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457558.776896, \"EndTime\": 1631457559.3458624, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 568.4120655059814, \"count\": 1, \"min\": 568.4120655059814, \"max\": 568.4120655059814}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:19 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=1086.97862047709 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:19 INFO 140186635052416] #progress_metric: host=algo-1, completed 10.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:19 INFO 140186635052416] #quality_metric: host=algo-1, epoch=20, train loss <loss>=10.338267803192139\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:19 INFO 140186635052416] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:19 INFO 140186635052416] Epoch[21] Batch[0] avg_epoch_loss=9.982057\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:19 INFO 140186635052416] #quality_metric: host=algo-1, epoch=21, batch=0 train loss <loss>=9.982056617736816\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:19 INFO 140186635052416] Epoch[21] Batch[5] avg_epoch_loss=9.903666\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:19 INFO 140186635052416] #quality_metric: host=algo-1, epoch=21, batch=5 train loss <loss>=9.903666019439697\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:19 INFO 140186635052416] Epoch[21] Batch [5]#011Speed: 2095.65 samples/sec#011loss=9.903666\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:19 INFO 140186635052416] Epoch[21] Batch[10] avg_epoch_loss=10.277141\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:19 INFO 140186635052416] #quality_metric: host=algo-1, epoch=21, batch=10 train loss <loss>=10.725311660766602\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:19 INFO 140186635052416] Epoch[21] Batch [10]#011Speed: 1888.16 samples/sec#011loss=10.725312\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:19 INFO 140186635052416] processed a total of 680 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457559.3459525, \"EndTime\": 1631457559.914567, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 568.1090354919434, \"count\": 1, \"min\": 568.1090354919434, \"max\": 568.1090354919434}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:19 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=1196.681128019052 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:19 INFO 140186635052416] #progress_metric: host=algo-1, completed 11.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:19 INFO 140186635052416] #quality_metric: host=algo-1, epoch=21, train loss <loss>=10.277141310951926\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:19 INFO 140186635052416] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:20 INFO 140186635052416] Epoch[22] Batch[0] avg_epoch_loss=10.490591\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:20 INFO 140186635052416] #quality_metric: host=algo-1, epoch=22, batch=0 train loss <loss>=10.490591049194336\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:20 INFO 140186635052416] Epoch[22] Batch[5] avg_epoch_loss=10.013352\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:20 INFO 140186635052416] #quality_metric: host=algo-1, epoch=22, batch=5 train loss <loss>=10.013351599375406\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:20 INFO 140186635052416] Epoch[22] Batch [5]#011Speed: 1986.50 samples/sec#011loss=10.013352\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:20 INFO 140186635052416] processed a total of 623 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457559.914654, \"EndTime\": 1631457560.4569182, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 541.7821407318115, \"count\": 1, \"min\": 541.7821407318115, \"max\": 541.7821407318115}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:20 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=1149.6071887531648 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:20 INFO 140186635052416] #progress_metric: host=algo-1, completed 11.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:20 INFO 140186635052416] #quality_metric: host=algo-1, epoch=22, train loss <loss>=10.213611793518066\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:20 INFO 140186635052416] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:20 INFO 140186635052416] Epoch[23] Batch[0] avg_epoch_loss=10.489149\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:20 INFO 140186635052416] #quality_metric: host=algo-1, epoch=23, batch=0 train loss <loss>=10.48914909362793\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:20 INFO 140186635052416] Epoch[23] Batch[5] avg_epoch_loss=9.978299\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:20 INFO 140186635052416] #quality_metric: host=algo-1, epoch=23, batch=5 train loss <loss>=9.978299140930176\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:20 INFO 140186635052416] Epoch[23] Batch [5]#011Speed: 1982.28 samples/sec#011loss=9.978299\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:21 INFO 140186635052416] Epoch[23] Batch[10] avg_epoch_loss=9.439238\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:21 INFO 140186635052416] #quality_metric: host=algo-1, epoch=23, batch=10 train loss <loss>=8.792365503311157\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:21 INFO 140186635052416] Epoch[23] Batch [10]#011Speed: 1958.88 samples/sec#011loss=8.792366\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:21 INFO 140186635052416] processed a total of 648 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457560.457014, \"EndTime\": 1631457561.0598495, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 602.2660732269287, \"count\": 1, \"min\": 602.2660732269287, \"max\": 602.2660732269287}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:21 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=1075.7056109147802 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:21 INFO 140186635052416] #progress_metric: host=algo-1, completed 12.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:21 INFO 140186635052416] #quality_metric: host=algo-1, epoch=23, train loss <loss>=9.439238396557895\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:21 INFO 140186635052416] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:21 INFO 140186635052416] Saved checkpoint to \"/opt/ml/model/state_9035c21e-f6c6-4d4e-ad86-facc1b20f26b-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457561.0599356, \"EndTime\": 1631457561.0789773, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 18.53179931640625, \"count\": 1, \"min\": 18.53179931640625, \"max\": 18.53179931640625}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:21 INFO 140186635052416] Epoch[24] Batch[0] avg_epoch_loss=11.195914\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:21 INFO 140186635052416] #quality_metric: host=algo-1, epoch=24, batch=0 train loss <loss>=11.195914268493652\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:21 INFO 140186635052416] Epoch[24] Batch[5] avg_epoch_loss=10.448704\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:21 INFO 140186635052416] #quality_metric: host=algo-1, epoch=24, batch=5 train loss <loss>=10.44870376586914\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:21 INFO 140186635052416] Epoch[24] Batch [5]#011Speed: 2105.70 samples/sec#011loss=10.448704\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:21 INFO 140186635052416] processed a total of 632 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457561.0790524, \"EndTime\": 1631457561.634212, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 555.0897121429443, \"count\": 1, \"min\": 555.0897121429443, \"max\": 555.0897121429443}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:21 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=1138.29313064949 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:21 INFO 140186635052416] #progress_metric: host=algo-1, completed 12.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:21 INFO 140186635052416] #quality_metric: host=algo-1, epoch=24, train loss <loss>=10.44752550125122\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:21 INFO 140186635052416] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:21 INFO 140186635052416] Epoch[25] Batch[0] avg_epoch_loss=10.130132\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:21 INFO 140186635052416] #quality_metric: host=algo-1, epoch=25, batch=0 train loss <loss>=10.130131721496582\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:22 INFO 140186635052416] Epoch[25] Batch[5] avg_epoch_loss=10.398739\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:22 INFO 140186635052416] #quality_metric: host=algo-1, epoch=25, batch=5 train loss <loss>=10.398739020029703\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:22 INFO 140186635052416] Epoch[25] Batch [5]#011Speed: 1746.46 samples/sec#011loss=10.398739\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:22 INFO 140186635052416] processed a total of 620 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457561.6342986, \"EndTime\": 1631457562.2126844, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 577.8758525848389, \"count\": 1, \"min\": 577.8758525848389, \"max\": 577.8758525848389}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:22 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=1072.6310488070574 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:22 INFO 140186635052416] #progress_metric: host=algo-1, completed 13.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:22 INFO 140186635052416] #quality_metric: host=algo-1, epoch=25, train loss <loss>=10.405168533325195\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:22 INFO 140186635052416] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:22 INFO 140186635052416] Epoch[26] Batch[0] avg_epoch_loss=10.448298\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:22 INFO 140186635052416] #quality_metric: host=algo-1, epoch=26, batch=0 train loss <loss>=10.448298454284668\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:22 INFO 140186635052416] Epoch[26] Batch[5] avg_epoch_loss=10.165292\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:22 INFO 140186635052416] #quality_metric: host=algo-1, epoch=26, batch=5 train loss <loss>=10.165292263031006\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:22 INFO 140186635052416] Epoch[26] Batch [5]#011Speed: 1829.06 samples/sec#011loss=10.165292\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:22 INFO 140186635052416] Epoch[26] Batch[10] avg_epoch_loss=10.510499\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:22 INFO 140186635052416] #quality_metric: host=algo-1, epoch=26, batch=10 train loss <loss>=10.924747467041016\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:22 INFO 140186635052416] Epoch[26] Batch [10]#011Speed: 1943.97 samples/sec#011loss=10.924747\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:22 INFO 140186635052416] processed a total of 644 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457562.2127817, \"EndTime\": 1631457562.811667, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 598.3338356018066, \"count\": 1, \"min\": 598.3338356018066, \"max\": 598.3338356018066}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:22 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=1076.0850962114969 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:22 INFO 140186635052416] #progress_metric: host=algo-1, completed 13.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:22 INFO 140186635052416] #quality_metric: host=algo-1, epoch=26, train loss <loss>=10.510499173944647\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:22 INFO 140186635052416] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:23 INFO 140186635052416] Epoch[27] Batch[0] avg_epoch_loss=9.694462\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:23 INFO 140186635052416] #quality_metric: host=algo-1, epoch=27, batch=0 train loss <loss>=9.694461822509766\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:23 INFO 140186635052416] Epoch[27] Batch[5] avg_epoch_loss=10.003550\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:23 INFO 140186635052416] #quality_metric: host=algo-1, epoch=27, batch=5 train loss <loss>=10.003549734751383\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:23 INFO 140186635052416] Epoch[27] Batch [5]#011Speed: 1632.31 samples/sec#011loss=10.003550\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:23 INFO 140186635052416] Epoch[27] Batch[10] avg_epoch_loss=10.505106\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:23 INFO 140186635052416] #quality_metric: host=algo-1, epoch=27, batch=10 train loss <loss>=11.106973266601562\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:23 INFO 140186635052416] Epoch[27] Batch [10]#011Speed: 1605.79 samples/sec#011loss=11.106973\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:23 INFO 140186635052416] processed a total of 641 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457562.8117552, \"EndTime\": 1631457563.5077002, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 695.4543590545654, \"count\": 1, \"min\": 695.4543590545654, \"max\": 695.4543590545654}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:23 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=921.5438411758254 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:23 INFO 140186635052416] #progress_metric: host=algo-1, completed 14.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:23 INFO 140186635052416] #quality_metric: host=algo-1, epoch=27, train loss <loss>=10.505105885592373\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:23 INFO 140186635052416] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:23 INFO 140186635052416] Epoch[28] Batch[0] avg_epoch_loss=9.845059\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:23 INFO 140186635052416] #quality_metric: host=algo-1, epoch=28, batch=0 train loss <loss>=9.845059394836426\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:23 INFO 140186635052416] Epoch[28] Batch[5] avg_epoch_loss=10.178890\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:23 INFO 140186635052416] #quality_metric: host=algo-1, epoch=28, batch=5 train loss <loss>=10.178890069325766\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:23 INFO 140186635052416] Epoch[28] Batch [5]#011Speed: 2007.47 samples/sec#011loss=10.178890\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:24 INFO 140186635052416] processed a total of 629 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457563.5077798, \"EndTime\": 1631457564.0888212, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 580.5330276489258, \"count\": 1, \"min\": 580.5330276489258, \"max\": 580.5330276489258}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:24 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=1083.206757208398 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:24 INFO 140186635052416] #progress_metric: host=algo-1, completed 14.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:24 INFO 140186635052416] #quality_metric: host=algo-1, epoch=28, train loss <loss>=10.256868648529053\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:24 INFO 140186635052416] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:24 INFO 140186635052416] Epoch[29] Batch[0] avg_epoch_loss=10.478994\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:24 INFO 140186635052416] #quality_metric: host=algo-1, epoch=29, batch=0 train loss <loss>=10.478994369506836\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:24 INFO 140186635052416] Epoch[29] Batch[5] avg_epoch_loss=9.743740\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:24 INFO 140186635052416] #quality_metric: host=algo-1, epoch=29, batch=5 train loss <loss>=9.743740240732828\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:24 INFO 140186635052416] Epoch[29] Batch [5]#011Speed: 2084.39 samples/sec#011loss=9.743740\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:24 INFO 140186635052416] processed a total of 620 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457564.0889227, \"EndTime\": 1631457564.6913602, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 601.8116474151611, \"count\": 1, \"min\": 601.8116474151611, \"max\": 601.8116474151611}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:24 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=1030.0149642134281 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:24 INFO 140186635052416] #progress_metric: host=algo-1, completed 15.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:24 INFO 140186635052416] #quality_metric: host=algo-1, epoch=29, train loss <loss>=9.949199008941651\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:24 INFO 140186635052416] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:24 INFO 140186635052416] Epoch[30] Batch[0] avg_epoch_loss=9.118380\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:24 INFO 140186635052416] #quality_metric: host=algo-1, epoch=30, batch=0 train loss <loss>=9.118379592895508\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:25 INFO 140186635052416] Epoch[30] Batch[5] avg_epoch_loss=9.962174\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:25 INFO 140186635052416] #quality_metric: host=algo-1, epoch=30, batch=5 train loss <loss>=9.962173779805502\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:25 INFO 140186635052416] Epoch[30] Batch [5]#011Speed: 2087.73 samples/sec#011loss=9.962174\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:25 INFO 140186635052416] Epoch[30] Batch[10] avg_epoch_loss=10.345974\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:25 INFO 140186635052416] #quality_metric: host=algo-1, epoch=30, batch=10 train loss <loss>=10.806533241271973\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:25 INFO 140186635052416] Epoch[30] Batch [10]#011Speed: 1953.14 samples/sec#011loss=10.806533\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:25 INFO 140186635052416] processed a total of 648 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457564.6914303, \"EndTime\": 1631457565.2465725, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 554.6078681945801, \"count\": 1, \"min\": 554.6078681945801, \"max\": 554.6078681945801}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:25 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=1168.0822618749337 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:25 INFO 140186635052416] #progress_metric: host=algo-1, completed 15.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:25 INFO 140186635052416] #quality_metric: host=algo-1, epoch=30, train loss <loss>=10.345973535017533\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:25 INFO 140186635052416] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:25 INFO 140186635052416] Epoch[31] Batch[0] avg_epoch_loss=9.926876\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:25 INFO 140186635052416] #quality_metric: host=algo-1, epoch=31, batch=0 train loss <loss>=9.926876068115234\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:25 INFO 140186635052416] Epoch[31] Batch[5] avg_epoch_loss=9.847350\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:25 INFO 140186635052416] #quality_metric: host=algo-1, epoch=31, batch=5 train loss <loss>=9.847350438435873\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:25 INFO 140186635052416] Epoch[31] Batch [5]#011Speed: 2095.97 samples/sec#011loss=9.847350\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:25 INFO 140186635052416] Epoch[31] Batch[10] avg_epoch_loss=10.246243\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:25 INFO 140186635052416] #quality_metric: host=algo-1, epoch=31, batch=10 train loss <loss>=10.72491397857666\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:25 INFO 140186635052416] Epoch[31] Batch [10]#011Speed: 2031.86 samples/sec#011loss=10.724914\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:25 INFO 140186635052416] processed a total of 647 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457565.2466736, \"EndTime\": 1631457565.8067153, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 559.5104694366455, \"count\": 1, \"min\": 559.5104694366455, \"max\": 559.5104694366455}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:25 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=1156.1285678559912 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:25 INFO 140186635052416] #progress_metric: host=algo-1, completed 16.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:25 INFO 140186635052416] #quality_metric: host=algo-1, epoch=31, train loss <loss>=10.246242956681685\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:25 INFO 140186635052416] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:26 INFO 140186635052416] Epoch[32] Batch[0] avg_epoch_loss=10.388182\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:26 INFO 140186635052416] #quality_metric: host=algo-1, epoch=32, batch=0 train loss <loss>=10.388181686401367\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:26 INFO 140186635052416] Epoch[32] Batch[5] avg_epoch_loss=10.327127\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:26 INFO 140186635052416] #quality_metric: host=algo-1, epoch=32, batch=5 train loss <loss>=10.327127456665039\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:26 INFO 140186635052416] Epoch[32] Batch [5]#011Speed: 1986.04 samples/sec#011loss=10.327127\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:26 INFO 140186635052416] processed a total of 617 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457565.806786, \"EndTime\": 1631457566.3775191, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 570.2114105224609, \"count\": 1, \"min\": 570.2114105224609, \"max\": 570.2114105224609}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:26 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=1081.796533834181 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:26 INFO 140186635052416] #progress_metric: host=algo-1, completed 16.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:26 INFO 140186635052416] #quality_metric: host=algo-1, epoch=32, train loss <loss>=10.234101104736329\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:26 INFO 140186635052416] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:26 INFO 140186635052416] Epoch[33] Batch[0] avg_epoch_loss=11.024644\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:26 INFO 140186635052416] #quality_metric: host=algo-1, epoch=33, batch=0 train loss <loss>=11.024643898010254\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:26 INFO 140186635052416] Epoch[33] Batch[5] avg_epoch_loss=10.150016\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:26 INFO 140186635052416] #quality_metric: host=algo-1, epoch=33, batch=5 train loss <loss>=10.150015513102213\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:26 INFO 140186635052416] Epoch[33] Batch [5]#011Speed: 2068.29 samples/sec#011loss=10.150016\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:26 INFO 140186635052416] Epoch[33] Batch[10] avg_epoch_loss=10.383542\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:26 INFO 140186635052416] #quality_metric: host=algo-1, epoch=33, batch=10 train loss <loss>=10.663774490356445\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:26 INFO 140186635052416] Epoch[33] Batch [10]#011Speed: 1689.75 samples/sec#011loss=10.663774\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:26 INFO 140186635052416] processed a total of 665 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457566.3776107, \"EndTime\": 1631457566.9815683, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 603.4278869628906, \"count\": 1, \"min\": 603.4278869628906, \"max\": 603.4278869628906}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:26 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=1101.8300126884503 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:26 INFO 140186635052416] #progress_metric: host=algo-1, completed 17.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:26 INFO 140186635052416] #quality_metric: host=algo-1, epoch=33, train loss <loss>=10.383542320945047\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:26 INFO 140186635052416] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:27 INFO 140186635052416] Epoch[34] Batch[0] avg_epoch_loss=10.801217\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:27 INFO 140186635052416] #quality_metric: host=algo-1, epoch=34, batch=0 train loss <loss>=10.801217079162598\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:27 INFO 140186635052416] Epoch[34] Batch[5] avg_epoch_loss=9.955356\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:27 INFO 140186635052416] #quality_metric: host=algo-1, epoch=34, batch=5 train loss <loss>=9.955356121063232\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:27 INFO 140186635052416] Epoch[34] Batch [5]#011Speed: 1843.91 samples/sec#011loss=9.955356\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:27 INFO 140186635052416] Epoch[34] Batch[10] avg_epoch_loss=9.843174\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:27 INFO 140186635052416] #quality_metric: host=algo-1, epoch=34, batch=10 train loss <loss>=9.70855484008789\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:27 INFO 140186635052416] Epoch[34] Batch [10]#011Speed: 1757.70 samples/sec#011loss=9.708555\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:27 INFO 140186635052416] processed a total of 689 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457566.9816399, \"EndTime\": 1631457567.5704095, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 588.245153427124, \"count\": 1, \"min\": 588.245153427124, \"max\": 588.245153427124}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:27 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=1171.0459008993128 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:27 INFO 140186635052416] #progress_metric: host=algo-1, completed 17.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:27 INFO 140186635052416] #quality_metric: host=algo-1, epoch=34, train loss <loss>=9.843173720619895\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:27 INFO 140186635052416] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:27 INFO 140186635052416] Epoch[35] Batch[0] avg_epoch_loss=9.339593\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:27 INFO 140186635052416] #quality_metric: host=algo-1, epoch=35, batch=0 train loss <loss>=9.339592933654785\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:27 INFO 140186635052416] Epoch[35] Batch[5] avg_epoch_loss=9.849774\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:27 INFO 140186635052416] #quality_metric: host=algo-1, epoch=35, batch=5 train loss <loss>=9.84977420171102\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:27 INFO 140186635052416] Epoch[35] Batch [5]#011Speed: 2112.32 samples/sec#011loss=9.849774\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:28 INFO 140186635052416] Epoch[35] Batch[10] avg_epoch_loss=9.810225\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:28 INFO 140186635052416] #quality_metric: host=algo-1, epoch=35, batch=10 train loss <loss>=9.76276683807373\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:28 INFO 140186635052416] Epoch[35] Batch [10]#011Speed: 1889.37 samples/sec#011loss=9.762767\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:28 INFO 140186635052416] processed a total of 685 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457567.5704846, \"EndTime\": 1631457568.1224527, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 551.4414310455322, \"count\": 1, \"min\": 551.4414310455322, \"max\": 551.4414310455322}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:28 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=1241.9219435738032 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:28 INFO 140186635052416] #progress_metric: host=algo-1, completed 18.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:28 INFO 140186635052416] #quality_metric: host=algo-1, epoch=35, train loss <loss>=9.810225400057705\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:28 INFO 140186635052416] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:28 INFO 140186635052416] Epoch[36] Batch[0] avg_epoch_loss=10.570539\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:28 INFO 140186635052416] #quality_metric: host=algo-1, epoch=36, batch=0 train loss <loss>=10.570538520812988\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:28 INFO 140186635052416] Epoch[36] Batch[5] avg_epoch_loss=10.304177\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:28 INFO 140186635052416] #quality_metric: host=algo-1, epoch=36, batch=5 train loss <loss>=10.304177125295004\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:28 INFO 140186635052416] Epoch[36] Batch [5]#011Speed: 1635.52 samples/sec#011loss=10.304177\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:28 INFO 140186635052416] Epoch[36] Batch[10] avg_epoch_loss=10.033951\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:28 INFO 140186635052416] #quality_metric: host=algo-1, epoch=36, batch=10 train loss <loss>=9.709679126739502\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:28 INFO 140186635052416] Epoch[36] Batch [10]#011Speed: 1708.31 samples/sec#011loss=9.709679\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:28 INFO 140186635052416] processed a total of 676 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457568.122535, \"EndTime\": 1631457568.742826, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 619.7981834411621, \"count\": 1, \"min\": 619.7981834411621, \"max\": 619.7981834411621}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:28 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=1090.4569678341386 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:28 INFO 140186635052416] #progress_metric: host=algo-1, completed 18.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:28 INFO 140186635052416] #quality_metric: host=algo-1, epoch=36, train loss <loss>=10.03395076231523\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:28 INFO 140186635052416] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:28 INFO 140186635052416] Epoch[37] Batch[0] avg_epoch_loss=10.982899\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:28 INFO 140186635052416] #quality_metric: host=algo-1, epoch=37, batch=0 train loss <loss>=10.982898712158203\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:29 INFO 140186635052416] Epoch[37] Batch[5] avg_epoch_loss=10.578911\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:29 INFO 140186635052416] #quality_metric: host=algo-1, epoch=37, batch=5 train loss <loss>=10.578911463419596\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:29 INFO 140186635052416] Epoch[37] Batch [5]#011Speed: 1881.30 samples/sec#011loss=10.578911\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:29 INFO 140186635052416] processed a total of 629 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457568.7429101, \"EndTime\": 1631457569.273302, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 529.8779010772705, \"count\": 1, \"min\": 529.8779010772705, \"max\": 529.8779010772705}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:29 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=1186.7108397424536 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:29 INFO 140186635052416] #progress_metric: host=algo-1, completed 19.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:29 INFO 140186635052416] #quality_metric: host=algo-1, epoch=37, train loss <loss>=10.455474853515625\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:29 INFO 140186635052416] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:29 INFO 140186635052416] Epoch[38] Batch[0] avg_epoch_loss=9.848721\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:29 INFO 140186635052416] #quality_metric: host=algo-1, epoch=38, batch=0 train loss <loss>=9.84872055053711\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:29 INFO 140186635052416] Epoch[38] Batch[5] avg_epoch_loss=10.278561\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:29 INFO 140186635052416] #quality_metric: host=algo-1, epoch=38, batch=5 train loss <loss>=10.278560797373453\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:29 INFO 140186635052416] Epoch[38] Batch [5]#011Speed: 1694.36 samples/sec#011loss=10.278561\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:29 INFO 140186635052416] processed a total of 603 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457569.2734125, \"EndTime\": 1631457569.8650804, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 591.1166667938232, \"count\": 1, \"min\": 591.1166667938232, \"max\": 591.1166667938232}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:29 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=1019.8646124465353 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:29 INFO 140186635052416] #progress_metric: host=algo-1, completed 19.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:29 INFO 140186635052416] #quality_metric: host=algo-1, epoch=38, train loss <loss>=9.966960430145264\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:29 INFO 140186635052416] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:30 INFO 140186635052416] Epoch[39] Batch[0] avg_epoch_loss=8.945670\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:30 INFO 140186635052416] #quality_metric: host=algo-1, epoch=39, batch=0 train loss <loss>=8.945670127868652\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:30 INFO 140186635052416] Epoch[39] Batch[5] avg_epoch_loss=10.001158\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:30 INFO 140186635052416] #quality_metric: host=algo-1, epoch=39, batch=5 train loss <loss>=10.001157601674398\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:30 INFO 140186635052416] Epoch[39] Batch [5]#011Speed: 1795.44 samples/sec#011loss=10.001158\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:30 INFO 140186635052416] Epoch[39] Batch[10] avg_epoch_loss=10.305236\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:30 INFO 140186635052416] #quality_metric: host=algo-1, epoch=39, batch=10 train loss <loss>=10.67012996673584\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:30 INFO 140186635052416] Epoch[39] Batch [10]#011Speed: 1630.60 samples/sec#011loss=10.670130\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:30 INFO 140186635052416] processed a total of 654 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457569.865172, \"EndTime\": 1631457570.5535676, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 687.830924987793, \"count\": 1, \"min\": 687.830924987793, \"max\": 687.830924987793}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:30 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=950.6344839058471 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:30 INFO 140186635052416] #progress_metric: host=algo-1, completed 20.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:30 INFO 140186635052416] #quality_metric: host=algo-1, epoch=39, train loss <loss>=10.3052359494296\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:30 INFO 140186635052416] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:30 INFO 140186635052416] Epoch[40] Batch[0] avg_epoch_loss=10.364967\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:30 INFO 140186635052416] #quality_metric: host=algo-1, epoch=40, batch=0 train loss <loss>=10.364967346191406\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:31 INFO 140186635052416] Epoch[40] Batch[5] avg_epoch_loss=10.359238\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:31 INFO 140186635052416] #quality_metric: host=algo-1, epoch=40, batch=5 train loss <loss>=10.359238306681315\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:31 INFO 140186635052416] Epoch[40] Batch [5]#011Speed: 1996.36 samples/sec#011loss=10.359238\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:31 INFO 140186635052416] Epoch[40] Batch[10] avg_epoch_loss=10.475917\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:31 INFO 140186635052416] #quality_metric: host=algo-1, epoch=40, batch=10 train loss <loss>=10.615930366516114\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:31 INFO 140186635052416] Epoch[40] Batch [10]#011Speed: 1692.14 samples/sec#011loss=10.615930\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:31 INFO 140186635052416] processed a total of 648 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457570.5536551, \"EndTime\": 1631457571.1941297, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 639.9829387664795, \"count\": 1, \"min\": 639.9829387664795, \"max\": 639.9829387664795}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:31 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=1012.3222108248356 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:31 INFO 140186635052416] #progress_metric: host=algo-1, completed 20.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:31 INFO 140186635052416] #quality_metric: host=algo-1, epoch=40, train loss <loss>=10.475916515697133\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:31 INFO 140186635052416] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:31 INFO 140186635052416] Epoch[41] Batch[0] avg_epoch_loss=9.685337\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:31 INFO 140186635052416] #quality_metric: host=algo-1, epoch=41, batch=0 train loss <loss>=9.68533706665039\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:31 INFO 140186635052416] Epoch[41] Batch[5] avg_epoch_loss=10.150924\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:31 INFO 140186635052416] #quality_metric: host=algo-1, epoch=41, batch=5 train loss <loss>=10.15092420578003\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:31 INFO 140186635052416] Epoch[41] Batch [5]#011Speed: 1889.23 samples/sec#011loss=10.150924\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:31 INFO 140186635052416] Epoch[41] Batch[10] avg_epoch_loss=10.385022\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:31 INFO 140186635052416] #quality_metric: host=algo-1, epoch=41, batch=10 train loss <loss>=10.665939140319825\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:31 INFO 140186635052416] Epoch[41] Batch [10]#011Speed: 1755.96 samples/sec#011loss=10.665939\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:31 INFO 140186635052416] processed a total of 675 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457571.194216, \"EndTime\": 1631457571.8045237, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 609.8227500915527, \"count\": 1, \"min\": 609.8227500915527, \"max\": 609.8227500915527}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:31 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=1106.6358550687476 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:31 INFO 140186635052416] #progress_metric: host=algo-1, completed 21.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:31 INFO 140186635052416] #quality_metric: host=algo-1, epoch=41, train loss <loss>=10.385021903298117\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:31 INFO 140186635052416] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:32 INFO 140186635052416] Epoch[42] Batch[0] avg_epoch_loss=10.423387\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:32 INFO 140186635052416] #quality_metric: host=algo-1, epoch=42, batch=0 train loss <loss>=10.423386573791504\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:32 INFO 140186635052416] Epoch[42] Batch[5] avg_epoch_loss=9.995187\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:32 INFO 140186635052416] #quality_metric: host=algo-1, epoch=42, batch=5 train loss <loss>=9.995186805725098\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:32 INFO 140186635052416] Epoch[42] Batch [5]#011Speed: 1794.60 samples/sec#011loss=9.995187\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:32 INFO 140186635052416] Epoch[42] Batch[10] avg_epoch_loss=9.787397\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:32 INFO 140186635052416] #quality_metric: host=algo-1, epoch=42, batch=10 train loss <loss>=9.53804931640625\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:32 INFO 140186635052416] Epoch[42] Batch [10]#011Speed: 1907.43 samples/sec#011loss=9.538049\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:32 INFO 140186635052416] processed a total of 661 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457571.8046145, \"EndTime\": 1631457572.3914094, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 586.2760543823242, \"count\": 1, \"min\": 586.2760543823242, \"max\": 586.2760543823242}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:32 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=1127.2168551843029 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:32 INFO 140186635052416] #progress_metric: host=algo-1, completed 21.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:32 INFO 140186635052416] #quality_metric: host=algo-1, epoch=42, train loss <loss>=9.787397037852894\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:32 INFO 140186635052416] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:32 INFO 140186635052416] Epoch[43] Batch[0] avg_epoch_loss=8.967278\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:32 INFO 140186635052416] #quality_metric: host=algo-1, epoch=43, batch=0 train loss <loss>=8.967278480529785\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:32 INFO 140186635052416] Epoch[43] Batch[5] avg_epoch_loss=9.653704\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:32 INFO 140186635052416] #quality_metric: host=algo-1, epoch=43, batch=5 train loss <loss>=9.653704325358072\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:32 INFO 140186635052416] Epoch[43] Batch [5]#011Speed: 2137.42 samples/sec#011loss=9.653704\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:32 INFO 140186635052416] Epoch[43] Batch[10] avg_epoch_loss=9.852788\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:32 INFO 140186635052416] #quality_metric: host=algo-1, epoch=43, batch=10 train loss <loss>=10.09168815612793\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:32 INFO 140186635052416] Epoch[43] Batch [10]#011Speed: 1910.40 samples/sec#011loss=10.091688\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:32 INFO 140186635052416] processed a total of 649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457572.3914926, \"EndTime\": 1631457572.971576, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 579.606294631958, \"count\": 1, \"min\": 579.606294631958, \"max\": 579.606294631958}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:32 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=1119.4727804065974 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:32 INFO 140186635052416] #progress_metric: host=algo-1, completed 22.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:32 INFO 140186635052416] #quality_metric: host=algo-1, epoch=43, train loss <loss>=9.852787884798916\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:32 INFO 140186635052416] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:33 INFO 140186635052416] Epoch[44] Batch[0] avg_epoch_loss=10.069580\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:33 INFO 140186635052416] #quality_metric: host=algo-1, epoch=44, batch=0 train loss <loss>=10.069580078125\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:33 INFO 140186635052416] Epoch[44] Batch[5] avg_epoch_loss=9.992171\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:33 INFO 140186635052416] #quality_metric: host=algo-1, epoch=44, batch=5 train loss <loss>=9.992170969645182\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:33 INFO 140186635052416] Epoch[44] Batch [5]#011Speed: 1921.35 samples/sec#011loss=9.992171\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:33 INFO 140186635052416] processed a total of 612 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457572.9716642, \"EndTime\": 1631457573.5470855, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 574.9068260192871, \"count\": 1, \"min\": 574.9068260192871, \"max\": 574.9068260192871}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:33 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=1064.2643344663202 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:33 INFO 140186635052416] #progress_metric: host=algo-1, completed 22.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:33 INFO 140186635052416] #quality_metric: host=algo-1, epoch=44, train loss <loss>=9.88439474105835\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:33 INFO 140186635052416] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:33 INFO 140186635052416] Epoch[45] Batch[0] avg_epoch_loss=10.169205\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:33 INFO 140186635052416] #quality_metric: host=algo-1, epoch=45, batch=0 train loss <loss>=10.169204711914062\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:33 INFO 140186635052416] Epoch[45] Batch[5] avg_epoch_loss=10.118863\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:33 INFO 140186635052416] #quality_metric: host=algo-1, epoch=45, batch=5 train loss <loss>=10.118862946828207\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:33 INFO 140186635052416] Epoch[45] Batch [5]#011Speed: 2015.74 samples/sec#011loss=10.118863\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:34 INFO 140186635052416] processed a total of 604 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457573.5471785, \"EndTime\": 1631457574.109396, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 561.6679191589355, \"count\": 1, \"min\": 561.6679191589355, \"max\": 561.6679191589355}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:34 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=1075.0591202825906 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:34 INFO 140186635052416] #progress_metric: host=algo-1, completed 23.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:34 INFO 140186635052416] #quality_metric: host=algo-1, epoch=45, train loss <loss>=9.895019245147704\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:34 INFO 140186635052416] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:34 INFO 140186635052416] Epoch[46] Batch[0] avg_epoch_loss=11.128858\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:34 INFO 140186635052416] #quality_metric: host=algo-1, epoch=46, batch=0 train loss <loss>=11.128857612609863\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:34 INFO 140186635052416] Epoch[46] Batch[5] avg_epoch_loss=10.205995\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:34 INFO 140186635052416] #quality_metric: host=algo-1, epoch=46, batch=5 train loss <loss>=10.205994923909506\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:34 INFO 140186635052416] Epoch[46] Batch [5]#011Speed: 1791.37 samples/sec#011loss=10.205995\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:34 INFO 140186635052416] processed a total of 575 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457574.1094904, \"EndTime\": 1631457574.6795654, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 569.5347785949707, \"count\": 1, \"min\": 569.5347785949707, \"max\": 569.5347785949707}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:34 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=1009.3500249436674 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:34 INFO 140186635052416] #progress_metric: host=algo-1, completed 23.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:34 INFO 140186635052416] #quality_metric: host=algo-1, epoch=46, train loss <loss>=10.273517926534018\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:34 INFO 140186635052416] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:34 INFO 140186635052416] Epoch[47] Batch[0] avg_epoch_loss=9.913043\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:34 INFO 140186635052416] #quality_metric: host=algo-1, epoch=47, batch=0 train loss <loss>=9.913043022155762\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:35 INFO 140186635052416] Epoch[47] Batch[5] avg_epoch_loss=9.751186\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:35 INFO 140186635052416] #quality_metric: host=algo-1, epoch=47, batch=5 train loss <loss>=9.751185735066732\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:35 INFO 140186635052416] Epoch[47] Batch [5]#011Speed: 1819.67 samples/sec#011loss=9.751186\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:35 INFO 140186635052416] Epoch[47] Batch[10] avg_epoch_loss=9.775266\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:35 INFO 140186635052416] #quality_metric: host=algo-1, epoch=47, batch=10 train loss <loss>=9.804163360595703\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:35 INFO 140186635052416] Epoch[47] Batch [10]#011Speed: 1750.43 samples/sec#011loss=9.804163\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:35 INFO 140186635052416] processed a total of 688 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457574.6796582, \"EndTime\": 1631457575.29049, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 610.2988719940186, \"count\": 1, \"min\": 610.2988719940186, \"max\": 610.2988719940186}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:35 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=1127.08667926154 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:35 INFO 140186635052416] #progress_metric: host=algo-1, completed 24.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:35 INFO 140186635052416] #quality_metric: host=algo-1, epoch=47, train loss <loss>=9.775266473943537\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:35 INFO 140186635052416] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:35 INFO 140186635052416] Epoch[48] Batch[0] avg_epoch_loss=10.771856\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:35 INFO 140186635052416] #quality_metric: host=algo-1, epoch=48, batch=0 train loss <loss>=10.771856307983398\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:35 INFO 140186635052416] Epoch[48] Batch[5] avg_epoch_loss=10.091147\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:35 INFO 140186635052416] #quality_metric: host=algo-1, epoch=48, batch=5 train loss <loss>=10.091147422790527\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:35 INFO 140186635052416] Epoch[48] Batch [5]#011Speed: 2136.37 samples/sec#011loss=10.091147\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:35 INFO 140186635052416] Epoch[48] Batch[10] avg_epoch_loss=10.345586\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:35 INFO 140186635052416] #quality_metric: host=algo-1, epoch=48, batch=10 train loss <loss>=10.650912475585937\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:35 INFO 140186635052416] Epoch[48] Batch [10]#011Speed: 1707.94 samples/sec#011loss=10.650912\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:35 INFO 140186635052416] processed a total of 652 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457575.2905693, \"EndTime\": 1631457575.87457, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 583.4887027740479, \"count\": 1, \"min\": 583.4887027740479, \"max\": 583.4887027740479}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:35 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=1117.1669676078363 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:35 INFO 140186635052416] #progress_metric: host=algo-1, completed 24.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:35 INFO 140186635052416] #quality_metric: host=algo-1, epoch=48, train loss <loss>=10.345586083152078\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:35 INFO 140186635052416] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:36 INFO 140186635052416] Epoch[49] Batch[0] avg_epoch_loss=11.000031\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:36 INFO 140186635052416] #quality_metric: host=algo-1, epoch=49, batch=0 train loss <loss>=11.000030517578125\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:36 INFO 140186635052416] Epoch[49] Batch[5] avg_epoch_loss=10.071643\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:36 INFO 140186635052416] #quality_metric: host=algo-1, epoch=49, batch=5 train loss <loss>=10.071643193562826\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:36 INFO 140186635052416] Epoch[49] Batch [5]#011Speed: 2058.12 samples/sec#011loss=10.071643\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:36 INFO 140186635052416] Epoch[49] Batch[10] avg_epoch_loss=10.246669\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:36 INFO 140186635052416] #quality_metric: host=algo-1, epoch=49, batch=10 train loss <loss>=10.456700134277344\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:36 INFO 140186635052416] Epoch[49] Batch [10]#011Speed: 1898.04 samples/sec#011loss=10.456700\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:36 INFO 140186635052416] processed a total of 662 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457575.8746576, \"EndTime\": 1631457576.4568548, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 581.6864967346191, \"count\": 1, \"min\": 581.6864967346191, \"max\": 581.6864967346191}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:36 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=1137.8219777716038 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:36 INFO 140186635052416] #progress_metric: host=algo-1, completed 25.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:36 INFO 140186635052416] #quality_metric: host=algo-1, epoch=49, train loss <loss>=10.246669075705789\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:36 INFO 140186635052416] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:36 INFO 140186635052416] Epoch[50] Batch[0] avg_epoch_loss=11.196795\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:36 INFO 140186635052416] #quality_metric: host=algo-1, epoch=50, batch=0 train loss <loss>=11.196794509887695\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:36 INFO 140186635052416] Epoch[50] Batch[5] avg_epoch_loss=10.079649\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:36 INFO 140186635052416] #quality_metric: host=algo-1, epoch=50, batch=5 train loss <loss>=10.079648653666178\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:36 INFO 140186635052416] Epoch[50] Batch [5]#011Speed: 2143.61 samples/sec#011loss=10.079649\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:37 INFO 140186635052416] Epoch[50] Batch[10] avg_epoch_loss=10.341850\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:37 INFO 140186635052416] #quality_metric: host=algo-1, epoch=50, batch=10 train loss <loss>=10.656491661071778\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:37 INFO 140186635052416] Epoch[50] Batch [10]#011Speed: 2040.13 samples/sec#011loss=10.656492\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:37 INFO 140186635052416] processed a total of 649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457576.4569392, \"EndTime\": 1631457577.0299897, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 572.5312232971191, \"count\": 1, \"min\": 572.5312232971191, \"max\": 572.5312232971191}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:37 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=1133.3007050594836 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:37 INFO 140186635052416] #progress_metric: host=algo-1, completed 25.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:37 INFO 140186635052416] #quality_metric: host=algo-1, epoch=50, train loss <loss>=10.341850020668723\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:37 INFO 140186635052416] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:37 INFO 140186635052416] Epoch[51] Batch[0] avg_epoch_loss=10.152127\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:37 INFO 140186635052416] #quality_metric: host=algo-1, epoch=51, batch=0 train loss <loss>=10.152127265930176\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:37 INFO 140186635052416] Epoch[51] Batch[5] avg_epoch_loss=9.881871\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:37 INFO 140186635052416] #quality_metric: host=algo-1, epoch=51, batch=5 train loss <loss>=9.881870905558268\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:37 INFO 140186635052416] Epoch[51] Batch [5]#011Speed: 2039.02 samples/sec#011loss=9.881871\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:37 INFO 140186635052416] processed a total of 612 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457577.0300777, \"EndTime\": 1631457577.555962, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 525.3961086273193, \"count\": 1, \"min\": 525.3961086273193, \"max\": 525.3961086273193}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:37 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=1164.5025103195978 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:37 INFO 140186635052416] #progress_metric: host=algo-1, completed 26.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:37 INFO 140186635052416] #quality_metric: host=algo-1, epoch=51, train loss <loss>=9.751909542083741\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:37 INFO 140186635052416] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:37 INFO 140186635052416] Epoch[52] Batch[0] avg_epoch_loss=10.460968\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:37 INFO 140186635052416] #quality_metric: host=algo-1, epoch=52, batch=0 train loss <loss>=10.460968017578125\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:37 INFO 140186635052416] Epoch[52] Batch[5] avg_epoch_loss=10.104667\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:37 INFO 140186635052416] #quality_metric: host=algo-1, epoch=52, batch=5 train loss <loss>=10.104667027791342\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:37 INFO 140186635052416] Epoch[52] Batch [5]#011Speed: 2047.63 samples/sec#011loss=10.104667\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:38 INFO 140186635052416] Epoch[52] Batch[10] avg_epoch_loss=9.788847\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:38 INFO 140186635052416] #quality_metric: host=algo-1, epoch=52, batch=10 train loss <loss>=9.409862804412843\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:38 INFO 140186635052416] Epoch[52] Batch [10]#011Speed: 1882.02 samples/sec#011loss=9.409863\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:38 INFO 140186635052416] processed a total of 664 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457577.556064, \"EndTime\": 1631457578.1176503, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 561.0265731811523, \"count\": 1, \"min\": 561.0265731811523, \"max\": 561.0265731811523}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:38 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=1183.2655694020107 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:38 INFO 140186635052416] #progress_metric: host=algo-1, completed 26.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:38 INFO 140186635052416] #quality_metric: host=algo-1, epoch=52, train loss <loss>=9.78884692625566\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:38 INFO 140186635052416] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:38 INFO 140186635052416] Epoch[53] Batch[0] avg_epoch_loss=10.123182\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:38 INFO 140186635052416] #quality_metric: host=algo-1, epoch=53, batch=0 train loss <loss>=10.12318229675293\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:38 INFO 140186635052416] Epoch[53] Batch[5] avg_epoch_loss=10.235174\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:38 INFO 140186635052416] #quality_metric: host=algo-1, epoch=53, batch=5 train loss <loss>=10.235174496968588\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:38 INFO 140186635052416] Epoch[53] Batch [5]#011Speed: 1778.13 samples/sec#011loss=10.235174\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:38 INFO 140186635052416] processed a total of 632 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457578.1177385, \"EndTime\": 1631457578.7174459, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 599.2121696472168, \"count\": 1, \"min\": 599.2121696472168, \"max\": 599.2121696472168}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:38 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=1054.46985245856 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:38 INFO 140186635052416] #progress_metric: host=algo-1, completed 27.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:38 INFO 140186635052416] #quality_metric: host=algo-1, epoch=53, train loss <loss>=10.368173789978027\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:38 INFO 140186635052416] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:38 INFO 140186635052416] Epoch[54] Batch[0] avg_epoch_loss=10.002705\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:38 INFO 140186635052416] #quality_metric: host=algo-1, epoch=54, batch=0 train loss <loss>=10.002704620361328\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:39 INFO 140186635052416] Epoch[54] Batch[5] avg_epoch_loss=9.947297\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:39 INFO 140186635052416] #quality_metric: host=algo-1, epoch=54, batch=5 train loss <loss>=9.94729741414388\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:39 INFO 140186635052416] Epoch[54] Batch [5]#011Speed: 1976.42 samples/sec#011loss=9.947297\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:39 INFO 140186635052416] Epoch[54] Batch[10] avg_epoch_loss=10.000728\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:39 INFO 140186635052416] #quality_metric: host=algo-1, epoch=54, batch=10 train loss <loss>=10.064843940734864\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:39 INFO 140186635052416] Epoch[54] Batch [10]#011Speed: 1729.91 samples/sec#011loss=10.064844\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:39 INFO 140186635052416] processed a total of 666 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457578.7175405, \"EndTime\": 1631457579.310516, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 592.432975769043, \"count\": 1, \"min\": 592.432975769043, \"max\": 592.432975769043}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:39 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=1123.9688247644353 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:39 INFO 140186635052416] #progress_metric: host=algo-1, completed 27.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:39 INFO 140186635052416] #quality_metric: host=algo-1, epoch=54, train loss <loss>=10.000727653503418\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:39 INFO 140186635052416] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:39 INFO 140186635052416] Epoch[55] Batch[0] avg_epoch_loss=9.649362\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:39 INFO 140186635052416] #quality_metric: host=algo-1, epoch=55, batch=0 train loss <loss>=9.649361610412598\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:39 INFO 140186635052416] Epoch[55] Batch[5] avg_epoch_loss=9.652005\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:39 INFO 140186635052416] #quality_metric: host=algo-1, epoch=55, batch=5 train loss <loss>=9.652004718780518\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:39 INFO 140186635052416] Epoch[55] Batch [5]#011Speed: 2101.63 samples/sec#011loss=9.652005\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:39 INFO 140186635052416] processed a total of 636 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457579.310587, \"EndTime\": 1631457579.8334122, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 522.334098815918, \"count\": 1, \"min\": 522.334098815918, \"max\": 522.334098815918}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:39 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=1217.3208874342813 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:39 INFO 140186635052416] #progress_metric: host=algo-1, completed 28.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:39 INFO 140186635052416] #quality_metric: host=algo-1, epoch=55, train loss <loss>=9.683319282531738\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:39 INFO 140186635052416] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:40 INFO 140186635052416] Epoch[56] Batch[0] avg_epoch_loss=9.938468\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:40 INFO 140186635052416] #quality_metric: host=algo-1, epoch=56, batch=0 train loss <loss>=9.938467979431152\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:40 INFO 140186635052416] Epoch[56] Batch[5] avg_epoch_loss=9.921747\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:40 INFO 140186635052416] #quality_metric: host=algo-1, epoch=56, batch=5 train loss <loss>=9.921746889750162\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:40 INFO 140186635052416] Epoch[56] Batch [5]#011Speed: 2126.31 samples/sec#011loss=9.921747\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:40 INFO 140186635052416] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457579.8334835, \"EndTime\": 1631457580.376343, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 542.2687530517578, \"count\": 1, \"min\": 542.2687530517578, \"max\": 542.2687530517578}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:40 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=1168.8567010427098 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:40 INFO 140186635052416] #progress_metric: host=algo-1, completed 28.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:40 INFO 140186635052416] #quality_metric: host=algo-1, epoch=56, train loss <loss>=10.001977252960206\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:40 INFO 140186635052416] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:40 INFO 140186635052416] Epoch[57] Batch[0] avg_epoch_loss=10.942116\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:40 INFO 140186635052416] #quality_metric: host=algo-1, epoch=57, batch=0 train loss <loss>=10.942115783691406\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:40 INFO 140186635052416] Epoch[57] Batch[5] avg_epoch_loss=10.030533\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:40 INFO 140186635052416] #quality_metric: host=algo-1, epoch=57, batch=5 train loss <loss>=10.03053347269694\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:40 INFO 140186635052416] Epoch[57] Batch [5]#011Speed: 2033.61 samples/sec#011loss=10.030533\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:40 INFO 140186635052416] Epoch[57] Batch[10] avg_epoch_loss=9.730781\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:40 INFO 140186635052416] #quality_metric: host=algo-1, epoch=57, batch=10 train loss <loss>=9.371078491210938\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:40 INFO 140186635052416] Epoch[57] Batch [10]#011Speed: 1745.00 samples/sec#011loss=9.371078\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:40 INFO 140186635052416] processed a total of 653 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457580.376438, \"EndTime\": 1631457580.981913, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 604.9494743347168, \"count\": 1, \"min\": 604.9494743347168, \"max\": 604.9494743347168}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:40 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=1079.2035658220746 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:40 INFO 140186635052416] #progress_metric: host=algo-1, completed 29.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:40 INFO 140186635052416] #quality_metric: host=algo-1, epoch=57, train loss <loss>=9.730781208385121\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:40 INFO 140186635052416] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:41 INFO 140186635052416] Epoch[58] Batch[0] avg_epoch_loss=9.690383\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:41 INFO 140186635052416] #quality_metric: host=algo-1, epoch=58, batch=0 train loss <loss>=9.690382957458496\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:41 INFO 140186635052416] Epoch[58] Batch[5] avg_epoch_loss=10.159065\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:41 INFO 140186635052416] #quality_metric: host=algo-1, epoch=58, batch=5 train loss <loss>=10.159065246582031\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:41 INFO 140186635052416] Epoch[58] Batch [5]#011Speed: 2046.57 samples/sec#011loss=10.159065\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:41 INFO 140186635052416] processed a total of 592 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457580.9819975, \"EndTime\": 1631457581.5401545, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 557.6896667480469, \"count\": 1, \"min\": 557.6896667480469, \"max\": 557.6896667480469}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:41 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=1061.262297233013 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:41 INFO 140186635052416] #progress_metric: host=algo-1, completed 29.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:41 INFO 140186635052416] #quality_metric: host=algo-1, epoch=58, train loss <loss>=9.682136583328248\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:41 INFO 140186635052416] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:41 INFO 140186635052416] Epoch[59] Batch[0] avg_epoch_loss=10.476301\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:41 INFO 140186635052416] #quality_metric: host=algo-1, epoch=59, batch=0 train loss <loss>=10.476301193237305\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:41 INFO 140186635052416] Epoch[59] Batch[5] avg_epoch_loss=9.614923\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:41 INFO 140186635052416] #quality_metric: host=algo-1, epoch=59, batch=5 train loss <loss>=9.614923159281412\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:41 INFO 140186635052416] Epoch[59] Batch [5]#011Speed: 2090.77 samples/sec#011loss=9.614923\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:42 INFO 140186635052416] Epoch[59] Batch[10] avg_epoch_loss=9.260171\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:42 INFO 140186635052416] #quality_metric: host=algo-1, epoch=59, batch=10 train loss <loss>=8.834467697143555\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:42 INFO 140186635052416] Epoch[59] Batch [10]#011Speed: 1751.26 samples/sec#011loss=8.834468\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:42 INFO 140186635052416] processed a total of 680 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457581.5402458, \"EndTime\": 1631457582.12565, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 584.8588943481445, \"count\": 1, \"min\": 584.8588943481445, \"max\": 584.8588943481445}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:42 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=1162.415826411588 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:42 INFO 140186635052416] #progress_metric: host=algo-1, completed 30.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:42 INFO 140186635052416] #quality_metric: host=algo-1, epoch=59, train loss <loss>=9.260170676491477\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:42 INFO 140186635052416] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:42 INFO 140186635052416] Saved checkpoint to \"/opt/ml/model/state_85becccf-33ef-48dd-bb79-73383f3a5f5b-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457582.125737, \"EndTime\": 1631457582.1449108, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 18.6614990234375, \"count\": 1, \"min\": 18.6614990234375, \"max\": 18.6614990234375}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:42 INFO 140186635052416] Epoch[60] Batch[0] avg_epoch_loss=10.452953\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:42 INFO 140186635052416] #quality_metric: host=algo-1, epoch=60, batch=0 train loss <loss>=10.452953338623047\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:42 INFO 140186635052416] Epoch[60] Batch[5] avg_epoch_loss=10.004697\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:42 INFO 140186635052416] #quality_metric: host=algo-1, epoch=60, batch=5 train loss <loss>=10.004696687062582\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:42 INFO 140186635052416] Epoch[60] Batch [5]#011Speed: 1839.64 samples/sec#011loss=10.004697\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:42 INFO 140186635052416] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457582.1449795, \"EndTime\": 1631457582.7255924, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 580.5456638336182, \"count\": 1, \"min\": 580.5456638336182, \"max\": 580.5456638336182}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:42 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=1091.8208900315904 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:42 INFO 140186635052416] #progress_metric: host=algo-1, completed 30.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:42 INFO 140186635052416] #quality_metric: host=algo-1, epoch=60, train loss <loss>=10.011611366271973\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:42 INFO 140186635052416] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:42 INFO 140186635052416] Epoch[61] Batch[0] avg_epoch_loss=9.045015\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:42 INFO 140186635052416] #quality_metric: host=algo-1, epoch=61, batch=0 train loss <loss>=9.045015335083008\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:43 INFO 140186635052416] Epoch[61] Batch[5] avg_epoch_loss=9.776147\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:43 INFO 140186635052416] #quality_metric: host=algo-1, epoch=61, batch=5 train loss <loss>=9.776146729787191\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:43 INFO 140186635052416] Epoch[61] Batch [5]#011Speed: 2094.49 samples/sec#011loss=9.776147\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:43 INFO 140186635052416] Epoch[61] Batch[10] avg_epoch_loss=9.691392\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:43 INFO 140186635052416] #quality_metric: host=algo-1, epoch=61, batch=10 train loss <loss>=9.589687252044678\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:43 INFO 140186635052416] Epoch[61] Batch [10]#011Speed: 1907.22 samples/sec#011loss=9.589687\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:43 INFO 140186635052416] processed a total of 685 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457582.7256844, \"EndTime\": 1631457583.3047965, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 578.5784721374512, \"count\": 1, \"min\": 578.5784721374512, \"max\": 578.5784721374512}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:43 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=1183.6801264306775 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:43 INFO 140186635052416] #progress_metric: host=algo-1, completed 31.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:43 INFO 140186635052416] #quality_metric: host=algo-1, epoch=61, train loss <loss>=9.691392421722412\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:43 INFO 140186635052416] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:43 INFO 140186635052416] Epoch[62] Batch[0] avg_epoch_loss=9.866301\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:43 INFO 140186635052416] #quality_metric: host=algo-1, epoch=62, batch=0 train loss <loss>=9.866300582885742\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:43 INFO 140186635052416] Epoch[62] Batch[5] avg_epoch_loss=9.867890\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:43 INFO 140186635052416] #quality_metric: host=algo-1, epoch=62, batch=5 train loss <loss>=9.867890199025473\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:43 INFO 140186635052416] Epoch[62] Batch [5]#011Speed: 1881.23 samples/sec#011loss=9.867890\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:43 INFO 140186635052416] processed a total of 620 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457583.30488, \"EndTime\": 1631457583.8446126, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 539.2696857452393, \"count\": 1, \"min\": 539.2696857452393, \"max\": 539.2696857452393}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:43 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=1149.4194841892681 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:43 INFO 140186635052416] #progress_metric: host=algo-1, completed 31.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:43 INFO 140186635052416] #quality_metric: host=algo-1, epoch=62, train loss <loss>=10.033232021331788\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:43 INFO 140186635052416] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:44 INFO 140186635052416] Epoch[63] Batch[0] avg_epoch_loss=10.299274\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:44 INFO 140186635052416] #quality_metric: host=algo-1, epoch=63, batch=0 train loss <loss>=10.299274444580078\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:44 INFO 140186635052416] Epoch[63] Batch[5] avg_epoch_loss=10.184935\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:44 INFO 140186635052416] #quality_metric: host=algo-1, epoch=63, batch=5 train loss <loss>=10.184935092926025\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:44 INFO 140186635052416] Epoch[63] Batch [5]#011Speed: 2131.49 samples/sec#011loss=10.184935\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:44 INFO 140186635052416] processed a total of 627 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457583.8446963, \"EndTime\": 1631457584.3792713, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 534.0240001678467, \"count\": 1, \"min\": 534.0240001678467, \"max\": 534.0240001678467}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:44 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=1173.7853912664675 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:44 INFO 140186635052416] #progress_metric: host=algo-1, completed 32.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:44 INFO 140186635052416] #quality_metric: host=algo-1, epoch=63, train loss <loss>=9.806812763214111\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:44 INFO 140186635052416] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:44 INFO 140186635052416] Epoch[64] Batch[0] avg_epoch_loss=9.688258\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:44 INFO 140186635052416] #quality_metric: host=algo-1, epoch=64, batch=0 train loss <loss>=9.688258171081543\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:44 INFO 140186635052416] Epoch[64] Batch[5] avg_epoch_loss=9.755229\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:44 INFO 140186635052416] #quality_metric: host=algo-1, epoch=64, batch=5 train loss <loss>=9.755229314168295\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:44 INFO 140186635052416] Epoch[64] Batch [5]#011Speed: 1780.41 samples/sec#011loss=9.755229\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:44 INFO 140186635052416] processed a total of 597 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457584.379371, \"EndTime\": 1631457584.9467714, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 566.8585300445557, \"count\": 1, \"min\": 566.8585300445557, \"max\": 566.8585300445557}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:44 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=1052.9137577675533 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:44 INFO 140186635052416] #progress_metric: host=algo-1, completed 32.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:44 INFO 140186635052416] #quality_metric: host=algo-1, epoch=64, train loss <loss>=9.504178190231324\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:44 INFO 140186635052416] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:45 INFO 140186635052416] Epoch[65] Batch[0] avg_epoch_loss=9.159543\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:45 INFO 140186635052416] #quality_metric: host=algo-1, epoch=65, batch=0 train loss <loss>=9.15954303741455\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:45 INFO 140186635052416] Epoch[65] Batch[5] avg_epoch_loss=9.848595\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:45 INFO 140186635052416] #quality_metric: host=algo-1, epoch=65, batch=5 train loss <loss>=9.848595460255941\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:45 INFO 140186635052416] Epoch[65] Batch [5]#011Speed: 2099.48 samples/sec#011loss=9.848595\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:45 INFO 140186635052416] Epoch[65] Batch[10] avg_epoch_loss=10.033969\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:45 INFO 140186635052416] #quality_metric: host=algo-1, epoch=65, batch=10 train loss <loss>=10.256416511535644\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:45 INFO 140186635052416] Epoch[65] Batch [10]#011Speed: 1951.07 samples/sec#011loss=10.256417\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:45 INFO 140186635052416] processed a total of 645 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457584.946864, \"EndTime\": 1631457585.558679, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 611.2775802612305, \"count\": 1, \"min\": 611.2775802612305, \"max\": 611.2775802612305}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:45 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=1054.9461574367078 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:45 INFO 140186635052416] #progress_metric: host=algo-1, completed 33.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:45 INFO 140186635052416] #quality_metric: host=algo-1, epoch=65, train loss <loss>=10.033968665383078\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:45 INFO 140186635052416] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:45 INFO 140186635052416] Epoch[66] Batch[0] avg_epoch_loss=9.670188\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:45 INFO 140186635052416] #quality_metric: host=algo-1, epoch=66, batch=0 train loss <loss>=9.670187950134277\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:45 INFO 140186635052416] Epoch[66] Batch[5] avg_epoch_loss=9.690976\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:45 INFO 140186635052416] #quality_metric: host=algo-1, epoch=66, batch=5 train loss <loss>=9.690975983937582\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:45 INFO 140186635052416] Epoch[66] Batch [5]#011Speed: 2023.32 samples/sec#011loss=9.690976\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:46 INFO 140186635052416] Epoch[66] Batch[10] avg_epoch_loss=9.881688\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:46 INFO 140186635052416] #quality_metric: host=algo-1, epoch=66, batch=10 train loss <loss>=10.110543251037598\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:46 INFO 140186635052416] Epoch[66] Batch [10]#011Speed: 1862.15 samples/sec#011loss=10.110543\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:46 INFO 140186635052416] processed a total of 665 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457585.558762, \"EndTime\": 1631457586.1354935, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 576.2495994567871, \"count\": 1, \"min\": 576.2495994567871, \"max\": 576.2495994567871}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:46 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=1153.7493629835533 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:46 INFO 140186635052416] #progress_metric: host=algo-1, completed 33.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:46 INFO 140186635052416] #quality_metric: host=algo-1, epoch=66, train loss <loss>=9.881688378073953\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:46 INFO 140186635052416] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:46 INFO 140186635052416] Epoch[67] Batch[0] avg_epoch_loss=9.577162\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:46 INFO 140186635052416] #quality_metric: host=algo-1, epoch=67, batch=0 train loss <loss>=9.57716178894043\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:46 INFO 140186635052416] Epoch[67] Batch[5] avg_epoch_loss=9.471744\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:46 INFO 140186635052416] #quality_metric: host=algo-1, epoch=67, batch=5 train loss <loss>=9.471744219462076\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:46 INFO 140186635052416] Epoch[67] Batch [5]#011Speed: 2041.82 samples/sec#011loss=9.471744\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:46 INFO 140186635052416] processed a total of 625 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457586.1355832, \"EndTime\": 1631457586.7358892, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 599.8167991638184, \"count\": 1, \"min\": 599.8167991638184, \"max\": 599.8167991638184}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:46 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=1041.748795096456 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:46 INFO 140186635052416] #progress_metric: host=algo-1, completed 34.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:46 INFO 140186635052416] #quality_metric: host=algo-1, epoch=67, train loss <loss>=9.688343334197999\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:46 INFO 140186635052416] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:46 INFO 140186635052416] Epoch[68] Batch[0] avg_epoch_loss=9.946183\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:46 INFO 140186635052416] #quality_metric: host=algo-1, epoch=68, batch=0 train loss <loss>=9.946183204650879\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:47 INFO 140186635052416] Epoch[68] Batch[5] avg_epoch_loss=9.618273\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:47 INFO 140186635052416] #quality_metric: host=algo-1, epoch=68, batch=5 train loss <loss>=9.618273417154947\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:47 INFO 140186635052416] Epoch[68] Batch [5]#011Speed: 1981.58 samples/sec#011loss=9.618273\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:47 INFO 140186635052416] processed a total of 624 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457586.7359798, \"EndTime\": 1631457587.2880943, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 551.5792369842529, \"count\": 1, \"min\": 551.5792369842529, \"max\": 551.5792369842529}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:47 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=1131.0213871321123 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:47 INFO 140186635052416] #progress_metric: host=algo-1, completed 34.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:47 INFO 140186635052416] #quality_metric: host=algo-1, epoch=68, train loss <loss>=9.735770988464356\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:47 INFO 140186635052416] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:47 INFO 140186635052416] Epoch[69] Batch[0] avg_epoch_loss=10.867234\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:47 INFO 140186635052416] #quality_metric: host=algo-1, epoch=69, batch=0 train loss <loss>=10.867234230041504\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:47 INFO 140186635052416] Epoch[69] Batch[5] avg_epoch_loss=10.766958\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:47 INFO 140186635052416] #quality_metric: host=algo-1, epoch=69, batch=5 train loss <loss>=10.766957918802897\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:47 INFO 140186635052416] Epoch[69] Batch [5]#011Speed: 1682.99 samples/sec#011loss=10.766958\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:47 INFO 140186635052416] Epoch[69] Batch[10] avg_epoch_loss=10.526204\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:47 INFO 140186635052416] #quality_metric: host=algo-1, epoch=69, batch=10 train loss <loss>=10.237298774719239\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:47 INFO 140186635052416] Epoch[69] Batch [10]#011Speed: 1776.73 samples/sec#011loss=10.237299\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:47 INFO 140186635052416] processed a total of 694 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457587.2881842, \"EndTime\": 1631457587.906087, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 617.3677444458008, \"count\": 1, \"min\": 617.3677444458008, \"max\": 617.3677444458008}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:47 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=1123.885650302994 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:47 INFO 140186635052416] #progress_metric: host=algo-1, completed 35.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:47 INFO 140186635052416] #quality_metric: host=algo-1, epoch=69, train loss <loss>=10.526203762401234\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:47 INFO 140186635052416] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:48 INFO 140186635052416] Epoch[70] Batch[0] avg_epoch_loss=9.563767\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:48 INFO 140186635052416] #quality_metric: host=algo-1, epoch=70, batch=0 train loss <loss>=9.563767433166504\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:48 INFO 140186635052416] Epoch[70] Batch[5] avg_epoch_loss=9.465081\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:48 INFO 140186635052416] #quality_metric: host=algo-1, epoch=70, batch=5 train loss <loss>=9.465080579121908\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:48 INFO 140186635052416] Epoch[70] Batch [5]#011Speed: 2058.35 samples/sec#011loss=9.465081\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:48 INFO 140186635052416] processed a total of 612 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457587.9061754, \"EndTime\": 1631457588.442655, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 535.9899997711182, \"count\": 1, \"min\": 535.9899997711182, \"max\": 535.9899997711182}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:48 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=1141.5112425056632 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:48 INFO 140186635052416] #progress_metric: host=algo-1, completed 35.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:48 INFO 140186635052416] #quality_metric: host=algo-1, epoch=70, train loss <loss>=9.988342475891113\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:48 INFO 140186635052416] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:48 INFO 140186635052416] Epoch[71] Batch[0] avg_epoch_loss=10.277129\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:48 INFO 140186635052416] #quality_metric: host=algo-1, epoch=71, batch=0 train loss <loss>=10.277129173278809\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:48 INFO 140186635052416] Epoch[71] Batch[5] avg_epoch_loss=10.383931\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:48 INFO 140186635052416] #quality_metric: host=algo-1, epoch=71, batch=5 train loss <loss>=10.383931159973145\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:48 INFO 140186635052416] Epoch[71] Batch [5]#011Speed: 2103.47 samples/sec#011loss=10.383931\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:48 INFO 140186635052416] processed a total of 587 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457588.442749, \"EndTime\": 1631457588.9584816, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 515.162467956543, \"count\": 1, \"min\": 515.162467956543, \"max\": 515.162467956543}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:48 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=1139.1395335279665 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:48 INFO 140186635052416] #progress_metric: host=algo-1, completed 36.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:48 INFO 140186635052416] #quality_metric: host=algo-1, epoch=71, train loss <loss>=9.888479471206665\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:48 INFO 140186635052416] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:49 INFO 140186635052416] Epoch[72] Batch[0] avg_epoch_loss=10.737208\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:49 INFO 140186635052416] #quality_metric: host=algo-1, epoch=72, batch=0 train loss <loss>=10.737208366394043\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:49 INFO 140186635052416] Epoch[72] Batch[5] avg_epoch_loss=10.305973\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:49 INFO 140186635052416] #quality_metric: host=algo-1, epoch=72, batch=5 train loss <loss>=10.305973370869955\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:49 INFO 140186635052416] Epoch[72] Batch [5]#011Speed: 2134.12 samples/sec#011loss=10.305973\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:49 INFO 140186635052416] processed a total of 639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457588.9585733, \"EndTime\": 1631457589.5192008, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 560.0998401641846, \"count\": 1, \"min\": 560.0998401641846, \"max\": 560.0998401641846}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:49 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=1140.54128895588 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:49 INFO 140186635052416] #progress_metric: host=algo-1, completed 36.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:49 INFO 140186635052416] #quality_metric: host=algo-1, epoch=72, train loss <loss>=10.254221439361572\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:49 INFO 140186635052416] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:49 INFO 140186635052416] Epoch[73] Batch[0] avg_epoch_loss=9.310386\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:49 INFO 140186635052416] #quality_metric: host=algo-1, epoch=73, batch=0 train loss <loss>=9.310385704040527\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:49 INFO 140186635052416] Epoch[73] Batch[5] avg_epoch_loss=9.812400\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:49 INFO 140186635052416] #quality_metric: host=algo-1, epoch=73, batch=5 train loss <loss>=9.812400341033936\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:49 INFO 140186635052416] Epoch[73] Batch [5]#011Speed: 2095.73 samples/sec#011loss=9.812400\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:50 INFO 140186635052416] processed a total of 631 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457589.5193143, \"EndTime\": 1631457590.0571032, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 537.2259616851807, \"count\": 1, \"min\": 537.2259616851807, \"max\": 537.2259616851807}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:50 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=1174.2552768385244 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:50 INFO 140186635052416] #progress_metric: host=algo-1, completed 37.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:50 INFO 140186635052416] #quality_metric: host=algo-1, epoch=73, train loss <loss>=9.93057975769043\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:50 INFO 140186635052416] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:50 INFO 140186635052416] Epoch[74] Batch[0] avg_epoch_loss=10.476953\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:50 INFO 140186635052416] #quality_metric: host=algo-1, epoch=74, batch=0 train loss <loss>=10.47695255279541\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:50 INFO 140186635052416] Epoch[74] Batch[5] avg_epoch_loss=9.930089\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:50 INFO 140186635052416] #quality_metric: host=algo-1, epoch=74, batch=5 train loss <loss>=9.930089314778646\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:50 INFO 140186635052416] Epoch[74] Batch [5]#011Speed: 2116.72 samples/sec#011loss=9.930089\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:50 INFO 140186635052416] Epoch[74] Batch[10] avg_epoch_loss=9.528237\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:50 INFO 140186635052416] #quality_metric: host=algo-1, epoch=74, batch=10 train loss <loss>=9.046013259887696\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:50 INFO 140186635052416] Epoch[74] Batch [10]#011Speed: 1896.54 samples/sec#011loss=9.046013\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:50 INFO 140186635052416] processed a total of 648 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457590.0571935, \"EndTime\": 1631457590.625505, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 567.7509307861328, \"count\": 1, \"min\": 567.7509307861328, \"max\": 567.7509307861328}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:50 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=1141.0867943539922 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:50 INFO 140186635052416] #progress_metric: host=algo-1, completed 37.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:50 INFO 140186635052416] #quality_metric: host=algo-1, epoch=74, train loss <loss>=9.528236562555486\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:50 INFO 140186635052416] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:50 INFO 140186635052416] Epoch[75] Batch[0] avg_epoch_loss=10.131413\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:50 INFO 140186635052416] #quality_metric: host=algo-1, epoch=75, batch=0 train loss <loss>=10.131413459777832\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:51 INFO 140186635052416] Epoch[75] Batch[5] avg_epoch_loss=9.947769\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:51 INFO 140186635052416] #quality_metric: host=algo-1, epoch=75, batch=5 train loss <loss>=9.947768529256185\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:51 INFO 140186635052416] Epoch[75] Batch [5]#011Speed: 2045.39 samples/sec#011loss=9.947769\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:51 INFO 140186635052416] Epoch[75] Batch[10] avg_epoch_loss=9.796046\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:51 INFO 140186635052416] #quality_metric: host=algo-1, epoch=75, batch=10 train loss <loss>=9.613979530334472\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:51 INFO 140186635052416] Epoch[75] Batch [10]#011Speed: 1891.66 samples/sec#011loss=9.613980\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:51 INFO 140186635052416] processed a total of 650 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457590.625589, \"EndTime\": 1631457591.1982038, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 572.0713138580322, \"count\": 1, \"min\": 572.0713138580322, \"max\": 572.0713138580322}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:51 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=1135.9606465518857 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:51 INFO 140186635052416] #progress_metric: host=algo-1, completed 38.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:51 INFO 140186635052416] #quality_metric: host=algo-1, epoch=75, train loss <loss>=9.796046257019043\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:51 INFO 140186635052416] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:51 INFO 140186635052416] Epoch[76] Batch[0] avg_epoch_loss=9.699683\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:51 INFO 140186635052416] #quality_metric: host=algo-1, epoch=76, batch=0 train loss <loss>=9.69968318939209\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:51 INFO 140186635052416] Epoch[76] Batch[5] avg_epoch_loss=9.776237\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:51 INFO 140186635052416] #quality_metric: host=algo-1, epoch=76, batch=5 train loss <loss>=9.776236693064371\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:51 INFO 140186635052416] Epoch[76] Batch [5]#011Speed: 2111.58 samples/sec#011loss=9.776237\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:51 INFO 140186635052416] Epoch[76] Batch[10] avg_epoch_loss=9.799506\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:51 INFO 140186635052416] #quality_metric: host=algo-1, epoch=76, batch=10 train loss <loss>=9.827429389953613\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:51 INFO 140186635052416] Epoch[76] Batch [10]#011Speed: 1720.82 samples/sec#011loss=9.827429\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:51 INFO 140186635052416] processed a total of 681 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457591.1982925, \"EndTime\": 1631457591.7901628, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 591.3791656494141, \"count\": 1, \"min\": 591.3791656494141, \"max\": 591.3791656494141}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:51 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=1151.307358469804 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:51 INFO 140186635052416] #progress_metric: host=algo-1, completed 38.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:51 INFO 140186635052416] #quality_metric: host=algo-1, epoch=76, train loss <loss>=9.799506100741299\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:51 INFO 140186635052416] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:52 INFO 140186635052416] Epoch[77] Batch[0] avg_epoch_loss=10.228976\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:52 INFO 140186635052416] #quality_metric: host=algo-1, epoch=77, batch=0 train loss <loss>=10.228976249694824\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:52 INFO 140186635052416] Epoch[77] Batch[5] avg_epoch_loss=9.608341\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:52 INFO 140186635052416] #quality_metric: host=algo-1, epoch=77, batch=5 train loss <loss>=9.608341375986734\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:52 INFO 140186635052416] Epoch[77] Batch [5]#011Speed: 2085.07 samples/sec#011loss=9.608341\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:52 INFO 140186635052416] processed a total of 632 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457591.7902381, \"EndTime\": 1631457592.3183494, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 527.6143550872803, \"count\": 1, \"min\": 527.6143550872803, \"max\": 527.6143550872803}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:52 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=1197.5236938734206 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:52 INFO 140186635052416] #progress_metric: host=algo-1, completed 39.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:52 INFO 140186635052416] #quality_metric: host=algo-1, epoch=77, train loss <loss>=9.428437232971191\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:52 INFO 140186635052416] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:52 INFO 140186635052416] Epoch[78] Batch[0] avg_epoch_loss=9.587904\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:52 INFO 140186635052416] #quality_metric: host=algo-1, epoch=78, batch=0 train loss <loss>=9.58790397644043\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:52 INFO 140186635052416] Epoch[78] Batch[5] avg_epoch_loss=10.147764\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:52 INFO 140186635052416] #quality_metric: host=algo-1, epoch=78, batch=5 train loss <loss>=10.147763888041178\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:52 INFO 140186635052416] Epoch[78] Batch [5]#011Speed: 1950.99 samples/sec#011loss=10.147764\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:52 INFO 140186635052416] Epoch[78] Batch[10] avg_epoch_loss=10.361584\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:52 INFO 140186635052416] #quality_metric: host=algo-1, epoch=78, batch=10 train loss <loss>=10.618167495727539\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:52 INFO 140186635052416] Epoch[78] Batch [10]#011Speed: 1789.35 samples/sec#011loss=10.618167\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:52 INFO 140186635052416] processed a total of 658 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457592.3184438, \"EndTime\": 1631457592.9118738, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 592.8990840911865, \"count\": 1, \"min\": 592.8990840911865, \"max\": 592.8990840911865}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:52 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=1109.5547158402653 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:52 INFO 140186635052416] #progress_metric: host=algo-1, completed 39.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:52 INFO 140186635052416] #quality_metric: host=algo-1, epoch=78, train loss <loss>=10.361583709716797\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:52 INFO 140186635052416] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:53 INFO 140186635052416] Epoch[79] Batch[0] avg_epoch_loss=9.313784\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:53 INFO 140186635052416] #quality_metric: host=algo-1, epoch=79, batch=0 train loss <loss>=9.313783645629883\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:53 INFO 140186635052416] Epoch[79] Batch[5] avg_epoch_loss=10.085895\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:53 INFO 140186635052416] #quality_metric: host=algo-1, epoch=79, batch=5 train loss <loss>=10.08589506149292\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:53 INFO 140186635052416] Epoch[79] Batch [5]#011Speed: 2055.39 samples/sec#011loss=10.085895\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:53 INFO 140186635052416] Epoch[79] Batch[10] avg_epoch_loss=9.977500\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:53 INFO 140186635052416] #quality_metric: host=algo-1, epoch=79, batch=10 train loss <loss>=9.847426223754884\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:53 INFO 140186635052416] Epoch[79] Batch [10]#011Speed: 1831.30 samples/sec#011loss=9.847426\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:53 INFO 140186635052416] processed a total of 672 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457592.9119632, \"EndTime\": 1631457593.4939187, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 581.458568572998, \"count\": 1, \"min\": 581.458568572998, \"max\": 581.458568572998}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:53 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=1155.4641735353223 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:53 INFO 140186635052416] #progress_metric: host=algo-1, completed 40.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:53 INFO 140186635052416] #quality_metric: host=algo-1, epoch=79, train loss <loss>=9.977500135248357\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:53 INFO 140186635052416] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:53 INFO 140186635052416] Epoch[80] Batch[0] avg_epoch_loss=10.760450\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:53 INFO 140186635052416] #quality_metric: host=algo-1, epoch=80, batch=0 train loss <loss>=10.76045036315918\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:53 INFO 140186635052416] Epoch[80] Batch[5] avg_epoch_loss=9.911628\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:53 INFO 140186635052416] #quality_metric: host=algo-1, epoch=80, batch=5 train loss <loss>=9.911627610524496\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:53 INFO 140186635052416] Epoch[80] Batch [5]#011Speed: 2035.27 samples/sec#011loss=9.911628\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:54 INFO 140186635052416] Epoch[80] Batch[10] avg_epoch_loss=10.445228\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:54 INFO 140186635052416] #quality_metric: host=algo-1, epoch=80, batch=10 train loss <loss>=11.085548591613769\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:54 INFO 140186635052416] Epoch[80] Batch [10]#011Speed: 2022.42 samples/sec#011loss=11.085549\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:54 INFO 140186635052416] processed a total of 645 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457593.4939992, \"EndTime\": 1631457594.065639, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 571.1550712585449, \"count\": 1, \"min\": 571.1550712585449, \"max\": 571.1550712585449}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:54 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=1129.0416614151343 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:54 INFO 140186635052416] #progress_metric: host=algo-1, completed 40.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:54 INFO 140186635052416] #quality_metric: host=algo-1, epoch=80, train loss <loss>=10.445228056474166\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:54 INFO 140186635052416] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:54 INFO 140186635052416] Epoch[81] Batch[0] avg_epoch_loss=10.163917\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:54 INFO 140186635052416] #quality_metric: host=algo-1, epoch=81, batch=0 train loss <loss>=10.16391658782959\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:54 INFO 140186635052416] Epoch[81] Batch[5] avg_epoch_loss=10.498400\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:54 INFO 140186635052416] #quality_metric: host=algo-1, epoch=81, batch=5 train loss <loss>=10.49839989344279\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:54 INFO 140186635052416] Epoch[81] Batch [5]#011Speed: 1916.05 samples/sec#011loss=10.498400\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:54 INFO 140186635052416] Epoch[81] Batch[10] avg_epoch_loss=10.599085\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:54 INFO 140186635052416] #quality_metric: host=algo-1, epoch=81, batch=10 train loss <loss>=10.719906425476074\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:54 INFO 140186635052416] Epoch[81] Batch [10]#011Speed: 1499.90 samples/sec#011loss=10.719906\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:54 INFO 140186635052416] processed a total of 687 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457594.0657234, \"EndTime\": 1631457594.6938655, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 627.6764869689941, \"count\": 1, \"min\": 627.6764869689941, \"max\": 627.6764869689941}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:54 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=1094.2842698813922 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:54 INFO 140186635052416] #progress_metric: host=algo-1, completed 41.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:54 INFO 140186635052416] #quality_metric: host=algo-1, epoch=81, train loss <loss>=10.599084680730646\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:54 INFO 140186635052416] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:54 INFO 140186635052416] Epoch[82] Batch[0] avg_epoch_loss=10.330276\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:54 INFO 140186635052416] #quality_metric: host=algo-1, epoch=82, batch=0 train loss <loss>=10.330276489257812\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:55 INFO 140186635052416] Epoch[82] Batch[5] avg_epoch_loss=9.990183\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:55 INFO 140186635052416] #quality_metric: host=algo-1, epoch=82, batch=5 train loss <loss>=9.990183353424072\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:55 INFO 140186635052416] Epoch[82] Batch [5]#011Speed: 1968.70 samples/sec#011loss=9.990183\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:55 INFO 140186635052416] Epoch[82] Batch[10] avg_epoch_loss=9.728211\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:55 INFO 140186635052416] #quality_metric: host=algo-1, epoch=82, batch=10 train loss <loss>=9.413845252990722\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:55 INFO 140186635052416] Epoch[82] Batch [10]#011Speed: 1896.17 samples/sec#011loss=9.413845\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:55 INFO 140186635052416] processed a total of 667 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457594.6939547, \"EndTime\": 1631457595.3225596, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 628.1068325042725, \"count\": 1, \"min\": 628.1068325042725, \"max\": 628.1068325042725}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:55 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=1061.7004158224286 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:55 INFO 140186635052416] #progress_metric: host=algo-1, completed 41.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:55 INFO 140186635052416] #quality_metric: host=algo-1, epoch=82, train loss <loss>=9.728211489590732\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:55 INFO 140186635052416] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:55 INFO 140186635052416] Epoch[83] Batch[0] avg_epoch_loss=10.283458\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:55 INFO 140186635052416] #quality_metric: host=algo-1, epoch=83, batch=0 train loss <loss>=10.28345775604248\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:55 INFO 140186635052416] Epoch[83] Batch[5] avg_epoch_loss=10.102567\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:55 INFO 140186635052416] #quality_metric: host=algo-1, epoch=83, batch=5 train loss <loss>=10.102566560109457\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:55 INFO 140186635052416] Epoch[83] Batch [5]#011Speed: 2046.06 samples/sec#011loss=10.102567\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:55 INFO 140186635052416] Epoch[83] Batch[10] avg_epoch_loss=10.224487\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:55 INFO 140186635052416] #quality_metric: host=algo-1, epoch=83, batch=10 train loss <loss>=10.370791244506837\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:55 INFO 140186635052416] Epoch[83] Batch [10]#011Speed: 2002.03 samples/sec#011loss=10.370791\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:55 INFO 140186635052416] processed a total of 665 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457595.3226466, \"EndTime\": 1631457595.8939693, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 570.8396434783936, \"count\": 1, \"min\": 570.8396434783936, \"max\": 570.8396434783936}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:55 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=1164.6937806575322 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:55 INFO 140186635052416] #progress_metric: host=algo-1, completed 42.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:55 INFO 140186635052416] #quality_metric: host=algo-1, epoch=83, train loss <loss>=10.224486871199174\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:55 INFO 140186635052416] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:56 INFO 140186635052416] Epoch[84] Batch[0] avg_epoch_loss=10.295627\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:56 INFO 140186635052416] #quality_metric: host=algo-1, epoch=84, batch=0 train loss <loss>=10.295626640319824\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:56 INFO 140186635052416] Epoch[84] Batch[5] avg_epoch_loss=9.958577\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:56 INFO 140186635052416] #quality_metric: host=algo-1, epoch=84, batch=5 train loss <loss>=9.958576679229736\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:56 INFO 140186635052416] Epoch[84] Batch [5]#011Speed: 2155.97 samples/sec#011loss=9.958577\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:56 INFO 140186635052416] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457595.8940492, \"EndTime\": 1631457596.428398, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 533.8146686553955, \"count\": 1, \"min\": 533.8146686553955, \"max\": 533.8146686553955}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:56 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=1185.4971784661466 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:56 INFO 140186635052416] #progress_metric: host=algo-1, completed 42.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:56 INFO 140186635052416] #quality_metric: host=algo-1, epoch=84, train loss <loss>=9.759149360656739\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:56 INFO 140186635052416] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:56 INFO 140186635052416] Epoch[85] Batch[0] avg_epoch_loss=9.843284\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:56 INFO 140186635052416] #quality_metric: host=algo-1, epoch=85, batch=0 train loss <loss>=9.843283653259277\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:56 INFO 140186635052416] Epoch[85] Batch[5] avg_epoch_loss=9.768601\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:56 INFO 140186635052416] #quality_metric: host=algo-1, epoch=85, batch=5 train loss <loss>=9.768601417541504\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:56 INFO 140186635052416] Epoch[85] Batch [5]#011Speed: 2016.73 samples/sec#011loss=9.768601\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:56 INFO 140186635052416] processed a total of 609 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457596.4284902, \"EndTime\": 1631457596.9723167, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 543.2965755462646, \"count\": 1, \"min\": 543.2965755462646, \"max\": 543.2965755462646}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:56 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=1120.6799794144526 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:56 INFO 140186635052416] #progress_metric: host=algo-1, completed 43.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:56 INFO 140186635052416] #quality_metric: host=algo-1, epoch=85, train loss <loss>=9.812414264678955\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:56 INFO 140186635052416] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:57 INFO 140186635052416] Epoch[86] Batch[0] avg_epoch_loss=9.973707\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:57 INFO 140186635052416] #quality_metric: host=algo-1, epoch=86, batch=0 train loss <loss>=9.97370719909668\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:57 INFO 140186635052416] Epoch[86] Batch[5] avg_epoch_loss=9.503582\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:57 INFO 140186635052416] #quality_metric: host=algo-1, epoch=86, batch=5 train loss <loss>=9.503581682840982\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:57 INFO 140186635052416] Epoch[86] Batch [5]#011Speed: 1708.95 samples/sec#011loss=9.503582\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:57 INFO 140186635052416] processed a total of 639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457596.9723952, \"EndTime\": 1631457597.5499494, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 576.9951343536377, \"count\": 1, \"min\": 576.9951343536377, \"max\": 576.9951343536377}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:57 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=1107.200901571181 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:57 INFO 140186635052416] #progress_metric: host=algo-1, completed 43.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:57 INFO 140186635052416] #quality_metric: host=algo-1, epoch=86, train loss <loss>=9.525897026062012\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:57 INFO 140186635052416] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:57 INFO 140186635052416] Epoch[87] Batch[0] avg_epoch_loss=9.926631\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:57 INFO 140186635052416] #quality_metric: host=algo-1, epoch=87, batch=0 train loss <loss>=9.926630973815918\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:57 INFO 140186635052416] Epoch[87] Batch[5] avg_epoch_loss=10.227658\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:57 INFO 140186635052416] #quality_metric: host=algo-1, epoch=87, batch=5 train loss <loss>=10.22765843073527\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:57 INFO 140186635052416] Epoch[87] Batch [5]#011Speed: 1997.64 samples/sec#011loss=10.227658\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:58 INFO 140186635052416] processed a total of 599 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457597.5500386, \"EndTime\": 1631457598.1076095, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 557.0430755615234, \"count\": 1, \"min\": 557.0430755615234, \"max\": 557.0430755615234}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:58 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=1075.0580540090432 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:58 INFO 140186635052416] #progress_metric: host=algo-1, completed 44.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:58 INFO 140186635052416] #quality_metric: host=algo-1, epoch=87, train loss <loss>=9.98936948776245\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:58 INFO 140186635052416] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:58 INFO 140186635052416] Epoch[88] Batch[0] avg_epoch_loss=9.691934\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:58 INFO 140186635052416] #quality_metric: host=algo-1, epoch=88, batch=0 train loss <loss>=9.691933631896973\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:58 INFO 140186635052416] Epoch[88] Batch[5] avg_epoch_loss=10.436026\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:58 INFO 140186635052416] #quality_metric: host=algo-1, epoch=88, batch=5 train loss <loss>=10.436025778452555\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:58 INFO 140186635052416] Epoch[88] Batch [5]#011Speed: 2148.14 samples/sec#011loss=10.436026\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:58 INFO 140186635052416] processed a total of 632 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457598.107699, \"EndTime\": 1631457598.6416135, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 533.379077911377, \"count\": 1, \"min\": 533.379077911377, \"max\": 533.379077911377}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:58 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=1184.6171950028713 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:58 INFO 140186635052416] #progress_metric: host=algo-1, completed 44.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:58 INFO 140186635052416] #quality_metric: host=algo-1, epoch=88, train loss <loss>=10.206443786621094\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:58 INFO 140186635052416] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:58 INFO 140186635052416] Epoch[89] Batch[0] avg_epoch_loss=10.330317\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:58 INFO 140186635052416] #quality_metric: host=algo-1, epoch=89, batch=0 train loss <loss>=10.330317497253418\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:59 INFO 140186635052416] Epoch[89] Batch[5] avg_epoch_loss=9.841233\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:59 INFO 140186635052416] #quality_metric: host=algo-1, epoch=89, batch=5 train loss <loss>=9.841232935587565\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:59 INFO 140186635052416] Epoch[89] Batch [5]#011Speed: 2126.86 samples/sec#011loss=9.841233\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:59 INFO 140186635052416] processed a total of 622 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457598.6416955, \"EndTime\": 1631457599.1788568, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 536.5972518920898, \"count\": 1, \"min\": 536.5972518920898, \"max\": 536.5972518920898}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:59 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=1158.8497788114764 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:59 INFO 140186635052416] #progress_metric: host=algo-1, completed 45.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:59 INFO 140186635052416] #quality_metric: host=algo-1, epoch=89, train loss <loss>=9.84140510559082\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:59 INFO 140186635052416] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:59 INFO 140186635052416] Epoch[90] Batch[0] avg_epoch_loss=9.147019\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:59 INFO 140186635052416] #quality_metric: host=algo-1, epoch=90, batch=0 train loss <loss>=9.147019386291504\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:59 INFO 140186635052416] Epoch[90] Batch[5] avg_epoch_loss=9.912150\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:59 INFO 140186635052416] #quality_metric: host=algo-1, epoch=90, batch=5 train loss <loss>=9.912150382995605\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:59 INFO 140186635052416] Epoch[90] Batch [5]#011Speed: 2007.16 samples/sec#011loss=9.912150\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:59 INFO 140186635052416] processed a total of 630 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457599.178952, \"EndTime\": 1631457599.7325153, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 552.9937744140625, \"count\": 1, \"min\": 552.9937744140625, \"max\": 552.9937744140625}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:59 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=1138.9650824931434 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:59 INFO 140186635052416] #progress_metric: host=algo-1, completed 45.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:59 INFO 140186635052416] #quality_metric: host=algo-1, epoch=90, train loss <loss>=9.995725154876709\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:59 INFO 140186635052416] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:59 INFO 140186635052416] Epoch[91] Batch[0] avg_epoch_loss=10.226317\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:39:59 INFO 140186635052416] #quality_metric: host=algo-1, epoch=91, batch=0 train loss <loss>=10.226317405700684\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:00 INFO 140186635052416] Epoch[91] Batch[5] avg_epoch_loss=9.698573\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:00 INFO 140186635052416] #quality_metric: host=algo-1, epoch=91, batch=5 train loss <loss>=9.698572794596354\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:00 INFO 140186635052416] Epoch[91] Batch [5]#011Speed: 1933.66 samples/sec#011loss=9.698573\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:00 INFO 140186635052416] processed a total of 636 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457599.7326088, \"EndTime\": 1631457600.3009548, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 567.7852630615234, \"count\": 1, \"min\": 567.7852630615234, \"max\": 567.7852630615234}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:00 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=1119.8568236869614 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:00 INFO 140186635052416] #progress_metric: host=algo-1, completed 46.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:00 INFO 140186635052416] #quality_metric: host=algo-1, epoch=91, train loss <loss>=9.86761178970337\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:00 INFO 140186635052416] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:00 INFO 140186635052416] Epoch[92] Batch[0] avg_epoch_loss=9.399529\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:00 INFO 140186635052416] #quality_metric: host=algo-1, epoch=92, batch=0 train loss <loss>=9.399528503417969\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:00 INFO 140186635052416] Epoch[92] Batch[5] avg_epoch_loss=9.603890\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:00 INFO 140186635052416] #quality_metric: host=algo-1, epoch=92, batch=5 train loss <loss>=9.603890419006348\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:00 INFO 140186635052416] Epoch[92] Batch [5]#011Speed: 2078.58 samples/sec#011loss=9.603890\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:00 INFO 140186635052416] processed a total of 621 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457600.301053, \"EndTime\": 1631457600.9218016, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 620.2080249786377, \"count\": 1, \"min\": 620.2080249786377, \"max\": 620.2080249786377}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:00 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=1001.1106979740354 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:00 INFO 140186635052416] #progress_metric: host=algo-1, completed 46.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:00 INFO 140186635052416] #quality_metric: host=algo-1, epoch=92, train loss <loss>=9.72676706314087\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:00 INFO 140186635052416] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:01 INFO 140186635052416] Epoch[93] Batch[0] avg_epoch_loss=10.106127\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:01 INFO 140186635052416] #quality_metric: host=algo-1, epoch=93, batch=0 train loss <loss>=10.10612678527832\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:01 INFO 140186635052416] Epoch[93] Batch[5] avg_epoch_loss=10.071318\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:01 INFO 140186635052416] #quality_metric: host=algo-1, epoch=93, batch=5 train loss <loss>=10.071317831675211\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:01 INFO 140186635052416] Epoch[93] Batch [5]#011Speed: 1623.91 samples/sec#011loss=10.071318\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:01 INFO 140186635052416] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457600.9218686, \"EndTime\": 1631457601.5442224, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 621.8411922454834, \"count\": 1, \"min\": 621.8411922454834, \"max\": 621.8411922454834}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:01 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=1017.7118386009136 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:01 INFO 140186635052416] #progress_metric: host=algo-1, completed 47.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:01 INFO 140186635052416] #quality_metric: host=algo-1, epoch=93, train loss <loss>=9.816405200958252\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:01 INFO 140186635052416] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:01 INFO 140186635052416] Epoch[94] Batch[0] avg_epoch_loss=9.423357\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:01 INFO 140186635052416] #quality_metric: host=algo-1, epoch=94, batch=0 train loss <loss>=9.423357009887695\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:02 INFO 140186635052416] Epoch[94] Batch[5] avg_epoch_loss=9.799356\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:02 INFO 140186635052416] #quality_metric: host=algo-1, epoch=94, batch=5 train loss <loss>=9.799355665842691\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:02 INFO 140186635052416] Epoch[94] Batch [5]#011Speed: 1915.77 samples/sec#011loss=9.799356\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:02 INFO 140186635052416] Epoch[94] Batch[10] avg_epoch_loss=10.280037\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:02 INFO 140186635052416] #quality_metric: host=algo-1, epoch=94, batch=10 train loss <loss>=10.856854820251465\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:02 INFO 140186635052416] Epoch[94] Batch [10]#011Speed: 1990.17 samples/sec#011loss=10.856855\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:02 INFO 140186635052416] processed a total of 642 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457601.544317, \"EndTime\": 1631457602.1916246, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 646.7294692993164, \"count\": 1, \"min\": 646.7294692993164, \"max\": 646.7294692993164}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:02 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=992.4822043831133 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:02 INFO 140186635052416] #progress_metric: host=algo-1, completed 47.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:02 INFO 140186635052416] #quality_metric: host=algo-1, epoch=94, train loss <loss>=10.280037099664861\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:02 INFO 140186635052416] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:02 INFO 140186635052416] Epoch[95] Batch[0] avg_epoch_loss=11.030096\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:02 INFO 140186635052416] #quality_metric: host=algo-1, epoch=95, batch=0 train loss <loss>=11.030096054077148\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:02 INFO 140186635052416] Epoch[95] Batch[5] avg_epoch_loss=10.156856\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:02 INFO 140186635052416] #quality_metric: host=algo-1, epoch=95, batch=5 train loss <loss>=10.156856060028076\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:02 INFO 140186635052416] Epoch[95] Batch [5]#011Speed: 2005.77 samples/sec#011loss=10.156856\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:02 INFO 140186635052416] Epoch[95] Batch[10] avg_epoch_loss=10.386654\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:02 INFO 140186635052416] #quality_metric: host=algo-1, epoch=95, batch=10 train loss <loss>=10.662412071228028\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:02 INFO 140186635052416] Epoch[95] Batch [10]#011Speed: 1905.59 samples/sec#011loss=10.662412\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:02 INFO 140186635052416] processed a total of 672 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457602.1917136, \"EndTime\": 1631457602.7633944, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 571.1796283721924, \"count\": 1, \"min\": 571.1796283721924, \"max\": 571.1796283721924}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:02 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=1176.260180326056 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:02 INFO 140186635052416] #progress_metric: host=algo-1, completed 48.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:02 INFO 140186635052416] #quality_metric: host=algo-1, epoch=95, train loss <loss>=10.386654246937145\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:02 INFO 140186635052416] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:03 INFO 140186635052416] Epoch[96] Batch[0] avg_epoch_loss=10.034892\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:03 INFO 140186635052416] #quality_metric: host=algo-1, epoch=96, batch=0 train loss <loss>=10.034892082214355\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:03 INFO 140186635052416] Epoch[96] Batch[5] avg_epoch_loss=9.681949\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:03 INFO 140186635052416] #quality_metric: host=algo-1, epoch=96, batch=5 train loss <loss>=9.6819486618042\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:03 INFO 140186635052416] Epoch[96] Batch [5]#011Speed: 1957.99 samples/sec#011loss=9.681949\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:03 INFO 140186635052416] Epoch[96] Batch[10] avg_epoch_loss=9.787228\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:03 INFO 140186635052416] #quality_metric: host=algo-1, epoch=96, batch=10 train loss <loss>=9.913564109802246\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:03 INFO 140186635052416] Epoch[96] Batch [10]#011Speed: 1876.91 samples/sec#011loss=9.913564\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:03 INFO 140186635052416] processed a total of 676 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457602.763476, \"EndTime\": 1631457603.3470674, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 583.122730255127, \"count\": 1, \"min\": 583.122730255127, \"max\": 583.122730255127}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:03 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=1159.0178887790648 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:03 INFO 140186635052416] #progress_metric: host=algo-1, completed 48.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:03 INFO 140186635052416] #quality_metric: host=algo-1, epoch=96, train loss <loss>=9.78722841089422\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:03 INFO 140186635052416] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:03 INFO 140186635052416] Epoch[97] Batch[0] avg_epoch_loss=9.982371\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:03 INFO 140186635052416] #quality_metric: host=algo-1, epoch=97, batch=0 train loss <loss>=9.98237133026123\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:03 INFO 140186635052416] Epoch[97] Batch[5] avg_epoch_loss=10.036041\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:03 INFO 140186635052416] #quality_metric: host=algo-1, epoch=97, batch=5 train loss <loss>=10.036040941874186\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:03 INFO 140186635052416] Epoch[97] Batch [5]#011Speed: 2128.63 samples/sec#011loss=10.036041\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:03 INFO 140186635052416] Epoch[97] Batch[10] avg_epoch_loss=9.771222\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:03 INFO 140186635052416] #quality_metric: host=algo-1, epoch=97, batch=10 train loss <loss>=9.453439617156983\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:03 INFO 140186635052416] Epoch[97] Batch [10]#011Speed: 1543.03 samples/sec#011loss=9.453440\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:03 INFO 140186635052416] processed a total of 656 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457603.3471537, \"EndTime\": 1631457603.9764647, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 628.8187503814697, \"count\": 1, \"min\": 628.8187503814697, \"max\": 628.8187503814697}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:03 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=1043.0206430991032 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:03 INFO 140186635052416] #progress_metric: host=algo-1, completed 49.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:03 INFO 140186635052416] #quality_metric: host=algo-1, epoch=97, train loss <loss>=9.77122215791182\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:03 INFO 140186635052416] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:04 INFO 140186635052416] Epoch[98] Batch[0] avg_epoch_loss=10.229782\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:04 INFO 140186635052416] #quality_metric: host=algo-1, epoch=98, batch=0 train loss <loss>=10.229782104492188\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:04 INFO 140186635052416] Epoch[98] Batch[5] avg_epoch_loss=9.566694\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:04 INFO 140186635052416] #quality_metric: host=algo-1, epoch=98, batch=5 train loss <loss>=9.566693941752115\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:04 INFO 140186635052416] Epoch[98] Batch [5]#011Speed: 1543.18 samples/sec#011loss=9.566694\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:04 INFO 140186635052416] Epoch[98] Batch[10] avg_epoch_loss=9.878171\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:04 INFO 140186635052416] #quality_metric: host=algo-1, epoch=98, batch=10 train loss <loss>=10.25194435119629\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:04 INFO 140186635052416] Epoch[98] Batch [10]#011Speed: 1649.09 samples/sec#011loss=10.251944\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:04 INFO 140186635052416] processed a total of 648 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457603.976544, \"EndTime\": 1631457604.664637, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 687.4463558197021, \"count\": 1, \"min\": 687.4463558197021, \"max\": 687.4463558197021}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:04 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=942.3954469472196 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:04 INFO 140186635052416] #progress_metric: host=algo-1, completed 49.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:04 INFO 140186635052416] #quality_metric: host=algo-1, epoch=98, train loss <loss>=9.878171400590377\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:04 INFO 140186635052416] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:04 INFO 140186635052416] Epoch[99] Batch[0] avg_epoch_loss=10.502357\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:04 INFO 140186635052416] #quality_metric: host=algo-1, epoch=99, batch=0 train loss <loss>=10.50235652923584\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:05 INFO 140186635052416] Epoch[99] Batch[5] avg_epoch_loss=10.289471\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:05 INFO 140186635052416] #quality_metric: host=algo-1, epoch=99, batch=5 train loss <loss>=10.289470672607422\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:05 INFO 140186635052416] Epoch[99] Batch [5]#011Speed: 2084.98 samples/sec#011loss=10.289471\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:05 INFO 140186635052416] Epoch[99] Batch[10] avg_epoch_loss=9.898569\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:05 INFO 140186635052416] #quality_metric: host=algo-1, epoch=99, batch=10 train loss <loss>=9.42948637008667\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:05 INFO 140186635052416] Epoch[99] Batch [10]#011Speed: 1506.62 samples/sec#011loss=9.429486\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:05 INFO 140186635052416] processed a total of 692 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457604.6647236, \"EndTime\": 1631457605.2725255, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 606.9304943084717, \"count\": 1, \"min\": 606.9304943084717, \"max\": 606.9304943084717}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:05 INFO 140186635052416] #throughput_metric: host=algo-1, train throughput=1139.8523091520162 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:05 INFO 140186635052416] #progress_metric: host=algo-1, completed 50.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:05 INFO 140186635052416] #quality_metric: host=algo-1, epoch=99, train loss <loss>=9.898568716916172\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:05 INFO 140186635052416] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:05 INFO 140186635052416] Loading parameters from best epoch (59)\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457605.2726429, \"EndTime\": 1631457605.2805138, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.deserialize.time\": {\"sum\": 7.199764251708984, \"count\": 1, \"min\": 7.199764251708984, \"max\": 7.199764251708984}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:05 INFO 140186635052416] stopping training now\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:05 INFO 140186635052416] #progress_metric: host=algo-1, completed 100 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:05 INFO 140186635052416] Final loss: 9.260170676491477 (occurred at epoch 59)\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:05 INFO 140186635052416] #quality_metric: host=algo-1, train final_loss <loss>=9.260170676491477\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:05 INFO 140186635052416] Worker algo-1 finished training.\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:05 WARNING 140186635052416] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:05 INFO 140186635052416] All workers finished. Serializing model for prediction.\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457605.2805796, \"EndTime\": 1631457605.4409487, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"get_graph.time\": {\"sum\": 159.58428382873535, \"count\": 1, \"min\": 159.58428382873535, \"max\": 159.58428382873535}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:05 INFO 140186635052416] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457605.441074, \"EndTime\": 1631457605.4787734, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"finalize.time\": {\"sum\": 197.45659828186035, \"count\": 1, \"min\": 197.45659828186035, \"max\": 197.45659828186035}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:05 INFO 140186635052416] Serializing to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:05 INFO 140186635052416] Saved checkpoint to \"/opt/ml/model/model_algo-1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457605.4788456, \"EndTime\": 1631457605.4847977, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"model.serialize.time\": {\"sum\": 5.902290344238281, \"count\": 1, \"min\": 5.902290344238281, \"max\": 5.902290344238281}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:05 INFO 140186635052416] Successfully serialized the model for prediction.\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:40:05 INFO 140186635052416] No test data passed, skipping evaluation.\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457605.4848542, \"EndTime\": 1631457605.4919045, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"setuptime\": {\"sum\": 6.66356086730957, \"count\": 1, \"min\": 6.66356086730957, \"max\": 6.66356086730957}, \"totaltime\": {\"sum\": 59290.037870407104, \"count\": 1, \"min\": 59290.037870407104, \"max\": 59290.037870407104}}}\n",
      "\u001b[0m\n",
      "\n",
      "2021-09-12 14:40:20 Uploading - Uploading generated training model\n",
      "2021-09-12 14:40:20 Completed - Training job completed\n",
      "Training seconds: 148\n",
      "Billable seconds: 148\n",
      "------!Overwriting existing file\n",
      "Uploading file to s3://sagemaker-us-east-2-222195595958/deepar-covid-demo-notebook/data/train/train.json\n",
      "2021-09-12 14:43:42 Starting - Starting the training job...\n",
      "2021-09-12 14:44:06 Starting - Launching requested ML instancesProfilerReport-1631457822: InProgress\n",
      "...\n",
      "2021-09-12 14:44:33 Starting - Preparing the instances for training.........\n",
      "2021-09-12 14:46:06 Downloading - Downloading input data\n",
      "2021-09-12 14:46:06 Training - Downloading the training image...\n",
      "2021-09-12 14:46:31 Training - Training image download completed. Training in progress.\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:35 INFO 139753018754432] Reading default configuration from /opt/amazon/lib/python3.6/site-packages/algorithm/resources/default-input.json: {'_kvstore': 'auto', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_tuning_objective_metric': '', 'cardinality': 'auto', 'dropout_rate': '0.10', 'early_stopping_patience': '', 'embedding_dimension': '10', 'learning_rate': '0.001', 'likelihood': 'student-t', 'mini_batch_size': '128', 'num_cells': '40', 'num_dynamic_feat': 'auto', 'num_eval_samples': '100', 'num_layers': '2', 'test_quantiles': '[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]'}\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:35 INFO 139753018754432] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {'prediction_length': '14', 'time_freq': 'D', 'context_length': '14', 'epochs': '200', 'learning_rate': '5E-4', 'early_stopping_patience': '40', 'mini_batch_size': '64'}\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:35 INFO 139753018754432] Final configuration: {'_kvstore': 'auto', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_tuning_objective_metric': '', 'cardinality': 'auto', 'dropout_rate': '0.10', 'early_stopping_patience': '40', 'embedding_dimension': '10', 'learning_rate': '5E-4', 'likelihood': 'student-t', 'mini_batch_size': '64', 'num_cells': '40', 'num_dynamic_feat': 'auto', 'num_eval_samples': '100', 'num_layers': '2', 'test_quantiles': '[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', 'prediction_length': '14', 'time_freq': 'D', 'context_length': '14', 'epochs': '200'}\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:35 INFO 139753018754432] Detected entry point for worker worker\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:35 INFO 139753018754432] Using early stopping with patience 40\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:35 INFO 139753018754432] random_seed is None\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:35 INFO 139753018754432] [cardinality=auto] `cat` field was NOT found in the file `/opt/ml/input/data/train/train.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:35 INFO 139753018754432] [num_dynamic_feat=auto] `dynamic_feat` field was NOT found in the file `/opt/ml/input/data/train/train.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:35 INFO 139753018754432] Training set statistics:\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:35 INFO 139753018754432] Integer time series\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:35 INFO 139753018754432] number of time series: 41\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:35 INFO 139753018754432] number of observations: 13491\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:35 INFO 139753018754432] mean target length: 329.0487804878049\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:35 INFO 139753018754432] min/mean/max target: 0.0/15102317.60195686/348102464.0\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:35 INFO 139753018754432] mean abs(target): 15102317.60195686\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:35 INFO 139753018754432] contains missing values: no\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:35 INFO 139753018754432] Small number of time series. Doing 16 passes over dataset with prob 0.975609756097561 per epoch.\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:35 INFO 139753018754432] No test channel found not running evaluations\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:35 INFO 139753018754432] #memory_usage::<batchbuffer> = 2.2568130493164062 mb\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:35 INFO 139753018754432] nvidia-smi took: 0.025330543518066406 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:35 INFO 139753018754432] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:35 INFO 139753018754432] Create Store: local\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457995.768211, \"EndTime\": 1631457995.8523705, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"get_graph.time\": {\"sum\": 82.01098442077637, \"count\": 1, \"min\": 82.01098442077637, \"max\": 82.01098442077637}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:35 INFO 139753018754432] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:35 INFO 139753018754432] #memory_usage::<model> = 12 mb\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457995.852448, \"EndTime\": 1631457995.9580648, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"initialize.time\": {\"sum\": 189.7413730621338, \"count\": 1, \"min\": 189.7413730621338, \"max\": 189.7413730621338}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:36 INFO 139753018754432] Epoch[0] Batch[0] avg_epoch_loss=15.572684\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:36 INFO 139753018754432] #quality_metric: host=algo-1, epoch=0, batch=0 train loss <loss>=15.572684288024902\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:36 INFO 139753018754432] Epoch[0] Batch[5] avg_epoch_loss=14.004516\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:36 INFO 139753018754432] #quality_metric: host=algo-1, epoch=0, batch=5 train loss <loss>=14.004515806833902\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:36 INFO 139753018754432] Epoch[0] Batch [5]#011Speed: 1603.36 samples/sec#011loss=14.004516\u001b[0m\n",
      "\u001b[34m/opt/amazon/python3.6/lib/python3.6/contextlib.py:99: DeprecationWarning: generator 'local_timer' raised StopIteration\n",
      "  self.gen.throw(type, value, traceback)\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:36 INFO 139753018754432] processed a total of 591 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457995.9581351, \"EndTime\": 1631457996.6101592, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"epochs\": {\"sum\": 200.0, \"count\": 1, \"min\": 200, \"max\": 200}, \"update.time\": {\"sum\": 651.9255638122559, \"count\": 1, \"min\": 651.9255638122559, \"max\": 651.9255638122559}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:36 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=906.3433535053051 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:36 INFO 139753018754432] #progress_metric: host=algo-1, completed 0.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:36 INFO 139753018754432] #quality_metric: host=algo-1, epoch=0, train loss <loss>=13.332122611999512\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:36 INFO 139753018754432] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:36 INFO 139753018754432] Saved checkpoint to \"/opt/ml/model/state_5618dac7-e82f-465d-8045-5b4f25a0dac5-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457996.6102612, \"EndTime\": 1631457996.630654, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 19.823551177978516, \"count\": 1, \"min\": 19.823551177978516, \"max\": 19.823551177978516}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:36 INFO 139753018754432] Epoch[1] Batch[0] avg_epoch_loss=12.486897\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:36 INFO 139753018754432] #quality_metric: host=algo-1, epoch=1, batch=0 train loss <loss>=12.486897468566895\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:37 INFO 139753018754432] Epoch[1] Batch[5] avg_epoch_loss=12.662752\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:37 INFO 139753018754432] #quality_metric: host=algo-1, epoch=1, batch=5 train loss <loss>=12.66275151570638\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:37 INFO 139753018754432] Epoch[1] Batch [5]#011Speed: 1946.25 samples/sec#011loss=12.662752\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:37 INFO 139753018754432] processed a total of 619 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457996.630734, \"EndTime\": 1631457997.2315953, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 600.7959842681885, \"count\": 1, \"min\": 600.7959842681885, \"max\": 600.7959842681885}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:37 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=1030.0635612233928 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:37 INFO 139753018754432] #progress_metric: host=algo-1, completed 1.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:37 INFO 139753018754432] #quality_metric: host=algo-1, epoch=1, train loss <loss>=12.34785327911377\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:37 INFO 139753018754432] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:37 INFO 139753018754432] Saved checkpoint to \"/opt/ml/model/state_0d647b96-264f-4641-b42d-9bbc02d1c75c-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457997.2316918, \"EndTime\": 1631457997.252578, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 20.256996154785156, \"count\": 1, \"min\": 20.256996154785156, \"max\": 20.256996154785156}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:37 INFO 139753018754432] Epoch[2] Batch[0] avg_epoch_loss=12.328909\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:37 INFO 139753018754432] #quality_metric: host=algo-1, epoch=2, batch=0 train loss <loss>=12.328908920288086\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:37 INFO 139753018754432] Epoch[2] Batch[5] avg_epoch_loss=12.159290\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:37 INFO 139753018754432] #quality_metric: host=algo-1, epoch=2, batch=5 train loss <loss>=12.159289836883545\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:37 INFO 139753018754432] Epoch[2] Batch [5]#011Speed: 1950.89 samples/sec#011loss=12.159290\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:37 INFO 139753018754432] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457997.2526913, \"EndTime\": 1631457997.835157, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 582.3929309844971, \"count\": 1, \"min\": 582.3929309844971, \"max\": 582.3929309844971}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:37 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=1088.3897787848227 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:37 INFO 139753018754432] #progress_metric: host=algo-1, completed 1.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:37 INFO 139753018754432] #quality_metric: host=algo-1, epoch=2, train loss <loss>=12.188819885253906\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:37 INFO 139753018754432] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:37 INFO 139753018754432] Saved checkpoint to \"/opt/ml/model/state_788b90d2-c054-4e77-adec-35082ea4c700-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457997.8352323, \"EndTime\": 1631457997.8488905, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 13.077735900878906, \"count\": 1, \"min\": 13.077735900878906, \"max\": 13.077735900878906}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:38 INFO 139753018754432] Epoch[3] Batch[0] avg_epoch_loss=11.993475\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:38 INFO 139753018754432] #quality_metric: host=algo-1, epoch=3, batch=0 train loss <loss>=11.993474960327148\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:38 INFO 139753018754432] Epoch[3] Batch[5] avg_epoch_loss=12.275134\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:38 INFO 139753018754432] #quality_metric: host=algo-1, epoch=3, batch=5 train loss <loss>=12.275134245554606\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:38 INFO 139753018754432] Epoch[3] Batch [5]#011Speed: 1721.86 samples/sec#011loss=12.275134\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:38 INFO 139753018754432] Epoch[3] Batch[10] avg_epoch_loss=11.548662\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:38 INFO 139753018754432] #quality_metric: host=algo-1, epoch=3, batch=10 train loss <loss>=10.676894664764404\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:38 INFO 139753018754432] Epoch[3] Batch [10]#011Speed: 1753.30 samples/sec#011loss=10.676895\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:38 INFO 139753018754432] processed a total of 653 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457997.8489597, \"EndTime\": 1631457998.4990253, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 650.0036716461182, \"count\": 1, \"min\": 650.0036716461182, \"max\": 650.0036716461182}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:38 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=1004.4347092623262 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:38 INFO 139753018754432] #progress_metric: host=algo-1, completed 2.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:38 INFO 139753018754432] #quality_metric: host=algo-1, epoch=3, train loss <loss>=11.548661708831787\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:38 INFO 139753018754432] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:38 INFO 139753018754432] Saved checkpoint to \"/opt/ml/model/state_edc9c51f-e81c-42f0-a341-52e29663b2f4-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457998.4991024, \"EndTime\": 1631457998.512879, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 13.167142868041992, \"count\": 1, \"min\": 13.167142868041992, \"max\": 13.167142868041992}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:38 INFO 139753018754432] Epoch[4] Batch[0] avg_epoch_loss=12.236448\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:38 INFO 139753018754432] #quality_metric: host=algo-1, epoch=4, batch=0 train loss <loss>=12.236448287963867\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:38 INFO 139753018754432] Epoch[4] Batch[5] avg_epoch_loss=12.210464\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:38 INFO 139753018754432] #quality_metric: host=algo-1, epoch=4, batch=5 train loss <loss>=12.210464318593344\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:38 INFO 139753018754432] Epoch[4] Batch [5]#011Speed: 1682.01 samples/sec#011loss=12.210464\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:39 INFO 139753018754432] processed a total of 601 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457998.5129538, \"EndTime\": 1631457999.1019256, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 588.9120101928711, \"count\": 1, \"min\": 588.9120101928711, \"max\": 588.9120101928711}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:39 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=1020.3338880026682 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:39 INFO 139753018754432] #progress_metric: host=algo-1, completed 2.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:39 INFO 139753018754432] #quality_metric: host=algo-1, epoch=4, train loss <loss>=12.093375492095948\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:39 INFO 139753018754432] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:39 INFO 139753018754432] Epoch[5] Batch[0] avg_epoch_loss=11.436016\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:39 INFO 139753018754432] #quality_metric: host=algo-1, epoch=5, batch=0 train loss <loss>=11.436016082763672\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:39 INFO 139753018754432] Epoch[5] Batch[5] avg_epoch_loss=10.861064\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:39 INFO 139753018754432] #quality_metric: host=algo-1, epoch=5, batch=5 train loss <loss>=10.861063957214355\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:39 INFO 139753018754432] Epoch[5] Batch [5]#011Speed: 1658.59 samples/sec#011loss=10.861064\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:39 INFO 139753018754432] Epoch[5] Batch[10] avg_epoch_loss=11.040294\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:39 INFO 139753018754432] #quality_metric: host=algo-1, epoch=5, batch=10 train loss <loss>=11.255370330810546\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:39 INFO 139753018754432] Epoch[5] Batch [10]#011Speed: 1464.35 samples/sec#011loss=11.255370\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:39 INFO 139753018754432] processed a total of 704 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457999.1020038, \"EndTime\": 1631457999.7584338, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 655.8492183685303, \"count\": 1, \"min\": 655.8492183685303, \"max\": 655.8492183685303}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:39 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=1073.2102922116392 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:39 INFO 139753018754432] #progress_metric: host=algo-1, completed 3.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:39 INFO 139753018754432] #quality_metric: host=algo-1, epoch=5, train loss <loss>=11.040294127030807\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:39 INFO 139753018754432] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:39 INFO 139753018754432] Saved checkpoint to \"/opt/ml/model/state_5d6ac2f1-5c9f-4a66-86b0-903971b22f95-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457999.7585208, \"EndTime\": 1631457999.778071, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 18.971920013427734, \"count\": 1, \"min\": 18.971920013427734, \"max\": 18.971920013427734}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:40 INFO 139753018754432] Epoch[6] Batch[0] avg_epoch_loss=11.472452\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:40 INFO 139753018754432] #quality_metric: host=algo-1, epoch=6, batch=0 train loss <loss>=11.472452163696289\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:40 INFO 139753018754432] Epoch[6] Batch[5] avg_epoch_loss=10.694576\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:40 INFO 139753018754432] #quality_metric: host=algo-1, epoch=6, batch=5 train loss <loss>=10.694576422373453\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:40 INFO 139753018754432] Epoch[6] Batch [5]#011Speed: 2089.13 samples/sec#011loss=10.694576\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:40 INFO 139753018754432] processed a total of 605 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631457999.77815, \"EndTime\": 1631458000.366889, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 588.6640548706055, \"count\": 1, \"min\": 588.6640548706055, \"max\": 588.6640548706055}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:40 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=1027.5090914907048 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:40 INFO 139753018754432] #progress_metric: host=algo-1, completed 3.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:40 INFO 139753018754432] #quality_metric: host=algo-1, epoch=6, train loss <loss>=11.083878135681152\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:40 INFO 139753018754432] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:40 INFO 139753018754432] Epoch[7] Batch[0] avg_epoch_loss=10.799895\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:40 INFO 139753018754432] #quality_metric: host=algo-1, epoch=7, batch=0 train loss <loss>=10.799895286560059\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:40 INFO 139753018754432] Epoch[7] Batch[5] avg_epoch_loss=10.790385\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:40 INFO 139753018754432] #quality_metric: host=algo-1, epoch=7, batch=5 train loss <loss>=10.790384928385416\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:40 INFO 139753018754432] Epoch[7] Batch [5]#011Speed: 1926.63 samples/sec#011loss=10.790385\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:40 INFO 139753018754432] Epoch[7] Batch[10] avg_epoch_loss=10.674015\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:40 INFO 139753018754432] #quality_metric: host=algo-1, epoch=7, batch=10 train loss <loss>=10.534370422363281\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:40 INFO 139753018754432] Epoch[7] Batch [10]#011Speed: 1978.89 samples/sec#011loss=10.534370\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:40 INFO 139753018754432] processed a total of 657 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458000.366978, \"EndTime\": 1631458000.9673285, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 599.8189449310303, \"count\": 1, \"min\": 599.8189449310303, \"max\": 599.8189449310303}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:40 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=1095.1050465735764 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:40 INFO 139753018754432] #progress_metric: host=algo-1, completed 4.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:40 INFO 139753018754432] #quality_metric: host=algo-1, epoch=7, train loss <loss>=10.674014698375355\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:40 INFO 139753018754432] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:40 INFO 139753018754432] Saved checkpoint to \"/opt/ml/model/state_fb0964db-0b23-499e-85a3-d5bf8bdbbd6b-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458000.9674132, \"EndTime\": 1631458000.9809062, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 12.938261032104492, \"count\": 1, \"min\": 12.938261032104492, \"max\": 12.938261032104492}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:41 INFO 139753018754432] Epoch[8] Batch[0] avg_epoch_loss=11.697097\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:41 INFO 139753018754432] #quality_metric: host=algo-1, epoch=8, batch=0 train loss <loss>=11.697096824645996\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:41 INFO 139753018754432] Epoch[8] Batch[5] avg_epoch_loss=11.461912\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:41 INFO 139753018754432] #quality_metric: host=algo-1, epoch=8, batch=5 train loss <loss>=11.46191151936849\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:41 INFO 139753018754432] Epoch[8] Batch [5]#011Speed: 2086.73 samples/sec#011loss=11.461912\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:41 INFO 139753018754432] Epoch[8] Batch[10] avg_epoch_loss=11.656944\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:41 INFO 139753018754432] #quality_metric: host=algo-1, epoch=8, batch=10 train loss <loss>=11.890982627868652\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:41 INFO 139753018754432] Epoch[8] Batch [10]#011Speed: 1923.09 samples/sec#011loss=11.890983\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:41 INFO 139753018754432] processed a total of 646 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458000.98098, \"EndTime\": 1631458001.544629, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 563.5864734649658, \"count\": 1, \"min\": 563.5864734649658, \"max\": 563.5864734649658}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:41 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=1145.9837300717911 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:41 INFO 139753018754432] #progress_metric: host=algo-1, completed 4.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:41 INFO 139753018754432] #quality_metric: host=algo-1, epoch=8, train loss <loss>=11.656943841414018\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:41 INFO 139753018754432] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:41 INFO 139753018754432] Epoch[9] Batch[0] avg_epoch_loss=11.143969\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:41 INFO 139753018754432] #quality_metric: host=algo-1, epoch=9, batch=0 train loss <loss>=11.14396858215332\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:42 INFO 139753018754432] Epoch[9] Batch[5] avg_epoch_loss=10.729166\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:42 INFO 139753018754432] #quality_metric: host=algo-1, epoch=9, batch=5 train loss <loss>=10.729166030883789\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:42 INFO 139753018754432] Epoch[9] Batch [5]#011Speed: 2090.91 samples/sec#011loss=10.729166\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:42 INFO 139753018754432] Epoch[9] Batch[10] avg_epoch_loss=10.691953\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:42 INFO 139753018754432] #quality_metric: host=algo-1, epoch=9, batch=10 train loss <loss>=10.647296905517578\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:42 INFO 139753018754432] Epoch[9] Batch [10]#011Speed: 1794.54 samples/sec#011loss=10.647297\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:42 INFO 139753018754432] processed a total of 674 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458001.5447145, \"EndTime\": 1631458002.179269, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 634.0417861938477, \"count\": 1, \"min\": 634.0417861938477, \"max\": 634.0417861938477}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:42 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=1062.8231921980855 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:42 INFO 139753018754432] #progress_metric: host=algo-1, completed 5.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:42 INFO 139753018754432] #quality_metric: host=algo-1, epoch=9, train loss <loss>=10.691952792080967\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:42 INFO 139753018754432] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:42 INFO 139753018754432] Epoch[10] Batch[0] avg_epoch_loss=10.366639\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:42 INFO 139753018754432] #quality_metric: host=algo-1, epoch=10, batch=0 train loss <loss>=10.366639137268066\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:42 INFO 139753018754432] Epoch[10] Batch[5] avg_epoch_loss=10.861407\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:42 INFO 139753018754432] #quality_metric: host=algo-1, epoch=10, batch=5 train loss <loss>=10.861407121022543\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:42 INFO 139753018754432] Epoch[10] Batch [5]#011Speed: 2151.92 samples/sec#011loss=10.861407\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:42 INFO 139753018754432] Epoch[10] Batch[10] avg_epoch_loss=10.829758\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:42 INFO 139753018754432] #quality_metric: host=algo-1, epoch=10, batch=10 train loss <loss>=10.791778373718262\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:42 INFO 139753018754432] Epoch[10] Batch [10]#011Speed: 1901.27 samples/sec#011loss=10.791778\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:42 INFO 139753018754432] processed a total of 660 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458002.179351, \"EndTime\": 1631458002.7993255, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 619.488000869751, \"count\": 1, \"min\": 619.488000869751, \"max\": 619.488000869751}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:42 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=1065.1876826062814 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:42 INFO 139753018754432] #progress_metric: host=algo-1, completed 5.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:42 INFO 139753018754432] #quality_metric: host=algo-1, epoch=10, train loss <loss>=10.829757690429688\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:42 INFO 139753018754432] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:43 INFO 139753018754432] Epoch[11] Batch[0] avg_epoch_loss=10.782565\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:43 INFO 139753018754432] #quality_metric: host=algo-1, epoch=11, batch=0 train loss <loss>=10.782565116882324\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:43 INFO 139753018754432] Epoch[11] Batch[5] avg_epoch_loss=10.928641\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:43 INFO 139753018754432] #quality_metric: host=algo-1, epoch=11, batch=5 train loss <loss>=10.928641160329184\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:43 INFO 139753018754432] Epoch[11] Batch [5]#011Speed: 2146.94 samples/sec#011loss=10.928641\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:43 INFO 139753018754432] Epoch[11] Batch[10] avg_epoch_loss=10.875670\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:43 INFO 139753018754432] #quality_metric: host=algo-1, epoch=11, batch=10 train loss <loss>=10.812104797363281\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:43 INFO 139753018754432] Epoch[11] Batch [10]#011Speed: 1872.78 samples/sec#011loss=10.812105\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:43 INFO 139753018754432] processed a total of 671 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458002.7994096, \"EndTime\": 1631458003.3771331, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 577.1474838256836, \"count\": 1, \"min\": 577.1474838256836, \"max\": 577.1474838256836}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:43 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=1162.3201088651142 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:43 INFO 139753018754432] #progress_metric: host=algo-1, completed 6.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:43 INFO 139753018754432] #quality_metric: host=algo-1, epoch=11, train loss <loss>=10.875670086253773\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:43 INFO 139753018754432] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:43 INFO 139753018754432] Epoch[12] Batch[0] avg_epoch_loss=10.667320\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:43 INFO 139753018754432] #quality_metric: host=algo-1, epoch=12, batch=0 train loss <loss>=10.667320251464844\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:43 INFO 139753018754432] Epoch[12] Batch[5] avg_epoch_loss=10.419816\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:43 INFO 139753018754432] #quality_metric: host=algo-1, epoch=12, batch=5 train loss <loss>=10.419816017150879\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:43 INFO 139753018754432] Epoch[12] Batch [5]#011Speed: 1544.56 samples/sec#011loss=10.419816\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:44 INFO 139753018754432] Epoch[12] Batch[10] avg_epoch_loss=10.681675\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:44 INFO 139753018754432] #quality_metric: host=algo-1, epoch=12, batch=10 train loss <loss>=10.995905303955078\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:44 INFO 139753018754432] Epoch[12] Batch [10]#011Speed: 1408.29 samples/sec#011loss=10.995905\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:44 INFO 139753018754432] processed a total of 671 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458003.37722, \"EndTime\": 1631458004.109194, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 731.4713001251221, \"count\": 1, \"min\": 731.4713001251221, \"max\": 731.4713001251221}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:44 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=917.1713801445052 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:44 INFO 139753018754432] #progress_metric: host=algo-1, completed 6.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:44 INFO 139753018754432] #quality_metric: host=algo-1, epoch=12, train loss <loss>=10.68167478388006\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:44 INFO 139753018754432] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:44 INFO 139753018754432] Epoch[13] Batch[0] avg_epoch_loss=11.326530\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:44 INFO 139753018754432] #quality_metric: host=algo-1, epoch=13, batch=0 train loss <loss>=11.326530456542969\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:44 INFO 139753018754432] Epoch[13] Batch[5] avg_epoch_loss=10.950191\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:44 INFO 139753018754432] #quality_metric: host=algo-1, epoch=13, batch=5 train loss <loss>=10.950191338857016\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:44 INFO 139753018754432] Epoch[13] Batch [5]#011Speed: 2094.50 samples/sec#011loss=10.950191\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:44 INFO 139753018754432] processed a total of 639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458004.1092813, \"EndTime\": 1631458004.6479867, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 538.2156372070312, \"count\": 1, \"min\": 538.2156372070312, \"max\": 538.2156372070312}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:44 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=1186.946256692781 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:44 INFO 139753018754432] #progress_metric: host=algo-1, completed 7.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:44 INFO 139753018754432] #quality_metric: host=algo-1, epoch=13, train loss <loss>=10.781118011474609\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:44 INFO 139753018754432] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:44 INFO 139753018754432] Epoch[14] Batch[0] avg_epoch_loss=10.621419\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:44 INFO 139753018754432] #quality_metric: host=algo-1, epoch=14, batch=0 train loss <loss>=10.621418952941895\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:45 INFO 139753018754432] Epoch[14] Batch[5] avg_epoch_loss=10.814337\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:45 INFO 139753018754432] #quality_metric: host=algo-1, epoch=14, batch=5 train loss <loss>=10.814337253570557\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:45 INFO 139753018754432] Epoch[14] Batch [5]#011Speed: 2012.41 samples/sec#011loss=10.814337\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:45 INFO 139753018754432] processed a total of 612 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458004.64809, \"EndTime\": 1631458005.1939068, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 545.2942848205566, \"count\": 1, \"min\": 545.2942848205566, \"max\": 545.2942848205566}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:45 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=1122.071696094168 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:45 INFO 139753018754432] #progress_metric: host=algo-1, completed 7.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:45 INFO 139753018754432] #quality_metric: host=algo-1, epoch=14, train loss <loss>=11.014495944976806\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:45 INFO 139753018754432] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:45 INFO 139753018754432] Epoch[15] Batch[0] avg_epoch_loss=9.993649\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:45 INFO 139753018754432] #quality_metric: host=algo-1, epoch=15, batch=0 train loss <loss>=9.993648529052734\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:45 INFO 139753018754432] Epoch[15] Batch[5] avg_epoch_loss=10.375607\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:45 INFO 139753018754432] #quality_metric: host=algo-1, epoch=15, batch=5 train loss <loss>=10.375607013702393\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:45 INFO 139753018754432] Epoch[15] Batch [5]#011Speed: 2019.43 samples/sec#011loss=10.375607\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:45 INFO 139753018754432] processed a total of 620 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458005.1939936, \"EndTime\": 1631458005.7462814, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 551.7864227294922, \"count\": 1, \"min\": 551.7864227294922, \"max\": 551.7864227294922}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:45 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=1123.3557056837174 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:45 INFO 139753018754432] #progress_metric: host=algo-1, completed 8.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:45 INFO 139753018754432] #quality_metric: host=algo-1, epoch=15, train loss <loss>=10.191892433166505\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:45 INFO 139753018754432] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:45 INFO 139753018754432] Saved checkpoint to \"/opt/ml/model/state_087fe0a3-f01d-45b8-ae72-d06ffa5743da-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458005.746373, \"EndTime\": 1631458005.7662218, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 19.272327423095703, \"count\": 1, \"min\": 19.272327423095703, \"max\": 19.272327423095703}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:46 INFO 139753018754432] Epoch[16] Batch[0] avg_epoch_loss=10.733161\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:46 INFO 139753018754432] #quality_metric: host=algo-1, epoch=16, batch=0 train loss <loss>=10.733160972595215\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:46 INFO 139753018754432] Epoch[16] Batch[5] avg_epoch_loss=10.167253\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:46 INFO 139753018754432] #quality_metric: host=algo-1, epoch=16, batch=5 train loss <loss>=10.167252699534098\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:46 INFO 139753018754432] Epoch[16] Batch [5]#011Speed: 2089.15 samples/sec#011loss=10.167253\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:46 INFO 139753018754432] processed a total of 625 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458005.7662945, \"EndTime\": 1631458006.3302708, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 563.9159679412842, \"count\": 1, \"min\": 563.9159679412842, \"max\": 563.9159679412842}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:46 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=1108.074737896178 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:46 INFO 139753018754432] #progress_metric: host=algo-1, completed 8.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:46 INFO 139753018754432] #quality_metric: host=algo-1, epoch=16, train loss <loss>=10.19803590774536\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:46 INFO 139753018754432] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:46 INFO 139753018754432] Epoch[17] Batch[0] avg_epoch_loss=11.348214\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:46 INFO 139753018754432] #quality_metric: host=algo-1, epoch=17, batch=0 train loss <loss>=11.348214149475098\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:46 INFO 139753018754432] Epoch[17] Batch[5] avg_epoch_loss=10.583747\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:46 INFO 139753018754432] #quality_metric: host=algo-1, epoch=17, batch=5 train loss <loss>=10.583746592203775\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:46 INFO 139753018754432] Epoch[17] Batch [5]#011Speed: 2028.24 samples/sec#011loss=10.583747\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:46 INFO 139753018754432] processed a total of 630 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458006.3303578, \"EndTime\": 1631458006.897821, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 566.9574737548828, \"count\": 1, \"min\": 566.9574737548828, \"max\": 566.9574737548828}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:46 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=1110.9641305284667 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:46 INFO 139753018754432] #progress_metric: host=algo-1, completed 9.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:46 INFO 139753018754432] #quality_metric: host=algo-1, epoch=17, train loss <loss>=10.557785606384277\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:46 INFO 139753018754432] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:47 INFO 139753018754432] Epoch[18] Batch[0] avg_epoch_loss=10.332242\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:47 INFO 139753018754432] #quality_metric: host=algo-1, epoch=18, batch=0 train loss <loss>=10.332242012023926\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:47 INFO 139753018754432] Epoch[18] Batch[5] avg_epoch_loss=10.548591\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:47 INFO 139753018754432] #quality_metric: host=algo-1, epoch=18, batch=5 train loss <loss>=10.548590819040934\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:47 INFO 139753018754432] Epoch[18] Batch [5]#011Speed: 2095.25 samples/sec#011loss=10.548591\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:47 INFO 139753018754432] Epoch[18] Batch[10] avg_epoch_loss=10.395525\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:47 INFO 139753018754432] #quality_metric: host=algo-1, epoch=18, batch=10 train loss <loss>=10.211846160888673\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:47 INFO 139753018754432] Epoch[18] Batch [10]#011Speed: 1674.08 samples/sec#011loss=10.211846\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:47 INFO 139753018754432] processed a total of 657 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458006.897905, \"EndTime\": 1631458007.491986, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 593.5132503509521, \"count\": 1, \"min\": 593.5132503509521, \"max\": 593.5132503509521}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:47 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=1106.713407152843 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:47 INFO 139753018754432] #progress_metric: host=algo-1, completed 9.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:47 INFO 139753018754432] #quality_metric: host=algo-1, epoch=18, train loss <loss>=10.395525065335361\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:47 INFO 139753018754432] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:47 INFO 139753018754432] Epoch[19] Batch[0] avg_epoch_loss=9.264392\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:47 INFO 139753018754432] #quality_metric: host=algo-1, epoch=19, batch=0 train loss <loss>=9.264391899108887\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:47 INFO 139753018754432] Epoch[19] Batch[5] avg_epoch_loss=10.297366\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:47 INFO 139753018754432] #quality_metric: host=algo-1, epoch=19, batch=5 train loss <loss>=10.297366301218668\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:47 INFO 139753018754432] Epoch[19] Batch [5]#011Speed: 1696.29 samples/sec#011loss=10.297366\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:48 INFO 139753018754432] processed a total of 621 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458007.492079, \"EndTime\": 1631458008.0620067, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 569.4189071655273, \"count\": 1, \"min\": 569.4189071655273, \"max\": 569.4189071655273}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:48 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=1090.3316644843258 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:48 INFO 139753018754432] #progress_metric: host=algo-1, completed 10.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:48 INFO 139753018754432] #quality_metric: host=algo-1, epoch=19, train loss <loss>=10.298197937011718\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:48 INFO 139753018754432] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:48 INFO 139753018754432] Epoch[20] Batch[0] avg_epoch_loss=10.933792\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:48 INFO 139753018754432] #quality_metric: host=algo-1, epoch=20, batch=0 train loss <loss>=10.933792114257812\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:48 INFO 139753018754432] Epoch[20] Batch[5] avg_epoch_loss=10.387551\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:48 INFO 139753018754432] #quality_metric: host=algo-1, epoch=20, batch=5 train loss <loss>=10.387550989786783\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:48 INFO 139753018754432] Epoch[20] Batch [5]#011Speed: 1886.25 samples/sec#011loss=10.387551\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:48 INFO 139753018754432] processed a total of 640 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458008.0620973, \"EndTime\": 1631458008.6196759, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 557.0704936981201, \"count\": 1, \"min\": 557.0704936981201, \"max\": 557.0704936981201}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:48 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=1148.6061340473948 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:48 INFO 139753018754432] #progress_metric: host=algo-1, completed 10.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:48 INFO 139753018754432] #quality_metric: host=algo-1, epoch=20, train loss <loss>=10.410653781890868\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:48 INFO 139753018754432] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:48 INFO 139753018754432] Epoch[21] Batch[0] avg_epoch_loss=9.760542\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:48 INFO 139753018754432] #quality_metric: host=algo-1, epoch=21, batch=0 train loss <loss>=9.760541915893555\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:49 INFO 139753018754432] Epoch[21] Batch[5] avg_epoch_loss=10.217813\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:49 INFO 139753018754432] #quality_metric: host=algo-1, epoch=21, batch=5 train loss <loss>=10.217813491821289\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:49 INFO 139753018754432] Epoch[21] Batch [5]#011Speed: 1991.29 samples/sec#011loss=10.217813\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:49 INFO 139753018754432] Epoch[21] Batch[10] avg_epoch_loss=9.535741\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:49 INFO 139753018754432] #quality_metric: host=algo-1, epoch=21, batch=10 train loss <loss>=8.717254972457885\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:49 INFO 139753018754432] Epoch[21] Batch [10]#011Speed: 2037.62 samples/sec#011loss=8.717255\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:49 INFO 139753018754432] processed a total of 648 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458008.6197631, \"EndTime\": 1631458009.2178833, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 597.5961685180664, \"count\": 1, \"min\": 597.5961685180664, \"max\": 597.5961685180664}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:49 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=1084.1215517258572 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:49 INFO 139753018754432] #progress_metric: host=algo-1, completed 11.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:49 INFO 139753018754432] #quality_metric: host=algo-1, epoch=21, train loss <loss>=9.535741437565196\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:49 INFO 139753018754432] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:49 INFO 139753018754432] Saved checkpoint to \"/opt/ml/model/state_7b800ce2-ce37-4a06-8adb-21ca8c21d4d3-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458009.2179685, \"EndTime\": 1631458009.2370245, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 18.543481826782227, \"count\": 1, \"min\": 18.543481826782227, \"max\": 18.543481826782227}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:49 INFO 139753018754432] Epoch[22] Batch[0] avg_epoch_loss=10.005483\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:49 INFO 139753018754432] #quality_metric: host=algo-1, epoch=22, batch=0 train loss <loss>=10.00548267364502\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:49 INFO 139753018754432] Epoch[22] Batch[5] avg_epoch_loss=9.725410\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:49 INFO 139753018754432] #quality_metric: host=algo-1, epoch=22, batch=5 train loss <loss>=9.725409825642904\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:49 INFO 139753018754432] Epoch[22] Batch [5]#011Speed: 2055.29 samples/sec#011loss=9.725410\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:49 INFO 139753018754432] processed a total of 617 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458009.2370944, \"EndTime\": 1631458009.842591, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 605.4189205169678, \"count\": 1, \"min\": 605.4189205169678, \"max\": 605.4189205169678}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:49 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=1018.9107519917476 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:49 INFO 139753018754432] #progress_metric: host=algo-1, completed 11.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:49 INFO 139753018754432] #quality_metric: host=algo-1, epoch=22, train loss <loss>=9.807731533050537\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:49 INFO 139753018754432] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:50 INFO 139753018754432] Epoch[23] Batch[0] avg_epoch_loss=10.254610\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:50 INFO 139753018754432] #quality_metric: host=algo-1, epoch=23, batch=0 train loss <loss>=10.254610061645508\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:50 INFO 139753018754432] Epoch[23] Batch[5] avg_epoch_loss=10.106423\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:50 INFO 139753018754432] #quality_metric: host=algo-1, epoch=23, batch=5 train loss <loss>=10.106422742207846\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:50 INFO 139753018754432] Epoch[23] Batch [5]#011Speed: 1984.59 samples/sec#011loss=10.106423\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:50 INFO 139753018754432] processed a total of 625 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458009.8426833, \"EndTime\": 1631458010.4149473, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 571.7337131500244, \"count\": 1, \"min\": 571.7337131500244, \"max\": 571.7337131500244}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:50 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=1092.9110287940598 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:50 INFO 139753018754432] #progress_metric: host=algo-1, completed 12.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:50 INFO 139753018754432] #quality_metric: host=algo-1, epoch=23, train loss <loss>=10.201108264923096\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:50 INFO 139753018754432] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:50 INFO 139753018754432] Epoch[24] Batch[0] avg_epoch_loss=9.626418\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:50 INFO 139753018754432] #quality_metric: host=algo-1, epoch=24, batch=0 train loss <loss>=9.626418113708496\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:50 INFO 139753018754432] Epoch[24] Batch[5] avg_epoch_loss=9.899850\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:50 INFO 139753018754432] #quality_metric: host=algo-1, epoch=24, batch=5 train loss <loss>=9.899850368499756\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:50 INFO 139753018754432] Epoch[24] Batch [5]#011Speed: 2146.94 samples/sec#011loss=9.899850\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:50 INFO 139753018754432] processed a total of 610 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458010.4150407, \"EndTime\": 1631458010.949845, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 534.2049598693848, \"count\": 1, \"min\": 534.2049598693848, \"max\": 534.2049598693848}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:50 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=1141.4350129087723 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:50 INFO 139753018754432] #progress_metric: host=algo-1, completed 12.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:50 INFO 139753018754432] #quality_metric: host=algo-1, epoch=24, train loss <loss>=10.326722526550293\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:50 INFO 139753018754432] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:51 INFO 139753018754432] Epoch[25] Batch[0] avg_epoch_loss=9.807824\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:51 INFO 139753018754432] #quality_metric: host=algo-1, epoch=25, batch=0 train loss <loss>=9.80782413482666\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:51 INFO 139753018754432] Epoch[25] Batch[5] avg_epoch_loss=10.216205\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:51 INFO 139753018754432] #quality_metric: host=algo-1, epoch=25, batch=5 train loss <loss>=10.216205279032389\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:51 INFO 139753018754432] Epoch[25] Batch [5]#011Speed: 2139.50 samples/sec#011loss=10.216205\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:51 INFO 139753018754432] Epoch[25] Batch[10] avg_epoch_loss=10.511804\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:51 INFO 139753018754432] #quality_metric: host=algo-1, epoch=25, batch=10 train loss <loss>=10.866522407531738\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:51 INFO 139753018754432] Epoch[25] Batch [10]#011Speed: 1613.18 samples/sec#011loss=10.866522\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:51 INFO 139753018754432] processed a total of 679 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458010.9499388, \"EndTime\": 1631458011.5990896, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 648.6001014709473, \"count\": 1, \"min\": 648.6001014709473, \"max\": 648.6001014709473}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:51 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=1046.6726435249009 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:51 INFO 139753018754432] #progress_metric: host=algo-1, completed 13.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:51 INFO 139753018754432] #quality_metric: host=algo-1, epoch=25, train loss <loss>=10.51180397380482\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:51 INFO 139753018754432] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:51 INFO 139753018754432] Epoch[26] Batch[0] avg_epoch_loss=10.395542\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:51 INFO 139753018754432] #quality_metric: host=algo-1, epoch=26, batch=0 train loss <loss>=10.39554214477539\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:52 INFO 139753018754432] Epoch[26] Batch[5] avg_epoch_loss=10.175165\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:52 INFO 139753018754432] #quality_metric: host=algo-1, epoch=26, batch=5 train loss <loss>=10.175165176391602\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:52 INFO 139753018754432] Epoch[26] Batch [5]#011Speed: 1640.14 samples/sec#011loss=10.175165\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:52 INFO 139753018754432] Epoch[26] Batch[10] avg_epoch_loss=10.460075\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:52 INFO 139753018754432] #quality_metric: host=algo-1, epoch=26, batch=10 train loss <loss>=10.801965713500977\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:52 INFO 139753018754432] Epoch[26] Batch [10]#011Speed: 1727.70 samples/sec#011loss=10.801966\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:52 INFO 139753018754432] processed a total of 649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458011.5991735, \"EndTime\": 1631458012.236559, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 636.8982791900635, \"count\": 1, \"min\": 636.8982791900635, \"max\": 636.8982791900635}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:52 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=1018.7947000732815 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:52 INFO 139753018754432] #progress_metric: host=algo-1, completed 13.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:52 INFO 139753018754432] #quality_metric: host=algo-1, epoch=26, train loss <loss>=10.460074511441318\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:52 INFO 139753018754432] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:52 INFO 139753018754432] Epoch[27] Batch[0] avg_epoch_loss=10.119578\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:52 INFO 139753018754432] #quality_metric: host=algo-1, epoch=27, batch=0 train loss <loss>=10.11957836151123\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:52 INFO 139753018754432] Epoch[27] Batch[5] avg_epoch_loss=10.120530\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:52 INFO 139753018754432] #quality_metric: host=algo-1, epoch=27, batch=5 train loss <loss>=10.120530287424723\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:52 INFO 139753018754432] Epoch[27] Batch [5]#011Speed: 1894.84 samples/sec#011loss=10.120530\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:52 INFO 139753018754432] Epoch[27] Batch[10] avg_epoch_loss=10.500278\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:52 INFO 139753018754432] #quality_metric: host=algo-1, epoch=27, batch=10 train loss <loss>=10.955974769592284\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:52 INFO 139753018754432] Epoch[27] Batch [10]#011Speed: 1938.57 samples/sec#011loss=10.955975\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:52 INFO 139753018754432] processed a total of 667 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458012.236647, \"EndTime\": 1631458012.825384, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 588.2260799407959, \"count\": 1, \"min\": 588.2260799407959, \"max\": 588.2260799407959}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:52 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=1133.6246145916443 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:52 INFO 139753018754432] #progress_metric: host=algo-1, completed 14.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:52 INFO 139753018754432] #quality_metric: host=algo-1, epoch=27, train loss <loss>=10.50027777931907\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:52 INFO 139753018754432] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:53 INFO 139753018754432] Epoch[28] Batch[0] avg_epoch_loss=9.997248\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:53 INFO 139753018754432] #quality_metric: host=algo-1, epoch=28, batch=0 train loss <loss>=9.997247695922852\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:53 INFO 139753018754432] Epoch[28] Batch[5] avg_epoch_loss=10.128975\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:53 INFO 139753018754432] #quality_metric: host=algo-1, epoch=28, batch=5 train loss <loss>=10.128974914550781\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:53 INFO 139753018754432] Epoch[28] Batch [5]#011Speed: 2067.30 samples/sec#011loss=10.128975\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:53 INFO 139753018754432] processed a total of 624 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458012.8254702, \"EndTime\": 1631458013.3649251, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 538.90061378479, \"count\": 1, \"min\": 538.90061378479, \"max\": 538.90061378479}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:53 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=1157.6200010173006 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:53 INFO 139753018754432] #progress_metric: host=algo-1, completed 14.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:53 INFO 139753018754432] #quality_metric: host=algo-1, epoch=28, train loss <loss>=10.478517246246337\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:53 INFO 139753018754432] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:53 INFO 139753018754432] Epoch[29] Batch[0] avg_epoch_loss=9.828016\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:53 INFO 139753018754432] #quality_metric: host=algo-1, epoch=29, batch=0 train loss <loss>=9.82801628112793\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:53 INFO 139753018754432] Epoch[29] Batch[5] avg_epoch_loss=10.010138\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:53 INFO 139753018754432] #quality_metric: host=algo-1, epoch=29, batch=5 train loss <loss>=10.010137716929117\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:53 INFO 139753018754432] Epoch[29] Batch [5]#011Speed: 2090.28 samples/sec#011loss=10.010138\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:53 INFO 139753018754432] Epoch[29] Batch[10] avg_epoch_loss=10.237568\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:53 INFO 139753018754432] #quality_metric: host=algo-1, epoch=29, batch=10 train loss <loss>=10.510484504699708\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:53 INFO 139753018754432] Epoch[29] Batch [10]#011Speed: 1946.68 samples/sec#011loss=10.510485\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:53 INFO 139753018754432] processed a total of 654 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458013.365018, \"EndTime\": 1631458013.9355164, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 569.9317455291748, \"count\": 1, \"min\": 569.9317455291748, \"max\": 569.9317455291748}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:53 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=1147.2706602419032 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:53 INFO 139753018754432] #progress_metric: host=algo-1, completed 15.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:53 INFO 139753018754432] #quality_metric: host=algo-1, epoch=29, train loss <loss>=10.237568075006658\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:53 INFO 139753018754432] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:54 INFO 139753018754432] Epoch[30] Batch[0] avg_epoch_loss=10.776215\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:54 INFO 139753018754432] #quality_metric: host=algo-1, epoch=30, batch=0 train loss <loss>=10.776214599609375\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:54 INFO 139753018754432] Epoch[30] Batch[5] avg_epoch_loss=10.124580\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:54 INFO 139753018754432] #quality_metric: host=algo-1, epoch=30, batch=5 train loss <loss>=10.124579906463623\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:54 INFO 139753018754432] Epoch[30] Batch [5]#011Speed: 2060.53 samples/sec#011loss=10.124580\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:54 INFO 139753018754432] processed a total of 595 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458013.9355955, \"EndTime\": 1631458014.4528413, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 516.6971683502197, \"count\": 1, \"min\": 516.6971683502197, \"max\": 516.6971683502197}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:54 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=1151.3111723660688 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:54 INFO 139753018754432] #progress_metric: host=algo-1, completed 15.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:54 INFO 139753018754432] #quality_metric: host=algo-1, epoch=30, train loss <loss>=9.783526802062989\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:54 INFO 139753018754432] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:54 INFO 139753018754432] Epoch[31] Batch[0] avg_epoch_loss=11.084323\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:54 INFO 139753018754432] #quality_metric: host=algo-1, epoch=31, batch=0 train loss <loss>=11.084322929382324\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:54 INFO 139753018754432] Epoch[31] Batch[5] avg_epoch_loss=10.447675\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:54 INFO 139753018754432] #quality_metric: host=algo-1, epoch=31, batch=5 train loss <loss>=10.447674751281738\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:54 INFO 139753018754432] Epoch[31] Batch [5]#011Speed: 1993.72 samples/sec#011loss=10.447675\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:54 INFO 139753018754432] processed a total of 613 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458014.4529083, \"EndTime\": 1631458014.9896712, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 536.2262725830078, \"count\": 1, \"min\": 536.2262725830078, \"max\": 536.2262725830078}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:54 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=1142.8946385342406 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:54 INFO 139753018754432] #progress_metric: host=algo-1, completed 16.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:54 INFO 139753018754432] #quality_metric: host=algo-1, epoch=31, train loss <loss>=10.565205764770507\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:54 INFO 139753018754432] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:55 INFO 139753018754432] Epoch[32] Batch[0] avg_epoch_loss=11.002800\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:55 INFO 139753018754432] #quality_metric: host=algo-1, epoch=32, batch=0 train loss <loss>=11.002799987792969\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:55 INFO 139753018754432] Epoch[32] Batch[5] avg_epoch_loss=10.407293\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:55 INFO 139753018754432] #quality_metric: host=algo-1, epoch=32, batch=5 train loss <loss>=10.407293319702148\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:55 INFO 139753018754432] Epoch[32] Batch [5]#011Speed: 2116.30 samples/sec#011loss=10.407293\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:55 INFO 139753018754432] Epoch[32] Batch[10] avg_epoch_loss=10.634537\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:55 INFO 139753018754432] #quality_metric: host=algo-1, epoch=32, batch=10 train loss <loss>=10.907229804992676\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:55 INFO 139753018754432] Epoch[32] Batch [10]#011Speed: 1932.53 samples/sec#011loss=10.907230\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:55 INFO 139753018754432] processed a total of 672 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458014.9897618, \"EndTime\": 1631458015.6155868, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 625.2849102020264, \"count\": 1, \"min\": 625.2849102020264, \"max\": 625.2849102020264}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:55 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=1074.5068728935003 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:55 INFO 139753018754432] #progress_metric: host=algo-1, completed 16.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:55 INFO 139753018754432] #quality_metric: host=algo-1, epoch=32, train loss <loss>=10.634537176652389\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:55 INFO 139753018754432] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:55 INFO 139753018754432] Epoch[33] Batch[0] avg_epoch_loss=9.673915\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:55 INFO 139753018754432] #quality_metric: host=algo-1, epoch=33, batch=0 train loss <loss>=9.673914909362793\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:56 INFO 139753018754432] Epoch[33] Batch[5] avg_epoch_loss=10.440133\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:56 INFO 139753018754432] #quality_metric: host=algo-1, epoch=33, batch=5 train loss <loss>=10.440133253733316\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:56 INFO 139753018754432] Epoch[33] Batch [5]#011Speed: 1918.66 samples/sec#011loss=10.440133\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:56 INFO 139753018754432] processed a total of 635 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458015.6156635, \"EndTime\": 1631458016.1544259, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 538.2428169250488, \"count\": 1, \"min\": 538.2428169250488, \"max\": 538.2428169250488}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:56 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=1179.477057089905 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:56 INFO 139753018754432] #progress_metric: host=algo-1, completed 17.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:56 INFO 139753018754432] #quality_metric: host=algo-1, epoch=33, train loss <loss>=10.157299900054932\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:56 INFO 139753018754432] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:56 INFO 139753018754432] Epoch[34] Batch[0] avg_epoch_loss=9.554925\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:56 INFO 139753018754432] #quality_metric: host=algo-1, epoch=34, batch=0 train loss <loss>=9.554924964904785\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:56 INFO 139753018754432] Epoch[34] Batch[5] avg_epoch_loss=9.812311\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:56 INFO 139753018754432] #quality_metric: host=algo-1, epoch=34, batch=5 train loss <loss>=9.812311490376791\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:56 INFO 139753018754432] Epoch[34] Batch [5]#011Speed: 1952.21 samples/sec#011loss=9.812311\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:56 INFO 139753018754432] Epoch[34] Batch[10] avg_epoch_loss=9.558209\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:56 INFO 139753018754432] #quality_metric: host=algo-1, epoch=34, batch=10 train loss <loss>=9.25328540802002\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:56 INFO 139753018754432] Epoch[34] Batch [10]#011Speed: 1888.01 samples/sec#011loss=9.253285\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:56 INFO 139753018754432] processed a total of 649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458016.1545103, \"EndTime\": 1631458016.7426887, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 587.684154510498, \"count\": 1, \"min\": 587.684154510498, \"max\": 587.684154510498}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:56 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=1104.100875784036 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:56 INFO 139753018754432] #progress_metric: host=algo-1, completed 17.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:56 INFO 139753018754432] #quality_metric: host=algo-1, epoch=34, train loss <loss>=9.558208725669168\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:56 INFO 139753018754432] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:56 INFO 139753018754432] Epoch[35] Batch[0] avg_epoch_loss=10.475408\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:56 INFO 139753018754432] #quality_metric: host=algo-1, epoch=35, batch=0 train loss <loss>=10.475407600402832\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:57 INFO 139753018754432] Epoch[35] Batch[5] avg_epoch_loss=10.208318\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:57 INFO 139753018754432] #quality_metric: host=algo-1, epoch=35, batch=5 train loss <loss>=10.20831791559855\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:57 INFO 139753018754432] Epoch[35] Batch [5]#011Speed: 2070.49 samples/sec#011loss=10.208318\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:57 INFO 139753018754432] Epoch[35] Batch[10] avg_epoch_loss=10.181910\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:57 INFO 139753018754432] #quality_metric: host=algo-1, epoch=35, batch=10 train loss <loss>=10.150220680236817\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:57 INFO 139753018754432] Epoch[35] Batch [10]#011Speed: 1923.78 samples/sec#011loss=10.150221\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:57 INFO 139753018754432] processed a total of 668 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458016.7427752, \"EndTime\": 1631458017.3123083, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 568.9897537231445, \"count\": 1, \"min\": 568.9897537231445, \"max\": 568.9897537231445}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:57 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=1173.7538623180694 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:57 INFO 139753018754432] #progress_metric: host=algo-1, completed 18.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:57 INFO 139753018754432] #quality_metric: host=algo-1, epoch=35, train loss <loss>=10.181910081343217\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:57 INFO 139753018754432] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:57 INFO 139753018754432] Epoch[36] Batch[0] avg_epoch_loss=9.948217\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:57 INFO 139753018754432] #quality_metric: host=algo-1, epoch=36, batch=0 train loss <loss>=9.948217391967773\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:57 INFO 139753018754432] Epoch[36] Batch[5] avg_epoch_loss=10.083886\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:57 INFO 139753018754432] #quality_metric: host=algo-1, epoch=36, batch=5 train loss <loss>=10.083885987599691\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:57 INFO 139753018754432] Epoch[36] Batch [5]#011Speed: 1996.51 samples/sec#011loss=10.083886\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:57 INFO 139753018754432] processed a total of 615 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458017.3123944, \"EndTime\": 1631458017.9295561, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 616.6622638702393, \"count\": 1, \"min\": 616.6622638702393, \"max\": 616.6622638702393}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:57 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=997.0966305477216 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:57 INFO 139753018754432] #progress_metric: host=algo-1, completed 18.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:57 INFO 139753018754432] #quality_metric: host=algo-1, epoch=36, train loss <loss>=10.276859951019286\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:57 INFO 139753018754432] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:58 INFO 139753018754432] Epoch[37] Batch[0] avg_epoch_loss=10.644729\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:58 INFO 139753018754432] #quality_metric: host=algo-1, epoch=37, batch=0 train loss <loss>=10.644728660583496\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:58 INFO 139753018754432] Epoch[37] Batch[5] avg_epoch_loss=10.111106\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:58 INFO 139753018754432] #quality_metric: host=algo-1, epoch=37, batch=5 train loss <loss>=10.111106395721436\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:58 INFO 139753018754432] Epoch[37] Batch [5]#011Speed: 2073.88 samples/sec#011loss=10.111106\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:58 INFO 139753018754432] processed a total of 613 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458017.9296405, \"EndTime\": 1631458018.4634013, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 533.207893371582, \"count\": 1, \"min\": 533.207893371582, \"max\": 533.207893371582}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:58 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=1149.36581998945 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:58 INFO 139753018754432] #progress_metric: host=algo-1, completed 19.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:58 INFO 139753018754432] #quality_metric: host=algo-1, epoch=37, train loss <loss>=9.869303607940674\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:58 INFO 139753018754432] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:58 INFO 139753018754432] Epoch[38] Batch[0] avg_epoch_loss=9.217719\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:58 INFO 139753018754432] #quality_metric: host=algo-1, epoch=38, batch=0 train loss <loss>=9.217719078063965\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:58 INFO 139753018754432] Epoch[38] Batch[5] avg_epoch_loss=9.886807\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:58 INFO 139753018754432] #quality_metric: host=algo-1, epoch=38, batch=5 train loss <loss>=9.886806805928549\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:58 INFO 139753018754432] Epoch[38] Batch [5]#011Speed: 2117.14 samples/sec#011loss=9.886807\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:59 INFO 139753018754432] Epoch[38] Batch[10] avg_epoch_loss=10.108416\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:59 INFO 139753018754432] #quality_metric: host=algo-1, epoch=38, batch=10 train loss <loss>=10.374347496032716\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:59 INFO 139753018754432] Epoch[38] Batch [10]#011Speed: 1748.04 samples/sec#011loss=10.374347\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:59 INFO 139753018754432] processed a total of 658 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458018.463494, \"EndTime\": 1631458019.044191, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 580.1639556884766, \"count\": 1, \"min\": 580.1639556884766, \"max\": 580.1639556884766}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:59 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=1133.9217026171987 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:59 INFO 139753018754432] #progress_metric: host=algo-1, completed 19.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:59 INFO 139753018754432] #quality_metric: host=algo-1, epoch=38, train loss <loss>=10.108416210521352\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:59 INFO 139753018754432] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:59 INFO 139753018754432] Epoch[39] Batch[0] avg_epoch_loss=10.173862\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:59 INFO 139753018754432] #quality_metric: host=algo-1, epoch=39, batch=0 train loss <loss>=10.173861503601074\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:59 INFO 139753018754432] Epoch[39] Batch[5] avg_epoch_loss=9.835932\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:59 INFO 139753018754432] #quality_metric: host=algo-1, epoch=39, batch=5 train loss <loss>=9.83593193689982\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:59 INFO 139753018754432] Epoch[39] Batch [5]#011Speed: 2141.04 samples/sec#011loss=9.835932\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:59 INFO 139753018754432] Epoch[39] Batch[10] avg_epoch_loss=10.192115\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:59 INFO 139753018754432] #quality_metric: host=algo-1, epoch=39, batch=10 train loss <loss>=10.619535636901855\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:59 INFO 139753018754432] Epoch[39] Batch [10]#011Speed: 2081.01 samples/sec#011loss=10.619536\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:59 INFO 139753018754432] processed a total of 642 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458019.044277, \"EndTime\": 1631458019.616315, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 571.5415477752686, \"count\": 1, \"min\": 571.5415477752686, \"max\": 571.5415477752686}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:59 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=1123.0427426710614 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:59 INFO 139753018754432] #progress_metric: host=algo-1, completed 20.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:59 INFO 139753018754432] #quality_metric: host=algo-1, epoch=39, train loss <loss>=10.192115436900746\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:59 INFO 139753018754432] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:59 INFO 139753018754432] Epoch[40] Batch[0] avg_epoch_loss=10.263640\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:46:59 INFO 139753018754432] #quality_metric: host=algo-1, epoch=40, batch=0 train loss <loss>=10.263640403747559\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:00 INFO 139753018754432] Epoch[40] Batch[5] avg_epoch_loss=10.029191\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:00 INFO 139753018754432] #quality_metric: host=algo-1, epoch=40, batch=5 train loss <loss>=10.029191493988037\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:00 INFO 139753018754432] Epoch[40] Batch [5]#011Speed: 2046.33 samples/sec#011loss=10.029191\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:00 INFO 139753018754432] processed a total of 613 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458019.6163979, \"EndTime\": 1631458020.1748772, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 557.9876899719238, \"count\": 1, \"min\": 557.9876899719238, \"max\": 557.9876899719238}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:00 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=1098.3361835045487 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:00 INFO 139753018754432] #progress_metric: host=algo-1, completed 20.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:00 INFO 139753018754432] #quality_metric: host=algo-1, epoch=40, train loss <loss>=9.870225715637208\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:00 INFO 139753018754432] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:00 INFO 139753018754432] Epoch[41] Batch[0] avg_epoch_loss=10.181763\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:00 INFO 139753018754432] #quality_metric: host=algo-1, epoch=41, batch=0 train loss <loss>=10.1817626953125\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:00 INFO 139753018754432] Epoch[41] Batch[5] avg_epoch_loss=10.023333\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:00 INFO 139753018754432] #quality_metric: host=algo-1, epoch=41, batch=5 train loss <loss>=10.023333072662354\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:00 INFO 139753018754432] Epoch[41] Batch [5]#011Speed: 1723.47 samples/sec#011loss=10.023333\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:00 INFO 139753018754432] Epoch[41] Batch[10] avg_epoch_loss=10.234353\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:00 INFO 139753018754432] #quality_metric: host=algo-1, epoch=41, batch=10 train loss <loss>=10.487577438354492\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:00 INFO 139753018754432] Epoch[41] Batch [10]#011Speed: 1778.75 samples/sec#011loss=10.487577\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:00 INFO 139753018754432] processed a total of 673 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458020.1749663, \"EndTime\": 1631458020.8177214, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 642.223596572876, \"count\": 1, \"min\": 642.223596572876, \"max\": 642.223596572876}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:00 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=1047.732598610257 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:00 INFO 139753018754432] #progress_metric: host=algo-1, completed 21.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:00 INFO 139753018754432] #quality_metric: host=algo-1, epoch=41, train loss <loss>=10.234353238886053\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:00 INFO 139753018754432] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:01 INFO 139753018754432] Epoch[42] Batch[0] avg_epoch_loss=10.097486\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:01 INFO 139753018754432] #quality_metric: host=algo-1, epoch=42, batch=0 train loss <loss>=10.09748649597168\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:01 INFO 139753018754432] Epoch[42] Batch[5] avg_epoch_loss=10.159963\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:01 INFO 139753018754432] #quality_metric: host=algo-1, epoch=42, batch=5 train loss <loss>=10.159963130950928\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:01 INFO 139753018754432] Epoch[42] Batch [5]#011Speed: 2135.12 samples/sec#011loss=10.159963\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:01 INFO 139753018754432] processed a total of 618 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458020.8178, \"EndTime\": 1631458021.3490942, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 530.7457447052002, \"count\": 1, \"min\": 530.7457447052002, \"max\": 530.7457447052002}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:01 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=1164.0463879747654 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:01 INFO 139753018754432] #progress_metric: host=algo-1, completed 21.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:01 INFO 139753018754432] #quality_metric: host=algo-1, epoch=42, train loss <loss>=9.73973708152771\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:01 INFO 139753018754432] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:01 INFO 139753018754432] Epoch[43] Batch[0] avg_epoch_loss=10.091849\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:01 INFO 139753018754432] #quality_metric: host=algo-1, epoch=43, batch=0 train loss <loss>=10.091849327087402\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:01 INFO 139753018754432] Epoch[43] Batch[5] avg_epoch_loss=10.143690\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:01 INFO 139753018754432] #quality_metric: host=algo-1, epoch=43, batch=5 train loss <loss>=10.14369010925293\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:01 INFO 139753018754432] Epoch[43] Batch [5]#011Speed: 1811.76 samples/sec#011loss=10.143690\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:01 INFO 139753018754432] processed a total of 613 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458021.3492143, \"EndTime\": 1631458021.9673913, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 617.6300048828125, \"count\": 1, \"min\": 617.6300048828125, \"max\": 617.6300048828125}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:01 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=992.2837279273452 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:01 INFO 139753018754432] #progress_metric: host=algo-1, completed 22.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:01 INFO 139753018754432] #quality_metric: host=algo-1, epoch=43, train loss <loss>=10.183688735961914\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:01 INFO 139753018754432] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:02 INFO 139753018754432] Epoch[44] Batch[0] avg_epoch_loss=9.838233\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:02 INFO 139753018754432] #quality_metric: host=algo-1, epoch=44, batch=0 train loss <loss>=9.83823299407959\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:02 INFO 139753018754432] Epoch[44] Batch[5] avg_epoch_loss=10.112856\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:02 INFO 139753018754432] #quality_metric: host=algo-1, epoch=44, batch=5 train loss <loss>=10.112856388092041\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:02 INFO 139753018754432] Epoch[44] Batch [5]#011Speed: 1930.77 samples/sec#011loss=10.112856\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:02 INFO 139753018754432] Epoch[44] Batch[10] avg_epoch_loss=9.823136\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:02 INFO 139753018754432] #quality_metric: host=algo-1, epoch=44, batch=10 train loss <loss>=9.475471305847169\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:02 INFO 139753018754432] Epoch[44] Batch [10]#011Speed: 1765.55 samples/sec#011loss=9.475471\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:02 INFO 139753018754432] processed a total of 676 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458021.9674869, \"EndTime\": 1631458022.6490963, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 681.0743808746338, \"count\": 1, \"min\": 681.0743808746338, \"max\": 681.0743808746338}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:02 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=992.285104539311 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:02 INFO 139753018754432] #progress_metric: host=algo-1, completed 22.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:02 INFO 139753018754432] #quality_metric: host=algo-1, epoch=44, train loss <loss>=9.823135896162553\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:02 INFO 139753018754432] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:02 INFO 139753018754432] Epoch[45] Batch[0] avg_epoch_loss=9.465463\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:02 INFO 139753018754432] #quality_metric: host=algo-1, epoch=45, batch=0 train loss <loss>=9.465462684631348\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:03 INFO 139753018754432] Epoch[45] Batch[5] avg_epoch_loss=9.547781\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:03 INFO 139753018754432] #quality_metric: host=algo-1, epoch=45, batch=5 train loss <loss>=9.547781149546305\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:03 INFO 139753018754432] Epoch[45] Batch [5]#011Speed: 1993.77 samples/sec#011loss=9.547781\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:03 INFO 139753018754432] processed a total of 631 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458022.6492195, \"EndTime\": 1631458023.253009, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 602.9517650604248, \"count\": 1, \"min\": 602.9517650604248, \"max\": 602.9517650604248}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:03 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=1046.2509518269424 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:03 INFO 139753018754432] #progress_metric: host=algo-1, completed 23.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:03 INFO 139753018754432] #quality_metric: host=algo-1, epoch=45, train loss <loss>=9.769152355194091\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:03 INFO 139753018754432] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:03 INFO 139753018754432] Epoch[46] Batch[0] avg_epoch_loss=9.840996\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:03 INFO 139753018754432] #quality_metric: host=algo-1, epoch=46, batch=0 train loss <loss>=9.840995788574219\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:03 INFO 139753018754432] Epoch[46] Batch[5] avg_epoch_loss=10.098595\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:03 INFO 139753018754432] #quality_metric: host=algo-1, epoch=46, batch=5 train loss <loss>=10.09859530131022\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:03 INFO 139753018754432] Epoch[46] Batch [5]#011Speed: 1810.11 samples/sec#011loss=10.098595\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:03 INFO 139753018754432] Epoch[46] Batch[10] avg_epoch_loss=10.353068\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:03 INFO 139753018754432] #quality_metric: host=algo-1, epoch=46, batch=10 train loss <loss>=10.65843505859375\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:03 INFO 139753018754432] Epoch[46] Batch [10]#011Speed: 1663.23 samples/sec#011loss=10.658435\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:03 INFO 139753018754432] processed a total of 645 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458023.2531009, \"EndTime\": 1631458023.9031942, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 649.5344638824463, \"count\": 1, \"min\": 649.5344638824463, \"max\": 649.5344638824463}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:03 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=992.8315850471529 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:03 INFO 139753018754432] #progress_metric: host=algo-1, completed 23.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:03 INFO 139753018754432] #quality_metric: host=algo-1, epoch=46, train loss <loss>=10.35306791825728\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:03 INFO 139753018754432] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:04 INFO 139753018754432] Epoch[47] Batch[0] avg_epoch_loss=9.692902\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:04 INFO 139753018754432] #quality_metric: host=algo-1, epoch=47, batch=0 train loss <loss>=9.692901611328125\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:04 INFO 139753018754432] Epoch[47] Batch[5] avg_epoch_loss=10.274474\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:04 INFO 139753018754432] #quality_metric: host=algo-1, epoch=47, batch=5 train loss <loss>=10.274473667144775\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:04 INFO 139753018754432] Epoch[47] Batch [5]#011Speed: 1781.46 samples/sec#011loss=10.274474\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:04 INFO 139753018754432] Epoch[47] Batch[10] avg_epoch_loss=10.275537\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:04 INFO 139753018754432] #quality_metric: host=algo-1, epoch=47, batch=10 train loss <loss>=10.276813125610351\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:04 INFO 139753018754432] Epoch[47] Batch [10]#011Speed: 1802.43 samples/sec#011loss=10.276813\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:04 INFO 139753018754432] processed a total of 672 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458023.9032784, \"EndTime\": 1631458024.5726223, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 668.8747406005859, \"count\": 1, \"min\": 668.8747406005859, \"max\": 668.8747406005859}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:04 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=1004.4894512602572 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:04 INFO 139753018754432] #progress_metric: host=algo-1, completed 24.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:04 INFO 139753018754432] #quality_metric: host=algo-1, epoch=47, train loss <loss>=10.2755370573564\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:04 INFO 139753018754432] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:04 INFO 139753018754432] Epoch[48] Batch[0] avg_epoch_loss=9.871405\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:04 INFO 139753018754432] #quality_metric: host=algo-1, epoch=48, batch=0 train loss <loss>=9.871404647827148\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:05 INFO 139753018754432] Epoch[48] Batch[5] avg_epoch_loss=10.005264\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:05 INFO 139753018754432] #quality_metric: host=algo-1, epoch=48, batch=5 train loss <loss>=10.005264282226562\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:05 INFO 139753018754432] Epoch[48] Batch [5]#011Speed: 1677.39 samples/sec#011loss=10.005264\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:05 INFO 139753018754432] processed a total of 638 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458024.572702, \"EndTime\": 1631458025.1999907, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 626.7285346984863, \"count\": 1, \"min\": 626.7285346984863, \"max\": 626.7285346984863}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:05 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=1017.7654636901085 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:05 INFO 139753018754432] #progress_metric: host=algo-1, completed 24.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:05 INFO 139753018754432] #quality_metric: host=algo-1, epoch=48, train loss <loss>=10.017918109893799\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:05 INFO 139753018754432] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:05 INFO 139753018754432] Epoch[49] Batch[0] avg_epoch_loss=9.481561\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:05 INFO 139753018754432] #quality_metric: host=algo-1, epoch=49, batch=0 train loss <loss>=9.481560707092285\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:05 INFO 139753018754432] Epoch[49] Batch[5] avg_epoch_loss=10.110601\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:05 INFO 139753018754432] #quality_metric: host=algo-1, epoch=49, batch=5 train loss <loss>=10.1106006304423\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:05 INFO 139753018754432] Epoch[49] Batch [5]#011Speed: 1507.78 samples/sec#011loss=10.110601\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:05 INFO 139753018754432] processed a total of 576 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458025.2000852, \"EndTime\": 1631458025.8508961, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 650.2804756164551, \"count\": 1, \"min\": 650.2804756164551, \"max\": 650.2804756164551}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:05 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=885.5872101675853 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:05 INFO 139753018754432] #progress_metric: host=algo-1, completed 25.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:05 INFO 139753018754432] #quality_metric: host=algo-1, epoch=49, train loss <loss>=9.96074432796902\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:05 INFO 139753018754432] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:06 INFO 139753018754432] Epoch[50] Batch[0] avg_epoch_loss=9.719064\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:06 INFO 139753018754432] #quality_metric: host=algo-1, epoch=50, batch=0 train loss <loss>=9.719063758850098\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:06 INFO 139753018754432] Epoch[50] Batch[5] avg_epoch_loss=10.039621\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:06 INFO 139753018754432] #quality_metric: host=algo-1, epoch=50, batch=5 train loss <loss>=10.039620876312256\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:06 INFO 139753018754432] Epoch[50] Batch [5]#011Speed: 1508.68 samples/sec#011loss=10.039621\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:06 INFO 139753018754432] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458025.8509893, \"EndTime\": 1631458026.5506473, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 699.103832244873, \"count\": 1, \"min\": 699.103832244873, \"max\": 699.103832244873}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:06 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=905.2800379706955 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:06 INFO 139753018754432] #progress_metric: host=algo-1, completed 25.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:06 INFO 139753018754432] #quality_metric: host=algo-1, epoch=50, train loss <loss>=9.908703994750976\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:06 INFO 139753018754432] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:06 INFO 139753018754432] Epoch[51] Batch[0] avg_epoch_loss=9.670924\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:06 INFO 139753018754432] #quality_metric: host=algo-1, epoch=51, batch=0 train loss <loss>=9.670924186706543\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:06 INFO 139753018754432] Epoch[51] Batch[5] avg_epoch_loss=9.983346\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:06 INFO 139753018754432] #quality_metric: host=algo-1, epoch=51, batch=5 train loss <loss>=9.983346462249756\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:06 INFO 139753018754432] Epoch[51] Batch [5]#011Speed: 2115.19 samples/sec#011loss=9.983346\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:07 INFO 139753018754432] Epoch[51] Batch[10] avg_epoch_loss=10.093697\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:07 INFO 139753018754432] #quality_metric: host=algo-1, epoch=51, batch=10 train loss <loss>=10.226118278503417\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:07 INFO 139753018754432] Epoch[51] Batch [10]#011Speed: 1948.69 samples/sec#011loss=10.226118\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:07 INFO 139753018754432] processed a total of 653 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458026.550733, \"EndTime\": 1631458027.113288, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 562.1671676635742, \"count\": 1, \"min\": 562.1671676635742, \"max\": 562.1671676635742}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:07 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=1161.3151596613002 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:07 INFO 139753018754432] #progress_metric: host=algo-1, completed 26.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:07 INFO 139753018754432] #quality_metric: host=algo-1, epoch=51, train loss <loss>=10.093697287819602\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:07 INFO 139753018754432] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:07 INFO 139753018754432] Epoch[52] Batch[0] avg_epoch_loss=10.321099\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:07 INFO 139753018754432] #quality_metric: host=algo-1, epoch=52, batch=0 train loss <loss>=10.321099281311035\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:07 INFO 139753018754432] Epoch[52] Batch[5] avg_epoch_loss=9.748069\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:07 INFO 139753018754432] #quality_metric: host=algo-1, epoch=52, batch=5 train loss <loss>=9.748068809509277\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:07 INFO 139753018754432] Epoch[52] Batch [5]#011Speed: 1973.74 samples/sec#011loss=9.748069\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:07 INFO 139753018754432] Epoch[52] Batch[10] avg_epoch_loss=9.672762\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:07 INFO 139753018754432] #quality_metric: host=algo-1, epoch=52, batch=10 train loss <loss>=9.582394027709961\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:07 INFO 139753018754432] Epoch[52] Batch [10]#011Speed: 1731.53 samples/sec#011loss=9.582394\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:07 INFO 139753018754432] processed a total of 684 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458027.113376, \"EndTime\": 1631458027.7465265, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 632.6284408569336, \"count\": 1, \"min\": 632.6284408569336, \"max\": 632.6284408569336}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:07 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=1080.9960025546875 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:07 INFO 139753018754432] #progress_metric: host=algo-1, completed 26.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:07 INFO 139753018754432] #quality_metric: host=algo-1, epoch=52, train loss <loss>=9.672762090509588\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:07 INFO 139753018754432] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:08 INFO 139753018754432] Epoch[53] Batch[0] avg_epoch_loss=9.637884\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:08 INFO 139753018754432] #quality_metric: host=algo-1, epoch=53, batch=0 train loss <loss>=9.637884140014648\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:08 INFO 139753018754432] Epoch[53] Batch[5] avg_epoch_loss=9.950233\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:08 INFO 139753018754432] #quality_metric: host=algo-1, epoch=53, batch=5 train loss <loss>=9.950232982635498\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:08 INFO 139753018754432] Epoch[53] Batch [5]#011Speed: 1804.74 samples/sec#011loss=9.950233\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:08 INFO 139753018754432] processed a total of 639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458027.7466094, \"EndTime\": 1631458028.3533826, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 606.1785221099854, \"count\": 1, \"min\": 606.1785221099854, \"max\": 606.1785221099854}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:08 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=1053.9165033455915 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:08 INFO 139753018754432] #progress_metric: host=algo-1, completed 27.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:08 INFO 139753018754432] #quality_metric: host=algo-1, epoch=53, train loss <loss>=9.981851291656493\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:08 INFO 139753018754432] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:08 INFO 139753018754432] Epoch[54] Batch[0] avg_epoch_loss=10.346920\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:08 INFO 139753018754432] #quality_metric: host=algo-1, epoch=54, batch=0 train loss <loss>=10.346920013427734\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:08 INFO 139753018754432] Epoch[54] Batch[5] avg_epoch_loss=9.919254\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:08 INFO 139753018754432] #quality_metric: host=algo-1, epoch=54, batch=5 train loss <loss>=9.919254461924234\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:08 INFO 139753018754432] Epoch[54] Batch [5]#011Speed: 1947.37 samples/sec#011loss=9.919254\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:08 INFO 139753018754432] processed a total of 625 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458028.3534744, \"EndTime\": 1631458028.8957362, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 541.6979789733887, \"count\": 1, \"min\": 541.6979789733887, \"max\": 541.6979789733887}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:08 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=1153.5002261730638 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:08 INFO 139753018754432] #progress_metric: host=algo-1, completed 27.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:08 INFO 139753018754432] #quality_metric: host=algo-1, epoch=54, train loss <loss>=9.917601299285888\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:08 INFO 139753018754432] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:09 INFO 139753018754432] Epoch[55] Batch[0] avg_epoch_loss=10.335537\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:09 INFO 139753018754432] #quality_metric: host=algo-1, epoch=55, batch=0 train loss <loss>=10.33553695678711\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:09 INFO 139753018754432] Epoch[55] Batch[5] avg_epoch_loss=10.017453\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:09 INFO 139753018754432] #quality_metric: host=algo-1, epoch=55, batch=5 train loss <loss>=10.01745319366455\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:09 INFO 139753018754432] Epoch[55] Batch [5]#011Speed: 2093.57 samples/sec#011loss=10.017453\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:09 INFO 139753018754432] Epoch[55] Batch[10] avg_epoch_loss=9.214243\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:09 INFO 139753018754432] #quality_metric: host=algo-1, epoch=55, batch=10 train loss <loss>=8.250390815734864\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:09 INFO 139753018754432] Epoch[55] Batch [10]#011Speed: 2096.71 samples/sec#011loss=8.250391\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:09 INFO 139753018754432] processed a total of 641 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458028.8958259, \"EndTime\": 1631458029.5001814, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 603.8365364074707, \"count\": 1, \"min\": 603.8365364074707, \"max\": 603.8365364074707}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:09 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=1061.327661442432 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:09 INFO 139753018754432] #progress_metric: host=algo-1, completed 28.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:09 INFO 139753018754432] #quality_metric: host=algo-1, epoch=55, train loss <loss>=9.21424302187833\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:09 INFO 139753018754432] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:09 INFO 139753018754432] Saved checkpoint to \"/opt/ml/model/state_611bb0cd-a5e6-4c25-90f5-07bc005f9e02-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458029.5002675, \"EndTime\": 1631458029.5195196, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 18.733501434326172, \"count\": 1, \"min\": 18.733501434326172, \"max\": 18.733501434326172}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:09 INFO 139753018754432] Epoch[56] Batch[0] avg_epoch_loss=9.440557\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:09 INFO 139753018754432] #quality_metric: host=algo-1, epoch=56, batch=0 train loss <loss>=9.440557479858398\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:09 INFO 139753018754432] Epoch[56] Batch[5] avg_epoch_loss=10.048995\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:09 INFO 139753018754432] #quality_metric: host=algo-1, epoch=56, batch=5 train loss <loss>=10.04899517695109\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:09 INFO 139753018754432] Epoch[56] Batch [5]#011Speed: 1565.28 samples/sec#011loss=10.048995\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:10 INFO 139753018754432] Epoch[56] Batch[10] avg_epoch_loss=10.629372\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:10 INFO 139753018754432] #quality_metric: host=algo-1, epoch=56, batch=10 train loss <loss>=11.325823783874512\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:10 INFO 139753018754432] Epoch[56] Batch [10]#011Speed: 1892.97 samples/sec#011loss=11.325824\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:10 INFO 139753018754432] processed a total of 649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458029.5195904, \"EndTime\": 1631458030.1607764, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 641.1242485046387, \"count\": 1, \"min\": 641.1242485046387, \"max\": 641.1242485046387}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:10 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=1012.0682499245253 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:10 INFO 139753018754432] #progress_metric: host=algo-1, completed 28.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:10 INFO 139753018754432] #quality_metric: host=algo-1, epoch=56, train loss <loss>=10.629371816461736\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:10 INFO 139753018754432] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:10 INFO 139753018754432] Epoch[57] Batch[0] avg_epoch_loss=10.253592\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:10 INFO 139753018754432] #quality_metric: host=algo-1, epoch=57, batch=0 train loss <loss>=10.253591537475586\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:10 INFO 139753018754432] Epoch[57] Batch[5] avg_epoch_loss=10.875923\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:10 INFO 139753018754432] #quality_metric: host=algo-1, epoch=57, batch=5 train loss <loss>=10.875923315684\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:10 INFO 139753018754432] Epoch[57] Batch [5]#011Speed: 1998.35 samples/sec#011loss=10.875923\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:10 INFO 139753018754432] processed a total of 635 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458030.1608741, \"EndTime\": 1631458030.7169478, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 555.4299354553223, \"count\": 1, \"min\": 555.4299354553223, \"max\": 555.4299354553223}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:10 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=1142.9749294058072 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:10 INFO 139753018754432] #progress_metric: host=algo-1, completed 29.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:10 INFO 139753018754432] #quality_metric: host=algo-1, epoch=57, train loss <loss>=10.658960247039795\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:10 INFO 139753018754432] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:10 INFO 139753018754432] Epoch[58] Batch[0] avg_epoch_loss=10.086878\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:10 INFO 139753018754432] #quality_metric: host=algo-1, epoch=58, batch=0 train loss <loss>=10.086877822875977\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:11 INFO 139753018754432] Epoch[58] Batch[5] avg_epoch_loss=10.161355\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:11 INFO 139753018754432] #quality_metric: host=algo-1, epoch=58, batch=5 train loss <loss>=10.161354700724283\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:11 INFO 139753018754432] Epoch[58] Batch [5]#011Speed: 1588.99 samples/sec#011loss=10.161355\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:11 INFO 139753018754432] Epoch[58] Batch[10] avg_epoch_loss=10.174200\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:11 INFO 139753018754432] #quality_metric: host=algo-1, epoch=58, batch=10 train loss <loss>=10.189615058898926\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:11 INFO 139753018754432] Epoch[58] Batch [10]#011Speed: 1603.16 samples/sec#011loss=10.189615\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:11 INFO 139753018754432] processed a total of 677 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458030.7170422, \"EndTime\": 1631458031.3724847, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 654.8049449920654, \"count\": 1, \"min\": 654.8049449920654, \"max\": 654.8049449920654}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:11 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=1033.70071085622 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:11 INFO 139753018754432] #progress_metric: host=algo-1, completed 29.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:11 INFO 139753018754432] #quality_metric: host=algo-1, epoch=58, train loss <loss>=10.174200318076394\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:11 INFO 139753018754432] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:11 INFO 139753018754432] Epoch[59] Batch[0] avg_epoch_loss=10.422509\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:11 INFO 139753018754432] #quality_metric: host=algo-1, epoch=59, batch=0 train loss <loss>=10.42250919342041\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:11 INFO 139753018754432] Epoch[59] Batch[5] avg_epoch_loss=10.049029\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:11 INFO 139753018754432] #quality_metric: host=algo-1, epoch=59, batch=5 train loss <loss>=10.049028873443604\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:11 INFO 139753018754432] Epoch[59] Batch [5]#011Speed: 2000.77 samples/sec#011loss=10.049029\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:11 INFO 139753018754432] Epoch[59] Batch[10] avg_epoch_loss=10.280644\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:11 INFO 139753018754432] #quality_metric: host=algo-1, epoch=59, batch=10 train loss <loss>=10.55858211517334\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:11 INFO 139753018754432] Epoch[59] Batch [10]#011Speed: 1931.67 samples/sec#011loss=10.558582\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:11 INFO 139753018754432] processed a total of 660 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458031.3725698, \"EndTime\": 1631458031.9715538, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 598.4835624694824, \"count\": 1, \"min\": 598.4835624694824, \"max\": 598.4835624694824}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:11 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=1102.5478031215162 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:11 INFO 139753018754432] #progress_metric: host=algo-1, completed 30.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:11 INFO 139753018754432] #quality_metric: host=algo-1, epoch=59, train loss <loss>=10.280643983320756\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:11 INFO 139753018754432] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:12 INFO 139753018754432] Epoch[60] Batch[0] avg_epoch_loss=10.087117\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:12 INFO 139753018754432] #quality_metric: host=algo-1, epoch=60, batch=0 train loss <loss>=10.087117195129395\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:12 INFO 139753018754432] Epoch[60] Batch[5] avg_epoch_loss=10.054243\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:12 INFO 139753018754432] #quality_metric: host=algo-1, epoch=60, batch=5 train loss <loss>=10.054242610931396\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:12 INFO 139753018754432] Epoch[60] Batch [5]#011Speed: 1840.58 samples/sec#011loss=10.054243\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:12 INFO 139753018754432] Epoch[60] Batch[10] avg_epoch_loss=9.478601\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:12 INFO 139753018754432] #quality_metric: host=algo-1, epoch=60, batch=10 train loss <loss>=8.787830352783203\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:12 INFO 139753018754432] Epoch[60] Batch [10]#011Speed: 1689.22 samples/sec#011loss=8.787830\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:12 INFO 139753018754432] processed a total of 648 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458031.9716456, \"EndTime\": 1631458032.6050417, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 632.8525543212891, \"count\": 1, \"min\": 632.8525543212891, \"max\": 632.8525543212891}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:12 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=1023.6945355932204 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:12 INFO 139753018754432] #progress_metric: host=algo-1, completed 30.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:12 INFO 139753018754432] #quality_metric: host=algo-1, epoch=60, train loss <loss>=9.47860067540949\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:12 INFO 139753018754432] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:12 INFO 139753018754432] Epoch[61] Batch[0] avg_epoch_loss=10.341672\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:12 INFO 139753018754432] #quality_metric: host=algo-1, epoch=61, batch=0 train loss <loss>=10.34167194366455\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:13 INFO 139753018754432] Epoch[61] Batch[5] avg_epoch_loss=10.506733\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:13 INFO 139753018754432] #quality_metric: host=algo-1, epoch=61, batch=5 train loss <loss>=10.50673278172811\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:13 INFO 139753018754432] Epoch[61] Batch [5]#011Speed: 1781.57 samples/sec#011loss=10.506733\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:13 INFO 139753018754432] Epoch[61] Batch[10] avg_epoch_loss=9.945698\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:13 INFO 139753018754432] #quality_metric: host=algo-1, epoch=61, batch=10 train loss <loss>=9.27245569229126\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:13 INFO 139753018754432] Epoch[61] Batch [10]#011Speed: 1944.85 samples/sec#011loss=9.272456\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:13 INFO 139753018754432] processed a total of 657 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458032.6051502, \"EndTime\": 1631458033.2201283, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 614.4809722900391, \"count\": 1, \"min\": 614.4809722900391, \"max\": 614.4809722900391}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:13 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=1068.9971817020503 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:13 INFO 139753018754432] #progress_metric: host=algo-1, completed 31.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:13 INFO 139753018754432] #quality_metric: host=algo-1, epoch=61, train loss <loss>=9.945697741074996\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:13 INFO 139753018754432] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:13 INFO 139753018754432] Epoch[62] Batch[0] avg_epoch_loss=10.133980\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:13 INFO 139753018754432] #quality_metric: host=algo-1, epoch=62, batch=0 train loss <loss>=10.133979797363281\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:13 INFO 139753018754432] Epoch[62] Batch[5] avg_epoch_loss=9.889474\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:13 INFO 139753018754432] #quality_metric: host=algo-1, epoch=62, batch=5 train loss <loss>=9.889474232991537\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:13 INFO 139753018754432] Epoch[62] Batch [5]#011Speed: 2081.79 samples/sec#011loss=9.889474\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:13 INFO 139753018754432] Epoch[62] Batch[10] avg_epoch_loss=9.738581\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:13 INFO 139753018754432] #quality_metric: host=algo-1, epoch=62, batch=10 train loss <loss>=9.557509613037109\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:13 INFO 139753018754432] Epoch[62] Batch [10]#011Speed: 1756.50 samples/sec#011loss=9.557510\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:13 INFO 139753018754432] processed a total of 692 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458033.220202, \"EndTime\": 1631458033.7952704, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 574.561357498169, \"count\": 1, \"min\": 574.561357498169, \"max\": 574.561357498169}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:13 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=1204.1342194339472 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:13 INFO 139753018754432] #progress_metric: host=algo-1, completed 31.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:13 INFO 139753018754432] #quality_metric: host=algo-1, epoch=62, train loss <loss>=9.738581223921342\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:13 INFO 139753018754432] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:14 INFO 139753018754432] Epoch[63] Batch[0] avg_epoch_loss=8.998338\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:14 INFO 139753018754432] #quality_metric: host=algo-1, epoch=63, batch=0 train loss <loss>=8.998337745666504\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:14 INFO 139753018754432] Epoch[63] Batch[5] avg_epoch_loss=9.943511\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:14 INFO 139753018754432] #quality_metric: host=algo-1, epoch=63, batch=5 train loss <loss>=9.943511168162027\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:14 INFO 139753018754432] Epoch[63] Batch [5]#011Speed: 1488.79 samples/sec#011loss=9.943511\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:14 INFO 139753018754432] Epoch[63] Batch[10] avg_epoch_loss=9.834403\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:14 INFO 139753018754432] #quality_metric: host=algo-1, epoch=63, batch=10 train loss <loss>=9.703473091125488\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:14 INFO 139753018754432] Epoch[63] Batch [10]#011Speed: 1500.82 samples/sec#011loss=9.703473\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:14 INFO 139753018754432] processed a total of 657 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458033.7953546, \"EndTime\": 1631458034.4644172, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 668.5130596160889, \"count\": 1, \"min\": 668.5130596160889, \"max\": 668.5130596160889}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:14 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=982.5944814547913 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:14 INFO 139753018754432] #progress_metric: host=algo-1, completed 32.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:14 INFO 139753018754432] #quality_metric: host=algo-1, epoch=63, train loss <loss>=9.834402951327236\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:14 INFO 139753018754432] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:14 INFO 139753018754432] Epoch[64] Batch[0] avg_epoch_loss=10.325674\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:14 INFO 139753018754432] #quality_metric: host=algo-1, epoch=64, batch=0 train loss <loss>=10.325674057006836\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:14 INFO 139753018754432] Epoch[64] Batch[5] avg_epoch_loss=10.106211\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:14 INFO 139753018754432] #quality_metric: host=algo-1, epoch=64, batch=5 train loss <loss>=10.106211344401041\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:14 INFO 139753018754432] Epoch[64] Batch [5]#011Speed: 1928.68 samples/sec#011loss=10.106211\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:15 INFO 139753018754432] processed a total of 627 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458034.4645033, \"EndTime\": 1631458035.051446, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 586.4005088806152, \"count\": 1, \"min\": 586.4005088806152, \"max\": 586.4005088806152}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:15 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=1068.996909869151 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:15 INFO 139753018754432] #progress_metric: host=algo-1, completed 32.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:15 INFO 139753018754432] #quality_metric: host=algo-1, epoch=64, train loss <loss>=9.637888145446777\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:15 INFO 139753018754432] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:15 INFO 139753018754432] Epoch[65] Batch[0] avg_epoch_loss=10.042877\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:15 INFO 139753018754432] #quality_metric: host=algo-1, epoch=65, batch=0 train loss <loss>=10.042877197265625\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:15 INFO 139753018754432] Epoch[65] Batch[5] avg_epoch_loss=9.888931\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:15 INFO 139753018754432] #quality_metric: host=algo-1, epoch=65, batch=5 train loss <loss>=9.888930797576904\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:15 INFO 139753018754432] Epoch[65] Batch [5]#011Speed: 2081.81 samples/sec#011loss=9.888931\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:15 INFO 139753018754432] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458035.0515375, \"EndTime\": 1631458035.6400363, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 587.9807472229004, \"count\": 1, \"min\": 587.9807472229004, \"max\": 587.9807472229004}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:15 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=1076.3367307267417 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:15 INFO 139753018754432] #progress_metric: host=algo-1, completed 33.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:15 INFO 139753018754432] #quality_metric: host=algo-1, epoch=65, train loss <loss>=10.12419548034668\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:15 INFO 139753018754432] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:15 INFO 139753018754432] Epoch[66] Batch[0] avg_epoch_loss=9.818265\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:15 INFO 139753018754432] #quality_metric: host=algo-1, epoch=66, batch=0 train loss <loss>=9.818264961242676\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:16 INFO 139753018754432] Epoch[66] Batch[5] avg_epoch_loss=9.967688\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:16 INFO 139753018754432] #quality_metric: host=algo-1, epoch=66, batch=5 train loss <loss>=9.967687924702963\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:16 INFO 139753018754432] Epoch[66] Batch [5]#011Speed: 2101.98 samples/sec#011loss=9.967688\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:16 INFO 139753018754432] Epoch[66] Batch[10] avg_epoch_loss=10.444291\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:16 INFO 139753018754432] #quality_metric: host=algo-1, epoch=66, batch=10 train loss <loss>=11.016213607788085\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:16 INFO 139753018754432] Epoch[66] Batch [10]#011Speed: 1896.68 samples/sec#011loss=11.016214\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:16 INFO 139753018754432] processed a total of 661 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458035.640123, \"EndTime\": 1631458036.2128017, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 572.1356868743896, \"count\": 1, \"min\": 572.1356868743896, \"max\": 572.1356868743896}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:16 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=1155.0739010500274 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:16 INFO 139753018754432] #progress_metric: host=algo-1, completed 33.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:16 INFO 139753018754432] #quality_metric: host=algo-1, epoch=66, train loss <loss>=10.444290507923473\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:16 INFO 139753018754432] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:16 INFO 139753018754432] Epoch[67] Batch[0] avg_epoch_loss=9.661889\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:16 INFO 139753018754432] #quality_metric: host=algo-1, epoch=67, batch=0 train loss <loss>=9.66188907623291\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:16 INFO 139753018754432] Epoch[67] Batch[5] avg_epoch_loss=9.794936\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:16 INFO 139753018754432] #quality_metric: host=algo-1, epoch=67, batch=5 train loss <loss>=9.794935544331869\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:16 INFO 139753018754432] Epoch[67] Batch [5]#011Speed: 1910.46 samples/sec#011loss=9.794936\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:16 INFO 139753018754432] Epoch[67] Batch[10] avg_epoch_loss=9.789463\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:16 INFO 139753018754432] #quality_metric: host=algo-1, epoch=67, batch=10 train loss <loss>=9.782895278930663\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:16 INFO 139753018754432] Epoch[67] Batch [10]#011Speed: 1869.71 samples/sec#011loss=9.782895\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:16 INFO 139753018754432] processed a total of 649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458036.2128859, \"EndTime\": 1631458036.818571, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 605.1511764526367, \"count\": 1, \"min\": 605.1511764526367, \"max\": 605.1511764526367}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:16 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=1072.2408850181491 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:16 INFO 139753018754432] #progress_metric: host=algo-1, completed 34.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:16 INFO 139753018754432] #quality_metric: host=algo-1, epoch=67, train loss <loss>=9.78946269642223\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:16 INFO 139753018754432] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:17 INFO 139753018754432] Epoch[68] Batch[0] avg_epoch_loss=10.811155\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:17 INFO 139753018754432] #quality_metric: host=algo-1, epoch=68, batch=0 train loss <loss>=10.811155319213867\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:17 INFO 139753018754432] Epoch[68] Batch[5] avg_epoch_loss=9.963878\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:17 INFO 139753018754432] #quality_metric: host=algo-1, epoch=68, batch=5 train loss <loss>=9.9638778368632\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:17 INFO 139753018754432] Epoch[68] Batch [5]#011Speed: 1898.61 samples/sec#011loss=9.963878\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:17 INFO 139753018754432] Epoch[68] Batch[10] avg_epoch_loss=9.905570\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:17 INFO 139753018754432] #quality_metric: host=algo-1, epoch=68, batch=10 train loss <loss>=9.835600280761719\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:17 INFO 139753018754432] Epoch[68] Batch [10]#011Speed: 1681.60 samples/sec#011loss=9.835600\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:17 INFO 139753018754432] processed a total of 662 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458036.8186567, \"EndTime\": 1631458037.4587154, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 639.563798904419, \"count\": 1, \"min\": 639.563798904419, \"max\": 639.563798904419}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:17 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=1034.8709939517 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:17 INFO 139753018754432] #progress_metric: host=algo-1, completed 34.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:17 INFO 139753018754432] #quality_metric: host=algo-1, epoch=68, train loss <loss>=9.905569856817072\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:17 INFO 139753018754432] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:17 INFO 139753018754432] Epoch[69] Batch[0] avg_epoch_loss=9.246747\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:17 INFO 139753018754432] #quality_metric: host=algo-1, epoch=69, batch=0 train loss <loss>=9.246747016906738\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:17 INFO 139753018754432] Epoch[69] Batch[5] avg_epoch_loss=9.811948\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:17 INFO 139753018754432] #quality_metric: host=algo-1, epoch=69, batch=5 train loss <loss>=9.8119478225708\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:17 INFO 139753018754432] Epoch[69] Batch [5]#011Speed: 1854.98 samples/sec#011loss=9.811948\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:18 INFO 139753018754432] Epoch[69] Batch[10] avg_epoch_loss=9.939725\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:18 INFO 139753018754432] #quality_metric: host=algo-1, epoch=69, batch=10 train loss <loss>=10.093058586120605\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:18 INFO 139753018754432] Epoch[69] Batch [10]#011Speed: 1623.77 samples/sec#011loss=10.093059\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:18 INFO 139753018754432] processed a total of 680 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458037.4588065, \"EndTime\": 1631458038.0745862, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 615.293025970459, \"count\": 1, \"min\": 615.293025970459, \"max\": 615.293025970459}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:18 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=1104.9401336561743 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:18 INFO 139753018754432] #progress_metric: host=algo-1, completed 35.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:18 INFO 139753018754432] #quality_metric: host=algo-1, epoch=69, train loss <loss>=9.939725442366166\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:18 INFO 139753018754432] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:18 INFO 139753018754432] Epoch[70] Batch[0] avg_epoch_loss=9.385681\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:18 INFO 139753018754432] #quality_metric: host=algo-1, epoch=70, batch=0 train loss <loss>=9.38568115234375\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:18 INFO 139753018754432] Epoch[70] Batch[5] avg_epoch_loss=9.437553\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:18 INFO 139753018754432] #quality_metric: host=algo-1, epoch=70, batch=5 train loss <loss>=9.437552769978842\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:18 INFO 139753018754432] Epoch[70] Batch [5]#011Speed: 2036.72 samples/sec#011loss=9.437553\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:18 INFO 139753018754432] Epoch[70] Batch[10] avg_epoch_loss=9.502770\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:18 INFO 139753018754432] #quality_metric: host=algo-1, epoch=70, batch=10 train loss <loss>=9.581030654907227\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:18 INFO 139753018754432] Epoch[70] Batch [10]#011Speed: 1887.08 samples/sec#011loss=9.581031\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:18 INFO 139753018754432] processed a total of 659 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458038.0746708, \"EndTime\": 1631458038.6610007, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 585.8321189880371, \"count\": 1, \"min\": 585.8321189880371, \"max\": 585.8321189880371}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:18 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=1124.675943324444 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:18 INFO 139753018754432] #progress_metric: host=algo-1, completed 35.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:18 INFO 139753018754432] #quality_metric: host=algo-1, epoch=70, train loss <loss>=9.502769990400834\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:18 INFO 139753018754432] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:18 INFO 139753018754432] Epoch[71] Batch[0] avg_epoch_loss=10.207595\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:18 INFO 139753018754432] #quality_metric: host=algo-1, epoch=71, batch=0 train loss <loss>=10.207594871520996\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:19 INFO 139753018754432] Epoch[71] Batch[5] avg_epoch_loss=9.813503\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:19 INFO 139753018754432] #quality_metric: host=algo-1, epoch=71, batch=5 train loss <loss>=9.813502629597982\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:19 INFO 139753018754432] Epoch[71] Batch [5]#011Speed: 2136.89 samples/sec#011loss=9.813503\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:19 INFO 139753018754432] Epoch[71] Batch[10] avg_epoch_loss=9.604677\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:19 INFO 139753018754432] #quality_metric: host=algo-1, epoch=71, batch=10 train loss <loss>=9.354085159301757\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:19 INFO 139753018754432] Epoch[71] Batch [10]#011Speed: 1795.35 samples/sec#011loss=9.354085\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:19 INFO 139753018754432] processed a total of 687 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458038.661075, \"EndTime\": 1631458039.2347114, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 573.087215423584, \"count\": 1, \"min\": 573.087215423584, \"max\": 573.087215423584}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:19 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=1198.5664755079945 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:19 INFO 139753018754432] #progress_metric: host=algo-1, completed 36.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:19 INFO 139753018754432] #quality_metric: host=algo-1, epoch=71, train loss <loss>=9.604676506736062\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:19 INFO 139753018754432] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:19 INFO 139753018754432] Epoch[72] Batch[0] avg_epoch_loss=10.137995\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:19 INFO 139753018754432] #quality_metric: host=algo-1, epoch=72, batch=0 train loss <loss>=10.137994766235352\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:19 INFO 139753018754432] Epoch[72] Batch[5] avg_epoch_loss=10.051400\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:19 INFO 139753018754432] #quality_metric: host=algo-1, epoch=72, batch=5 train loss <loss>=10.051399866739908\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:19 INFO 139753018754432] Epoch[72] Batch [5]#011Speed: 2054.27 samples/sec#011loss=10.051400\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:19 INFO 139753018754432] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458039.2347748, \"EndTime\": 1631458039.7966483, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 561.3703727722168, \"count\": 1, \"min\": 561.3703727722168, \"max\": 561.3703727722168}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:19 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=1134.460154625868 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:19 INFO 139753018754432] #progress_metric: host=algo-1, completed 36.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:19 INFO 139753018754432] #quality_metric: host=algo-1, epoch=72, train loss <loss>=9.921901512145997\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:19 INFO 139753018754432] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:20 INFO 139753018754432] Epoch[73] Batch[0] avg_epoch_loss=11.565110\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:20 INFO 139753018754432] #quality_metric: host=algo-1, epoch=73, batch=0 train loss <loss>=11.565110206604004\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:20 INFO 139753018754432] Epoch[73] Batch[5] avg_epoch_loss=10.173181\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:20 INFO 139753018754432] #quality_metric: host=algo-1, epoch=73, batch=5 train loss <loss>=10.173181215922037\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:20 INFO 139753018754432] Epoch[73] Batch [5]#011Speed: 1773.88 samples/sec#011loss=10.173181\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:20 INFO 139753018754432] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458039.796738, \"EndTime\": 1631458040.3936489, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 596.3478088378906, \"count\": 1, \"min\": 596.3478088378906, \"max\": 596.3478088378906}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:20 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=1062.8966037687583 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:20 INFO 139753018754432] #progress_metric: host=algo-1, completed 37.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:20 INFO 139753018754432] #quality_metric: host=algo-1, epoch=73, train loss <loss>=9.838179588317871\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:20 INFO 139753018754432] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:20 INFO 139753018754432] Epoch[74] Batch[0] avg_epoch_loss=9.984686\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:20 INFO 139753018754432] #quality_metric: host=algo-1, epoch=74, batch=0 train loss <loss>=9.984685897827148\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:20 INFO 139753018754432] Epoch[74] Batch[5] avg_epoch_loss=10.151657\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:20 INFO 139753018754432] #quality_metric: host=algo-1, epoch=74, batch=5 train loss <loss>=10.15165662765503\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:20 INFO 139753018754432] Epoch[74] Batch [5]#011Speed: 2128.16 samples/sec#011loss=10.151657\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:20 INFO 139753018754432] Epoch[74] Batch[10] avg_epoch_loss=10.566380\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:20 INFO 139753018754432] #quality_metric: host=algo-1, epoch=74, batch=10 train loss <loss>=11.064048194885254\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:20 INFO 139753018754432] Epoch[74] Batch [10]#011Speed: 2018.36 samples/sec#011loss=11.064048\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:20 INFO 139753018754432] processed a total of 645 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458040.3937435, \"EndTime\": 1631458040.9655821, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 571.2945461273193, \"count\": 1, \"min\": 571.2945461273193, \"max\": 571.2945461273193}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:20 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=1128.767021778614 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:20 INFO 139753018754432] #progress_metric: host=algo-1, completed 37.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:20 INFO 139753018754432] #quality_metric: host=algo-1, epoch=74, train loss <loss>=10.566380067305131\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:20 INFO 139753018754432] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:21 INFO 139753018754432] Epoch[75] Batch[0] avg_epoch_loss=10.125574\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:21 INFO 139753018754432] #quality_metric: host=algo-1, epoch=75, batch=0 train loss <loss>=10.125574111938477\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:21 INFO 139753018754432] Epoch[75] Batch[5] avg_epoch_loss=10.219930\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:21 INFO 139753018754432] #quality_metric: host=algo-1, epoch=75, batch=5 train loss <loss>=10.219929854075113\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:21 INFO 139753018754432] Epoch[75] Batch [5]#011Speed: 1941.06 samples/sec#011loss=10.219930\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:21 INFO 139753018754432] processed a total of 629 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458040.965669, \"EndTime\": 1631458041.5577693, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 591.588020324707, \"count\": 1, \"min\": 591.588020324707, \"max\": 591.588020324707}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:21 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=1062.9845956478434 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:21 INFO 139753018754432] #progress_metric: host=algo-1, completed 38.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:21 INFO 139753018754432] #quality_metric: host=algo-1, epoch=75, train loss <loss>=10.133108425140382\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:21 INFO 139753018754432] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:21 INFO 139753018754432] Epoch[76] Batch[0] avg_epoch_loss=9.759372\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:21 INFO 139753018754432] #quality_metric: host=algo-1, epoch=76, batch=0 train loss <loss>=9.759371757507324\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:21 INFO 139753018754432] Epoch[76] Batch[5] avg_epoch_loss=10.047781\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:21 INFO 139753018754432] #quality_metric: host=algo-1, epoch=76, batch=5 train loss <loss>=10.047780513763428\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:21 INFO 139753018754432] Epoch[76] Batch [5]#011Speed: 1686.82 samples/sec#011loss=10.047781\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:22 INFO 139753018754432] Epoch[76] Batch[10] avg_epoch_loss=10.210119\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:22 INFO 139753018754432] #quality_metric: host=algo-1, epoch=76, batch=10 train loss <loss>=10.404924392700195\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:22 INFO 139753018754432] Epoch[76] Batch [10]#011Speed: 1626.80 samples/sec#011loss=10.404924\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:22 INFO 139753018754432] processed a total of 681 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458041.55787, \"EndTime\": 1631458042.1929939, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 634.5515251159668, \"count\": 1, \"min\": 634.5515251159668, \"max\": 634.5515251159668}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:22 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=1072.974349554236 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:22 INFO 139753018754432] #progress_metric: host=algo-1, completed 38.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:22 INFO 139753018754432] #quality_metric: host=algo-1, epoch=76, train loss <loss>=10.210118640552867\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:22 INFO 139753018754432] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:22 INFO 139753018754432] Epoch[77] Batch[0] avg_epoch_loss=10.493984\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:22 INFO 139753018754432] #quality_metric: host=algo-1, epoch=77, batch=0 train loss <loss>=10.49398422241211\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:22 INFO 139753018754432] Epoch[77] Batch[5] avg_epoch_loss=9.847500\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:22 INFO 139753018754432] #quality_metric: host=algo-1, epoch=77, batch=5 train loss <loss>=9.84749952952067\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:22 INFO 139753018754432] Epoch[77] Batch [5]#011Speed: 1681.22 samples/sec#011loss=9.847500\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:22 INFO 139753018754432] processed a total of 612 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458042.1930819, \"EndTime\": 1631458042.776577, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 582.9257965087891, \"count\": 1, \"min\": 582.9257965087891, \"max\": 582.9257965087891}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:22 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=1049.6569580029106 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:22 INFO 139753018754432] #progress_metric: host=algo-1, completed 39.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:22 INFO 139753018754432] #quality_metric: host=algo-1, epoch=77, train loss <loss>=10.018917274475097\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:22 INFO 139753018754432] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:23 INFO 139753018754432] Epoch[78] Batch[0] avg_epoch_loss=9.330701\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:23 INFO 139753018754432] #quality_metric: host=algo-1, epoch=78, batch=0 train loss <loss>=9.330700874328613\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:23 INFO 139753018754432] Epoch[78] Batch[5] avg_epoch_loss=9.886588\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:23 INFO 139753018754432] #quality_metric: host=algo-1, epoch=78, batch=5 train loss <loss>=9.886587937672934\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:23 INFO 139753018754432] Epoch[78] Batch [5]#011Speed: 1891.56 samples/sec#011loss=9.886588\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:23 INFO 139753018754432] processed a total of 603 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458042.7766557, \"EndTime\": 1631458043.326792, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 549.5412349700928, \"count\": 1, \"min\": 549.5412349700928, \"max\": 549.5412349700928}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:23 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=1096.9690726120905 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:23 INFO 139753018754432] #progress_metric: host=algo-1, completed 39.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:23 INFO 139753018754432] #quality_metric: host=algo-1, epoch=78, train loss <loss>=9.358037233352661\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:23 INFO 139753018754432] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:23 INFO 139753018754432] Epoch[79] Batch[0] avg_epoch_loss=9.926128\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:23 INFO 139753018754432] #quality_metric: host=algo-1, epoch=79, batch=0 train loss <loss>=9.926128387451172\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:23 INFO 139753018754432] Epoch[79] Batch[5] avg_epoch_loss=9.938555\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:23 INFO 139753018754432] #quality_metric: host=algo-1, epoch=79, batch=5 train loss <loss>=9.938555081685385\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:23 INFO 139753018754432] Epoch[79] Batch [5]#011Speed: 2123.81 samples/sec#011loss=9.938555\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:23 INFO 139753018754432] processed a total of 635 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458043.3269024, \"EndTime\": 1631458043.8750312, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 547.5232601165771, \"count\": 1, \"min\": 547.5232601165771, \"max\": 547.5232601165771}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:23 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=1159.4913947463394 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:23 INFO 139753018754432] #progress_metric: host=algo-1, completed 40.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:23 INFO 139753018754432] #quality_metric: host=algo-1, epoch=79, train loss <loss>=9.87154426574707\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:23 INFO 139753018754432] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:24 INFO 139753018754432] Epoch[80] Batch[0] avg_epoch_loss=9.564863\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:24 INFO 139753018754432] #quality_metric: host=algo-1, epoch=80, batch=0 train loss <loss>=9.564863204956055\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:24 INFO 139753018754432] Epoch[80] Batch[5] avg_epoch_loss=9.848232\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:24 INFO 139753018754432] #quality_metric: host=algo-1, epoch=80, batch=5 train loss <loss>=9.848231792449951\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:24 INFO 139753018754432] Epoch[80] Batch [5]#011Speed: 2043.93 samples/sec#011loss=9.848232\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:24 INFO 139753018754432] Epoch[80] Batch[10] avg_epoch_loss=9.763435\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:24 INFO 139753018754432] #quality_metric: host=algo-1, epoch=80, batch=10 train loss <loss>=9.661679077148438\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:24 INFO 139753018754432] Epoch[80] Batch [10]#011Speed: 1790.63 samples/sec#011loss=9.661679\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:24 INFO 139753018754432] processed a total of 690 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458043.8751209, \"EndTime\": 1631458044.4782376, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 602.5948524475098, \"count\": 1, \"min\": 602.5948524475098, \"max\": 602.5948524475098}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:24 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=1144.8119630362023 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:24 INFO 139753018754432] #progress_metric: host=algo-1, completed 40.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:24 INFO 139753018754432] #quality_metric: host=algo-1, epoch=80, train loss <loss>=9.763435103676535\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:24 INFO 139753018754432] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:24 INFO 139753018754432] Epoch[81] Batch[0] avg_epoch_loss=10.892720\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:24 INFO 139753018754432] #quality_metric: host=algo-1, epoch=81, batch=0 train loss <loss>=10.892720222473145\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:24 INFO 139753018754432] Epoch[81] Batch[5] avg_epoch_loss=10.150848\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:24 INFO 139753018754432] #quality_metric: host=algo-1, epoch=81, batch=5 train loss <loss>=10.150847911834717\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:24 INFO 139753018754432] Epoch[81] Batch [5]#011Speed: 2077.19 samples/sec#011loss=10.150848\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:25 INFO 139753018754432] Epoch[81] Batch[10] avg_epoch_loss=9.991188\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:25 INFO 139753018754432] #quality_metric: host=algo-1, epoch=81, batch=10 train loss <loss>=9.79959602355957\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:25 INFO 139753018754432] Epoch[81] Batch [10]#011Speed: 1776.63 samples/sec#011loss=9.799596\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:25 INFO 139753018754432] processed a total of 672 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458044.478324, \"EndTime\": 1631458045.059637, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 580.756425857544, \"count\": 1, \"min\": 580.756425857544, \"max\": 580.756425857544}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:25 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=1156.8622677612898 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:25 INFO 139753018754432] #progress_metric: host=algo-1, completed 41.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:25 INFO 139753018754432] #quality_metric: host=algo-1, epoch=81, train loss <loss>=9.99118796261874\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:25 INFO 139753018754432] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:25 INFO 139753018754432] Epoch[82] Batch[0] avg_epoch_loss=9.512371\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:25 INFO 139753018754432] #quality_metric: host=algo-1, epoch=82, batch=0 train loss <loss>=9.512371063232422\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:25 INFO 139753018754432] Epoch[82] Batch[5] avg_epoch_loss=9.754818\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:25 INFO 139753018754432] #quality_metric: host=algo-1, epoch=82, batch=5 train loss <loss>=9.754817962646484\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:25 INFO 139753018754432] Epoch[82] Batch [5]#011Speed: 1738.38 samples/sec#011loss=9.754818\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:25 INFO 139753018754432] Epoch[82] Batch[10] avg_epoch_loss=9.807518\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:25 INFO 139753018754432] #quality_metric: host=algo-1, epoch=82, batch=10 train loss <loss>=9.870757293701171\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:25 INFO 139753018754432] Epoch[82] Batch [10]#011Speed: 1687.40 samples/sec#011loss=9.870757\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:25 INFO 139753018754432] processed a total of 712 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458045.0597246, \"EndTime\": 1631458045.7315476, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 671.337366104126, \"count\": 1, \"min\": 671.337366104126, \"max\": 671.337366104126}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:25 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=1060.363986011627 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:25 INFO 139753018754432] #progress_metric: host=algo-1, completed 41.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:25 INFO 139753018754432] #quality_metric: host=algo-1, epoch=82, train loss <loss>=10.450686931610107\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:25 INFO 139753018754432] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:26 INFO 139753018754432] Epoch[83] Batch[0] avg_epoch_loss=9.371031\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:26 INFO 139753018754432] #quality_metric: host=algo-1, epoch=83, batch=0 train loss <loss>=9.371030807495117\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:26 INFO 139753018754432] Epoch[83] Batch[5] avg_epoch_loss=10.238356\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:26 INFO 139753018754432] #quality_metric: host=algo-1, epoch=83, batch=5 train loss <loss>=10.238355795542398\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:26 INFO 139753018754432] Epoch[83] Batch [5]#011Speed: 2108.00 samples/sec#011loss=10.238356\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:26 INFO 139753018754432] Epoch[83] Batch[10] avg_epoch_loss=10.052802\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:26 INFO 139753018754432] #quality_metric: host=algo-1, epoch=83, batch=10 train loss <loss>=9.830138015747071\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:26 INFO 139753018754432] Epoch[83] Batch [10]#011Speed: 1831.94 samples/sec#011loss=9.830138\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:26 INFO 139753018754432] processed a total of 647 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458045.731638, \"EndTime\": 1631458046.347154, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 614.9628162384033, \"count\": 1, \"min\": 614.9628162384033, \"max\": 614.9628162384033}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:26 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=1051.8865161026492 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:26 INFO 139753018754432] #progress_metric: host=algo-1, completed 42.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:26 INFO 139753018754432] #quality_metric: host=algo-1, epoch=83, train loss <loss>=10.052802259271795\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:26 INFO 139753018754432] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:26 INFO 139753018754432] Epoch[84] Batch[0] avg_epoch_loss=9.265635\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:26 INFO 139753018754432] #quality_metric: host=algo-1, epoch=84, batch=0 train loss <loss>=9.265634536743164\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:26 INFO 139753018754432] Epoch[84] Batch[5] avg_epoch_loss=9.820578\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:26 INFO 139753018754432] #quality_metric: host=algo-1, epoch=84, batch=5 train loss <loss>=9.820578416188559\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:26 INFO 139753018754432] Epoch[84] Batch [5]#011Speed: 1949.55 samples/sec#011loss=9.820578\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:26 INFO 139753018754432] processed a total of 626 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458046.3472385, \"EndTime\": 1631458046.9168453, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 569.1149234771729, \"count\": 1, \"min\": 569.1149234771729, \"max\": 569.1149234771729}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:26 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=1099.6992804049937 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:26 INFO 139753018754432] #progress_metric: host=algo-1, completed 42.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:26 INFO 139753018754432] #quality_metric: host=algo-1, epoch=84, train loss <loss>=9.995167541503907\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:26 INFO 139753018754432] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:27 INFO 139753018754432] Epoch[85] Batch[0] avg_epoch_loss=9.708350\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:27 INFO 139753018754432] #quality_metric: host=algo-1, epoch=85, batch=0 train loss <loss>=9.70835018157959\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:27 INFO 139753018754432] Epoch[85] Batch[5] avg_epoch_loss=9.872369\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:27 INFO 139753018754432] #quality_metric: host=algo-1, epoch=85, batch=5 train loss <loss>=9.872369130452475\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:27 INFO 139753018754432] Epoch[85] Batch [5]#011Speed: 2038.51 samples/sec#011loss=9.872369\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:27 INFO 139753018754432] Epoch[85] Batch[10] avg_epoch_loss=9.882606\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:27 INFO 139753018754432] #quality_metric: host=algo-1, epoch=85, batch=10 train loss <loss>=9.894889450073242\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:27 INFO 139753018754432] Epoch[85] Batch [10]#011Speed: 1806.65 samples/sec#011loss=9.894889\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:27 INFO 139753018754432] processed a total of 662 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458046.916937, \"EndTime\": 1631458047.511747, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 594.2516326904297, \"count\": 1, \"min\": 594.2516326904297, \"max\": 594.2516326904297}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:27 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=1113.7724897161847 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:27 INFO 139753018754432] #progress_metric: host=algo-1, completed 43.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:27 INFO 139753018754432] #quality_metric: host=algo-1, epoch=85, train loss <loss>=9.882605639371006\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:27 INFO 139753018754432] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:27 INFO 139753018754432] Epoch[86] Batch[0] avg_epoch_loss=11.123814\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:27 INFO 139753018754432] #quality_metric: host=algo-1, epoch=86, batch=0 train loss <loss>=11.12381362915039\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:27 INFO 139753018754432] Epoch[86] Batch[5] avg_epoch_loss=10.067371\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:27 INFO 139753018754432] #quality_metric: host=algo-1, epoch=86, batch=5 train loss <loss>=10.067370891571045\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:27 INFO 139753018754432] Epoch[86] Batch [5]#011Speed: 2108.39 samples/sec#011loss=10.067371\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:28 INFO 139753018754432] Epoch[86] Batch[10] avg_epoch_loss=10.272962\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:28 INFO 139753018754432] #quality_metric: host=algo-1, epoch=86, batch=10 train loss <loss>=10.51967124938965\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:28 INFO 139753018754432] Epoch[86] Batch [10]#011Speed: 1692.07 samples/sec#011loss=10.519671\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:28 INFO 139753018754432] processed a total of 646 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458047.5118334, \"EndTime\": 1631458048.1185124, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 606.1930656433105, \"count\": 1, \"min\": 606.1930656433105, \"max\": 606.1930656433105}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:28 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=1065.483541460545 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:28 INFO 139753018754432] #progress_metric: host=algo-1, completed 43.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:28 INFO 139753018754432] #quality_metric: host=algo-1, epoch=86, train loss <loss>=10.272961963306773\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:28 INFO 139753018754432] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:28 INFO 139753018754432] Epoch[87] Batch[0] avg_epoch_loss=9.504551\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:28 INFO 139753018754432] #quality_metric: host=algo-1, epoch=87, batch=0 train loss <loss>=9.50455093383789\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:28 INFO 139753018754432] Epoch[87] Batch[5] avg_epoch_loss=9.880412\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:28 INFO 139753018754432] #quality_metric: host=algo-1, epoch=87, batch=5 train loss <loss>=9.880412419637045\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:28 INFO 139753018754432] Epoch[87] Batch [5]#011Speed: 2015.31 samples/sec#011loss=9.880412\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:28 INFO 139753018754432] processed a total of 627 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458048.1185784, \"EndTime\": 1631458048.668686, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 549.5836734771729, \"count\": 1, \"min\": 549.5836734771729, \"max\": 549.5836734771729}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:28 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=1140.6166705701721 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:28 INFO 139753018754432] #progress_metric: host=algo-1, completed 44.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:28 INFO 139753018754432] #quality_metric: host=algo-1, epoch=87, train loss <loss>=9.651956939697266\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:28 INFO 139753018754432] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:28 INFO 139753018754432] Epoch[88] Batch[0] avg_epoch_loss=10.206473\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:28 INFO 139753018754432] #quality_metric: host=algo-1, epoch=88, batch=0 train loss <loss>=10.206473350524902\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:29 INFO 139753018754432] Epoch[88] Batch[5] avg_epoch_loss=9.838939\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:29 INFO 139753018754432] #quality_metric: host=algo-1, epoch=88, batch=5 train loss <loss>=9.838938554128012\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:29 INFO 139753018754432] Epoch[88] Batch [5]#011Speed: 1930.95 samples/sec#011loss=9.838939\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:29 INFO 139753018754432] Epoch[88] Batch[10] avg_epoch_loss=9.642744\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:29 INFO 139753018754432] #quality_metric: host=algo-1, epoch=88, batch=10 train loss <loss>=9.407311344146729\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:29 INFO 139753018754432] Epoch[88] Batch [10]#011Speed: 1901.66 samples/sec#011loss=9.407311\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:29 INFO 139753018754432] processed a total of 663 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458048.6687703, \"EndTime\": 1631458049.26061, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 591.2628173828125, \"count\": 1, \"min\": 591.2628173828125, \"max\": 591.2628173828125}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:29 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=1121.0932308038398 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:29 INFO 139753018754432] #progress_metric: host=algo-1, completed 44.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:29 INFO 139753018754432] #quality_metric: host=algo-1, epoch=88, train loss <loss>=9.642744367772883\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:29 INFO 139753018754432] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:29 INFO 139753018754432] Epoch[89] Batch[0] avg_epoch_loss=9.403181\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:29 INFO 139753018754432] #quality_metric: host=algo-1, epoch=89, batch=0 train loss <loss>=9.403181076049805\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:29 INFO 139753018754432] Epoch[89] Batch[5] avg_epoch_loss=9.735927\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:29 INFO 139753018754432] #quality_metric: host=algo-1, epoch=89, batch=5 train loss <loss>=9.735926946004232\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:29 INFO 139753018754432] Epoch[89] Batch [5]#011Speed: 2051.65 samples/sec#011loss=9.735927\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:29 INFO 139753018754432] processed a total of 630 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458049.2606957, \"EndTime\": 1631458049.8000908, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 538.7899875640869, \"count\": 1, \"min\": 538.7899875640869, \"max\": 538.7899875640869}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:29 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=1168.9825993823317 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:29 INFO 139753018754432] #progress_metric: host=algo-1, completed 45.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:29 INFO 139753018754432] #quality_metric: host=algo-1, epoch=89, train loss <loss>=9.743365859985351\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:29 INFO 139753018754432] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:30 INFO 139753018754432] Epoch[90] Batch[0] avg_epoch_loss=9.970828\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:30 INFO 139753018754432] #quality_metric: host=algo-1, epoch=90, batch=0 train loss <loss>=9.97082805633545\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:30 INFO 139753018754432] Epoch[90] Batch[5] avg_epoch_loss=9.928875\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:30 INFO 139753018754432] #quality_metric: host=algo-1, epoch=90, batch=5 train loss <loss>=9.928874651590982\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:30 INFO 139753018754432] Epoch[90] Batch [5]#011Speed: 1744.02 samples/sec#011loss=9.928875\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:30 INFO 139753018754432] processed a total of 575 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458049.800189, \"EndTime\": 1631458050.332152, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 531.4145088195801, \"count\": 1, \"min\": 531.4145088195801, \"max\": 531.4145088195801}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:30 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=1081.7611394415555 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:30 INFO 139753018754432] #progress_metric: host=algo-1, completed 45.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:30 INFO 139753018754432] #quality_metric: host=algo-1, epoch=90, train loss <loss>=9.826532681783041\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:30 INFO 139753018754432] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:30 INFO 139753018754432] Epoch[91] Batch[0] avg_epoch_loss=10.724751\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:30 INFO 139753018754432] #quality_metric: host=algo-1, epoch=91, batch=0 train loss <loss>=10.724751472473145\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:30 INFO 139753018754432] Epoch[91] Batch[5] avg_epoch_loss=10.403173\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:30 INFO 139753018754432] #quality_metric: host=algo-1, epoch=91, batch=5 train loss <loss>=10.403172810872396\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:30 INFO 139753018754432] Epoch[91] Batch [5]#011Speed: 2105.99 samples/sec#011loss=10.403173\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:30 INFO 139753018754432] processed a total of 620 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458050.33224, \"EndTime\": 1631458050.8913054, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 558.539867401123, \"count\": 1, \"min\": 558.539867401123, \"max\": 558.539867401123}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:30 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=1109.7761259317772 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:30 INFO 139753018754432] #progress_metric: host=algo-1, completed 46.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:30 INFO 139753018754432] #quality_metric: host=algo-1, epoch=91, train loss <loss>=10.367809677124024\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:30 INFO 139753018754432] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:31 INFO 139753018754432] Epoch[92] Batch[0] avg_epoch_loss=8.945981\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:31 INFO 139753018754432] #quality_metric: host=algo-1, epoch=92, batch=0 train loss <loss>=8.9459810256958\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:31 INFO 139753018754432] Epoch[92] Batch[5] avg_epoch_loss=9.816707\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:31 INFO 139753018754432] #quality_metric: host=algo-1, epoch=92, batch=5 train loss <loss>=9.816706975301107\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:31 INFO 139753018754432] Epoch[92] Batch [5]#011Speed: 2065.32 samples/sec#011loss=9.816707\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:31 INFO 139753018754432] Epoch[92] Batch[10] avg_epoch_loss=10.194569\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:31 INFO 139753018754432] #quality_metric: host=algo-1, epoch=92, batch=10 train loss <loss>=10.648003768920898\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:31 INFO 139753018754432] Epoch[92] Batch [10]#011Speed: 1847.91 samples/sec#011loss=10.648004\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:31 INFO 139753018754432] processed a total of 650 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458050.8913963, \"EndTime\": 1631458051.5058706, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 613.9614582061768, \"count\": 1, \"min\": 613.9614582061768, \"max\": 613.9614582061768}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:31 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=1058.4829937123034 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:31 INFO 139753018754432] #progress_metric: host=algo-1, completed 46.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:31 INFO 139753018754432] #quality_metric: host=algo-1, epoch=92, train loss <loss>=10.194569154219193\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:31 INFO 139753018754432] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:31 INFO 139753018754432] Epoch[93] Batch[0] avg_epoch_loss=9.357523\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:31 INFO 139753018754432] #quality_metric: host=algo-1, epoch=93, batch=0 train loss <loss>=9.357522964477539\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:31 INFO 139753018754432] Epoch[93] Batch[5] avg_epoch_loss=9.879882\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:31 INFO 139753018754432] #quality_metric: host=algo-1, epoch=93, batch=5 train loss <loss>=9.879882335662842\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:31 INFO 139753018754432] Epoch[93] Batch [5]#011Speed: 1754.71 samples/sec#011loss=9.879882\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:32 INFO 139753018754432] Epoch[93] Batch[10] avg_epoch_loss=9.707538\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:32 INFO 139753018754432] #quality_metric: host=algo-1, epoch=93, batch=10 train loss <loss>=9.500724601745606\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:32 INFO 139753018754432] Epoch[93] Batch [10]#011Speed: 1700.62 samples/sec#011loss=9.500725\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:32 INFO 139753018754432] processed a total of 659 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458051.5059576, \"EndTime\": 1631458052.1677258, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 661.2739562988281, \"count\": 1, \"min\": 661.2739562988281, \"max\": 661.2739562988281}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:32 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=996.3748051171856 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:32 INFO 139753018754432] #progress_metric: host=algo-1, completed 47.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:32 INFO 139753018754432] #quality_metric: host=algo-1, epoch=93, train loss <loss>=9.707537911155008\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:32 INFO 139753018754432] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:32 INFO 139753018754432] Epoch[94] Batch[0] avg_epoch_loss=9.573078\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:32 INFO 139753018754432] #quality_metric: host=algo-1, epoch=94, batch=0 train loss <loss>=9.573078155517578\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:32 INFO 139753018754432] Epoch[94] Batch[5] avg_epoch_loss=9.737644\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:32 INFO 139753018754432] #quality_metric: host=algo-1, epoch=94, batch=5 train loss <loss>=9.737643718719482\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:32 INFO 139753018754432] Epoch[94] Batch [5]#011Speed: 2008.10 samples/sec#011loss=9.737644\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:32 INFO 139753018754432] Epoch[94] Batch[10] avg_epoch_loss=9.664960\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:32 INFO 139753018754432] #quality_metric: host=algo-1, epoch=94, batch=10 train loss <loss>=9.577740478515626\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:32 INFO 139753018754432] Epoch[94] Batch [10]#011Speed: 1782.71 samples/sec#011loss=9.577740\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:32 INFO 139753018754432] processed a total of 669 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458052.1678102, \"EndTime\": 1631458052.7789085, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 610.6040477752686, \"count\": 1, \"min\": 610.6040477752686, \"max\": 610.6040477752686}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:32 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=1095.4092481622743 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:32 INFO 139753018754432] #progress_metric: host=algo-1, completed 47.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:32 INFO 139753018754432] #quality_metric: host=algo-1, epoch=94, train loss <loss>=9.664960427717729\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:32 INFO 139753018754432] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:33 INFO 139753018754432] Epoch[95] Batch[0] avg_epoch_loss=10.687354\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:33 INFO 139753018754432] #quality_metric: host=algo-1, epoch=95, batch=0 train loss <loss>=10.68735408782959\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:33 INFO 139753018754432] Epoch[95] Batch[5] avg_epoch_loss=9.753097\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:33 INFO 139753018754432] #quality_metric: host=algo-1, epoch=95, batch=5 train loss <loss>=9.75309673945109\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:33 INFO 139753018754432] Epoch[95] Batch [5]#011Speed: 1642.94 samples/sec#011loss=9.753097\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:33 INFO 139753018754432] processed a total of 606 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458052.7789955, \"EndTime\": 1631458053.368793, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 589.2815589904785, \"count\": 1, \"min\": 589.2815589904785, \"max\": 589.2815589904785}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:33 INFO 139753018754432] #throughput_metric: host=algo-1, train throughput=1028.1425158008635 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:33 INFO 139753018754432] #progress_metric: host=algo-1, completed 48.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:33 INFO 139753018754432] #quality_metric: host=algo-1, epoch=95, train loss <loss>=10.227310371398925\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:33 INFO 139753018754432] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:33 INFO 139753018754432] Loading parameters from best epoch (55)\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458053.3688834, \"EndTime\": 1631458053.3786786, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.deserialize.time\": {\"sum\": 9.150505065917969, \"count\": 1, \"min\": 9.150505065917969, \"max\": 9.150505065917969}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:33 INFO 139753018754432] stopping training now\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:33 INFO 139753018754432] #progress_metric: host=algo-1, completed 100 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:33 INFO 139753018754432] Final loss: 9.21424302187833 (occurred at epoch 55)\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:33 INFO 139753018754432] #quality_metric: host=algo-1, train final_loss <loss>=9.21424302187833\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:33 INFO 139753018754432] Worker algo-1 finished training.\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:33 WARNING 139753018754432] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:33 INFO 139753018754432] All workers finished. Serializing model for prediction.\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458053.3787556, \"EndTime\": 1631458053.5164866, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"get_graph.time\": {\"sum\": 136.97457313537598, \"count\": 1, \"min\": 136.97457313537598, \"max\": 136.97457313537598}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:33 INFO 139753018754432] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458053.5165591, \"EndTime\": 1631458053.5499084, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"finalize.time\": {\"sum\": 170.43709754943848, \"count\": 1, \"min\": 170.43709754943848, \"max\": 170.43709754943848}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:33 INFO 139753018754432] Serializing to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:33 INFO 139753018754432] Saved checkpoint to \"/opt/ml/model/model_algo-1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458053.5499902, \"EndTime\": 1631458053.5554464, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"model.serialize.time\": {\"sum\": 5.411863327026367, \"count\": 1, \"min\": 5.411863327026367, \"max\": 5.411863327026367}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:33 INFO 139753018754432] Successfully serialized the model for prediction.\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:47:33 INFO 139753018754432] No test data passed, skipping evaluation.\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458053.5554962, \"EndTime\": 1631458053.5611591, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"setuptime\": {\"sum\": 7.255792617797852, \"count\": 1, \"min\": 7.255792617797852, \"max\": 7.255792617797852}, \"totaltime\": {\"sum\": 58051.97024345398, \"count\": 1, \"min\": 58051.97024345398, \"max\": 58051.97024345398}}}\n",
      "\u001b[0m\n",
      "\n",
      "2021-09-12 14:47:38 Uploading - Uploading generated training model\n",
      "2021-09-12 14:48:06 Completed - Training job completed\n",
      "ProfilerReport-1631457822: NoIssuesFound\n",
      "Training seconds: 122\n",
      "Billable seconds: 122\n",
      "-------------!Overwriting existing file\n",
      "Uploading file to s3://sagemaker-us-east-2-222195595958/deepar-covid-demo-notebook/data/train/train.json\n",
      "2021-09-12 14:54:58 Starting - Starting the training job...\n",
      "2021-09-12 14:55:00 Starting - Launching requested ML instancesProfilerReport-1631458498: InProgress\n",
      "......\n",
      "2021-09-12 14:56:17 Starting - Preparing the instances for training......\n",
      "2021-09-12 14:57:23 Downloading - Downloading input data...\n",
      "2021-09-12 14:57:57 Training - Downloading the training image...\n",
      "2021-09-12 14:58:17 Training - Training image download completed. Training in progress.\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:14 INFO 140277876397440] Reading default configuration from /opt/amazon/lib/python3.6/site-packages/algorithm/resources/default-input.json: {'_kvstore': 'auto', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_tuning_objective_metric': '', 'cardinality': 'auto', 'dropout_rate': '0.10', 'early_stopping_patience': '', 'embedding_dimension': '10', 'learning_rate': '0.001', 'likelihood': 'student-t', 'mini_batch_size': '128', 'num_cells': '40', 'num_dynamic_feat': 'auto', 'num_eval_samples': '100', 'num_layers': '2', 'test_quantiles': '[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]'}\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:14 INFO 140277876397440] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {'prediction_length': '14', 'time_freq': 'D', 'context_length': '14', 'epochs': '200', 'learning_rate': '5E-4', 'early_stopping_patience': '40', 'mini_batch_size': '64'}\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:14 INFO 140277876397440] Final configuration: {'_kvstore': 'auto', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_tuning_objective_metric': '', 'cardinality': 'auto', 'dropout_rate': '0.10', 'early_stopping_patience': '40', 'embedding_dimension': '10', 'learning_rate': '5E-4', 'likelihood': 'student-t', 'mini_batch_size': '64', 'num_cells': '40', 'num_dynamic_feat': 'auto', 'num_eval_samples': '100', 'num_layers': '2', 'test_quantiles': '[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', 'prediction_length': '14', 'time_freq': 'D', 'context_length': '14', 'epochs': '200'}\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:14 INFO 140277876397440] Detected entry point for worker worker\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:14 INFO 140277876397440] Using early stopping with patience 40\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:14 INFO 140277876397440] random_seed is None\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:14 INFO 140277876397440] [cardinality=auto] `cat` field was NOT found in the file `/opt/ml/input/data/train/train.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:14 INFO 140277876397440] [num_dynamic_feat=auto] `dynamic_feat` field was NOT found in the file `/opt/ml/input/data/train/train.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:14 INFO 140277876397440] Training set statistics:\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:14 INFO 140277876397440] Integer time series\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:14 INFO 140277876397440] number of time series: 41\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:14 INFO 140277876397440] number of observations: 13490\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:14 INFO 140277876397440] mean target length: 329.0243902439024\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:14 INFO 140277876397440] min/mean/max target: 0.0/15156836.111489993/348966432.0\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:14 INFO 140277876397440] mean abs(target): 15156836.111489993\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:14 INFO 140277876397440] contains missing values: no\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:14 INFO 140277876397440] Small number of time series. Doing 16 passes over dataset with prob 0.975609756097561 per epoch.\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:14 INFO 140277876397440] No test channel found not running evaluations\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:14 INFO 140277876397440] #memory_usage::<batchbuffer> = 2.2568130493164062 mb\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:14 INFO 140277876397440] nvidia-smi took: 0.02534651756286621 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:14 INFO 140277876397440] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:14 INFO 140277876397440] Create Store: local\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458694.4257836, \"EndTime\": 1631458694.5118334, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"get_graph.time\": {\"sum\": 83.97603034973145, \"count\": 1, \"min\": 83.97603034973145, \"max\": 83.97603034973145}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:14 INFO 140277876397440] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:14 INFO 140277876397440] #memory_usage::<model> = 12 mb\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458694.511905, \"EndTime\": 1631458694.6165833, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"initialize.time\": {\"sum\": 190.68098068237305, \"count\": 1, \"min\": 190.68098068237305, \"max\": 190.68098068237305}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:14 INFO 140277876397440] Epoch[0] Batch[0] avg_epoch_loss=14.933855\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:14 INFO 140277876397440] #quality_metric: host=algo-1, epoch=0, batch=0 train loss <loss>=14.933855056762695\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:15 INFO 140277876397440] Epoch[0] Batch[5] avg_epoch_loss=14.153173\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:15 INFO 140277876397440] #quality_metric: host=algo-1, epoch=0, batch=5 train loss <loss>=14.153173446655273\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:15 INFO 140277876397440] Epoch[0] Batch [5]#011Speed: 2091.09 samples/sec#011loss=14.153173\u001b[0m\n",
      "\u001b[34m/opt/amazon/python3.6/lib/python3.6/contextlib.py:99: DeprecationWarning: generator 'local_timer' raised StopIteration\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:15 INFO 140277876397440] processed a total of 640 examples\n",
      "  self.gen.throw(type, value, traceback)\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458694.6166456, \"EndTime\": 1631458695.24803, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"epochs\": {\"sum\": 200.0, \"count\": 1, \"min\": 200, \"max\": 200}, \"update.time\": {\"sum\": 631.3016414642334, \"count\": 1, \"min\": 631.3016414642334, \"max\": 631.3016414642334}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:15 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1013.5491661223273 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:15 INFO 140277876397440] #progress_metric: host=algo-1, completed 0.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:15 INFO 140277876397440] #quality_metric: host=algo-1, epoch=0, train loss <loss>=13.7101900100708\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:15 INFO 140277876397440] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:15 INFO 140277876397440] Saved checkpoint to \"/opt/ml/model/state_277e7e08-bb1a-4aeb-b50c-477c392486ea-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458695.2481303, \"EndTime\": 1631458695.268715, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 20.027875900268555, \"count\": 1, \"min\": 20.027875900268555, \"max\": 20.027875900268555}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:15 INFO 140277876397440] Epoch[1] Batch[0] avg_epoch_loss=12.648597\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:15 INFO 140277876397440] #quality_metric: host=algo-1, epoch=1, batch=0 train loss <loss>=12.64859676361084\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:15 INFO 140277876397440] Epoch[1] Batch[5] avg_epoch_loss=12.859846\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:15 INFO 140277876397440] #quality_metric: host=algo-1, epoch=1, batch=5 train loss <loss>=12.859845956166586\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:15 INFO 140277876397440] Epoch[1] Batch [5]#011Speed: 2083.32 samples/sec#011loss=12.859846\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:15 INFO 140277876397440] Epoch[1] Batch[10] avg_epoch_loss=13.326890\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:15 INFO 140277876397440] #quality_metric: host=algo-1, epoch=1, batch=10 train loss <loss>=13.887343597412109\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:15 INFO 140277876397440] Epoch[1] Batch [10]#011Speed: 1957.52 samples/sec#011loss=13.887344\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:15 INFO 140277876397440] processed a total of 651 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458695.2687843, \"EndTime\": 1631458695.8713174, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 602.4792194366455, \"count\": 1, \"min\": 602.4792194366455, \"max\": 602.4792194366455}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:15 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1080.3265654296847 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:15 INFO 140277876397440] #progress_metric: host=algo-1, completed 1.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:15 INFO 140277876397440] #quality_metric: host=algo-1, epoch=1, train loss <loss>=13.326890338550914\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:15 INFO 140277876397440] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:15 INFO 140277876397440] Saved checkpoint to \"/opt/ml/model/state_62250c87-3a5d-4ce0-a516-da00cdd34687-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458695.871401, \"EndTime\": 1631458695.8906584, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 18.729448318481445, \"count\": 1, \"min\": 18.729448318481445, \"max\": 18.729448318481445}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:16 INFO 140277876397440] Epoch[2] Batch[0] avg_epoch_loss=13.540771\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:16 INFO 140277876397440] #quality_metric: host=algo-1, epoch=2, batch=0 train loss <loss>=13.540771484375\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:16 INFO 140277876397440] Epoch[2] Batch[5] avg_epoch_loss=12.578814\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:16 INFO 140277876397440] #quality_metric: host=algo-1, epoch=2, batch=5 train loss <loss>=12.578813870747885\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:16 INFO 140277876397440] Epoch[2] Batch [5]#011Speed: 2045.57 samples/sec#011loss=12.578814\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:16 INFO 140277876397440] Epoch[2] Batch[10] avg_epoch_loss=12.238396\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:16 INFO 140277876397440] #quality_metric: host=algo-1, epoch=2, batch=10 train loss <loss>=11.829893684387207\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:16 INFO 140277876397440] Epoch[2] Batch [10]#011Speed: 1706.87 samples/sec#011loss=11.829894\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:16 INFO 140277876397440] processed a total of 675 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458695.890729, \"EndTime\": 1631458696.520337, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 629.4703483581543, \"count\": 1, \"min\": 629.4703483581543, \"max\": 629.4703483581543}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:16 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1072.1124418057761 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:16 INFO 140277876397440] #progress_metric: host=algo-1, completed 1.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:16 INFO 140277876397440] #quality_metric: host=algo-1, epoch=2, train loss <loss>=12.238395604220303\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:16 INFO 140277876397440] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:16 INFO 140277876397440] Saved checkpoint to \"/opt/ml/model/state_63c43758-fbe2-4aa6-82fb-48ad6d3c5141-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458696.5204208, \"EndTime\": 1631458696.5394247, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 18.506765365600586, \"count\": 1, \"min\": 18.506765365600586, \"max\": 18.506765365600586}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:16 INFO 140277876397440] Epoch[3] Batch[0] avg_epoch_loss=12.395502\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:16 INFO 140277876397440] #quality_metric: host=algo-1, epoch=3, batch=0 train loss <loss>=12.395502090454102\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:16 INFO 140277876397440] Epoch[3] Batch[5] avg_epoch_loss=12.165768\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:16 INFO 140277876397440] #quality_metric: host=algo-1, epoch=3, batch=5 train loss <loss>=12.165768305460611\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:16 INFO 140277876397440] Epoch[3] Batch [5]#011Speed: 1913.60 samples/sec#011loss=12.165768\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:17 INFO 140277876397440] Epoch[3] Batch[10] avg_epoch_loss=12.176456\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:17 INFO 140277876397440] #quality_metric: host=algo-1, epoch=3, batch=10 train loss <loss>=12.189280891418457\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:17 INFO 140277876397440] Epoch[3] Batch [10]#011Speed: 1918.05 samples/sec#011loss=12.189281\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:17 INFO 140277876397440] processed a total of 676 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458696.5394921, \"EndTime\": 1631458697.1397805, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 600.2304553985596, \"count\": 1, \"min\": 600.2304553985596, \"max\": 600.2304553985596}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:17 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1125.993016923952 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:17 INFO 140277876397440] #progress_metric: host=algo-1, completed 2.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:17 INFO 140277876397440] #quality_metric: host=algo-1, epoch=3, train loss <loss>=12.17645584453236\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:17 INFO 140277876397440] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:17 INFO 140277876397440] Saved checkpoint to \"/opt/ml/model/state_79f97732-42f1-4e84-9206-92ce25c6f0c7-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458697.1398716, \"EndTime\": 1631458697.1531942, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 12.823104858398438, \"count\": 1, \"min\": 12.823104858398438, \"max\": 12.823104858398438}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:17 INFO 140277876397440] Epoch[4] Batch[0] avg_epoch_loss=11.507617\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:17 INFO 140277876397440] #quality_metric: host=algo-1, epoch=4, batch=0 train loss <loss>=11.507616996765137\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:17 INFO 140277876397440] Epoch[4] Batch[5] avg_epoch_loss=11.691660\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:17 INFO 140277876397440] #quality_metric: host=algo-1, epoch=4, batch=5 train loss <loss>=11.691659927368164\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:17 INFO 140277876397440] Epoch[4] Batch [5]#011Speed: 1643.12 samples/sec#011loss=11.691660\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:17 INFO 140277876397440] processed a total of 603 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458697.153259, \"EndTime\": 1631458697.7595813, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 606.2648296356201, \"count\": 1, \"min\": 606.2648296356201, \"max\": 606.2648296356201}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:17 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=994.4197049727172 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:17 INFO 140277876397440] #progress_metric: host=algo-1, completed 2.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:17 INFO 140277876397440] #quality_metric: host=algo-1, epoch=4, train loss <loss>=11.76639928817749\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:17 INFO 140277876397440] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:17 INFO 140277876397440] Saved checkpoint to \"/opt/ml/model/state_b540bc37-02b2-4f21-8c11-8a8dc824531f-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458697.7596622, \"EndTime\": 1631458697.7716641, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 11.394023895263672, \"count\": 1, \"min\": 11.394023895263672, \"max\": 11.394023895263672}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:18 INFO 140277876397440] Epoch[5] Batch[0] avg_epoch_loss=10.992550\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:18 INFO 140277876397440] #quality_metric: host=algo-1, epoch=5, batch=0 train loss <loss>=10.992549896240234\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:18 INFO 140277876397440] Epoch[5] Batch[5] avg_epoch_loss=11.527435\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:18 INFO 140277876397440] #quality_metric: host=algo-1, epoch=5, batch=5 train loss <loss>=11.527434825897217\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:18 INFO 140277876397440] Epoch[5] Batch [5]#011Speed: 1709.54 samples/sec#011loss=11.527435\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:18 INFO 140277876397440] Epoch[5] Batch[10] avg_epoch_loss=11.863660\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:18 INFO 140277876397440] #quality_metric: host=algo-1, epoch=5, batch=10 train loss <loss>=12.267131042480468\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:18 INFO 140277876397440] Epoch[5] Batch [10]#011Speed: 2013.67 samples/sec#011loss=12.267131\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:18 INFO 140277876397440] processed a total of 645 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458697.7717395, \"EndTime\": 1631458698.3789747, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 607.1765422821045, \"count\": 1, \"min\": 607.1765422821045, \"max\": 607.1765422821045}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:18 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1062.0984063770045 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:18 INFO 140277876397440] #progress_metric: host=algo-1, completed 3.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:18 INFO 140277876397440] #quality_metric: host=algo-1, epoch=5, train loss <loss>=11.863660378889604\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:18 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:18 INFO 140277876397440] Epoch[6] Batch[0] avg_epoch_loss=12.176056\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:18 INFO 140277876397440] #quality_metric: host=algo-1, epoch=6, batch=0 train loss <loss>=12.176055908203125\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:18 INFO 140277876397440] Epoch[6] Batch[5] avg_epoch_loss=11.108238\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:18 INFO 140277876397440] #quality_metric: host=algo-1, epoch=6, batch=5 train loss <loss>=11.108238379160563\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:18 INFO 140277876397440] Epoch[6] Batch [5]#011Speed: 2092.70 samples/sec#011loss=11.108238\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:18 INFO 140277876397440] Epoch[6] Batch[10] avg_epoch_loss=11.165216\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:18 INFO 140277876397440] #quality_metric: host=algo-1, epoch=6, batch=10 train loss <loss>=11.233588981628419\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:18 INFO 140277876397440] Epoch[6] Batch [10]#011Speed: 1922.50 samples/sec#011loss=11.233589\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:18 INFO 140277876397440] processed a total of 644 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458698.379054, \"EndTime\": 1631458698.94468, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 565.133810043335, \"count\": 1, \"min\": 565.133810043335, \"max\": 565.133810043335}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:18 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1139.3224108252705 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:18 INFO 140277876397440] #progress_metric: host=algo-1, completed 3.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:18 INFO 140277876397440] #quality_metric: host=algo-1, epoch=6, train loss <loss>=11.165215925736861\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:18 INFO 140277876397440] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:18 INFO 140277876397440] Saved checkpoint to \"/opt/ml/model/state_8e467e49-667a-491b-b6b1-f38fbc9d8504-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458698.9447584, \"EndTime\": 1631458698.963434, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 18.190383911132812, \"count\": 1, \"min\": 18.190383911132812, \"max\": 18.190383911132812}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:19 INFO 140277876397440] Epoch[7] Batch[0] avg_epoch_loss=11.325264\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:19 INFO 140277876397440] #quality_metric: host=algo-1, epoch=7, batch=0 train loss <loss>=11.325263977050781\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:19 INFO 140277876397440] Epoch[7] Batch[5] avg_epoch_loss=11.502670\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:19 INFO 140277876397440] #quality_metric: host=algo-1, epoch=7, batch=5 train loss <loss>=11.502670129140219\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:19 INFO 140277876397440] Epoch[7] Batch [5]#011Speed: 1765.09 samples/sec#011loss=11.502670\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:19 INFO 140277876397440] Epoch[7] Batch[10] avg_epoch_loss=11.334198\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:19 INFO 140277876397440] #quality_metric: host=algo-1, epoch=7, batch=10 train loss <loss>=11.132032012939453\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:19 INFO 140277876397440] Epoch[7] Batch [10]#011Speed: 1570.87 samples/sec#011loss=11.132032\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:19 INFO 140277876397440] processed a total of 644 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458698.9634964, \"EndTime\": 1631458699.6434014, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 679.8508167266846, \"count\": 1, \"min\": 679.8508167266846, \"max\": 679.8508167266846}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:19 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=947.1009032258064 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:19 INFO 140277876397440] #progress_metric: host=algo-1, completed 4.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:19 INFO 140277876397440] #quality_metric: host=algo-1, epoch=7, train loss <loss>=11.334198258139871\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:19 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:19 INFO 140277876397440] Epoch[8] Batch[0] avg_epoch_loss=11.259499\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:19 INFO 140277876397440] #quality_metric: host=algo-1, epoch=8, batch=0 train loss <loss>=11.259498596191406\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:20 INFO 140277876397440] Epoch[8] Batch[5] avg_epoch_loss=10.764437\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:20 INFO 140277876397440] #quality_metric: host=algo-1, epoch=8, batch=5 train loss <loss>=10.764436721801758\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:20 INFO 140277876397440] Epoch[8] Batch [5]#011Speed: 1706.04 samples/sec#011loss=10.764437\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:20 INFO 140277876397440] Epoch[8] Batch[10] avg_epoch_loss=11.553765\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:20 INFO 140277876397440] #quality_metric: host=algo-1, epoch=8, batch=10 train loss <loss>=12.500959014892578\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:20 INFO 140277876397440] Epoch[8] Batch [10]#011Speed: 2025.40 samples/sec#011loss=12.500959\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:20 INFO 140277876397440] processed a total of 644 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458699.643487, \"EndTime\": 1631458700.3231359, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 679.1262626647949, \"count\": 1, \"min\": 679.1262626647949, \"max\": 679.1262626647949}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:20 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=948.1165048816316 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:20 INFO 140277876397440] #progress_metric: host=algo-1, completed 4.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:20 INFO 140277876397440] #quality_metric: host=algo-1, epoch=8, train loss <loss>=11.55376503684304\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:20 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:20 INFO 140277876397440] Epoch[9] Batch[0] avg_epoch_loss=10.713845\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:20 INFO 140277876397440] #quality_metric: host=algo-1, epoch=9, batch=0 train loss <loss>=10.713845252990723\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:20 INFO 140277876397440] Epoch[9] Batch[5] avg_epoch_loss=10.544256\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:20 INFO 140277876397440] #quality_metric: host=algo-1, epoch=9, batch=5 train loss <loss>=10.544256369272867\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:20 INFO 140277876397440] Epoch[9] Batch [5]#011Speed: 2104.19 samples/sec#011loss=10.544256\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:20 INFO 140277876397440] Epoch[9] Batch[10] avg_epoch_loss=10.653357\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:20 INFO 140277876397440] #quality_metric: host=algo-1, epoch=9, batch=10 train loss <loss>=10.784277153015136\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:20 INFO 140277876397440] Epoch[9] Batch [10]#011Speed: 1521.28 samples/sec#011loss=10.784277\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:20 INFO 140277876397440] processed a total of 691 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458700.323214, \"EndTime\": 1631458700.957354, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 633.6731910705566, \"count\": 1, \"min\": 633.6731910705566, \"max\": 633.6731910705566}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:20 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1090.2566210691516 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:20 INFO 140277876397440] #progress_metric: host=algo-1, completed 5.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:20 INFO 140277876397440] #quality_metric: host=algo-1, epoch=9, train loss <loss>=10.653356725519354\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:20 INFO 140277876397440] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:20 INFO 140277876397440] Saved checkpoint to \"/opt/ml/model/state_947d10b7-909f-45d2-ac90-bfb822e3b6d0-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458700.957439, \"EndTime\": 1631458700.9767933, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 18.851518630981445, \"count\": 1, \"min\": 18.851518630981445, \"max\": 18.851518630981445}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:21 INFO 140277876397440] Epoch[10] Batch[0] avg_epoch_loss=11.333416\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:21 INFO 140277876397440] #quality_metric: host=algo-1, epoch=10, batch=0 train loss <loss>=11.333415985107422\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:21 INFO 140277876397440] Epoch[10] Batch[5] avg_epoch_loss=10.702752\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:21 INFO 140277876397440] #quality_metric: host=algo-1, epoch=10, batch=5 train loss <loss>=10.702751954396566\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:21 INFO 140277876397440] Epoch[10] Batch [5]#011Speed: 2050.29 samples/sec#011loss=10.702752\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:21 INFO 140277876397440] processed a total of 635 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458700.9768596, \"EndTime\": 1631458701.5523598, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 575.4439830780029, \"count\": 1, \"min\": 575.4439830780029, \"max\": 575.4439830780029}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:21 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1103.274526722702 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:21 INFO 140277876397440] #progress_metric: host=algo-1, completed 5.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:21 INFO 140277876397440] #quality_metric: host=algo-1, epoch=10, train loss <loss>=10.627990627288819\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:21 INFO 140277876397440] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:21 INFO 140277876397440] Saved checkpoint to \"/opt/ml/model/state_771490b2-f2e0-4ac2-b706-2d2305cdc5d3-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458701.5524387, \"EndTime\": 1631458701.5656326, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 12.633562088012695, \"count\": 1, \"min\": 12.633562088012695, \"max\": 12.633562088012695}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:21 INFO 140277876397440] Epoch[11] Batch[0] avg_epoch_loss=11.051871\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:21 INFO 140277876397440] #quality_metric: host=algo-1, epoch=11, batch=0 train loss <loss>=11.051871299743652\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:22 INFO 140277876397440] Epoch[11] Batch[5] avg_epoch_loss=10.839939\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:22 INFO 140277876397440] #quality_metric: host=algo-1, epoch=11, batch=5 train loss <loss>=10.83993911743164\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:22 INFO 140277876397440] Epoch[11] Batch [5]#011Speed: 1679.33 samples/sec#011loss=10.839939\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:22 INFO 140277876397440] Epoch[11] Batch[10] avg_epoch_loss=10.454101\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:22 INFO 140277876397440] #quality_metric: host=algo-1, epoch=11, batch=10 train loss <loss>=9.991095161437988\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:22 INFO 140277876397440] Epoch[11] Batch [10]#011Speed: 1962.08 samples/sec#011loss=9.991095\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:22 INFO 140277876397440] processed a total of 652 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458701.5657153, \"EndTime\": 1631458702.2062829, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 640.5088901519775, \"count\": 1, \"min\": 640.5088901519775, \"max\": 640.5088901519775}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:22 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1017.7534461513154 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:22 INFO 140277876397440] #progress_metric: host=algo-1, completed 6.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:22 INFO 140277876397440] #quality_metric: host=algo-1, epoch=11, train loss <loss>=10.454100955616344\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:22 INFO 140277876397440] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:22 INFO 140277876397440] Saved checkpoint to \"/opt/ml/model/state_b3380f4c-e329-4796-8d0a-0a00bf0fceb2-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458702.2063665, \"EndTime\": 1631458702.2256787, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 18.774986267089844, \"count\": 1, \"min\": 18.774986267089844, \"max\": 18.774986267089844}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:22 INFO 140277876397440] Epoch[12] Batch[0] avg_epoch_loss=9.745655\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:22 INFO 140277876397440] #quality_metric: host=algo-1, epoch=12, batch=0 train loss <loss>=9.745655059814453\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:22 INFO 140277876397440] Epoch[12] Batch[5] avg_epoch_loss=10.530721\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:22 INFO 140277876397440] #quality_metric: host=algo-1, epoch=12, batch=5 train loss <loss>=10.530721346537272\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:22 INFO 140277876397440] Epoch[12] Batch [5]#011Speed: 2032.04 samples/sec#011loss=10.530721\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:22 INFO 140277876397440] processed a total of 608 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458702.225747, \"EndTime\": 1631458702.7572165, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 531.4161777496338, \"count\": 1, \"min\": 531.4161777496338, \"max\": 531.4161777496338}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:22 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1143.8561097253503 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:22 INFO 140277876397440] #progress_metric: host=algo-1, completed 6.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:22 INFO 140277876397440] #quality_metric: host=algo-1, epoch=12, train loss <loss>=10.470762348175048\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:22 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:23 INFO 140277876397440] Epoch[13] Batch[0] avg_epoch_loss=11.018128\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:23 INFO 140277876397440] #quality_metric: host=algo-1, epoch=13, batch=0 train loss <loss>=11.018128395080566\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:23 INFO 140277876397440] Epoch[13] Batch[5] avg_epoch_loss=10.372710\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:23 INFO 140277876397440] #quality_metric: host=algo-1, epoch=13, batch=5 train loss <loss>=10.37270991007487\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:23 INFO 140277876397440] Epoch[13] Batch [5]#011Speed: 1708.94 samples/sec#011loss=10.372710\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:23 INFO 140277876397440] processed a total of 638 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458702.7573066, \"EndTime\": 1631458703.3157763, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 557.9390525817871, \"count\": 1, \"min\": 557.9390525817871, \"max\": 557.9390525817871}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:23 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1143.2439455576546 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:23 INFO 140277876397440] #progress_metric: host=algo-1, completed 7.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:23 INFO 140277876397440] #quality_metric: host=algo-1, epoch=13, train loss <loss>=10.340427207946778\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:23 INFO 140277876397440] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:23 INFO 140277876397440] Saved checkpoint to \"/opt/ml/model/state_0c0115ee-7670-443f-b4ff-2fe77bada5e9-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458703.315861, \"EndTime\": 1631458703.3342528, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 17.888784408569336, \"count\": 1, \"min\": 17.888784408569336, \"max\": 17.888784408569336}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:23 INFO 140277876397440] Epoch[14] Batch[0] avg_epoch_loss=10.110916\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:23 INFO 140277876397440] #quality_metric: host=algo-1, epoch=14, batch=0 train loss <loss>=10.110916137695312\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:23 INFO 140277876397440] Epoch[14] Batch[5] avg_epoch_loss=10.288867\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:23 INFO 140277876397440] #quality_metric: host=algo-1, epoch=14, batch=5 train loss <loss>=10.288867155710856\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:23 INFO 140277876397440] Epoch[14] Batch [5]#011Speed: 2088.63 samples/sec#011loss=10.288867\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:23 INFO 140277876397440] Epoch[14] Batch[10] avg_epoch_loss=10.544870\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:23 INFO 140277876397440] #quality_metric: host=algo-1, epoch=14, batch=10 train loss <loss>=10.852073097229004\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:23 INFO 140277876397440] Epoch[14] Batch [10]#011Speed: 1937.45 samples/sec#011loss=10.852073\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:23 INFO 140277876397440] processed a total of 645 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458703.3343155, \"EndTime\": 1631458703.8964965, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 562.1242523193359, \"count\": 1, \"min\": 562.1242523193359, \"max\": 562.1242523193359}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:23 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1147.2161332153885 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:23 INFO 140277876397440] #progress_metric: host=algo-1, completed 7.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:23 INFO 140277876397440] #quality_metric: host=algo-1, epoch=14, train loss <loss>=10.544869856400924\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:23 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:24 INFO 140277876397440] Epoch[15] Batch[0] avg_epoch_loss=9.667875\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:24 INFO 140277876397440] #quality_metric: host=algo-1, epoch=15, batch=0 train loss <loss>=9.667875289916992\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:24 INFO 140277876397440] Epoch[15] Batch[5] avg_epoch_loss=10.360093\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:24 INFO 140277876397440] #quality_metric: host=algo-1, epoch=15, batch=5 train loss <loss>=10.360092957814535\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:24 INFO 140277876397440] Epoch[15] Batch [5]#011Speed: 2104.06 samples/sec#011loss=10.360093\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:24 INFO 140277876397440] Epoch[15] Batch[10] avg_epoch_loss=10.356404\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:24 INFO 140277876397440] #quality_metric: host=algo-1, epoch=15, batch=10 train loss <loss>=10.351977729797364\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:24 INFO 140277876397440] Epoch[15] Batch [10]#011Speed: 1774.98 samples/sec#011loss=10.351978\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:24 INFO 140277876397440] processed a total of 642 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458703.8965664, \"EndTime\": 1631458704.4758136, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 578.7539482116699, \"count\": 1, \"min\": 578.7539482116699, \"max\": 578.7539482116699}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:24 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1109.0782026194536 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:24 INFO 140277876397440] #progress_metric: host=algo-1, completed 8.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:24 INFO 140277876397440] #quality_metric: host=algo-1, epoch=15, train loss <loss>=10.356404217806729\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:24 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:24 INFO 140277876397440] Epoch[16] Batch[0] avg_epoch_loss=10.694357\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:24 INFO 140277876397440] #quality_metric: host=algo-1, epoch=16, batch=0 train loss <loss>=10.694356918334961\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:24 INFO 140277876397440] Epoch[16] Batch[5] avg_epoch_loss=10.494088\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:24 INFO 140277876397440] #quality_metric: host=algo-1, epoch=16, batch=5 train loss <loss>=10.49408753712972\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:24 INFO 140277876397440] Epoch[16] Batch [5]#011Speed: 1619.29 samples/sec#011loss=10.494088\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:25 INFO 140277876397440] Epoch[16] Batch[10] avg_epoch_loss=10.295147\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:25 INFO 140277876397440] #quality_metric: host=algo-1, epoch=16, batch=10 train loss <loss>=10.05641803741455\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:25 INFO 140277876397440] Epoch[16] Batch [10]#011Speed: 1762.99 samples/sec#011loss=10.056418\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:25 INFO 140277876397440] processed a total of 659 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458704.4758816, \"EndTime\": 1631458705.1052213, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 628.889799118042, \"count\": 1, \"min\": 628.889799118042, \"max\": 628.889799118042}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:25 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1047.6793518142288 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:25 INFO 140277876397440] #progress_metric: host=algo-1, completed 8.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:25 INFO 140277876397440] #quality_metric: host=algo-1, epoch=16, train loss <loss>=10.295146855441006\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:25 INFO 140277876397440] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:25 INFO 140277876397440] Saved checkpoint to \"/opt/ml/model/state_855ae257-3e6e-4668-a153-b4d36f881d7e-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458705.105309, \"EndTime\": 1631458705.1175427, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 11.686563491821289, \"count\": 1, \"min\": 11.686563491821289, \"max\": 11.686563491821289}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:25 INFO 140277876397440] Epoch[17] Batch[0] avg_epoch_loss=11.055189\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:25 INFO 140277876397440] #quality_metric: host=algo-1, epoch=17, batch=0 train loss <loss>=11.05518913269043\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:25 INFO 140277876397440] Epoch[17] Batch[5] avg_epoch_loss=10.408677\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:25 INFO 140277876397440] #quality_metric: host=algo-1, epoch=17, batch=5 train loss <loss>=10.408677419026693\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:25 INFO 140277876397440] Epoch[17] Batch [5]#011Speed: 1947.55 samples/sec#011loss=10.408677\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:25 INFO 140277876397440] processed a total of 601 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458705.1175964, \"EndTime\": 1631458705.659941, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 542.2866344451904, \"count\": 1, \"min\": 542.2866344451904, \"max\": 542.2866344451904}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:25 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1108.0171566666695 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:25 INFO 140277876397440] #progress_metric: host=algo-1, completed 9.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:25 INFO 140277876397440] #quality_metric: host=algo-1, epoch=17, train loss <loss>=10.238390254974366\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:25 INFO 140277876397440] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:25 INFO 140277876397440] Saved checkpoint to \"/opt/ml/model/state_5bf218b0-534f-4a59-9252-fb1800b48add-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458705.6600296, \"EndTime\": 1631458705.6794713, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 18.923282623291016, \"count\": 1, \"min\": 18.923282623291016, \"max\": 18.923282623291016}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:25 INFO 140277876397440] Epoch[18] Batch[0] avg_epoch_loss=10.413087\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:25 INFO 140277876397440] #quality_metric: host=algo-1, epoch=18, batch=0 train loss <loss>=10.413086891174316\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:26 INFO 140277876397440] Epoch[18] Batch[5] avg_epoch_loss=10.565363\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:26 INFO 140277876397440] #quality_metric: host=algo-1, epoch=18, batch=5 train loss <loss>=10.56536340713501\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:26 INFO 140277876397440] Epoch[18] Batch [5]#011Speed: 1564.72 samples/sec#011loss=10.565363\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:26 INFO 140277876397440] processed a total of 623 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458705.6795368, \"EndTime\": 1631458706.3066359, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 627.0425319671631, \"count\": 1, \"min\": 627.0425319671631, \"max\": 627.0425319671631}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:26 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=993.3554804545091 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:26 INFO 140277876397440] #progress_metric: host=algo-1, completed 9.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:26 INFO 140277876397440] #quality_metric: host=algo-1, epoch=18, train loss <loss>=10.44723482131958\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:26 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:26 INFO 140277876397440] Epoch[19] Batch[0] avg_epoch_loss=11.389862\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:26 INFO 140277876397440] #quality_metric: host=algo-1, epoch=19, batch=0 train loss <loss>=11.389862060546875\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:26 INFO 140277876397440] Epoch[19] Batch[5] avg_epoch_loss=10.624867\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:26 INFO 140277876397440] #quality_metric: host=algo-1, epoch=19, batch=5 train loss <loss>=10.6248672803243\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:26 INFO 140277876397440] Epoch[19] Batch [5]#011Speed: 2105.58 samples/sec#011loss=10.624867\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:26 INFO 140277876397440] processed a total of 628 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458706.3067238, \"EndTime\": 1631458706.9180307, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 610.7475757598877, \"count\": 1, \"min\": 610.7475757598877, \"max\": 610.7475757598877}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:26 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1028.0389574198866 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:26 INFO 140277876397440] #progress_metric: host=algo-1, completed 10.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:26 INFO 140277876397440] #quality_metric: host=algo-1, epoch=19, train loss <loss>=10.665637397766114\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:26 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:27 INFO 140277876397440] Epoch[20] Batch[0] avg_epoch_loss=10.005072\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:27 INFO 140277876397440] #quality_metric: host=algo-1, epoch=20, batch=0 train loss <loss>=10.005071640014648\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:27 INFO 140277876397440] Epoch[20] Batch[5] avg_epoch_loss=10.354512\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:27 INFO 140277876397440] #quality_metric: host=algo-1, epoch=20, batch=5 train loss <loss>=10.354512214660645\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:27 INFO 140277876397440] Epoch[20] Batch [5]#011Speed: 1852.62 samples/sec#011loss=10.354512\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:27 INFO 140277876397440] Epoch[20] Batch[10] avg_epoch_loss=9.986503\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:27 INFO 140277876397440] #quality_metric: host=algo-1, epoch=20, batch=10 train loss <loss>=9.544892597198487\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:27 INFO 140277876397440] Epoch[20] Batch [10]#011Speed: 1946.19 samples/sec#011loss=9.544893\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:27 INFO 140277876397440] processed a total of 662 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458706.9181187, \"EndTime\": 1631458707.5121965, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 593.533992767334, \"count\": 1, \"min\": 593.533992767334, \"max\": 593.533992767334}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:27 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1115.134092196995 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:27 INFO 140277876397440] #progress_metric: host=algo-1, completed 10.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:27 INFO 140277876397440] #quality_metric: host=algo-1, epoch=20, train loss <loss>=9.98650329763239\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:27 INFO 140277876397440] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:27 INFO 140277876397440] Saved checkpoint to \"/opt/ml/model/state_ae2c31a4-8949-4b24-b867-17ed1e61d625-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458707.5122762, \"EndTime\": 1631458707.5305405, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 17.795324325561523, \"count\": 1, \"min\": 17.795324325561523, \"max\": 17.795324325561523}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:27 INFO 140277876397440] Epoch[21] Batch[0] avg_epoch_loss=11.513657\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:27 INFO 140277876397440] #quality_metric: host=algo-1, epoch=21, batch=0 train loss <loss>=11.513656616210938\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:27 INFO 140277876397440] Epoch[21] Batch[5] avg_epoch_loss=10.374412\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:27 INFO 140277876397440] #quality_metric: host=algo-1, epoch=21, batch=5 train loss <loss>=10.374411741892496\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:27 INFO 140277876397440] Epoch[21] Batch [5]#011Speed: 2078.88 samples/sec#011loss=10.374412\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:28 INFO 140277876397440] Epoch[21] Batch[10] avg_epoch_loss=10.377316\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:28 INFO 140277876397440] #quality_metric: host=algo-1, epoch=21, batch=10 train loss <loss>=10.380800437927245\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:28 INFO 140277876397440] Epoch[21] Batch [10]#011Speed: 1864.61 samples/sec#011loss=10.380800\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:28 INFO 140277876397440] processed a total of 646 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458707.530602, \"EndTime\": 1631458708.1174893, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 586.8344306945801, \"count\": 1, \"min\": 586.8344306945801, \"max\": 586.8344306945801}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:28 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1100.6051491441788 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:28 INFO 140277876397440] #progress_metric: host=algo-1, completed 11.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:28 INFO 140277876397440] #quality_metric: host=algo-1, epoch=21, train loss <loss>=10.377315694635564\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:28 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:28 INFO 140277876397440] Epoch[22] Batch[0] avg_epoch_loss=10.708764\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:28 INFO 140277876397440] #quality_metric: host=algo-1, epoch=22, batch=0 train loss <loss>=10.70876407623291\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:28 INFO 140277876397440] Epoch[22] Batch[5] avg_epoch_loss=10.201191\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:28 INFO 140277876397440] #quality_metric: host=algo-1, epoch=22, batch=5 train loss <loss>=10.201190630594889\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:28 INFO 140277876397440] Epoch[22] Batch [5]#011Speed: 2134.94 samples/sec#011loss=10.201191\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:28 INFO 140277876397440] processed a total of 624 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458708.1175728, \"EndTime\": 1631458708.6774764, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 559.4291687011719, \"count\": 1, \"min\": 559.4291687011719, \"max\": 559.4291687011719}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:28 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1115.1698699758153 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:28 INFO 140277876397440] #progress_metric: host=algo-1, completed 11.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:28 INFO 140277876397440] #quality_metric: host=algo-1, epoch=22, train loss <loss>=10.451303958892822\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:28 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:28 INFO 140277876397440] Epoch[23] Batch[0] avg_epoch_loss=9.988502\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:28 INFO 140277876397440] #quality_metric: host=algo-1, epoch=23, batch=0 train loss <loss>=9.98850154876709\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:29 INFO 140277876397440] Epoch[23] Batch[5] avg_epoch_loss=10.313242\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:29 INFO 140277876397440] #quality_metric: host=algo-1, epoch=23, batch=5 train loss <loss>=10.313241958618164\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:29 INFO 140277876397440] Epoch[23] Batch [5]#011Speed: 2003.84 samples/sec#011loss=10.313242\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:29 INFO 140277876397440] processed a total of 635 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458708.6775649, \"EndTime\": 1631458709.2441447, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 566.0665035247803, \"count\": 1, \"min\": 566.0665035247803, \"max\": 566.0665035247803}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:29 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1121.558552685879 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:29 INFO 140277876397440] #progress_metric: host=algo-1, completed 12.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:29 INFO 140277876397440] #quality_metric: host=algo-1, epoch=23, train loss <loss>=10.127198696136475\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:29 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:29 INFO 140277876397440] Epoch[24] Batch[0] avg_epoch_loss=10.623839\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:29 INFO 140277876397440] #quality_metric: host=algo-1, epoch=24, batch=0 train loss <loss>=10.623839378356934\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:29 INFO 140277876397440] Epoch[24] Batch[5] avg_epoch_loss=10.655453\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:29 INFO 140277876397440] #quality_metric: host=algo-1, epoch=24, batch=5 train loss <loss>=10.655453046162924\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:29 INFO 140277876397440] Epoch[24] Batch [5]#011Speed: 2099.38 samples/sec#011loss=10.655453\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:29 INFO 140277876397440] processed a total of 593 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458709.2442214, \"EndTime\": 1631458709.754358, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 509.6871852874756, \"count\": 1, \"min\": 509.6871852874756, \"max\": 509.6871852874756}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:29 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1163.1795934706888 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:29 INFO 140277876397440] #progress_metric: host=algo-1, completed 12.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:29 INFO 140277876397440] #quality_metric: host=algo-1, epoch=24, train loss <loss>=10.032490348815918\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:29 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:30 INFO 140277876397440] Epoch[25] Batch[0] avg_epoch_loss=9.913286\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:30 INFO 140277876397440] #quality_metric: host=algo-1, epoch=25, batch=0 train loss <loss>=9.913286209106445\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:30 INFO 140277876397440] Epoch[25] Batch[5] avg_epoch_loss=10.510665\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:30 INFO 140277876397440] #quality_metric: host=algo-1, epoch=25, batch=5 train loss <loss>=10.510664939880371\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:30 INFO 140277876397440] Epoch[25] Batch [5]#011Speed: 1702.00 samples/sec#011loss=10.510665\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:30 INFO 140277876397440] processed a total of 627 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458709.754441, \"EndTime\": 1631458710.3222532, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 567.3003196716309, \"count\": 1, \"min\": 567.3003196716309, \"max\": 567.3003196716309}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:30 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1105.0285048704259 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:30 INFO 140277876397440] #progress_metric: host=algo-1, completed 13.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:30 INFO 140277876397440] #quality_metric: host=algo-1, epoch=25, train loss <loss>=10.358488368988038\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:30 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:30 INFO 140277876397440] Epoch[26] Batch[0] avg_epoch_loss=10.217525\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:30 INFO 140277876397440] #quality_metric: host=algo-1, epoch=26, batch=0 train loss <loss>=10.217524528503418\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:30 INFO 140277876397440] Epoch[26] Batch[5] avg_epoch_loss=10.124339\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:30 INFO 140277876397440] #quality_metric: host=algo-1, epoch=26, batch=5 train loss <loss>=10.12433926264445\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:30 INFO 140277876397440] Epoch[26] Batch [5]#011Speed: 2057.06 samples/sec#011loss=10.124339\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:30 INFO 140277876397440] Epoch[26] Batch[10] avg_epoch_loss=10.458730\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:30 INFO 140277876397440] #quality_metric: host=algo-1, epoch=26, batch=10 train loss <loss>=10.859997940063476\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:30 INFO 140277876397440] Epoch[26] Batch [10]#011Speed: 2071.69 samples/sec#011loss=10.859998\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:30 INFO 140277876397440] processed a total of 647 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458710.322325, \"EndTime\": 1631458710.8766108, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 553.769588470459, \"count\": 1, \"min\": 553.769588470459, \"max\": 553.769588470459}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:30 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1168.106013537508 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:30 INFO 140277876397440] #progress_metric: host=algo-1, completed 13.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:30 INFO 140277876397440] #quality_metric: host=algo-1, epoch=26, train loss <loss>=10.45872957056219\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:30 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:31 INFO 140277876397440] Epoch[27] Batch[0] avg_epoch_loss=9.914904\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:31 INFO 140277876397440] #quality_metric: host=algo-1, epoch=27, batch=0 train loss <loss>=9.91490364074707\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:31 INFO 140277876397440] Epoch[27] Batch[5] avg_epoch_loss=10.032687\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:31 INFO 140277876397440] #quality_metric: host=algo-1, epoch=27, batch=5 train loss <loss>=10.032687187194824\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:31 INFO 140277876397440] Epoch[27] Batch [5]#011Speed: 2108.44 samples/sec#011loss=10.032687\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:31 INFO 140277876397440] processed a total of 605 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458710.876694, \"EndTime\": 1631458711.401248, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 524.0828990936279, \"count\": 1, \"min\": 524.0828990936279, \"max\": 524.0828990936279}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:31 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1154.1161022950391 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:31 INFO 140277876397440] #progress_metric: host=algo-1, completed 14.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:31 INFO 140277876397440] #quality_metric: host=algo-1, epoch=27, train loss <loss>=10.574427700042724\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:31 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:31 INFO 140277876397440] Epoch[28] Batch[0] avg_epoch_loss=10.638758\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:31 INFO 140277876397440] #quality_metric: host=algo-1, epoch=28, batch=0 train loss <loss>=10.638757705688477\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:31 INFO 140277876397440] Epoch[28] Batch[5] avg_epoch_loss=10.167826\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:31 INFO 140277876397440] #quality_metric: host=algo-1, epoch=28, batch=5 train loss <loss>=10.167826016743978\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:31 INFO 140277876397440] Epoch[28] Batch [5]#011Speed: 2099.46 samples/sec#011loss=10.167826\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:31 INFO 140277876397440] processed a total of 638 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458711.401336, \"EndTime\": 1631458711.946207, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 544.4114208221436, \"count\": 1, \"min\": 544.4114208221436, \"max\": 544.4114208221436}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:31 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1171.6538270571411 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:31 INFO 140277876397440] #progress_metric: host=algo-1, completed 14.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:31 INFO 140277876397440] #quality_metric: host=algo-1, epoch=28, train loss <loss>=10.189812088012696\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:31 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:32 INFO 140277876397440] Epoch[29] Batch[0] avg_epoch_loss=10.237370\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:32 INFO 140277876397440] #quality_metric: host=algo-1, epoch=29, batch=0 train loss <loss>=10.237369537353516\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:32 INFO 140277876397440] Epoch[29] Batch[5] avg_epoch_loss=10.217750\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:32 INFO 140277876397440] #quality_metric: host=algo-1, epoch=29, batch=5 train loss <loss>=10.217750072479248\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:32 INFO 140277876397440] Epoch[29] Batch [5]#011Speed: 2099.95 samples/sec#011loss=10.217750\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:32 INFO 140277876397440] processed a total of 626 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458711.9462848, \"EndTime\": 1631458712.4749215, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 528.0952453613281, \"count\": 1, \"min\": 528.0952453613281, \"max\": 528.0952453613281}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:32 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1185.1279150488876 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:32 INFO 140277876397440] #progress_metric: host=algo-1, completed 15.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:32 INFO 140277876397440] #quality_metric: host=algo-1, epoch=29, train loss <loss>=10.3513765335083\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:32 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:32 INFO 140277876397440] Epoch[30] Batch[0] avg_epoch_loss=9.863366\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:32 INFO 140277876397440] #quality_metric: host=algo-1, epoch=30, batch=0 train loss <loss>=9.86336612701416\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:32 INFO 140277876397440] Epoch[30] Batch[5] avg_epoch_loss=10.277720\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:32 INFO 140277876397440] #quality_metric: host=algo-1, epoch=30, batch=5 train loss <loss>=10.277720292409262\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:32 INFO 140277876397440] Epoch[30] Batch [5]#011Speed: 2113.04 samples/sec#011loss=10.277720\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:33 INFO 140277876397440] Epoch[30] Batch[10] avg_epoch_loss=10.601833\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:33 INFO 140277876397440] #quality_metric: host=algo-1, epoch=30, batch=10 train loss <loss>=10.990769004821777\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:33 INFO 140277876397440] Epoch[30] Batch [10]#011Speed: 2041.83 samples/sec#011loss=10.990769\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:33 INFO 140277876397440] processed a total of 643 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458712.4750094, \"EndTime\": 1631458713.022834, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 547.2993850708008, \"count\": 1, \"min\": 547.2993850708008, \"max\": 547.2993850708008}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:33 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1174.656565924936 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:33 INFO 140277876397440] #progress_metric: host=algo-1, completed 15.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:33 INFO 140277876397440] #quality_metric: host=algo-1, epoch=30, train loss <loss>=10.60183334350586\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:33 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:33 INFO 140277876397440] Epoch[31] Batch[0] avg_epoch_loss=10.759737\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:33 INFO 140277876397440] #quality_metric: host=algo-1, epoch=31, batch=0 train loss <loss>=10.759737014770508\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:33 INFO 140277876397440] Epoch[31] Batch[5] avg_epoch_loss=10.173723\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:33 INFO 140277876397440] #quality_metric: host=algo-1, epoch=31, batch=5 train loss <loss>=10.173722902933756\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:33 INFO 140277876397440] Epoch[31] Batch [5]#011Speed: 1715.33 samples/sec#011loss=10.173723\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:33 INFO 140277876397440] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458713.0228975, \"EndTime\": 1631458713.5784502, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 555.0820827484131, \"count\": 1, \"min\": 555.0820827484131, \"max\": 555.0820827484131}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:33 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1141.8963047955383 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:33 INFO 140277876397440] #progress_metric: host=algo-1, completed 16.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:33 INFO 140277876397440] #quality_metric: host=algo-1, epoch=31, train loss <loss>=10.1431453704834\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:33 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:33 INFO 140277876397440] Epoch[32] Batch[0] avg_epoch_loss=10.698726\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:33 INFO 140277876397440] #quality_metric: host=algo-1, epoch=32, batch=0 train loss <loss>=10.698725700378418\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:33 INFO 140277876397440] Epoch[32] Batch[5] avg_epoch_loss=10.403839\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:33 INFO 140277876397440] #quality_metric: host=algo-1, epoch=32, batch=5 train loss <loss>=10.403839270273844\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:33 INFO 140277876397440] Epoch[32] Batch [5]#011Speed: 1855.20 samples/sec#011loss=10.403839\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:34 INFO 140277876397440] processed a total of 623 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458713.578541, \"EndTime\": 1631458714.1415434, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 562.4644756317139, \"count\": 1, \"min\": 562.4644756317139, \"max\": 562.4644756317139}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:34 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1107.3603015961269 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:34 INFO 140277876397440] #progress_metric: host=algo-1, completed 16.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:34 INFO 140277876397440] #quality_metric: host=algo-1, epoch=32, train loss <loss>=10.241064643859863\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:34 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:34 INFO 140277876397440] Epoch[33] Batch[0] avg_epoch_loss=9.430902\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:34 INFO 140277876397440] #quality_metric: host=algo-1, epoch=33, batch=0 train loss <loss>=9.430901527404785\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:34 INFO 140277876397440] Epoch[33] Batch[5] avg_epoch_loss=9.926313\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:34 INFO 140277876397440] #quality_metric: host=algo-1, epoch=33, batch=5 train loss <loss>=9.926312764485678\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:34 INFO 140277876397440] Epoch[33] Batch [5]#011Speed: 2010.16 samples/sec#011loss=9.926313\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:34 INFO 140277876397440] Epoch[33] Batch[10] avg_epoch_loss=9.944070\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:34 INFO 140277876397440] #quality_metric: host=algo-1, epoch=33, batch=10 train loss <loss>=9.965379333496093\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:34 INFO 140277876397440] Epoch[33] Batch [10]#011Speed: 1893.32 samples/sec#011loss=9.965379\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:34 INFO 140277876397440] processed a total of 674 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458714.141637, \"EndTime\": 1631458714.7168212, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 574.721097946167, \"count\": 1, \"min\": 574.721097946167, \"max\": 574.721097946167}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:34 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1172.4990609923618 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:34 INFO 140277876397440] #progress_metric: host=algo-1, completed 17.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:34 INFO 140277876397440] #quality_metric: host=algo-1, epoch=33, train loss <loss>=9.944070295854049\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:34 INFO 140277876397440] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:34 INFO 140277876397440] Saved checkpoint to \"/opt/ml/model/state_31dfd088-185d-4f59-aa4a-9ba7549df5c4-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458714.7169032, \"EndTime\": 1631458714.7356398, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 18.266677856445312, \"count\": 1, \"min\": 18.266677856445312, \"max\": 18.266677856445312}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:34 INFO 140277876397440] Epoch[34] Batch[0] avg_epoch_loss=9.340958\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:34 INFO 140277876397440] #quality_metric: host=algo-1, epoch=34, batch=0 train loss <loss>=9.340957641601562\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:35 INFO 140277876397440] Epoch[34] Batch[5] avg_epoch_loss=9.624100\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:35 INFO 140277876397440] #quality_metric: host=algo-1, epoch=34, batch=5 train loss <loss>=9.62410020828247\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:35 INFO 140277876397440] Epoch[34] Batch [5]#011Speed: 2055.77 samples/sec#011loss=9.624100\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:35 INFO 140277876397440] processed a total of 619 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458714.735705, \"EndTime\": 1631458715.264753, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 528.9919376373291, \"count\": 1, \"min\": 528.9919376373291, \"max\": 528.9919376373291}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:35 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1169.9265881000028 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:35 INFO 140277876397440] #progress_metric: host=algo-1, completed 17.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:35 INFO 140277876397440] #quality_metric: host=algo-1, epoch=34, train loss <loss>=9.851822662353516\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:35 INFO 140277876397440] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:35 INFO 140277876397440] Saved checkpoint to \"/opt/ml/model/state_395966f3-3281-4cce-9fef-56b17c0108cc-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458715.2648208, \"EndTime\": 1631458715.2779167, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 12.537717819213867, \"count\": 1, \"min\": 12.537717819213867, \"max\": 12.537717819213867}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:35 INFO 140277876397440] Epoch[35] Batch[0] avg_epoch_loss=10.253321\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:35 INFO 140277876397440] #quality_metric: host=algo-1, epoch=35, batch=0 train loss <loss>=10.253320693969727\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:35 INFO 140277876397440] Epoch[35] Batch[5] avg_epoch_loss=10.302442\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:35 INFO 140277876397440] #quality_metric: host=algo-1, epoch=35, batch=5 train loss <loss>=10.302441914876303\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:35 INFO 140277876397440] Epoch[35] Batch [5]#011Speed: 2119.91 samples/sec#011loss=10.302442\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:35 INFO 140277876397440] Epoch[35] Batch[10] avg_epoch_loss=9.963204\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:35 INFO 140277876397440] #quality_metric: host=algo-1, epoch=35, batch=10 train loss <loss>=9.55611810684204\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:35 INFO 140277876397440] Epoch[35] Batch [10]#011Speed: 1743.07 samples/sec#011loss=9.556118\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:35 INFO 140277876397440] processed a total of 659 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458715.2779837, \"EndTime\": 1631458715.8512926, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 573.2550621032715, \"count\": 1, \"min\": 573.2550621032715, \"max\": 573.2550621032715}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:35 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1149.357566463273 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:35 INFO 140277876397440] #progress_metric: host=algo-1, completed 18.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:35 INFO 140277876397440] #quality_metric: host=algo-1, epoch=35, train loss <loss>=9.963203820315274\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:35 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:36 INFO 140277876397440] Epoch[36] Batch[0] avg_epoch_loss=10.288062\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:36 INFO 140277876397440] #quality_metric: host=algo-1, epoch=36, batch=0 train loss <loss>=10.28806209564209\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:36 INFO 140277876397440] Epoch[36] Batch[5] avg_epoch_loss=10.197250\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:36 INFO 140277876397440] #quality_metric: host=algo-1, epoch=36, batch=5 train loss <loss>=10.19724957148234\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:36 INFO 140277876397440] Epoch[36] Batch [5]#011Speed: 1964.65 samples/sec#011loss=10.197250\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:36 INFO 140277876397440] processed a total of 640 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458715.8513682, \"EndTime\": 1631458716.3880258, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 536.1795425415039, \"count\": 1, \"min\": 536.1795425415039, \"max\": 536.1795425415039}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:36 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1193.3402980111548 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:36 INFO 140277876397440] #progress_metric: host=algo-1, completed 18.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:36 INFO 140277876397440] #quality_metric: host=algo-1, epoch=36, train loss <loss>=10.25169734954834\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:36 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:36 INFO 140277876397440] Epoch[37] Batch[0] avg_epoch_loss=10.015485\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:36 INFO 140277876397440] #quality_metric: host=algo-1, epoch=37, batch=0 train loss <loss>=10.015484809875488\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:36 INFO 140277876397440] Epoch[37] Batch[5] avg_epoch_loss=10.225142\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:36 INFO 140277876397440] #quality_metric: host=algo-1, epoch=37, batch=5 train loss <loss>=10.225142478942871\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:36 INFO 140277876397440] Epoch[37] Batch [5]#011Speed: 1949.54 samples/sec#011loss=10.225142\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:37 INFO 140277876397440] Epoch[37] Batch[10] avg_epoch_loss=10.465405\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:37 INFO 140277876397440] #quality_metric: host=algo-1, epoch=37, batch=10 train loss <loss>=10.753720664978028\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:37 INFO 140277876397440] Epoch[37] Batch [10]#011Speed: 1634.34 samples/sec#011loss=10.753721\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:37 INFO 140277876397440] processed a total of 678 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458716.388117, \"EndTime\": 1631458717.0594027, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 670.7823276519775, \"count\": 1, \"min\": 670.7823276519775, \"max\": 670.7823276519775}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:37 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1010.5761745742769 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:37 INFO 140277876397440] #progress_metric: host=algo-1, completed 19.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:37 INFO 140277876397440] #quality_metric: host=algo-1, epoch=37, train loss <loss>=10.465405290777033\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:37 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:37 INFO 140277876397440] Epoch[38] Batch[0] avg_epoch_loss=9.399756\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:37 INFO 140277876397440] #quality_metric: host=algo-1, epoch=38, batch=0 train loss <loss>=9.39975643157959\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:37 INFO 140277876397440] Epoch[38] Batch[5] avg_epoch_loss=9.668886\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:37 INFO 140277876397440] #quality_metric: host=algo-1, epoch=38, batch=5 train loss <loss>=9.668885548909506\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:37 INFO 140277876397440] Epoch[38] Batch [5]#011Speed: 1755.41 samples/sec#011loss=9.668886\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:37 INFO 140277876397440] Epoch[38] Batch[10] avg_epoch_loss=9.453155\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:37 INFO 140277876397440] #quality_metric: host=algo-1, epoch=38, batch=10 train loss <loss>=9.1942777633667\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:37 INFO 140277876397440] Epoch[38] Batch [10]#011Speed: 2046.98 samples/sec#011loss=9.194278\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:37 INFO 140277876397440] processed a total of 647 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458717.0594873, \"EndTime\": 1631458717.66443, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 604.4700145721436, \"count\": 1, \"min\": 604.4700145721436, \"max\": 604.4700145721436}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:37 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1070.146818758641 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:37 INFO 140277876397440] #progress_metric: host=algo-1, completed 19.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:37 INFO 140277876397440] #quality_metric: host=algo-1, epoch=38, train loss <loss>=9.453154737299139\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:37 INFO 140277876397440] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:37 INFO 140277876397440] Saved checkpoint to \"/opt/ml/model/state_5b03c862-afed-4e83-9995-872c96dd0ff2-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458717.6645129, \"EndTime\": 1631458717.6775372, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 12.537479400634766, \"count\": 1, \"min\": 12.537479400634766, \"max\": 12.537479400634766}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:37 INFO 140277876397440] Epoch[39] Batch[0] avg_epoch_loss=10.196134\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:37 INFO 140277876397440] #quality_metric: host=algo-1, epoch=39, batch=0 train loss <loss>=10.196133613586426\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:38 INFO 140277876397440] Epoch[39] Batch[5] avg_epoch_loss=9.782953\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:38 INFO 140277876397440] #quality_metric: host=algo-1, epoch=39, batch=5 train loss <loss>=9.78295342127482\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:38 INFO 140277876397440] Epoch[39] Batch [5]#011Speed: 1973.69 samples/sec#011loss=9.782953\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:38 INFO 140277876397440] processed a total of 618 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458717.6775818, \"EndTime\": 1631458718.2217233, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 544.083833694458, \"count\": 1, \"min\": 544.083833694458, \"max\": 544.083833694458}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:38 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1135.5961047449982 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:38 INFO 140277876397440] #progress_metric: host=algo-1, completed 20.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:38 INFO 140277876397440] #quality_metric: host=algo-1, epoch=39, train loss <loss>=10.044210720062257\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:38 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:38 INFO 140277876397440] Epoch[40] Batch[0] avg_epoch_loss=10.362975\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:38 INFO 140277876397440] #quality_metric: host=algo-1, epoch=40, batch=0 train loss <loss>=10.362975120544434\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:38 INFO 140277876397440] Epoch[40] Batch[5] avg_epoch_loss=10.088827\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:38 INFO 140277876397440] #quality_metric: host=algo-1, epoch=40, batch=5 train loss <loss>=10.088826815287272\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:38 INFO 140277876397440] Epoch[40] Batch [5]#011Speed: 2114.64 samples/sec#011loss=10.088827\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:38 INFO 140277876397440] Epoch[40] Batch[10] avg_epoch_loss=9.533366\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:38 INFO 140277876397440] #quality_metric: host=algo-1, epoch=40, batch=10 train loss <loss>=8.866812705993652\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:38 INFO 140277876397440] Epoch[40] Batch [10]#011Speed: 1937.94 samples/sec#011loss=8.866813\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:38 INFO 140277876397440] processed a total of 649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458718.22181, \"EndTime\": 1631458718.8061948, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 583.8823318481445, \"count\": 1, \"min\": 583.8823318481445, \"max\": 583.8823318481445}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:38 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1111.2952614159617 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:38 INFO 140277876397440] #progress_metric: host=algo-1, completed 20.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:38 INFO 140277876397440] #quality_metric: host=algo-1, epoch=40, train loss <loss>=9.533365856517445\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:38 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:39 INFO 140277876397440] Epoch[41] Batch[0] avg_epoch_loss=11.079585\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:39 INFO 140277876397440] #quality_metric: host=algo-1, epoch=41, batch=0 train loss <loss>=11.079585075378418\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:39 INFO 140277876397440] Epoch[41] Batch[5] avg_epoch_loss=10.261508\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:39 INFO 140277876397440] #quality_metric: host=algo-1, epoch=41, batch=5 train loss <loss>=10.261507511138916\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:39 INFO 140277876397440] Epoch[41] Batch [5]#011Speed: 2011.11 samples/sec#011loss=10.261508\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:39 INFO 140277876397440] processed a total of 622 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458718.806279, \"EndTime\": 1631458719.3703687, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 563.6210441589355, \"count\": 1, \"min\": 563.6210441589355, \"max\": 563.6210441589355}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:39 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1103.3776828898997 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:39 INFO 140277876397440] #progress_metric: host=algo-1, completed 21.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:39 INFO 140277876397440] #quality_metric: host=algo-1, epoch=41, train loss <loss>=10.387391185760498\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:39 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:39 INFO 140277876397440] Epoch[42] Batch[0] avg_epoch_loss=9.576097\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:39 INFO 140277876397440] #quality_metric: host=algo-1, epoch=42, batch=0 train loss <loss>=9.57609748840332\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:39 INFO 140277876397440] Epoch[42] Batch[5] avg_epoch_loss=10.079492\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:39 INFO 140277876397440] #quality_metric: host=algo-1, epoch=42, batch=5 train loss <loss>=10.079492251078287\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:39 INFO 140277876397440] Epoch[42] Batch [5]#011Speed: 2123.43 samples/sec#011loss=10.079492\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:39 INFO 140277876397440] Epoch[42] Batch[10] avg_epoch_loss=10.337145\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:39 INFO 140277876397440] #quality_metric: host=algo-1, epoch=42, batch=10 train loss <loss>=10.646328163146972\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:39 INFO 140277876397440] Epoch[42] Batch [10]#011Speed: 2028.11 samples/sec#011loss=10.646328\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:39 INFO 140277876397440] processed a total of 650 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458719.3704374, \"EndTime\": 1631458719.932061, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 561.0921382904053, \"count\": 1, \"min\": 561.0921382904053, \"max\": 561.0921382904053}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:39 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1158.202543519192 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:39 INFO 140277876397440] #progress_metric: host=algo-1, completed 21.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:39 INFO 140277876397440] #quality_metric: host=algo-1, epoch=42, train loss <loss>=10.337144938382236\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:39 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:40 INFO 140277876397440] Epoch[43] Batch[0] avg_epoch_loss=10.748478\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:40 INFO 140277876397440] #quality_metric: host=algo-1, epoch=43, batch=0 train loss <loss>=10.748477935791016\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:40 INFO 140277876397440] Epoch[43] Batch[5] avg_epoch_loss=9.967384\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:40 INFO 140277876397440] #quality_metric: host=algo-1, epoch=43, batch=5 train loss <loss>=9.967383543650309\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:40 INFO 140277876397440] Epoch[43] Batch [5]#011Speed: 1732.45 samples/sec#011loss=9.967384\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:40 INFO 140277876397440] processed a total of 639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458719.9321444, \"EndTime\": 1631458720.513109, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 580.4970264434814, \"count\": 1, \"min\": 580.4970264434814, \"max\": 580.4970264434814}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:40 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1100.531326106122 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:40 INFO 140277876397440] #progress_metric: host=algo-1, completed 22.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:40 INFO 140277876397440] #quality_metric: host=algo-1, epoch=43, train loss <loss>=10.159981155395508\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:40 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:40 INFO 140277876397440] Epoch[44] Batch[0] avg_epoch_loss=9.555304\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:40 INFO 140277876397440] #quality_metric: host=algo-1, epoch=44, batch=0 train loss <loss>=9.555303573608398\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:40 INFO 140277876397440] Epoch[44] Batch[5] avg_epoch_loss=10.019504\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:40 INFO 140277876397440] #quality_metric: host=algo-1, epoch=44, batch=5 train loss <loss>=10.019504229227701\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:40 INFO 140277876397440] Epoch[44] Batch [5]#011Speed: 2070.77 samples/sec#011loss=10.019504\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:41 INFO 140277876397440] processed a total of 610 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458720.5131927, \"EndTime\": 1631458721.0468688, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 533.1945419311523, \"count\": 1, \"min\": 533.1945419311523, \"max\": 533.1945419311523}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:41 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1143.7735882795778 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:41 INFO 140277876397440] #progress_metric: host=algo-1, completed 22.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:41 INFO 140277876397440] #quality_metric: host=algo-1, epoch=44, train loss <loss>=9.703379011154174\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:41 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:41 INFO 140277876397440] Epoch[45] Batch[0] avg_epoch_loss=10.480002\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:41 INFO 140277876397440] #quality_metric: host=algo-1, epoch=45, batch=0 train loss <loss>=10.480002403259277\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:41 INFO 140277876397440] Epoch[45] Batch[5] avg_epoch_loss=9.934960\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:41 INFO 140277876397440] #quality_metric: host=algo-1, epoch=45, batch=5 train loss <loss>=9.93496004740397\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:41 INFO 140277876397440] Epoch[45] Batch [5]#011Speed: 2049.82 samples/sec#011loss=9.934960\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:41 INFO 140277876397440] processed a total of 638 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458721.046958, \"EndTime\": 1631458721.5990427, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 551.5873432159424, \"count\": 1, \"min\": 551.5873432159424, \"max\": 551.5873432159424}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:41 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1156.397366699308 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:41 INFO 140277876397440] #progress_metric: host=algo-1, completed 23.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:41 INFO 140277876397440] #quality_metric: host=algo-1, epoch=45, train loss <loss>=10.068581199645996\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:41 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:41 INFO 140277876397440] Epoch[46] Batch[0] avg_epoch_loss=10.111564\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:41 INFO 140277876397440] #quality_metric: host=algo-1, epoch=46, batch=0 train loss <loss>=10.111563682556152\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:42 INFO 140277876397440] Epoch[46] Batch[5] avg_epoch_loss=10.131723\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:42 INFO 140277876397440] #quality_metric: host=algo-1, epoch=46, batch=5 train loss <loss>=10.131722927093506\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:42 INFO 140277876397440] Epoch[46] Batch [5]#011Speed: 2074.92 samples/sec#011loss=10.131723\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:42 INFO 140277876397440] Epoch[46] Batch[10] avg_epoch_loss=10.055723\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:42 INFO 140277876397440] #quality_metric: host=algo-1, epoch=46, batch=10 train loss <loss>=9.96452293395996\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:42 INFO 140277876397440] Epoch[46] Batch [10]#011Speed: 1531.26 samples/sec#011loss=9.964523\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:42 INFO 140277876397440] processed a total of 682 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458721.5991297, \"EndTime\": 1631458722.2141159, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 614.4907474517822, \"count\": 1, \"min\": 614.4907474517822, \"max\": 614.4907474517822}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:42 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1109.6451241628247 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:42 INFO 140277876397440] #progress_metric: host=algo-1, completed 23.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:42 INFO 140277876397440] #quality_metric: host=algo-1, epoch=46, train loss <loss>=10.055722930214621\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:42 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:42 INFO 140277876397440] Epoch[47] Batch[0] avg_epoch_loss=9.917677\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:42 INFO 140277876397440] #quality_metric: host=algo-1, epoch=47, batch=0 train loss <loss>=9.91767692565918\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:42 INFO 140277876397440] Epoch[47] Batch[5] avg_epoch_loss=10.005961\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:42 INFO 140277876397440] #quality_metric: host=algo-1, epoch=47, batch=5 train loss <loss>=10.005960941314697\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:42 INFO 140277876397440] Epoch[47] Batch [5]#011Speed: 2139.23 samples/sec#011loss=10.005961\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:42 INFO 140277876397440] processed a total of 638 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458722.2141988, \"EndTime\": 1631458722.79138, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 576.7176151275635, \"count\": 1, \"min\": 576.7176151275635, \"max\": 576.7176151275635}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:42 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1106.043909179248 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:42 INFO 140277876397440] #progress_metric: host=algo-1, completed 24.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:42 INFO 140277876397440] #quality_metric: host=algo-1, epoch=47, train loss <loss>=9.933853721618652\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:42 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:43 INFO 140277876397440] Epoch[48] Batch[0] avg_epoch_loss=9.478661\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:43 INFO 140277876397440] #quality_metric: host=algo-1, epoch=48, batch=0 train loss <loss>=9.478660583496094\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:43 INFO 140277876397440] Epoch[48] Batch[5] avg_epoch_loss=9.810040\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:43 INFO 140277876397440] #quality_metric: host=algo-1, epoch=48, batch=5 train loss <loss>=9.81003967920939\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:43 INFO 140277876397440] Epoch[48] Batch [5]#011Speed: 2128.86 samples/sec#011loss=9.810040\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:43 INFO 140277876397440] processed a total of 632 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458722.7914565, \"EndTime\": 1631458723.3633077, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 571.3708400726318, \"count\": 1, \"min\": 571.3708400726318, \"max\": 571.3708400726318}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:43 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1105.8704110844021 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:43 INFO 140277876397440] #progress_metric: host=algo-1, completed 24.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:43 INFO 140277876397440] #quality_metric: host=algo-1, epoch=48, train loss <loss>=10.010283374786377\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:43 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:43 INFO 140277876397440] Epoch[49] Batch[0] avg_epoch_loss=10.847335\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:43 INFO 140277876397440] #quality_metric: host=algo-1, epoch=49, batch=0 train loss <loss>=10.847334861755371\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:43 INFO 140277876397440] Epoch[49] Batch[5] avg_epoch_loss=10.182806\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:43 INFO 140277876397440] #quality_metric: host=algo-1, epoch=49, batch=5 train loss <loss>=10.18280585606893\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:43 INFO 140277876397440] Epoch[49] Batch [5]#011Speed: 2084.79 samples/sec#011loss=10.182806\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:43 INFO 140277876397440] Epoch[49] Batch[10] avg_epoch_loss=10.593021\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:43 INFO 140277876397440] #quality_metric: host=algo-1, epoch=49, batch=10 train loss <loss>=11.085279083251953\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:43 INFO 140277876397440] Epoch[49] Batch [10]#011Speed: 1819.48 samples/sec#011loss=11.085279\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:43 INFO 140277876397440] processed a total of 650 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458723.3633943, \"EndTime\": 1631458723.9945867, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 630.6755542755127, \"count\": 1, \"min\": 630.6755542755127, \"max\": 630.6755542755127}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:43 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1030.4340201135697 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:43 INFO 140277876397440] #progress_metric: host=algo-1, completed 25.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:43 INFO 140277876397440] #quality_metric: host=algo-1, epoch=49, train loss <loss>=10.59302095933394\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:43 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:44 INFO 140277876397440] Epoch[50] Batch[0] avg_epoch_loss=9.935065\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:44 INFO 140277876397440] #quality_metric: host=algo-1, epoch=50, batch=0 train loss <loss>=9.935065269470215\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:44 INFO 140277876397440] Epoch[50] Batch[5] avg_epoch_loss=9.945057\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:44 INFO 140277876397440] #quality_metric: host=algo-1, epoch=50, batch=5 train loss <loss>=9.945057074228922\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:44 INFO 140277876397440] Epoch[50] Batch [5]#011Speed: 2059.81 samples/sec#011loss=9.945057\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:44 INFO 140277876397440] Epoch[50] Batch[10] avg_epoch_loss=10.036969\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:44 INFO 140277876397440] #quality_metric: host=algo-1, epoch=50, batch=10 train loss <loss>=10.147264099121093\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:44 INFO 140277876397440] Epoch[50] Batch [10]#011Speed: 1839.14 samples/sec#011loss=10.147264\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:44 INFO 140277876397440] processed a total of 681 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458723.9946735, \"EndTime\": 1631458724.567662, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 572.4639892578125, \"count\": 1, \"min\": 572.4639892578125, \"max\": 572.4639892578125}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:44 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1189.3403708609383 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:44 INFO 140277876397440] #progress_metric: host=algo-1, completed 25.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:44 INFO 140277876397440] #quality_metric: host=algo-1, epoch=50, train loss <loss>=10.036969358270818\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:44 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:44 INFO 140277876397440] Epoch[51] Batch[0] avg_epoch_loss=10.836087\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:44 INFO 140277876397440] #quality_metric: host=algo-1, epoch=51, batch=0 train loss <loss>=10.836087226867676\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:44 INFO 140277876397440] Epoch[51] Batch[5] avg_epoch_loss=10.144884\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:44 INFO 140277876397440] #quality_metric: host=algo-1, epoch=51, batch=5 train loss <loss>=10.14488426844279\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:44 INFO 140277876397440] Epoch[51] Batch [5]#011Speed: 2003.60 samples/sec#011loss=10.144884\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:45 INFO 140277876397440] Epoch[51] Batch[10] avg_epoch_loss=9.736718\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:45 INFO 140277876397440] #quality_metric: host=algo-1, epoch=51, batch=10 train loss <loss>=9.246918487548829\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:45 INFO 140277876397440] Epoch[51] Batch [10]#011Speed: 1570.42 samples/sec#011loss=9.246918\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:45 INFO 140277876397440] processed a total of 654 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458724.5677466, \"EndTime\": 1631458725.186576, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 618.3576583862305, \"count\": 1, \"min\": 618.3576583862305, \"max\": 618.3576583862305}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:45 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1057.4307700491963 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:45 INFO 140277876397440] #progress_metric: host=algo-1, completed 26.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:45 INFO 140277876397440] #quality_metric: host=algo-1, epoch=51, train loss <loss>=9.73671800440008\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:45 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:45 INFO 140277876397440] Epoch[52] Batch[0] avg_epoch_loss=10.183532\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:45 INFO 140277876397440] #quality_metric: host=algo-1, epoch=52, batch=0 train loss <loss>=10.183531761169434\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:45 INFO 140277876397440] Epoch[52] Batch[5] avg_epoch_loss=10.372637\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:45 INFO 140277876397440] #quality_metric: host=algo-1, epoch=52, batch=5 train loss <loss>=10.372636795043945\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:45 INFO 140277876397440] Epoch[52] Batch [5]#011Speed: 1694.86 samples/sec#011loss=10.372637\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:45 INFO 140277876397440] processed a total of 630 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458725.1866603, \"EndTime\": 1631458725.8102272, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 623.0647563934326, \"count\": 1, \"min\": 623.0647563934326, \"max\": 623.0647563934326}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:45 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1010.9200311568909 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:45 INFO 140277876397440] #progress_metric: host=algo-1, completed 26.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:45 INFO 140277876397440] #quality_metric: host=algo-1, epoch=52, train loss <loss>=10.36571044921875\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:45 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:46 INFO 140277876397440] Epoch[53] Batch[0] avg_epoch_loss=10.244867\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:46 INFO 140277876397440] #quality_metric: host=algo-1, epoch=53, batch=0 train loss <loss>=10.244867324829102\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:46 INFO 140277876397440] Epoch[53] Batch[5] avg_epoch_loss=10.298241\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:46 INFO 140277876397440] #quality_metric: host=algo-1, epoch=53, batch=5 train loss <loss>=10.29824129740397\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:46 INFO 140277876397440] Epoch[53] Batch [5]#011Speed: 2083.21 samples/sec#011loss=10.298241\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:46 INFO 140277876397440] Epoch[53] Batch[10] avg_epoch_loss=10.429610\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:46 INFO 140277876397440] #quality_metric: host=algo-1, epoch=53, batch=10 train loss <loss>=10.587251472473145\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:46 INFO 140277876397440] Epoch[53] Batch [10]#011Speed: 1838.64 samples/sec#011loss=10.587251\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:46 INFO 140277876397440] processed a total of 677 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458725.8103173, \"EndTime\": 1631458726.3787124, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 567.828893661499, \"count\": 1, \"min\": 567.828893661499, \"max\": 567.828893661499}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:46 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1191.9167539060434 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:46 INFO 140277876397440] #progress_metric: host=algo-1, completed 27.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:46 INFO 140277876397440] #quality_metric: host=algo-1, epoch=53, train loss <loss>=10.42960955879905\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:46 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:46 INFO 140277876397440] Epoch[54] Batch[0] avg_epoch_loss=9.720778\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:46 INFO 140277876397440] #quality_metric: host=algo-1, epoch=54, batch=0 train loss <loss>=9.720778465270996\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:46 INFO 140277876397440] Epoch[54] Batch[5] avg_epoch_loss=10.187639\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:46 INFO 140277876397440] #quality_metric: host=algo-1, epoch=54, batch=5 train loss <loss>=10.187639236450195\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:46 INFO 140277876397440] Epoch[54] Batch [5]#011Speed: 2058.96 samples/sec#011loss=10.187639\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:46 INFO 140277876397440] processed a total of 627 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458726.378837, \"EndTime\": 1631458726.907944, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 528.6102294921875, \"count\": 1, \"min\": 528.6102294921875, \"max\": 528.6102294921875}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:46 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1185.8419318488752 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:46 INFO 140277876397440] #progress_metric: host=algo-1, completed 27.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:46 INFO 140277876397440] #quality_metric: host=algo-1, epoch=54, train loss <loss>=10.265741348266602\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:46 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:47 INFO 140277876397440] Epoch[55] Batch[0] avg_epoch_loss=10.938730\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:47 INFO 140277876397440] #quality_metric: host=algo-1, epoch=55, batch=0 train loss <loss>=10.938730239868164\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:47 INFO 140277876397440] Epoch[55] Batch[5] avg_epoch_loss=9.882440\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:47 INFO 140277876397440] #quality_metric: host=algo-1, epoch=55, batch=5 train loss <loss>=9.882440090179443\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:47 INFO 140277876397440] Epoch[55] Batch [5]#011Speed: 1907.36 samples/sec#011loss=9.882440\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:47 INFO 140277876397440] Epoch[55] Batch[10] avg_epoch_loss=9.675896\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:47 INFO 140277876397440] #quality_metric: host=algo-1, epoch=55, batch=10 train loss <loss>=9.428042221069337\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:47 INFO 140277876397440] Epoch[55] Batch [10]#011Speed: 1827.07 samples/sec#011loss=9.428042\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:47 INFO 140277876397440] processed a total of 654 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458726.9080315, \"EndTime\": 1631458727.5229108, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 614.3577098846436, \"count\": 1, \"min\": 614.3577098846436, \"max\": 614.3577098846436}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:47 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1064.314923088563 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:47 INFO 140277876397440] #progress_metric: host=algo-1, completed 28.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:47 INFO 140277876397440] #quality_metric: host=algo-1, epoch=55, train loss <loss>=9.675895604220303\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:47 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:47 INFO 140277876397440] Epoch[56] Batch[0] avg_epoch_loss=9.137540\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:47 INFO 140277876397440] #quality_metric: host=algo-1, epoch=56, batch=0 train loss <loss>=9.137539863586426\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:47 INFO 140277876397440] Epoch[56] Batch[5] avg_epoch_loss=9.692810\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:47 INFO 140277876397440] #quality_metric: host=algo-1, epoch=56, batch=5 train loss <loss>=9.692810217539469\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:47 INFO 140277876397440] Epoch[56] Batch [5]#011Speed: 2104.36 samples/sec#011loss=9.692810\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:48 INFO 140277876397440] Epoch[56] Batch[10] avg_epoch_loss=9.744362\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:48 INFO 140277876397440] #quality_metric: host=algo-1, epoch=56, batch=10 train loss <loss>=9.806225204467774\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:48 INFO 140277876397440] Epoch[56] Batch [10]#011Speed: 1480.58 samples/sec#011loss=9.806225\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:48 INFO 140277876397440] processed a total of 706 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458727.522995, \"EndTime\": 1631458728.1820157, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 658.545970916748, \"count\": 1, \"min\": 658.545970916748, \"max\": 658.545970916748}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:48 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1071.8581714210839 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:48 INFO 140277876397440] #progress_metric: host=algo-1, completed 28.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:48 INFO 140277876397440] #quality_metric: host=algo-1, epoch=56, train loss <loss>=10.18891461690267\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:48 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:48 INFO 140277876397440] Epoch[57] Batch[0] avg_epoch_loss=10.548836\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:48 INFO 140277876397440] #quality_metric: host=algo-1, epoch=57, batch=0 train loss <loss>=10.548835754394531\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:48 INFO 140277876397440] Epoch[57] Batch[5] avg_epoch_loss=9.871579\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:48 INFO 140277876397440] #quality_metric: host=algo-1, epoch=57, batch=5 train loss <loss>=9.871579011281332\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:48 INFO 140277876397440] Epoch[57] Batch [5]#011Speed: 2105.05 samples/sec#011loss=9.871579\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:48 INFO 140277876397440] Epoch[57] Batch[10] avg_epoch_loss=9.471113\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:48 INFO 140277876397440] #quality_metric: host=algo-1, epoch=57, batch=10 train loss <loss>=8.990553855895996\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:48 INFO 140277876397440] Epoch[57] Batch [10]#011Speed: 1938.24 samples/sec#011loss=8.990554\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:48 INFO 140277876397440] processed a total of 655 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458728.1821012, \"EndTime\": 1631458728.755685, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 573.0762481689453, \"count\": 1, \"min\": 573.0762481689453, \"max\": 573.0762481689453}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:48 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1142.7147610807933 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:48 INFO 140277876397440] #progress_metric: host=algo-1, completed 29.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:48 INFO 140277876397440] #quality_metric: host=algo-1, epoch=57, train loss <loss>=9.471113031560725\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:48 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:49 INFO 140277876397440] Epoch[58] Batch[0] avg_epoch_loss=9.611541\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:49 INFO 140277876397440] #quality_metric: host=algo-1, epoch=58, batch=0 train loss <loss>=9.611540794372559\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:49 INFO 140277876397440] Epoch[58] Batch[5] avg_epoch_loss=9.933458\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:49 INFO 140277876397440] #quality_metric: host=algo-1, epoch=58, batch=5 train loss <loss>=9.933457533518473\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:49 INFO 140277876397440] Epoch[58] Batch [5]#011Speed: 2104.30 samples/sec#011loss=9.933458\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:49 INFO 140277876397440] Epoch[58] Batch[10] avg_epoch_loss=10.285289\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:49 INFO 140277876397440] #quality_metric: host=algo-1, epoch=58, batch=10 train loss <loss>=10.707486152648926\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:49 INFO 140277876397440] Epoch[58] Batch [10]#011Speed: 2001.70 samples/sec#011loss=10.707486\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:49 INFO 140277876397440] processed a total of 657 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458728.7557666, \"EndTime\": 1631458729.3183472, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 562.0839595794678, \"count\": 1, \"min\": 562.0839595794678, \"max\": 562.0839595794678}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:49 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1168.618165533526 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:49 INFO 140277876397440] #progress_metric: host=algo-1, completed 29.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:49 INFO 140277876397440] #quality_metric: host=algo-1, epoch=58, train loss <loss>=10.285288724032315\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:49 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:49 INFO 140277876397440] Epoch[59] Batch[0] avg_epoch_loss=9.956596\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:49 INFO 140277876397440] #quality_metric: host=algo-1, epoch=59, batch=0 train loss <loss>=9.956596374511719\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:49 INFO 140277876397440] Epoch[59] Batch[5] avg_epoch_loss=9.846031\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:49 INFO 140277876397440] #quality_metric: host=algo-1, epoch=59, batch=5 train loss <loss>=9.846031347910563\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:49 INFO 140277876397440] Epoch[59] Batch [5]#011Speed: 1939.97 samples/sec#011loss=9.846031\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:49 INFO 140277876397440] Epoch[59] Batch[10] avg_epoch_loss=9.868347\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:49 INFO 140277876397440] #quality_metric: host=algo-1, epoch=59, batch=10 train loss <loss>=9.89512596130371\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:49 INFO 140277876397440] Epoch[59] Batch [10]#011Speed: 2078.33 samples/sec#011loss=9.895126\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:49 INFO 140277876397440] processed a total of 641 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458729.3184278, \"EndTime\": 1631458729.892669, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 573.753833770752, \"count\": 1, \"min\": 573.753833770752, \"max\": 573.753833770752}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:49 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1116.971325704747 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:49 INFO 140277876397440] #progress_metric: host=algo-1, completed 30.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:49 INFO 140277876397440] #quality_metric: host=algo-1, epoch=59, train loss <loss>=9.868347081271084\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:49 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:50 INFO 140277876397440] Epoch[60] Batch[0] avg_epoch_loss=10.028088\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:50 INFO 140277876397440] #quality_metric: host=algo-1, epoch=60, batch=0 train loss <loss>=10.028087615966797\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:50 INFO 140277876397440] Epoch[60] Batch[5] avg_epoch_loss=9.954784\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:50 INFO 140277876397440] #quality_metric: host=algo-1, epoch=60, batch=5 train loss <loss>=9.954783916473389\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:50 INFO 140277876397440] Epoch[60] Batch [5]#011Speed: 2097.55 samples/sec#011loss=9.954784\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:50 INFO 140277876397440] Epoch[60] Batch[10] avg_epoch_loss=9.612985\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:50 INFO 140277876397440] #quality_metric: host=algo-1, epoch=60, batch=10 train loss <loss>=9.202825546264648\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:50 INFO 140277876397440] Epoch[60] Batch [10]#011Speed: 1951.36 samples/sec#011loss=9.202826\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:50 INFO 140277876397440] processed a total of 657 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458729.8927517, \"EndTime\": 1631458730.4657395, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 572.5142955780029, \"count\": 1, \"min\": 572.5142955780029, \"max\": 572.5142955780029}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:50 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1147.3249677014553 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:50 INFO 140277876397440] #progress_metric: host=algo-1, completed 30.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:50 INFO 140277876397440] #quality_metric: host=algo-1, epoch=60, train loss <loss>=9.612984657287598\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:50 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:50 INFO 140277876397440] Epoch[61] Batch[0] avg_epoch_loss=9.729125\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:50 INFO 140277876397440] #quality_metric: host=algo-1, epoch=61, batch=0 train loss <loss>=9.729125022888184\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:50 INFO 140277876397440] Epoch[61] Batch[5] avg_epoch_loss=10.160443\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:50 INFO 140277876397440] #quality_metric: host=algo-1, epoch=61, batch=5 train loss <loss>=10.160442988077799\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:50 INFO 140277876397440] Epoch[61] Batch [5]#011Speed: 2103.34 samples/sec#011loss=10.160443\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:50 INFO 140277876397440] processed a total of 614 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458730.465823, \"EndTime\": 1631458730.989206, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 522.9105949401855, \"count\": 1, \"min\": 522.9105949401855, \"max\": 522.9105949401855}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:50 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1173.8843283207905 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:50 INFO 140277876397440] #progress_metric: host=algo-1, completed 31.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:50 INFO 140277876397440] #quality_metric: host=algo-1, epoch=61, train loss <loss>=10.189930438995361\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:50 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:51 INFO 140277876397440] Epoch[62] Batch[0] avg_epoch_loss=9.501307\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:51 INFO 140277876397440] #quality_metric: host=algo-1, epoch=62, batch=0 train loss <loss>=9.501307487487793\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:51 INFO 140277876397440] Epoch[62] Batch[5] avg_epoch_loss=9.645570\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:51 INFO 140277876397440] #quality_metric: host=algo-1, epoch=62, batch=5 train loss <loss>=9.645570437113443\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:51 INFO 140277876397440] Epoch[62] Batch [5]#011Speed: 2095.57 samples/sec#011loss=9.645570\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:51 INFO 140277876397440] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458730.9893048, \"EndTime\": 1631458731.5024812, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 512.603759765625, \"count\": 1, \"min\": 512.603759765625, \"max\": 512.603759765625}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:51 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1242.3730274131276 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:51 INFO 140277876397440] #progress_metric: host=algo-1, completed 31.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:51 INFO 140277876397440] #quality_metric: host=algo-1, epoch=62, train loss <loss>=9.789559459686279\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:51 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:51 INFO 140277876397440] Epoch[63] Batch[0] avg_epoch_loss=10.321430\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:51 INFO 140277876397440] #quality_metric: host=algo-1, epoch=63, batch=0 train loss <loss>=10.321430206298828\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:51 INFO 140277876397440] Epoch[63] Batch[5] avg_epoch_loss=10.303125\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:51 INFO 140277876397440] #quality_metric: host=algo-1, epoch=63, batch=5 train loss <loss>=10.303125063578287\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:51 INFO 140277876397440] Epoch[63] Batch [5]#011Speed: 1785.80 samples/sec#011loss=10.303125\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:52 INFO 140277876397440] processed a total of 600 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458731.5025663, \"EndTime\": 1631458732.061239, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 558.1538677215576, \"count\": 1, \"min\": 558.1538677215576, \"max\": 558.1538677215576}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:52 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1074.7281665221074 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:52 INFO 140277876397440] #progress_metric: host=algo-1, completed 32.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:52 INFO 140277876397440] #quality_metric: host=algo-1, epoch=63, train loss <loss>=10.375236988067627\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:52 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:52 INFO 140277876397440] Epoch[64] Batch[0] avg_epoch_loss=10.068181\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:52 INFO 140277876397440] #quality_metric: host=algo-1, epoch=64, batch=0 train loss <loss>=10.068181037902832\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:52 INFO 140277876397440] Epoch[64] Batch[5] avg_epoch_loss=10.019018\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:52 INFO 140277876397440] #quality_metric: host=algo-1, epoch=64, batch=5 train loss <loss>=10.019017855326334\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:52 INFO 140277876397440] Epoch[64] Batch [5]#011Speed: 1947.08 samples/sec#011loss=10.019018\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:52 INFO 140277876397440] processed a total of 616 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458732.0613275, \"EndTime\": 1631458732.6148567, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 553.0338287353516, \"count\": 1, \"min\": 553.0338287353516, \"max\": 553.0338287353516}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:52 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1113.6192268817738 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:52 INFO 140277876397440] #progress_metric: host=algo-1, completed 32.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:52 INFO 140277876397440] #quality_metric: host=algo-1, epoch=64, train loss <loss>=9.830728435516358\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:52 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:52 INFO 140277876397440] Epoch[65] Batch[0] avg_epoch_loss=9.877365\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:52 INFO 140277876397440] #quality_metric: host=algo-1, epoch=65, batch=0 train loss <loss>=9.877365112304688\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:53 INFO 140277876397440] Epoch[65] Batch[5] avg_epoch_loss=9.758254\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:53 INFO 140277876397440] #quality_metric: host=algo-1, epoch=65, batch=5 train loss <loss>=9.758254210154215\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:53 INFO 140277876397440] Epoch[65] Batch [5]#011Speed: 1878.09 samples/sec#011loss=9.758254\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:53 INFO 140277876397440] processed a total of 621 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458732.6149402, \"EndTime\": 1631458733.155638, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 540.1773452758789, \"count\": 1, \"min\": 540.1773452758789, \"max\": 540.1773452758789}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:53 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1149.352565528197 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:53 INFO 140277876397440] #progress_metric: host=algo-1, completed 33.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:53 INFO 140277876397440] #quality_metric: host=algo-1, epoch=65, train loss <loss>=9.80074863433838\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:53 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:53 INFO 140277876397440] Epoch[66] Batch[0] avg_epoch_loss=9.941035\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:53 INFO 140277876397440] #quality_metric: host=algo-1, epoch=66, batch=0 train loss <loss>=9.941035270690918\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:53 INFO 140277876397440] Epoch[66] Batch[5] avg_epoch_loss=10.091299\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:53 INFO 140277876397440] #quality_metric: host=algo-1, epoch=66, batch=5 train loss <loss>=10.091299374898275\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:53 INFO 140277876397440] Epoch[66] Batch [5]#011Speed: 2091.45 samples/sec#011loss=10.091299\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:53 INFO 140277876397440] processed a total of 607 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458733.1557257, \"EndTime\": 1631458733.7009983, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 544.7123050689697, \"count\": 1, \"min\": 544.7123050689697, \"max\": 544.7123050689697}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:53 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1114.0820028373412 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:53 INFO 140277876397440] #progress_metric: host=algo-1, completed 33.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:53 INFO 140277876397440] #quality_metric: host=algo-1, epoch=66, train loss <loss>=10.021959495544433\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:53 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:53 INFO 140277876397440] Epoch[67] Batch[0] avg_epoch_loss=8.878386\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:53 INFO 140277876397440] #quality_metric: host=algo-1, epoch=67, batch=0 train loss <loss>=8.878386497497559\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:54 INFO 140277876397440] Epoch[67] Batch[5] avg_epoch_loss=9.698240\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:54 INFO 140277876397440] #quality_metric: host=algo-1, epoch=67, batch=5 train loss <loss>=9.698240280151367\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:54 INFO 140277876397440] Epoch[67] Batch [5]#011Speed: 2022.47 samples/sec#011loss=9.698240\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:54 INFO 140277876397440] Epoch[67] Batch[10] avg_epoch_loss=10.139087\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:54 INFO 140277876397440] #quality_metric: host=algo-1, epoch=67, batch=10 train loss <loss>=10.66810359954834\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:54 INFO 140277876397440] Epoch[67] Batch [10]#011Speed: 2072.49 samples/sec#011loss=10.668104\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:54 INFO 140277876397440] processed a total of 642 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458733.701092, \"EndTime\": 1631458734.273248, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 571.6705322265625, \"count\": 1, \"min\": 571.6705322265625, \"max\": 571.6705322265625}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:54 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1122.7931520180932 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:54 INFO 140277876397440] #progress_metric: host=algo-1, completed 34.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:54 INFO 140277876397440] #quality_metric: host=algo-1, epoch=67, train loss <loss>=10.139087243513627\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:54 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:54 INFO 140277876397440] Epoch[68] Batch[0] avg_epoch_loss=9.540505\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:54 INFO 140277876397440] #quality_metric: host=algo-1, epoch=68, batch=0 train loss <loss>=9.540505409240723\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:54 INFO 140277876397440] Epoch[68] Batch[5] avg_epoch_loss=10.004479\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:54 INFO 140277876397440] #quality_metric: host=algo-1, epoch=68, batch=5 train loss <loss>=10.00447909037272\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:54 INFO 140277876397440] Epoch[68] Batch [5]#011Speed: 2121.65 samples/sec#011loss=10.004479\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:54 INFO 140277876397440] Epoch[68] Batch[10] avg_epoch_loss=10.142730\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:54 INFO 140277876397440] #quality_metric: host=algo-1, epoch=68, batch=10 train loss <loss>=10.308631706237794\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:54 INFO 140277876397440] Epoch[68] Batch [10]#011Speed: 1949.75 samples/sec#011loss=10.308632\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:54 INFO 140277876397440] processed a total of 666 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458734.2733302, \"EndTime\": 1631458734.8413906, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 567.5923824310303, \"count\": 1, \"min\": 567.5923824310303, \"max\": 567.5923824310303}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:54 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1173.1303887691524 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:54 INFO 140277876397440] #progress_metric: host=algo-1, completed 34.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:54 INFO 140277876397440] #quality_metric: host=algo-1, epoch=68, train loss <loss>=10.142730279402299\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:54 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:55 INFO 140277876397440] Epoch[69] Batch[0] avg_epoch_loss=10.922283\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:55 INFO 140277876397440] #quality_metric: host=algo-1, epoch=69, batch=0 train loss <loss>=10.922283172607422\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:55 INFO 140277876397440] Epoch[69] Batch[5] avg_epoch_loss=10.388604\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:55 INFO 140277876397440] #quality_metric: host=algo-1, epoch=69, batch=5 train loss <loss>=10.388604164123535\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:55 INFO 140277876397440] Epoch[69] Batch [5]#011Speed: 1592.71 samples/sec#011loss=10.388604\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:55 INFO 140277876397440] processed a total of 623 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458734.841473, \"EndTime\": 1631458735.422252, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 580.3027153015137, \"count\": 1, \"min\": 580.3027153015137, \"max\": 580.3027153015137}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:55 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1073.3253668663212 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:55 INFO 140277876397440] #progress_metric: host=algo-1, completed 35.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:55 INFO 140277876397440] #quality_metric: host=algo-1, epoch=69, train loss <loss>=10.089328575134278\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:55 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:55 INFO 140277876397440] Epoch[70] Batch[0] avg_epoch_loss=9.782084\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:55 INFO 140277876397440] #quality_metric: host=algo-1, epoch=70, batch=0 train loss <loss>=9.782083511352539\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:55 INFO 140277876397440] Epoch[70] Batch[5] avg_epoch_loss=9.959685\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:55 INFO 140277876397440] #quality_metric: host=algo-1, epoch=70, batch=5 train loss <loss>=9.959684689839682\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:55 INFO 140277876397440] Epoch[70] Batch [5]#011Speed: 2136.44 samples/sec#011loss=9.959685\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:56 INFO 140277876397440] Epoch[70] Batch[10] avg_epoch_loss=10.122798\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:56 INFO 140277876397440] #quality_metric: host=algo-1, epoch=70, batch=10 train loss <loss>=10.318535041809081\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:56 INFO 140277876397440] Epoch[70] Batch [10]#011Speed: 1843.86 samples/sec#011loss=10.318535\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:56 INFO 140277876397440] processed a total of 647 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458735.4223487, \"EndTime\": 1631458736.0304952, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 607.6006889343262, \"count\": 1, \"min\": 607.6006889343262, \"max\": 607.6006889343262}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:56 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1064.6243481726865 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:56 INFO 140277876397440] #progress_metric: host=algo-1, completed 35.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:56 INFO 140277876397440] #quality_metric: host=algo-1, epoch=70, train loss <loss>=10.122798486189408\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:56 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:56 INFO 140277876397440] Epoch[71] Batch[0] avg_epoch_loss=10.617148\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:56 INFO 140277876397440] #quality_metric: host=algo-1, epoch=71, batch=0 train loss <loss>=10.617148399353027\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:56 INFO 140277876397440] Epoch[71] Batch[5] avg_epoch_loss=10.161376\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:56 INFO 140277876397440] #quality_metric: host=algo-1, epoch=71, batch=5 train loss <loss>=10.161375999450684\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:56 INFO 140277876397440] Epoch[71] Batch [5]#011Speed: 2124.54 samples/sec#011loss=10.161376\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:56 INFO 140277876397440] Epoch[71] Batch[10] avg_epoch_loss=10.273445\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:56 INFO 140277876397440] #quality_metric: host=algo-1, epoch=71, batch=10 train loss <loss>=10.407928848266602\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:56 INFO 140277876397440] Epoch[71] Batch [10]#011Speed: 1404.01 samples/sec#011loss=10.407929\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:56 INFO 140277876397440] processed a total of 681 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458736.0305803, \"EndTime\": 1631458736.6497126, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 618.5817718505859, \"count\": 1, \"min\": 618.5817718505859, \"max\": 618.5817718505859}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:56 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1100.685625412807 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:56 INFO 140277876397440] #progress_metric: host=algo-1, completed 36.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:56 INFO 140277876397440] #quality_metric: host=algo-1, epoch=71, train loss <loss>=10.273445476185191\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:56 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:56 INFO 140277876397440] Epoch[72] Batch[0] avg_epoch_loss=9.729920\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:56 INFO 140277876397440] #quality_metric: host=algo-1, epoch=72, batch=0 train loss <loss>=9.729920387268066\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:57 INFO 140277876397440] Epoch[72] Batch[5] avg_epoch_loss=10.179741\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:57 INFO 140277876397440] #quality_metric: host=algo-1, epoch=72, batch=5 train loss <loss>=10.179741223653158\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:57 INFO 140277876397440] Epoch[72] Batch [5]#011Speed: 1987.32 samples/sec#011loss=10.179741\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:57 INFO 140277876397440] Epoch[72] Batch[10] avg_epoch_loss=10.069313\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:57 INFO 140277876397440] #quality_metric: host=algo-1, epoch=72, batch=10 train loss <loss>=9.936798667907714\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:57 INFO 140277876397440] Epoch[72] Batch [10]#011Speed: 1707.57 samples/sec#011loss=9.936799\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:57 INFO 140277876397440] processed a total of 679 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458736.6497986, \"EndTime\": 1631458737.3003037, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 650.0251293182373, \"count\": 1, \"min\": 650.0251293182373, \"max\": 650.0251293182373}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:57 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1044.3501501655671 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:57 INFO 140277876397440] #progress_metric: host=algo-1, completed 36.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:57 INFO 140277876397440] #quality_metric: host=algo-1, epoch=72, train loss <loss>=10.06931278922341\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:57 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:57 INFO 140277876397440] Epoch[73] Batch[0] avg_epoch_loss=10.209142\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:57 INFO 140277876397440] #quality_metric: host=algo-1, epoch=73, batch=0 train loss <loss>=10.209141731262207\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:57 INFO 140277876397440] Epoch[73] Batch[5] avg_epoch_loss=9.823408\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:57 INFO 140277876397440] #quality_metric: host=algo-1, epoch=73, batch=5 train loss <loss>=9.823407967885336\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:57 INFO 140277876397440] Epoch[73] Batch [5]#011Speed: 1621.28 samples/sec#011loss=9.823408\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:57 INFO 140277876397440] Epoch[73] Batch[10] avg_epoch_loss=10.080512\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:57 INFO 140277876397440] #quality_metric: host=algo-1, epoch=73, batch=10 train loss <loss>=10.38903579711914\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:57 INFO 140277876397440] Epoch[73] Batch [10]#011Speed: 1793.83 samples/sec#011loss=10.389036\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:57 INFO 140277876397440] processed a total of 652 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458737.3004022, \"EndTime\": 1631458737.938605, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 637.6593112945557, \"count\": 1, \"min\": 637.6593112945557, \"max\": 637.6593112945557}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:57 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1022.3045755528648 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:57 INFO 140277876397440] #progress_metric: host=algo-1, completed 37.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:57 INFO 140277876397440] #quality_metric: host=algo-1, epoch=73, train loss <loss>=10.080511526627975\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:57 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:58 INFO 140277876397440] Epoch[74] Batch[0] avg_epoch_loss=8.847339\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:58 INFO 140277876397440] #quality_metric: host=algo-1, epoch=74, batch=0 train loss <loss>=8.847338676452637\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:58 INFO 140277876397440] Epoch[74] Batch[5] avg_epoch_loss=9.248562\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:58 INFO 140277876397440] #quality_metric: host=algo-1, epoch=74, batch=5 train loss <loss>=9.248562018076578\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:58 INFO 140277876397440] Epoch[74] Batch [5]#011Speed: 2138.46 samples/sec#011loss=9.248562\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:58 INFO 140277876397440] Epoch[74] Batch[10] avg_epoch_loss=9.296098\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:58 INFO 140277876397440] #quality_metric: host=algo-1, epoch=74, batch=10 train loss <loss>=9.353141975402831\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:58 INFO 140277876397440] Epoch[74] Batch [10]#011Speed: 1998.09 samples/sec#011loss=9.353142\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:58 INFO 140277876397440] processed a total of 656 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458737.938686, \"EndTime\": 1631458738.4952998, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 556.0927391052246, \"count\": 1, \"min\": 556.0927391052246, \"max\": 556.0927391052246}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:58 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1179.4009617935878 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:58 INFO 140277876397440] #progress_metric: host=algo-1, completed 37.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:58 INFO 140277876397440] #quality_metric: host=algo-1, epoch=74, train loss <loss>=9.296098362315785\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:58 INFO 140277876397440] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:58 INFO 140277876397440] Saved checkpoint to \"/opt/ml/model/state_6260c4cb-9b27-40e1-86d4-51162655d010-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458738.4953835, \"EndTime\": 1631458738.5143123, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 18.435239791870117, \"count\": 1, \"min\": 18.435239791870117, \"max\": 18.435239791870117}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:58 INFO 140277876397440] Epoch[75] Batch[0] avg_epoch_loss=10.151061\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:58 INFO 140277876397440] #quality_metric: host=algo-1, epoch=75, batch=0 train loss <loss>=10.151061058044434\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:58 INFO 140277876397440] Epoch[75] Batch[5] avg_epoch_loss=9.964967\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:58 INFO 140277876397440] #quality_metric: host=algo-1, epoch=75, batch=5 train loss <loss>=9.964967409769693\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:58 INFO 140277876397440] Epoch[75] Batch [5]#011Speed: 1868.99 samples/sec#011loss=9.964967\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:59 INFO 140277876397440] processed a total of 607 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458738.514379, \"EndTime\": 1631458739.0556908, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 541.2552356719971, \"count\": 1, \"min\": 541.2552356719971, \"max\": 541.2552356719971}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:59 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1121.210885767787 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:59 INFO 140277876397440] #progress_metric: host=algo-1, completed 38.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:59 INFO 140277876397440] #quality_metric: host=algo-1, epoch=75, train loss <loss>=10.052732849121094\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:59 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:59 INFO 140277876397440] Epoch[76] Batch[0] avg_epoch_loss=10.037102\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:59 INFO 140277876397440] #quality_metric: host=algo-1, epoch=76, batch=0 train loss <loss>=10.037101745605469\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:59 INFO 140277876397440] Epoch[76] Batch[5] avg_epoch_loss=9.890774\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:59 INFO 140277876397440] #quality_metric: host=algo-1, epoch=76, batch=5 train loss <loss>=9.890773932139078\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:59 INFO 140277876397440] Epoch[76] Batch [5]#011Speed: 2049.90 samples/sec#011loss=9.890774\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:59 INFO 140277876397440] processed a total of 618 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458739.05578, \"EndTime\": 1631458739.615896, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 559.6063137054443, \"count\": 1, \"min\": 559.6063137054443, \"max\": 559.6063137054443}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:59 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1104.0862591924488 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:59 INFO 140277876397440] #progress_metric: host=algo-1, completed 38.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:59 INFO 140277876397440] #quality_metric: host=algo-1, epoch=76, train loss <loss>=9.877413463592529\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:59 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:59 INFO 140277876397440] Epoch[77] Batch[0] avg_epoch_loss=9.795965\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:58:59 INFO 140277876397440] #quality_metric: host=algo-1, epoch=77, batch=0 train loss <loss>=9.795965194702148\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:00 INFO 140277876397440] Epoch[77] Batch[5] avg_epoch_loss=9.705319\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:00 INFO 140277876397440] #quality_metric: host=algo-1, epoch=77, batch=5 train loss <loss>=9.705319245656332\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:00 INFO 140277876397440] Epoch[77] Batch [5]#011Speed: 1831.14 samples/sec#011loss=9.705319\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:00 INFO 140277876397440] processed a total of 635 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458739.615989, \"EndTime\": 1631458740.1988034, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 582.3071002960205, \"count\": 1, \"min\": 582.3071002960205, \"max\": 582.3071002960205}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:00 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1090.2434745233686 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:00 INFO 140277876397440] #progress_metric: host=algo-1, completed 39.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:00 INFO 140277876397440] #quality_metric: host=algo-1, epoch=77, train loss <loss>=9.815095615386962\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:00 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:00 INFO 140277876397440] Epoch[78] Batch[0] avg_epoch_loss=10.446689\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:00 INFO 140277876397440] #quality_metric: host=algo-1, epoch=78, batch=0 train loss <loss>=10.446688652038574\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:00 INFO 140277876397440] Epoch[78] Batch[5] avg_epoch_loss=10.048386\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:00 INFO 140277876397440] #quality_metric: host=algo-1, epoch=78, batch=5 train loss <loss>=10.048386414845785\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:00 INFO 140277876397440] Epoch[78] Batch [5]#011Speed: 1998.43 samples/sec#011loss=10.048386\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:00 INFO 140277876397440] Epoch[78] Batch[10] avg_epoch_loss=9.753425\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:00 INFO 140277876397440] #quality_metric: host=algo-1, epoch=78, batch=10 train loss <loss>=9.399471092224122\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:00 INFO 140277876397440] Epoch[78] Batch [10]#011Speed: 1754.20 samples/sec#011loss=9.399471\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:00 INFO 140277876397440] processed a total of 696 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458740.1988957, \"EndTime\": 1631458740.816666, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 617.2897815704346, \"count\": 1, \"min\": 617.2897815704346, \"max\": 617.2897815704346}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:00 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1127.285990885128 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:00 INFO 140277876397440] #progress_metric: host=algo-1, completed 39.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:00 INFO 140277876397440] #quality_metric: host=algo-1, epoch=78, train loss <loss>=9.75342490456321\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:00 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:01 INFO 140277876397440] Epoch[79] Batch[0] avg_epoch_loss=10.347203\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:01 INFO 140277876397440] #quality_metric: host=algo-1, epoch=79, batch=0 train loss <loss>=10.347203254699707\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:01 INFO 140277876397440] Epoch[79] Batch[5] avg_epoch_loss=9.869306\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:01 INFO 140277876397440] #quality_metric: host=algo-1, epoch=79, batch=5 train loss <loss>=9.869305928548178\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:01 INFO 140277876397440] Epoch[79] Batch [5]#011Speed: 1665.98 samples/sec#011loss=9.869306\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:01 INFO 140277876397440] Epoch[79] Batch[10] avg_epoch_loss=9.970994\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:01 INFO 140277876397440] #quality_metric: host=algo-1, epoch=79, batch=10 train loss <loss>=10.093018722534179\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:01 INFO 140277876397440] Epoch[79] Batch [10]#011Speed: 1621.98 samples/sec#011loss=10.093019\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:01 INFO 140277876397440] processed a total of 671 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458740.8167517, \"EndTime\": 1631458741.4593828, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 642.1654224395752, \"count\": 1, \"min\": 642.1654224395752, \"max\": 642.1654224395752}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:01 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1044.702737354911 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:01 INFO 140277876397440] #progress_metric: host=algo-1, completed 40.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:01 INFO 140277876397440] #quality_metric: host=algo-1, epoch=79, train loss <loss>=9.970993562178178\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:01 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:01 INFO 140277876397440] Epoch[80] Batch[0] avg_epoch_loss=9.644109\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:01 INFO 140277876397440] #quality_metric: host=algo-1, epoch=80, batch=0 train loss <loss>=9.644108772277832\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:01 INFO 140277876397440] Epoch[80] Batch[5] avg_epoch_loss=9.898189\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:01 INFO 140277876397440] #quality_metric: host=algo-1, epoch=80, batch=5 train loss <loss>=9.898188591003418\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:01 INFO 140277876397440] Epoch[80] Batch [5]#011Speed: 1673.85 samples/sec#011loss=9.898189\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:02 INFO 140277876397440] Epoch[80] Batch[10] avg_epoch_loss=10.383504\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:02 INFO 140277876397440] #quality_metric: host=algo-1, epoch=80, batch=10 train loss <loss>=10.965883255004883\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:02 INFO 140277876397440] Epoch[80] Batch [10]#011Speed: 1470.23 samples/sec#011loss=10.965883\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:02 INFO 140277876397440] processed a total of 644 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458741.4594681, \"EndTime\": 1631458742.1741211, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 714.1759395599365, \"count\": 1, \"min\": 714.1759395599365, \"max\": 714.1759395599365}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:02 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=901.582072843343 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:02 INFO 140277876397440] #progress_metric: host=algo-1, completed 40.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:02 INFO 140277876397440] #quality_metric: host=algo-1, epoch=80, train loss <loss>=10.38350434736772\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:02 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:02 INFO 140277876397440] Epoch[81] Batch[0] avg_epoch_loss=8.958817\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:02 INFO 140277876397440] #quality_metric: host=algo-1, epoch=81, batch=0 train loss <loss>=8.958817481994629\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:02 INFO 140277876397440] Epoch[81] Batch[5] avg_epoch_loss=10.014245\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:02 INFO 140277876397440] #quality_metric: host=algo-1, epoch=81, batch=5 train loss <loss>=10.014244874318441\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:02 INFO 140277876397440] Epoch[81] Batch [5]#011Speed: 1774.75 samples/sec#011loss=10.014245\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:02 INFO 140277876397440] processed a total of 589 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458742.1742077, \"EndTime\": 1631458742.7880008, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 613.3182048797607, \"count\": 1, \"min\": 613.3182048797607, \"max\": 613.3182048797607}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:02 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=960.1515817882341 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:02 INFO 140277876397440] #progress_metric: host=algo-1, completed 41.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:02 INFO 140277876397440] #quality_metric: host=algo-1, epoch=81, train loss <loss>=10.554895782470703\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:02 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:03 INFO 140277876397440] Epoch[82] Batch[0] avg_epoch_loss=10.477890\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:03 INFO 140277876397440] #quality_metric: host=algo-1, epoch=82, batch=0 train loss <loss>=10.477890014648438\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:03 INFO 140277876397440] Epoch[82] Batch[5] avg_epoch_loss=10.726388\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:03 INFO 140277876397440] #quality_metric: host=algo-1, epoch=82, batch=5 train loss <loss>=10.726387977600098\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:03 INFO 140277876397440] Epoch[82] Batch [5]#011Speed: 1811.89 samples/sec#011loss=10.726388\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:03 INFO 140277876397440] Epoch[82] Batch[10] avg_epoch_loss=10.456219\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:03 INFO 140277876397440] #quality_metric: host=algo-1, epoch=82, batch=10 train loss <loss>=10.132015419006347\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:03 INFO 140277876397440] Epoch[82] Batch [10]#011Speed: 1353.81 samples/sec#011loss=10.132015\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:03 INFO 140277876397440] processed a total of 703 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458742.7880888, \"EndTime\": 1631458743.4647968, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 676.1832237243652, \"count\": 1, \"min\": 676.1832237243652, \"max\": 676.1832237243652}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:03 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1039.4658873422652 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:03 INFO 140277876397440] #progress_metric: host=algo-1, completed 41.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:03 INFO 140277876397440] #quality_metric: host=algo-1, epoch=82, train loss <loss>=10.456218632784756\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:03 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:03 INFO 140277876397440] Epoch[83] Batch[0] avg_epoch_loss=9.730129\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:03 INFO 140277876397440] #quality_metric: host=algo-1, epoch=83, batch=0 train loss <loss>=9.73012924194336\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:03 INFO 140277876397440] Epoch[83] Batch[5] avg_epoch_loss=10.447847\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:03 INFO 140277876397440] #quality_metric: host=algo-1, epoch=83, batch=5 train loss <loss>=10.44784673055013\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:03 INFO 140277876397440] Epoch[83] Batch [5]#011Speed: 1567.68 samples/sec#011loss=10.447847\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:04 INFO 140277876397440] processed a total of 623 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458743.4648843, \"EndTime\": 1631458744.1518948, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 686.54465675354, \"count\": 1, \"min\": 686.54465675354, \"max\": 686.54465675354}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:04 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=907.26509292427 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:04 INFO 140277876397440] #progress_metric: host=algo-1, completed 42.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:04 INFO 140277876397440] #quality_metric: host=algo-1, epoch=83, train loss <loss>=10.248416137695312\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:04 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:04 INFO 140277876397440] Epoch[84] Batch[0] avg_epoch_loss=9.836299\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:04 INFO 140277876397440] #quality_metric: host=algo-1, epoch=84, batch=0 train loss <loss>=9.836298942565918\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:04 INFO 140277876397440] Epoch[84] Batch[5] avg_epoch_loss=9.730286\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:04 INFO 140277876397440] #quality_metric: host=algo-1, epoch=84, batch=5 train loss <loss>=9.730286280314127\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:04 INFO 140277876397440] Epoch[84] Batch [5]#011Speed: 1616.39 samples/sec#011loss=9.730286\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:04 INFO 140277876397440] Epoch[84] Batch[10] avg_epoch_loss=9.626166\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:04 INFO 140277876397440] #quality_metric: host=algo-1, epoch=84, batch=10 train loss <loss>=9.501222038269043\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:04 INFO 140277876397440] Epoch[84] Batch [10]#011Speed: 1999.64 samples/sec#011loss=9.501222\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:04 INFO 140277876397440] processed a total of 656 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458744.1519895, \"EndTime\": 1631458744.825342, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 672.6779937744141, \"count\": 1, \"min\": 672.6779937744141, \"max\": 672.6779937744141}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:04 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=975.0330888424114 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:04 INFO 140277876397440] #progress_metric: host=algo-1, completed 42.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:04 INFO 140277876397440] #quality_metric: host=algo-1, epoch=84, train loss <loss>=9.626166170293635\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:04 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:05 INFO 140277876397440] Epoch[85] Batch[0] avg_epoch_loss=10.418841\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:05 INFO 140277876397440] #quality_metric: host=algo-1, epoch=85, batch=0 train loss <loss>=10.418841361999512\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:05 INFO 140277876397440] Epoch[85] Batch[5] avg_epoch_loss=10.133788\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:05 INFO 140277876397440] #quality_metric: host=algo-1, epoch=85, batch=5 train loss <loss>=10.133787949879965\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:05 INFO 140277876397440] Epoch[85] Batch [5]#011Speed: 2037.85 samples/sec#011loss=10.133788\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:05 INFO 140277876397440] Epoch[85] Batch[10] avg_epoch_loss=9.990565\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:05 INFO 140277876397440] #quality_metric: host=algo-1, epoch=85, batch=10 train loss <loss>=9.818696594238281\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:05 INFO 140277876397440] Epoch[85] Batch [10]#011Speed: 1801.04 samples/sec#011loss=9.818697\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:05 INFO 140277876397440] processed a total of 669 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458744.825425, \"EndTime\": 1631458745.4120872, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 586.1935615539551, \"count\": 1, \"min\": 586.1935615539551, \"max\": 586.1935615539551}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:05 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1141.068654286201 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:05 INFO 140277876397440] #progress_metric: host=algo-1, completed 43.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:05 INFO 140277876397440] #quality_metric: host=algo-1, epoch=85, train loss <loss>=9.990564606406473\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:05 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:05 INFO 140277876397440] Epoch[86] Batch[0] avg_epoch_loss=10.382256\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:05 INFO 140277876397440] #quality_metric: host=algo-1, epoch=86, batch=0 train loss <loss>=10.382255554199219\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:05 INFO 140277876397440] Epoch[86] Batch[5] avg_epoch_loss=10.002649\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:05 INFO 140277876397440] #quality_metric: host=algo-1, epoch=86, batch=5 train loss <loss>=10.002648512522379\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:05 INFO 140277876397440] Epoch[86] Batch [5]#011Speed: 2032.17 samples/sec#011loss=10.002649\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:05 INFO 140277876397440] Epoch[86] Batch[10] avg_epoch_loss=9.947043\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:06 INFO 140277876397440] #quality_metric: host=algo-1, epoch=86, batch=10 train loss <loss>=9.880317306518554\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:06 INFO 140277876397440] Epoch[86] Batch [10]#011Speed: 1750.65 samples/sec#011loss=9.880317\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:06 INFO 140277876397440] processed a total of 699 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458745.4121487, \"EndTime\": 1631458746.0005672, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 587.9342555999756, \"count\": 1, \"min\": 587.9342555999756, \"max\": 587.9342555999756}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:06 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1188.6582720249876 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:06 INFO 140277876397440] #progress_metric: host=algo-1, completed 43.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:06 INFO 140277876397440] #quality_metric: host=algo-1, epoch=86, train loss <loss>=9.947043418884277\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:06 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:06 INFO 140277876397440] Epoch[87] Batch[0] avg_epoch_loss=9.403345\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:06 INFO 140277876397440] #quality_metric: host=algo-1, epoch=87, batch=0 train loss <loss>=9.403345108032227\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:06 INFO 140277876397440] Epoch[87] Batch[5] avg_epoch_loss=9.667081\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:06 INFO 140277876397440] #quality_metric: host=algo-1, epoch=87, batch=5 train loss <loss>=9.667081038157145\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:06 INFO 140277876397440] Epoch[87] Batch [5]#011Speed: 2119.39 samples/sec#011loss=9.667081\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:06 INFO 140277876397440] processed a total of 625 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458746.0006526, \"EndTime\": 1631458746.5622377, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 561.1021518707275, \"count\": 1, \"min\": 561.1021518707275, \"max\": 561.1021518707275}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:06 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1113.6638616620671 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:06 INFO 140277876397440] #progress_metric: host=algo-1, completed 44.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:06 INFO 140277876397440] #quality_metric: host=algo-1, epoch=87, train loss <loss>=9.652797222137451\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:06 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:06 INFO 140277876397440] Epoch[88] Batch[0] avg_epoch_loss=10.239574\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:06 INFO 140277876397440] #quality_metric: host=algo-1, epoch=88, batch=0 train loss <loss>=10.239574432373047\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:06 INFO 140277876397440] Epoch[88] Batch[5] avg_epoch_loss=9.993293\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:06 INFO 140277876397440] #quality_metric: host=algo-1, epoch=88, batch=5 train loss <loss>=9.993292649586996\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:06 INFO 140277876397440] Epoch[88] Batch [5]#011Speed: 2042.77 samples/sec#011loss=9.993293\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:07 INFO 140277876397440] processed a total of 632 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458746.562312, \"EndTime\": 1631458747.1092453, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 546.3626384735107, \"count\": 1, \"min\": 546.3626384735107, \"max\": 546.3626384735107}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:07 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1156.4693778633064 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:07 INFO 140277876397440] #progress_metric: host=algo-1, completed 44.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:07 INFO 140277876397440] #quality_metric: host=algo-1, epoch=88, train loss <loss>=9.93919677734375\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:07 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:07 INFO 140277876397440] Epoch[89] Batch[0] avg_epoch_loss=9.975951\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:07 INFO 140277876397440] #quality_metric: host=algo-1, epoch=89, batch=0 train loss <loss>=9.975951194763184\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:07 INFO 140277876397440] Epoch[89] Batch[5] avg_epoch_loss=9.670599\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:07 INFO 140277876397440] #quality_metric: host=algo-1, epoch=89, batch=5 train loss <loss>=9.67059882481893\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:07 INFO 140277876397440] Epoch[89] Batch [5]#011Speed: 1823.77 samples/sec#011loss=9.670599\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:07 INFO 140277876397440] Epoch[89] Batch[10] avg_epoch_loss=9.880931\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:07 INFO 140277876397440] #quality_metric: host=algo-1, epoch=89, batch=10 train loss <loss>=10.133329582214355\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:07 INFO 140277876397440] Epoch[89] Batch [10]#011Speed: 1835.83 samples/sec#011loss=10.133330\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:07 INFO 140277876397440] processed a total of 665 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458747.109334, \"EndTime\": 1631458747.71972, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 609.8721027374268, \"count\": 1, \"min\": 609.8721027374268, \"max\": 609.8721027374268}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:07 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1090.1820180161446 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:07 INFO 140277876397440] #progress_metric: host=algo-1, completed 45.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:07 INFO 140277876397440] #quality_metric: host=algo-1, epoch=89, train loss <loss>=9.880930987271396\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:07 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:07 INFO 140277876397440] Epoch[90] Batch[0] avg_epoch_loss=10.154954\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:07 INFO 140277876397440] #quality_metric: host=algo-1, epoch=90, batch=0 train loss <loss>=10.154953956604004\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:08 INFO 140277876397440] Epoch[90] Batch[5] avg_epoch_loss=10.114651\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:08 INFO 140277876397440] #quality_metric: host=algo-1, epoch=90, batch=5 train loss <loss>=10.114650885264078\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:08 INFO 140277876397440] Epoch[90] Batch [5]#011Speed: 1984.38 samples/sec#011loss=10.114651\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:08 INFO 140277876397440] Epoch[90] Batch[10] avg_epoch_loss=10.279814\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:08 INFO 140277876397440] #quality_metric: host=algo-1, epoch=90, batch=10 train loss <loss>=10.478009986877442\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:08 INFO 140277876397440] Epoch[90] Batch [10]#011Speed: 2073.42 samples/sec#011loss=10.478010\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:08 INFO 140277876397440] processed a total of 641 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458747.719801, \"EndTime\": 1631458748.2958512, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 575.5698680877686, \"count\": 1, \"min\": 575.5698680877686, \"max\": 575.5698680877686}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:08 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1113.4473351260262 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:08 INFO 140277876397440] #progress_metric: host=algo-1, completed 45.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:08 INFO 140277876397440] #quality_metric: host=algo-1, epoch=90, train loss <loss>=10.279814113270152\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:08 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:08 INFO 140277876397440] Epoch[91] Batch[0] avg_epoch_loss=9.759288\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:08 INFO 140277876397440] #quality_metric: host=algo-1, epoch=91, batch=0 train loss <loss>=9.75928783416748\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:08 INFO 140277876397440] Epoch[91] Batch[5] avg_epoch_loss=9.473976\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:08 INFO 140277876397440] #quality_metric: host=algo-1, epoch=91, batch=5 train loss <loss>=9.473976135253906\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:08 INFO 140277876397440] Epoch[91] Batch [5]#011Speed: 1767.88 samples/sec#011loss=9.473976\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:08 INFO 140277876397440] processed a total of 612 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458748.2959347, \"EndTime\": 1631458748.8658361, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 569.434642791748, \"count\": 1, \"min\": 569.434642791748, \"max\": 569.434642791748}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:08 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1074.5216598503907 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:08 INFO 140277876397440] #progress_metric: host=algo-1, completed 46.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:08 INFO 140277876397440] #quality_metric: host=algo-1, epoch=91, train loss <loss>=9.565010166168213\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:08 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:09 INFO 140277876397440] Epoch[92] Batch[0] avg_epoch_loss=10.173134\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:09 INFO 140277876397440] #quality_metric: host=algo-1, epoch=92, batch=0 train loss <loss>=10.173133850097656\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:09 INFO 140277876397440] Epoch[92] Batch[5] avg_epoch_loss=10.156781\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:09 INFO 140277876397440] #quality_metric: host=algo-1, epoch=92, batch=5 train loss <loss>=10.15678071975708\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:09 INFO 140277876397440] Epoch[92] Batch [5]#011Speed: 1871.43 samples/sec#011loss=10.156781\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:09 INFO 140277876397440] Epoch[92] Batch[10] avg_epoch_loss=9.756066\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:09 INFO 140277876397440] #quality_metric: host=algo-1, epoch=92, batch=10 train loss <loss>=9.275208473205566\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:09 INFO 140277876397440] Epoch[92] Batch [10]#011Speed: 1831.18 samples/sec#011loss=9.275208\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:09 INFO 140277876397440] processed a total of 659 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458748.8659203, \"EndTime\": 1631458749.4703603, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 603.961706161499, \"count\": 1, \"min\": 603.961706161499, \"max\": 603.961706161499}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:09 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1090.9177629930114 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:09 INFO 140277876397440] #progress_metric: host=algo-1, completed 46.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:09 INFO 140277876397440] #quality_metric: host=algo-1, epoch=92, train loss <loss>=9.756066062233664\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:09 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:09 INFO 140277876397440] Epoch[93] Batch[0] avg_epoch_loss=10.058450\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:09 INFO 140277876397440] #quality_metric: host=algo-1, epoch=93, batch=0 train loss <loss>=10.058449745178223\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:09 INFO 140277876397440] Epoch[93] Batch[5] avg_epoch_loss=9.570938\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:09 INFO 140277876397440] #quality_metric: host=algo-1, epoch=93, batch=5 train loss <loss>=9.570937792460123\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:09 INFO 140277876397440] Epoch[93] Batch [5]#011Speed: 2094.06 samples/sec#011loss=9.570938\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:10 INFO 140277876397440] processed a total of 636 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458749.470438, \"EndTime\": 1631458750.0016763, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 530.6911468505859, \"count\": 1, \"min\": 530.6911468505859, \"max\": 530.6911468505859}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:10 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1198.1066843207302 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:10 INFO 140277876397440] #progress_metric: host=algo-1, completed 47.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:10 INFO 140277876397440] #quality_metric: host=algo-1, epoch=93, train loss <loss>=9.7198956489563\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:10 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:10 INFO 140277876397440] Epoch[94] Batch[0] avg_epoch_loss=9.113033\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:10 INFO 140277876397440] #quality_metric: host=algo-1, epoch=94, batch=0 train loss <loss>=9.113033294677734\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:10 INFO 140277876397440] Epoch[94] Batch[5] avg_epoch_loss=9.763060\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:10 INFO 140277876397440] #quality_metric: host=algo-1, epoch=94, batch=5 train loss <loss>=9.763060410817465\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:10 INFO 140277876397440] Epoch[94] Batch [5]#011Speed: 1983.56 samples/sec#011loss=9.763060\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:10 INFO 140277876397440] processed a total of 631 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458750.0017815, \"EndTime\": 1631458750.5384717, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 536.1247062683105, \"count\": 1, \"min\": 536.1247062683105, \"max\": 536.1247062683105}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:10 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1176.6750772710943 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:10 INFO 140277876397440] #progress_metric: host=algo-1, completed 47.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:10 INFO 140277876397440] #quality_metric: host=algo-1, epoch=94, train loss <loss>=9.859270381927491\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:10 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:10 INFO 140277876397440] Epoch[95] Batch[0] avg_epoch_loss=10.006514\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:10 INFO 140277876397440] #quality_metric: host=algo-1, epoch=95, batch=0 train loss <loss>=10.006513595581055\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:10 INFO 140277876397440] Epoch[95] Batch[5] avg_epoch_loss=9.911340\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:10 INFO 140277876397440] #quality_metric: host=algo-1, epoch=95, batch=5 train loss <loss>=9.911339600880941\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:10 INFO 140277876397440] Epoch[95] Batch [5]#011Speed: 2097.44 samples/sec#011loss=9.911340\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:11 INFO 140277876397440] Epoch[95] Batch[10] avg_epoch_loss=10.269499\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:11 INFO 140277876397440] #quality_metric: host=algo-1, epoch=95, batch=10 train loss <loss>=10.69929027557373\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:11 INFO 140277876397440] Epoch[95] Batch [10]#011Speed: 1869.80 samples/sec#011loss=10.699290\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:11 INFO 140277876397440] processed a total of 674 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458750.5385635, \"EndTime\": 1631458751.1062691, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 567.1260356903076, \"count\": 1, \"min\": 567.1260356903076, \"max\": 567.1260356903076}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:11 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1188.1905990000903 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:11 INFO 140277876397440] #progress_metric: host=algo-1, completed 48.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:11 INFO 140277876397440] #quality_metric: host=algo-1, epoch=95, train loss <loss>=10.269498998468572\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:11 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:11 INFO 140277876397440] Epoch[96] Batch[0] avg_epoch_loss=10.558015\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:11 INFO 140277876397440] #quality_metric: host=algo-1, epoch=96, batch=0 train loss <loss>=10.558014869689941\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:11 INFO 140277876397440] Epoch[96] Batch[5] avg_epoch_loss=10.029166\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:11 INFO 140277876397440] #quality_metric: host=algo-1, epoch=96, batch=5 train loss <loss>=10.029165744781494\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:11 INFO 140277876397440] Epoch[96] Batch [5]#011Speed: 2065.05 samples/sec#011loss=10.029166\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:11 INFO 140277876397440] processed a total of 636 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458751.1063526, \"EndTime\": 1631458751.6499012, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 543.0564880371094, \"count\": 1, \"min\": 543.0564880371094, \"max\": 543.0564880371094}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:11 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1170.8723257261393 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:11 INFO 140277876397440] #progress_metric: host=algo-1, completed 48.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:11 INFO 140277876397440] #quality_metric: host=algo-1, epoch=96, train loss <loss>=9.842277336120606\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:11 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:11 INFO 140277876397440] Epoch[97] Batch[0] avg_epoch_loss=9.920688\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:11 INFO 140277876397440] #quality_metric: host=algo-1, epoch=97, batch=0 train loss <loss>=9.920687675476074\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:12 INFO 140277876397440] Epoch[97] Batch[5] avg_epoch_loss=9.701616\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:12 INFO 140277876397440] #quality_metric: host=algo-1, epoch=97, batch=5 train loss <loss>=9.701615969340006\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:12 INFO 140277876397440] Epoch[97] Batch [5]#011Speed: 2057.44 samples/sec#011loss=9.701616\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:12 INFO 140277876397440] processed a total of 620 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458751.6499767, \"EndTime\": 1631458752.1811347, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 530.6191444396973, \"count\": 1, \"min\": 530.6191444396973, \"max\": 530.6191444396973}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:12 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1168.139820911506 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:12 INFO 140277876397440] #progress_metric: host=algo-1, completed 49.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:12 INFO 140277876397440] #quality_metric: host=algo-1, epoch=97, train loss <loss>=9.592318630218506\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:12 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:12 INFO 140277876397440] Epoch[98] Batch[0] avg_epoch_loss=10.043293\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:12 INFO 140277876397440] #quality_metric: host=algo-1, epoch=98, batch=0 train loss <loss>=10.043292999267578\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:12 INFO 140277876397440] Epoch[98] Batch[5] avg_epoch_loss=10.208010\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:12 INFO 140277876397440] #quality_metric: host=algo-1, epoch=98, batch=5 train loss <loss>=10.208009560902914\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:12 INFO 140277876397440] Epoch[98] Batch [5]#011Speed: 1953.07 samples/sec#011loss=10.208010\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:12 INFO 140277876397440] processed a total of 604 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458752.1812356, \"EndTime\": 1631458752.7202852, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 538.5539531707764, \"count\": 1, \"min\": 538.5539531707764, \"max\": 538.5539531707764}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:12 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1121.2527638962715 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:12 INFO 140277876397440] #progress_metric: host=algo-1, completed 49.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:12 INFO 140277876397440] #quality_metric: host=algo-1, epoch=98, train loss <loss>=10.432083797454833\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:12 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:12 INFO 140277876397440] Epoch[99] Batch[0] avg_epoch_loss=9.846853\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:12 INFO 140277876397440] #quality_metric: host=algo-1, epoch=99, batch=0 train loss <loss>=9.846853256225586\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:13 INFO 140277876397440] Epoch[99] Batch[5] avg_epoch_loss=9.876995\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:13 INFO 140277876397440] #quality_metric: host=algo-1, epoch=99, batch=5 train loss <loss>=9.87699524561564\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:13 INFO 140277876397440] Epoch[99] Batch [5]#011Speed: 2127.50 samples/sec#011loss=9.876995\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:13 INFO 140277876397440] Epoch[99] Batch[10] avg_epoch_loss=9.766668\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:13 INFO 140277876397440] #quality_metric: host=algo-1, epoch=99, batch=10 train loss <loss>=9.63427448272705\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:13 INFO 140277876397440] Epoch[99] Batch [10]#011Speed: 1854.11 samples/sec#011loss=9.634274\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:13 INFO 140277876397440] processed a total of 678 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458752.7203755, \"EndTime\": 1631458753.3188515, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 597.9709625244141, \"count\": 1, \"min\": 597.9709625244141, \"max\": 597.9709625244141}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:13 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1133.6001945315895 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:13 INFO 140277876397440] #progress_metric: host=algo-1, completed 50.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:13 INFO 140277876397440] #quality_metric: host=algo-1, epoch=99, train loss <loss>=9.766667626120828\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:13 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:13 INFO 140277876397440] Epoch[100] Batch[0] avg_epoch_loss=10.255198\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:13 INFO 140277876397440] #quality_metric: host=algo-1, epoch=100, batch=0 train loss <loss>=10.255197525024414\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:13 INFO 140277876397440] Epoch[100] Batch[5] avg_epoch_loss=9.921023\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:13 INFO 140277876397440] #quality_metric: host=algo-1, epoch=100, batch=5 train loss <loss>=9.921022891998291\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:13 INFO 140277876397440] Epoch[100] Batch [5]#011Speed: 2034.26 samples/sec#011loss=9.921023\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:13 INFO 140277876397440] Epoch[100] Batch[10] avg_epoch_loss=9.447829\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:13 INFO 140277876397440] #quality_metric: host=algo-1, epoch=100, batch=10 train loss <loss>=8.879996013641357\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:13 INFO 140277876397440] Epoch[100] Batch [10]#011Speed: 1829.74 samples/sec#011loss=8.879996\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:13 INFO 140277876397440] processed a total of 661 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458753.3189378, \"EndTime\": 1631458753.919659, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 600.245475769043, \"count\": 1, \"min\": 600.245475769043, \"max\": 600.245475769043}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:13 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1100.98916378854 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:13 INFO 140277876397440] #progress_metric: host=algo-1, completed 50.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:13 INFO 140277876397440] #quality_metric: host=algo-1, epoch=100, train loss <loss>=9.447828856381504\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:13 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:14 INFO 140277876397440] Epoch[101] Batch[0] avg_epoch_loss=9.673265\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:14 INFO 140277876397440] #quality_metric: host=algo-1, epoch=101, batch=0 train loss <loss>=9.67326545715332\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:14 INFO 140277876397440] Epoch[101] Batch[5] avg_epoch_loss=9.641689\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:14 INFO 140277876397440] #quality_metric: host=algo-1, epoch=101, batch=5 train loss <loss>=9.641688664754232\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:14 INFO 140277876397440] Epoch[101] Batch [5]#011Speed: 2067.72 samples/sec#011loss=9.641689\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:14 INFO 140277876397440] Epoch[101] Batch[10] avg_epoch_loss=9.962657\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:14 INFO 140277876397440] #quality_metric: host=algo-1, epoch=101, batch=10 train loss <loss>=10.347818946838379\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:14 INFO 140277876397440] Epoch[101] Batch [10]#011Speed: 1757.08 samples/sec#011loss=10.347819\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:14 INFO 140277876397440] processed a total of 653 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458753.9197454, \"EndTime\": 1631458754.5221567, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 601.9454002380371, \"count\": 1, \"min\": 601.9454002380371, \"max\": 601.9454002380371}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:14 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1084.59948535707 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:14 INFO 140277876397440] #progress_metric: host=algo-1, completed 51.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:14 INFO 140277876397440] #quality_metric: host=algo-1, epoch=101, train loss <loss>=9.96265697479248\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:14 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:14 INFO 140277876397440] Epoch[102] Batch[0] avg_epoch_loss=9.519173\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:14 INFO 140277876397440] #quality_metric: host=algo-1, epoch=102, batch=0 train loss <loss>=9.519172668457031\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:14 INFO 140277876397440] Epoch[102] Batch[5] avg_epoch_loss=9.746703\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:14 INFO 140277876397440] #quality_metric: host=algo-1, epoch=102, batch=5 train loss <loss>=9.746702829996744\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:14 INFO 140277876397440] Epoch[102] Batch [5]#011Speed: 2097.13 samples/sec#011loss=9.746703\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:15 INFO 140277876397440] Epoch[102] Batch[10] avg_epoch_loss=9.984649\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:15 INFO 140277876397440] #quality_metric: host=algo-1, epoch=102, batch=10 train loss <loss>=10.270184898376465\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:15 INFO 140277876397440] Epoch[102] Batch [10]#011Speed: 1945.91 samples/sec#011loss=10.270185\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:15 INFO 140277876397440] processed a total of 646 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458754.52224, \"EndTime\": 1631458755.1079605, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 585.2518081665039, \"count\": 1, \"min\": 585.2518081665039, \"max\": 585.2518081665039}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:15 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1103.5744971330494 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:15 INFO 140277876397440] #progress_metric: host=algo-1, completed 51.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:15 INFO 140277876397440] #quality_metric: host=algo-1, epoch=102, train loss <loss>=9.984649224714799\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:15 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:15 INFO 140277876397440] Epoch[103] Batch[0] avg_epoch_loss=10.262940\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:15 INFO 140277876397440] #quality_metric: host=algo-1, epoch=103, batch=0 train loss <loss>=10.262940406799316\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:15 INFO 140277876397440] Epoch[103] Batch[5] avg_epoch_loss=9.876298\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:15 INFO 140277876397440] #quality_metric: host=algo-1, epoch=103, batch=5 train loss <loss>=9.876297950744629\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:15 INFO 140277876397440] Epoch[103] Batch [5]#011Speed: 2010.70 samples/sec#011loss=9.876298\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:15 INFO 140277876397440] processed a total of 631 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458755.1080427, \"EndTime\": 1631458755.6640491, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 555.5369853973389, \"count\": 1, \"min\": 555.5369853973389, \"max\": 555.5369853973389}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:15 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1135.5835670701229 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:15 INFO 140277876397440] #progress_metric: host=algo-1, completed 52.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:15 INFO 140277876397440] #quality_metric: host=algo-1, epoch=103, train loss <loss>=9.678283596038819\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:15 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:15 INFO 140277876397440] Epoch[104] Batch[0] avg_epoch_loss=10.507855\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:15 INFO 140277876397440] #quality_metric: host=algo-1, epoch=104, batch=0 train loss <loss>=10.507855415344238\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:16 INFO 140277876397440] Epoch[104] Batch[5] avg_epoch_loss=9.907354\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:16 INFO 140277876397440] #quality_metric: host=algo-1, epoch=104, batch=5 train loss <loss>=9.90735387802124\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:16 INFO 140277876397440] Epoch[104] Batch [5]#011Speed: 2122.22 samples/sec#011loss=9.907354\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:16 INFO 140277876397440] Epoch[104] Batch[10] avg_epoch_loss=9.617383\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:16 INFO 140277876397440] #quality_metric: host=algo-1, epoch=104, batch=10 train loss <loss>=9.26941738128662\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:16 INFO 140277876397440] Epoch[104] Batch [10]#011Speed: 1842.39 samples/sec#011loss=9.269417\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:16 INFO 140277876397440] processed a total of 653 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458755.6641364, \"EndTime\": 1631458756.2473779, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 582.726240158081, \"count\": 1, \"min\": 582.726240158081, \"max\": 582.726240158081}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:16 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1120.358739874812 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:16 INFO 140277876397440] #progress_metric: host=algo-1, completed 52.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:16 INFO 140277876397440] #quality_metric: host=algo-1, epoch=104, train loss <loss>=9.617382743141867\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:16 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:16 INFO 140277876397440] Epoch[105] Batch[0] avg_epoch_loss=9.320626\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:16 INFO 140277876397440] #quality_metric: host=algo-1, epoch=105, batch=0 train loss <loss>=9.320626258850098\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:16 INFO 140277876397440] Epoch[105] Batch[5] avg_epoch_loss=10.038554\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:16 INFO 140277876397440] #quality_metric: host=algo-1, epoch=105, batch=5 train loss <loss>=10.038554191589355\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:16 INFO 140277876397440] Epoch[105] Batch [5]#011Speed: 2051.77 samples/sec#011loss=10.038554\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:16 INFO 140277876397440] processed a total of 613 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458756.2474632, \"EndTime\": 1631458756.826019, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 578.0622959136963, \"count\": 1, \"min\": 578.0622959136963, \"max\": 578.0622959136963}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:16 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1060.20321230922 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:16 INFO 140277876397440] #progress_metric: host=algo-1, completed 53.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:16 INFO 140277876397440] #quality_metric: host=algo-1, epoch=105, train loss <loss>=10.223095035552978\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:16 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:17 INFO 140277876397440] Epoch[106] Batch[0] avg_epoch_loss=9.452851\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:17 INFO 140277876397440] #quality_metric: host=algo-1, epoch=106, batch=0 train loss <loss>=9.452851295471191\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:17 INFO 140277876397440] Epoch[106] Batch[5] avg_epoch_loss=10.220216\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:17 INFO 140277876397440] #quality_metric: host=algo-1, epoch=106, batch=5 train loss <loss>=10.220215638478598\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:17 INFO 140277876397440] Epoch[106] Batch [5]#011Speed: 1970.57 samples/sec#011loss=10.220216\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:17 INFO 140277876397440] Epoch[106] Batch[10] avg_epoch_loss=9.916799\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:17 INFO 140277876397440] #quality_metric: host=algo-1, epoch=106, batch=10 train loss <loss>=9.55269889831543\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:17 INFO 140277876397440] Epoch[106] Batch [10]#011Speed: 1947.00 samples/sec#011loss=9.552699\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:17 INFO 140277876397440] processed a total of 663 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458756.8261087, \"EndTime\": 1631458757.4016542, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 575.0057697296143, \"count\": 1, \"min\": 575.0057697296143, \"max\": 575.0057697296143}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:17 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1152.7948247379516 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:17 INFO 140277876397440] #progress_metric: host=algo-1, completed 53.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:17 INFO 140277876397440] #quality_metric: host=algo-1, epoch=106, train loss <loss>=9.91679893840443\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:17 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:17 INFO 140277876397440] Epoch[107] Batch[0] avg_epoch_loss=10.245447\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:17 INFO 140277876397440] #quality_metric: host=algo-1, epoch=107, batch=0 train loss <loss>=10.245447158813477\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:17 INFO 140277876397440] Epoch[107] Batch[5] avg_epoch_loss=9.805899\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:17 INFO 140277876397440] #quality_metric: host=algo-1, epoch=107, batch=5 train loss <loss>=9.805898825327555\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:17 INFO 140277876397440] Epoch[107] Batch [5]#011Speed: 1887.89 samples/sec#011loss=9.805899\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:17 INFO 140277876397440] Epoch[107] Batch[10] avg_epoch_loss=9.395255\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:17 INFO 140277876397440] #quality_metric: host=algo-1, epoch=107, batch=10 train loss <loss>=8.902481842041016\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:17 INFO 140277876397440] Epoch[107] Batch [10]#011Speed: 1947.74 samples/sec#011loss=8.902482\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:17 INFO 140277876397440] processed a total of 664 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458757.4017346, \"EndTime\": 1631458757.9872515, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 585.0224494934082, \"count\": 1, \"min\": 585.0224494934082, \"max\": 585.0224494934082}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:17 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1134.763325336393 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:17 INFO 140277876397440] #progress_metric: host=algo-1, completed 54.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:17 INFO 140277876397440] #quality_metric: host=algo-1, epoch=107, train loss <loss>=9.395254742015492\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:17 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:18 INFO 140277876397440] Epoch[108] Batch[0] avg_epoch_loss=10.481202\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:18 INFO 140277876397440] #quality_metric: host=algo-1, epoch=108, batch=0 train loss <loss>=10.481202125549316\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:18 INFO 140277876397440] Epoch[108] Batch[5] avg_epoch_loss=9.941145\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:18 INFO 140277876397440] #quality_metric: host=algo-1, epoch=108, batch=5 train loss <loss>=9.941145420074463\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:18 INFO 140277876397440] Epoch[108] Batch [5]#011Speed: 2108.93 samples/sec#011loss=9.941145\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:18 INFO 140277876397440] processed a total of 605 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458757.987335, \"EndTime\": 1631458758.5620506, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 574.244499206543, \"count\": 1, \"min\": 574.244499206543, \"max\": 574.244499206543}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:18 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1053.3032423616903 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:18 INFO 140277876397440] #progress_metric: host=algo-1, completed 54.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:18 INFO 140277876397440] #quality_metric: host=algo-1, epoch=108, train loss <loss>=10.063196277618408\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:18 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:18 INFO 140277876397440] Epoch[109] Batch[0] avg_epoch_loss=9.247248\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:18 INFO 140277876397440] #quality_metric: host=algo-1, epoch=109, batch=0 train loss <loss>=9.247247695922852\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:18 INFO 140277876397440] Epoch[109] Batch[5] avg_epoch_loss=9.753489\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:18 INFO 140277876397440] #quality_metric: host=algo-1, epoch=109, batch=5 train loss <loss>=9.753488540649414\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:18 INFO 140277876397440] Epoch[109] Batch [5]#011Speed: 2002.22 samples/sec#011loss=9.753489\u001b[0m\n",
      "\n",
      "2021-09-12 14:59:30 Uploading - Uploading generated training model\n",
      "2021-09-12 14:59:30 Completed - Training job completed\n",
      "\u001b[34m[09/12/2021 14:59:19 INFO 140277876397440] Epoch[109] Batch[10] avg_epoch_loss=9.655992\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:19 INFO 140277876397440] #quality_metric: host=algo-1, epoch=109, batch=10 train loss <loss>=9.538996505737305\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:19 INFO 140277876397440] Epoch[109] Batch [10]#011Speed: 1910.26 samples/sec#011loss=9.538997\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:19 INFO 140277876397440] processed a total of 667 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458758.56215, \"EndTime\": 1631458759.1468897, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 584.2256546020508, \"count\": 1, \"min\": 584.2256546020508, \"max\": 584.2256546020508}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:19 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1141.445978603198 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:19 INFO 140277876397440] #progress_metric: host=algo-1, completed 55.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:19 INFO 140277876397440] #quality_metric: host=algo-1, epoch=109, train loss <loss>=9.65599216114391\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:19 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:19 INFO 140277876397440] Epoch[110] Batch[0] avg_epoch_loss=8.790164\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:19 INFO 140277876397440] #quality_metric: host=algo-1, epoch=110, batch=0 train loss <loss>=8.79016399383545\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:19 INFO 140277876397440] Epoch[110] Batch[5] avg_epoch_loss=9.336764\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:19 INFO 140277876397440] #quality_metric: host=algo-1, epoch=110, batch=5 train loss <loss>=9.336764494578043\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:19 INFO 140277876397440] Epoch[110] Batch [5]#011Speed: 2010.25 samples/sec#011loss=9.336764\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:19 INFO 140277876397440] Epoch[110] Batch[10] avg_epoch_loss=9.908182\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:19 INFO 140277876397440] #quality_metric: host=algo-1, epoch=110, batch=10 train loss <loss>=10.593881988525391\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:19 INFO 140277876397440] Epoch[110] Batch [10]#011Speed: 2058.56 samples/sec#011loss=10.593882\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:19 INFO 140277876397440] processed a total of 647 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458759.1469731, \"EndTime\": 1631458759.7158003, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 568.3672428131104, \"count\": 1, \"min\": 568.3672428131104, \"max\": 568.3672428131104}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:19 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1138.1179103400957 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:19 INFO 140277876397440] #progress_metric: host=algo-1, completed 55.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:19 INFO 140277876397440] #quality_metric: host=algo-1, epoch=110, train loss <loss>=9.908181537281383\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:19 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:19 INFO 140277876397440] Epoch[111] Batch[0] avg_epoch_loss=8.865290\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:19 INFO 140277876397440] #quality_metric: host=algo-1, epoch=111, batch=0 train loss <loss>=8.865289688110352\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:20 INFO 140277876397440] Epoch[111] Batch[5] avg_epoch_loss=9.450676\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:20 INFO 140277876397440] #quality_metric: host=algo-1, epoch=111, batch=5 train loss <loss>=9.450676282246908\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:20 INFO 140277876397440] Epoch[111] Batch [5]#011Speed: 2021.45 samples/sec#011loss=9.450676\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:20 INFO 140277876397440] processed a total of 625 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458759.7158804, \"EndTime\": 1631458760.269164, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 552.8397560119629, \"count\": 1, \"min\": 552.8397560119629, \"max\": 552.8397560119629}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:20 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1130.2442875331396 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:20 INFO 140277876397440] #progress_metric: host=algo-1, completed 56.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:20 INFO 140277876397440] #quality_metric: host=algo-1, epoch=111, train loss <loss>=9.389745616912842\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:20 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:20 INFO 140277876397440] Epoch[112] Batch[0] avg_epoch_loss=10.056252\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:20 INFO 140277876397440] #quality_metric: host=algo-1, epoch=112, batch=0 train loss <loss>=10.056252479553223\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:20 INFO 140277876397440] Epoch[112] Batch[5] avg_epoch_loss=9.594134\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:20 INFO 140277876397440] #quality_metric: host=algo-1, epoch=112, batch=5 train loss <loss>=9.59413448969523\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:20 INFO 140277876397440] Epoch[112] Batch [5]#011Speed: 2128.35 samples/sec#011loss=9.594134\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:20 INFO 140277876397440] processed a total of 608 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458760.269263, \"EndTime\": 1631458760.8124743, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 542.7079200744629, \"count\": 1, \"min\": 542.7079200744629, \"max\": 542.7079200744629}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:20 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1120.0870159891529 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:20 INFO 140277876397440] #progress_metric: host=algo-1, completed 56.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:20 INFO 140277876397440] #quality_metric: host=algo-1, epoch=112, train loss <loss>=9.833938217163086\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:20 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:21 INFO 140277876397440] Epoch[113] Batch[0] avg_epoch_loss=9.582701\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:21 INFO 140277876397440] #quality_metric: host=algo-1, epoch=113, batch=0 train loss <loss>=9.582700729370117\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:21 INFO 140277876397440] Epoch[113] Batch[5] avg_epoch_loss=9.677851\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:21 INFO 140277876397440] #quality_metric: host=algo-1, epoch=113, batch=5 train loss <loss>=9.677850564320883\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:21 INFO 140277876397440] Epoch[113] Batch [5]#011Speed: 2009.03 samples/sec#011loss=9.677851\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:21 INFO 140277876397440] processed a total of 628 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458760.8125448, \"EndTime\": 1631458761.3509538, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 537.8603935241699, \"count\": 1, \"min\": 537.8603935241699, \"max\": 537.8603935241699}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:21 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1167.3392970232221 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:21 INFO 140277876397440] #progress_metric: host=algo-1, completed 57.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:21 INFO 140277876397440] #quality_metric: host=algo-1, epoch=113, train loss <loss>=9.708845329284667\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:21 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:21 INFO 140277876397440] Epoch[114] Batch[0] avg_epoch_loss=9.862407\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:21 INFO 140277876397440] #quality_metric: host=algo-1, epoch=114, batch=0 train loss <loss>=9.862406730651855\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:21 INFO 140277876397440] Epoch[114] Batch[5] avg_epoch_loss=9.717842\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:21 INFO 140277876397440] #quality_metric: host=algo-1, epoch=114, batch=5 train loss <loss>=9.717841625213623\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:21 INFO 140277876397440] Epoch[114] Batch [5]#011Speed: 2098.38 samples/sec#011loss=9.717842\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:21 INFO 140277876397440] Epoch[114] Batch[10] avg_epoch_loss=9.375310\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:21 INFO 140277876397440] #quality_metric: host=algo-1, epoch=114, batch=10 train loss <loss>=8.964272594451904\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:21 INFO 140277876397440] Epoch[114] Batch [10]#011Speed: 1860.47 samples/sec#011loss=8.964273\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:21 INFO 140277876397440] processed a total of 655 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458761.3510275, \"EndTime\": 1631458761.9150007, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 563.443660736084, \"count\": 1, \"min\": 563.443660736084, \"max\": 563.443660736084}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:21 INFO 140277876397440] #throughput_metric: host=algo-1, train throughput=1162.2414382460547 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:21 INFO 140277876397440] #progress_metric: host=algo-1, completed 57.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:21 INFO 140277876397440] #quality_metric: host=algo-1, epoch=114, train loss <loss>=9.37531024759466\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:21 INFO 140277876397440] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:21 INFO 140277876397440] Loading parameters from best epoch (74)\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458761.9150856, \"EndTime\": 1631458761.9250007, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.deserialize.time\": {\"sum\": 9.35506820678711, \"count\": 1, \"min\": 9.35506820678711, \"max\": 9.35506820678711}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:21 INFO 140277876397440] stopping training now\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:21 INFO 140277876397440] #progress_metric: host=algo-1, completed 100 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:21 INFO 140277876397440] Final loss: 9.296098362315785 (occurred at epoch 74)\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:21 INFO 140277876397440] #quality_metric: host=algo-1, train final_loss <loss>=9.296098362315785\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:21 INFO 140277876397440] Worker algo-1 finished training.\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:21 WARNING 140277876397440] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:21 INFO 140277876397440] All workers finished. Serializing model for prediction.\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458761.925072, \"EndTime\": 1631458762.0578566, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"get_graph.time\": {\"sum\": 132.08961486816406, \"count\": 1, \"min\": 132.08961486816406, \"max\": 132.08961486816406}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:22 INFO 140277876397440] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458762.0579135, \"EndTime\": 1631458762.0914009, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"finalize.time\": {\"sum\": 165.67540168762207, \"count\": 1, \"min\": 165.67540168762207, \"max\": 165.67540168762207}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:22 INFO 140277876397440] Serializing to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:22 INFO 140277876397440] Saved checkpoint to \"/opt/ml/model/model_algo-1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458762.0914454, \"EndTime\": 1631458762.0967667, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"model.serialize.time\": {\"sum\": 5.283594131469727, \"count\": 1, \"min\": 5.283594131469727, \"max\": 5.283594131469727}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:22 INFO 140277876397440] Successfully serialized the model for prediction.\u001b[0m\n",
      "\u001b[34m[09/12/2021 14:59:22 INFO 140277876397440] No test data passed, skipping evaluation.\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631458762.0968144, \"EndTime\": 1631458762.1021729, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"setuptime\": {\"sum\": 7.272005081176758, \"count\": 1, \"min\": 7.272005081176758, \"max\": 7.272005081176758}, \"totaltime\": {\"sum\": 67948.3094215393, \"count\": 1, \"min\": 67948.3094215393, \"max\": 67948.3094215393}}}\n",
      "\u001b[0m\n",
      "Training seconds: 127\n",
      "Billable seconds: 127\n",
      "------!Overwriting existing file\n",
      "Uploading file to s3://sagemaker-us-east-2-222195595958/deepar-covid-demo-notebook/data/train/train.json\n",
      "2021-09-12 15:02:44 Starting - Starting the training job...\n",
      "2021-09-12 15:03:07 Starting - Launching requested ML instancesProfilerReport-1631458964: InProgress\n",
      "...\n",
      "2021-09-12 15:03:35 Starting - Preparing the instances for training.........\n",
      "2021-09-12 15:05:09 Downloading - Downloading input data\n",
      "2021-09-12 15:05:09 Training - Downloading the training image...\n",
      "2021-09-12 15:05:30 Training - Training image download completed. Training in progress.\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:34 INFO 140374100940160] Reading default configuration from /opt/amazon/lib/python3.6/site-packages/algorithm/resources/default-input.json: {'_kvstore': 'auto', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_tuning_objective_metric': '', 'cardinality': 'auto', 'dropout_rate': '0.10', 'early_stopping_patience': '', 'embedding_dimension': '10', 'learning_rate': '0.001', 'likelihood': 'student-t', 'mini_batch_size': '128', 'num_cells': '40', 'num_dynamic_feat': 'auto', 'num_eval_samples': '100', 'num_layers': '2', 'test_quantiles': '[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]'}\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:34 INFO 140374100940160] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {'prediction_length': '14', 'time_freq': 'D', 'context_length': '14', 'epochs': '200', 'learning_rate': '5E-4', 'early_stopping_patience': '40', 'mini_batch_size': '64'}\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:34 INFO 140374100940160] Final configuration: {'_kvstore': 'auto', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_tuning_objective_metric': '', 'cardinality': 'auto', 'dropout_rate': '0.10', 'early_stopping_patience': '40', 'embedding_dimension': '10', 'learning_rate': '5E-4', 'likelihood': 'student-t', 'mini_batch_size': '64', 'num_cells': '40', 'num_dynamic_feat': 'auto', 'num_eval_samples': '100', 'num_layers': '2', 'test_quantiles': '[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', 'prediction_length': '14', 'time_freq': 'D', 'context_length': '14', 'epochs': '200'}\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:34 INFO 140374100940160] Detected entry point for worker worker\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:34 INFO 140374100940160] Using early stopping with patience 40\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:34 INFO 140374100940160] random_seed is None\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:34 INFO 140374100940160] [cardinality=auto] `cat` field was NOT found in the file `/opt/ml/input/data/train/train.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:34 INFO 140374100940160] [num_dynamic_feat=auto] `dynamic_feat` field was NOT found in the file `/opt/ml/input/data/train/train.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:34 INFO 140374100940160] Training set statistics:\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:34 INFO 140374100940160] Integer time series\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:34 INFO 140374100940160] number of time series: 41\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:34 INFO 140374100940160] number of observations: 13493\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:34 INFO 140374100940160] mean target length: 329.0975609756098\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:34 INFO 140374100940160] min/mean/max target: 0.0/15207287.977321574/349787488.0\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:34 INFO 140374100940160] mean abs(target): 15207287.977321574\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:34 INFO 140374100940160] contains missing values: no\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:34 INFO 140374100940160] Small number of time series. Doing 16 passes over dataset with prob 0.975609756097561 per epoch.\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:34 INFO 140374100940160] No test channel found not running evaluations\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:34 INFO 140374100940160] #memory_usage::<batchbuffer> = 2.2568130493164062 mb\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:34 INFO 140374100940160] nvidia-smi took: 0.025377511978149414 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:34 INFO 140374100940160] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:34 INFO 140374100940160] Create Store: local\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459134.4886591, \"EndTime\": 1631459134.5699668, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"get_graph.time\": {\"sum\": 79.33688163757324, \"count\": 1, \"min\": 79.33688163757324, \"max\": 79.33688163757324}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:34 INFO 140374100940160] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:34 INFO 140374100940160] #memory_usage::<model> = 12 mb\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459134.570037, \"EndTime\": 1631459134.6749978, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"initialize.time\": {\"sum\": 186.2330436706543, \"count\": 1, \"min\": 186.2330436706543, \"max\": 186.2330436706543}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:35 INFO 140374100940160] Epoch[0] Batch[0] avg_epoch_loss=13.797310\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:35 INFO 140374100940160] #quality_metric: host=algo-1, epoch=0, batch=0 train loss <loss>=13.797309875488281\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:35 INFO 140374100940160] Epoch[0] Batch[5] avg_epoch_loss=14.071261\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:35 INFO 140374100940160] #quality_metric: host=algo-1, epoch=0, batch=5 train loss <loss>=14.071261088053385\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:35 INFO 140374100940160] Epoch[0] Batch [5]#011Speed: 1910.28 samples/sec#011loss=14.071261\u001b[0m\n",
      "\u001b[34m/opt/amazon/python3.6/lib/python3.6/contextlib.py:99: DeprecationWarning: generator 'local_timer' raised StopIteration\n",
      "  self.gen.throw(type, value, traceback)\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:35 INFO 140374100940160] processed a total of 618 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459134.6750615, \"EndTime\": 1631459135.4128888, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"epochs\": {\"sum\": 200.0, \"count\": 1, \"min\": 200, \"max\": 200}, \"update.time\": {\"sum\": 737.7383708953857, \"count\": 1, \"min\": 737.7383708953857, \"max\": 737.7383708953857}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:35 INFO 140374100940160] #throughput_metric: host=algo-1, train throughput=837.5345680771516 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:35 INFO 140374100940160] #progress_metric: host=algo-1, completed 0.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:35 INFO 140374100940160] #quality_metric: host=algo-1, epoch=0, train loss <loss>=13.635882759094239\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:35 INFO 140374100940160] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:35 INFO 140374100940160] Saved checkpoint to \"/opt/ml/model/state_2eaa05b1-67d4-441b-a258-db7aaaf54163-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459135.4129882, \"EndTime\": 1631459135.433436, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 19.842147827148438, \"count\": 1, \"min\": 19.842147827148438, \"max\": 19.842147827148438}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:35 INFO 140374100940160] Epoch[1] Batch[0] avg_epoch_loss=12.978895\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:35 INFO 140374100940160] #quality_metric: host=algo-1, epoch=1, batch=0 train loss <loss>=12.97889518737793\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:35 INFO 140374100940160] Epoch[1] Batch[5] avg_epoch_loss=13.113867\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:35 INFO 140374100940160] #quality_metric: host=algo-1, epoch=1, batch=5 train loss <loss>=13.113867282867432\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:35 INFO 140374100940160] Epoch[1] Batch [5]#011Speed: 2150.01 samples/sec#011loss=13.113867\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:35 INFO 140374100940160] processed a total of 611 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459135.4335132, \"EndTime\": 1631459135.978303, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 544.7218418121338, \"count\": 1, \"min\": 544.7218418121338, \"max\": 544.7218418121338}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:35 INFO 140374100940160] #throughput_metric: host=algo-1, train throughput=1121.4024638545545 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:35 INFO 140374100940160] #progress_metric: host=algo-1, completed 1.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:35 INFO 140374100940160] #quality_metric: host=algo-1, epoch=1, train loss <loss>=12.914175510406494\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:35 INFO 140374100940160] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:35 INFO 140374100940160] Saved checkpoint to \"/opt/ml/model/state_34ad69ef-ef3b-49a4-95e9-8f8563de8627-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459135.9783938, \"EndTime\": 1631459135.9903693, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 11.458873748779297, \"count\": 1, \"min\": 11.458873748779297, \"max\": 11.458873748779297}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:36 INFO 140374100940160] Epoch[2] Batch[0] avg_epoch_loss=12.515577\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:36 INFO 140374100940160] #quality_metric: host=algo-1, epoch=2, batch=0 train loss <loss>=12.51557731628418\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:36 INFO 140374100940160] Epoch[2] Batch[5] avg_epoch_loss=12.061965\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:36 INFO 140374100940160] #quality_metric: host=algo-1, epoch=2, batch=5 train loss <loss>=12.061965147654215\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:36 INFO 140374100940160] Epoch[2] Batch [5]#011Speed: 1841.94 samples/sec#011loss=12.061965\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:36 INFO 140374100940160] processed a total of 635 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459135.9904535, \"EndTime\": 1631459136.5814185, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 590.9032821655273, \"count\": 1, \"min\": 590.9032821655273, \"max\": 590.9032821655273}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:36 INFO 140374100940160] #throughput_metric: host=algo-1, train throughput=1074.44560448338 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:36 INFO 140374100940160] #progress_metric: host=algo-1, completed 1.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:36 INFO 140374100940160] #quality_metric: host=algo-1, epoch=2, train loss <loss>=12.210450839996337\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:36 INFO 140374100940160] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:36 INFO 140374100940160] Saved checkpoint to \"/opt/ml/model/state_79c1d917-137f-4bcb-9423-33d1d1982c5c-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459136.5814848, \"EndTime\": 1631459136.593516, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 11.475801467895508, \"count\": 1, \"min\": 11.475801467895508, \"max\": 11.475801467895508}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:36 INFO 140374100940160] Epoch[3] Batch[0] avg_epoch_loss=12.129247\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:36 INFO 140374100940160] #quality_metric: host=algo-1, epoch=3, batch=0 train loss <loss>=12.129246711730957\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:36 INFO 140374100940160] Epoch[3] Batch[5] avg_epoch_loss=11.756033\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:36 INFO 140374100940160] #quality_metric: host=algo-1, epoch=3, batch=5 train loss <loss>=11.756032943725586\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:36 INFO 140374100940160] Epoch[3] Batch [5]#011Speed: 1964.82 samples/sec#011loss=11.756033\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:37 INFO 140374100940160] Epoch[3] Batch[10] avg_epoch_loss=11.754965\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:37 INFO 140374100940160] #quality_metric: host=algo-1, epoch=3, batch=10 train loss <loss>=11.753683853149415\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:37 INFO 140374100940160] Epoch[3] Batch [10]#011Speed: 1698.41 samples/sec#011loss=11.753684\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:37 INFO 140374100940160] processed a total of 706 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459136.5935912, \"EndTime\": 1631459137.2208154, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 627.1626949310303, \"count\": 1, \"min\": 627.1626949310303, \"max\": 627.1626949310303}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:37 INFO 140374100940160] #throughput_metric: host=algo-1, train throughput=1125.4758084807336 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:37 INFO 140374100940160] #progress_metric: host=algo-1, completed 2.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:37 INFO 140374100940160] #quality_metric: host=algo-1, epoch=3, train loss <loss>=11.600783665974935\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:37 INFO 140374100940160] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:37 INFO 140374100940160] Saved checkpoint to \"/opt/ml/model/state_73a38db0-0f96-4569-a24c-5f40c147623c-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459137.2209058, \"EndTime\": 1631459137.2400272, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 18.566370010375977, \"count\": 1, \"min\": 18.566370010375977, \"max\": 18.566370010375977}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:37 INFO 140374100940160] Epoch[4] Batch[0] avg_epoch_loss=11.594159\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:37 INFO 140374100940160] #quality_metric: host=algo-1, epoch=4, batch=0 train loss <loss>=11.594159126281738\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:37 INFO 140374100940160] Epoch[4] Batch[5] avg_epoch_loss=11.467476\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:37 INFO 140374100940160] #quality_metric: host=algo-1, epoch=4, batch=5 train loss <loss>=11.467476050059\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:37 INFO 140374100940160] Epoch[4] Batch [5]#011Speed: 2022.69 samples/sec#011loss=11.467476\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:37 INFO 140374100940160] Epoch[4] Batch[10] avg_epoch_loss=11.613116\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:37 INFO 140374100940160] #quality_metric: host=algo-1, epoch=4, batch=10 train loss <loss>=11.787883758544922\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:37 INFO 140374100940160] Epoch[4] Batch [10]#011Speed: 1654.92 samples/sec#011loss=11.787884\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:37 INFO 140374100940160] processed a total of 674 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459137.240094, \"EndTime\": 1631459137.843886, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 603.7342548370361, \"count\": 1, \"min\": 603.7342548370361, \"max\": 603.7342548370361}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:37 INFO 140374100940160] #throughput_metric: host=algo-1, train throughput=1116.1661535205717 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:37 INFO 140374100940160] #progress_metric: host=algo-1, completed 2.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:37 INFO 140374100940160] #quality_metric: host=algo-1, epoch=4, train loss <loss>=11.613115917552602\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:37 INFO 140374100940160] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:38 INFO 140374100940160] Epoch[5] Batch[0] avg_epoch_loss=11.352420\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:38 INFO 140374100940160] #quality_metric: host=algo-1, epoch=5, batch=0 train loss <loss>=11.35241985321045\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:38 INFO 140374100940160] Epoch[5] Batch[5] avg_epoch_loss=11.560802\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:38 INFO 140374100940160] #quality_metric: host=algo-1, epoch=5, batch=5 train loss <loss>=11.560802141825357\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:38 INFO 140374100940160] Epoch[5] Batch [5]#011Speed: 2106.92 samples/sec#011loss=11.560802\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:38 INFO 140374100940160] processed a total of 575 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459137.8439708, \"EndTime\": 1631459138.3752139, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 530.7633876800537, \"count\": 1, \"min\": 530.7633876800537, \"max\": 530.7633876800537}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:38 INFO 140374100940160] #throughput_metric: host=algo-1, train throughput=1083.0679469164295 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:38 INFO 140374100940160] #progress_metric: host=algo-1, completed 3.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:38 INFO 140374100940160] #quality_metric: host=algo-1, epoch=5, train loss <loss>=11.497024430168999\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:38 INFO 140374100940160] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:38 INFO 140374100940160] Saved checkpoint to \"/opt/ml/model/state_e40f3671-470a-46c6-a7dd-f7083c04a9f7-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459138.3752992, \"EndTime\": 1631459138.394502, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 18.65386962890625, \"count\": 1, \"min\": 18.65386962890625, \"max\": 18.65386962890625}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:38 INFO 140374100940160] Epoch[6] Batch[0] avg_epoch_loss=11.534489\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:38 INFO 140374100940160] #quality_metric: host=algo-1, epoch=6, batch=0 train loss <loss>=11.534488677978516\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:38 INFO 140374100940160] Epoch[6] Batch[5] avg_epoch_loss=11.627197\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:38 INFO 140374100940160] #quality_metric: host=algo-1, epoch=6, batch=5 train loss <loss>=11.627196788787842\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:38 INFO 140374100940160] Epoch[6] Batch [5]#011Speed: 1610.77 samples/sec#011loss=11.627197\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:39 INFO 140374100940160] Epoch[6] Batch[10] avg_epoch_loss=11.241148\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:39 INFO 140374100940160] #quality_metric: host=algo-1, epoch=6, batch=10 train loss <loss>=10.777889251708984\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:39 INFO 140374100940160] Epoch[6] Batch [10]#011Speed: 1443.22 samples/sec#011loss=10.777889\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:39 INFO 140374100940160] processed a total of 657 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459138.394574, \"EndTime\": 1631459139.0851688, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 690.5360221862793, \"count\": 1, \"min\": 690.5360221862793, \"max\": 690.5360221862793}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:39 INFO 140374100940160] #throughput_metric: host=algo-1, train throughput=951.2679491277695 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:39 INFO 140374100940160] #progress_metric: host=algo-1, completed 3.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:39 INFO 140374100940160] #quality_metric: host=algo-1, epoch=6, train loss <loss>=11.241147908297451\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:39 INFO 140374100940160] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:39 INFO 140374100940160] Saved checkpoint to \"/opt/ml/model/state_cc4bdbba-12ac-4641-82f0-49fc5447736f-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459139.085255, \"EndTime\": 1631459139.1047122, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 18.898963928222656, \"count\": 1, \"min\": 18.898963928222656, \"max\": 18.898963928222656}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:39 INFO 140374100940160] Epoch[7] Batch[0] avg_epoch_loss=10.271536\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:39 INFO 140374100940160] #quality_metric: host=algo-1, epoch=7, batch=0 train loss <loss>=10.271535873413086\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:39 INFO 140374100940160] Epoch[7] Batch[5] avg_epoch_loss=10.644558\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:39 INFO 140374100940160] #quality_metric: host=algo-1, epoch=7, batch=5 train loss <loss>=10.644558429718018\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:39 INFO 140374100940160] Epoch[7] Batch [5]#011Speed: 1693.17 samples/sec#011loss=10.644558\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:39 INFO 140374100940160] processed a total of 599 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459139.1047878, \"EndTime\": 1631459139.7510562, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 646.2063789367676, \"count\": 1, \"min\": 646.2063789367676, \"max\": 646.2063789367676}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:39 INFO 140374100940160] #throughput_metric: host=algo-1, train throughput=926.7713264652676 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:39 INFO 140374100940160] #progress_metric: host=algo-1, completed 4.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:39 INFO 140374100940160] #quality_metric: host=algo-1, epoch=7, train loss <loss>=10.362598943710328\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:39 INFO 140374100940160] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:39 INFO 140374100940160] Saved checkpoint to \"/opt/ml/model/state_5028a8b8-35f5-4aba-9e72-0cf9e23a61db-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459139.7511427, \"EndTime\": 1631459139.770241, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 18.578052520751953, \"count\": 1, \"min\": 18.578052520751953, \"max\": 18.578052520751953}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:40 INFO 140374100940160] Epoch[8] Batch[0] avg_epoch_loss=11.154544\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:40 INFO 140374100940160] #quality_metric: host=algo-1, epoch=8, batch=0 train loss <loss>=11.15454387664795\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:40 INFO 140374100940160] Epoch[8] Batch[5] avg_epoch_loss=10.937150\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:40 INFO 140374100940160] #quality_metric: host=algo-1, epoch=8, batch=5 train loss <loss>=10.937150001525879\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:40 INFO 140374100940160] Epoch[8] Batch [5]#011Speed: 1904.64 samples/sec#011loss=10.937150\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:40 INFO 140374100940160] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459139.7703078, \"EndTime\": 1631459140.3613477, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 590.9817218780518, \"count\": 1, \"min\": 590.9817218780518, \"max\": 590.9817218780518}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:40 INFO 140374100940160] #throughput_metric: host=algo-1, train throughput=1077.6349248091794 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:40 INFO 140374100940160] #progress_metric: host=algo-1, completed 4.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:40 INFO 140374100940160] #quality_metric: host=algo-1, epoch=8, train loss <loss>=10.813505363464355\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:40 INFO 140374100940160] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:40 INFO 140374100940160] Epoch[9] Batch[0] avg_epoch_loss=10.632440\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:40 INFO 140374100940160] #quality_metric: host=algo-1, epoch=9, batch=0 train loss <loss>=10.632439613342285\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:40 INFO 140374100940160] Epoch[9] Batch[5] avg_epoch_loss=10.514695\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:40 INFO 140374100940160] #quality_metric: host=algo-1, epoch=9, batch=5 train loss <loss>=10.514695485432943\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:40 INFO 140374100940160] Epoch[9] Batch [5]#011Speed: 1942.66 samples/sec#011loss=10.514695\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:41 INFO 140374100940160] Epoch[9] Batch[10] avg_epoch_loss=10.688500\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:41 INFO 140374100940160] #quality_metric: host=algo-1, epoch=9, batch=10 train loss <loss>=10.897065734863281\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:41 INFO 140374100940160] Epoch[9] Batch [10]#011Speed: 1809.73 samples/sec#011loss=10.897066\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:41 INFO 140374100940160] processed a total of 694 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459140.361439, \"EndTime\": 1631459141.001523, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 639.5957469940186, \"count\": 1, \"min\": 639.5957469940186, \"max\": 639.5957469940186}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:41 INFO 140374100940160] #throughput_metric: host=algo-1, train throughput=1084.854536130425 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:41 INFO 140374100940160] #progress_metric: host=algo-1, completed 5.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:41 INFO 140374100940160] #quality_metric: host=algo-1, epoch=9, train loss <loss>=10.688500144264914\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:41 INFO 140374100940160] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:41 INFO 140374100940160] Epoch[10] Batch[0] avg_epoch_loss=10.162472\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:41 INFO 140374100940160] #quality_metric: host=algo-1, epoch=10, batch=0 train loss <loss>=10.162471771240234\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:41 INFO 140374100940160] Epoch[10] Batch[5] avg_epoch_loss=10.462074\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:41 INFO 140374100940160] #quality_metric: host=algo-1, epoch=10, batch=5 train loss <loss>=10.462074279785156\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:41 INFO 140374100940160] Epoch[10] Batch [5]#011Speed: 1674.69 samples/sec#011loss=10.462074\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:41 INFO 140374100940160] processed a total of 621 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459141.0016046, \"EndTime\": 1631459141.5821989, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 580.0843238830566, \"count\": 1, \"min\": 580.0843238830566, \"max\": 580.0843238830566}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:41 INFO 140374100940160] #throughput_metric: host=algo-1, train throughput=1070.2898720211374 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:41 INFO 140374100940160] #progress_metric: host=algo-1, completed 5.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:41 INFO 140374100940160] #quality_metric: host=algo-1, epoch=10, train loss <loss>=10.426144981384278\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:41 INFO 140374100940160] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:41 INFO 140374100940160] Epoch[11] Batch[0] avg_epoch_loss=10.065514\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:41 INFO 140374100940160] #quality_metric: host=algo-1, epoch=11, batch=0 train loss <loss>=10.065513610839844\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:41 INFO 140374100940160] Epoch[11] Batch[5] avg_epoch_loss=10.778100\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:41 INFO 140374100940160] #quality_metric: host=algo-1, epoch=11, batch=5 train loss <loss>=10.778099854787191\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:41 INFO 140374100940160] Epoch[11] Batch [5]#011Speed: 1918.86 samples/sec#011loss=10.778100\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:42 INFO 140374100940160] processed a total of 619 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459141.5822916, \"EndTime\": 1631459142.1519346, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 569.0844058990479, \"count\": 1, \"min\": 569.0844058990479, \"max\": 569.0844058990479}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:42 INFO 140374100940160] #throughput_metric: host=algo-1, train throughput=1087.4638113005549 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:42 INFO 140374100940160] #progress_metric: host=algo-1, completed 6.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:42 INFO 140374100940160] #quality_metric: host=algo-1, epoch=11, train loss <loss>=10.867324256896973\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:42 INFO 140374100940160] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:42 INFO 140374100940160] Epoch[12] Batch[0] avg_epoch_loss=10.125969\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:42 INFO 140374100940160] #quality_metric: host=algo-1, epoch=12, batch=0 train loss <loss>=10.125968933105469\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:42 INFO 140374100940160] Epoch[12] Batch[5] avg_epoch_loss=10.250529\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:42 INFO 140374100940160] #quality_metric: host=algo-1, epoch=12, batch=5 train loss <loss>=10.250529289245605\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:42 INFO 140374100940160] Epoch[12] Batch [5]#011Speed: 2135.58 samples/sec#011loss=10.250529\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:42 INFO 140374100940160] processed a total of 621 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459142.152025, \"EndTime\": 1631459142.6898832, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 537.3542308807373, \"count\": 1, \"min\": 537.3542308807373, \"max\": 537.3542308807373}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:42 INFO 140374100940160] #throughput_metric: host=algo-1, train throughput=1155.3967045860327 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:42 INFO 140374100940160] #progress_metric: host=algo-1, completed 6.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:42 INFO 140374100940160] #quality_metric: host=algo-1, epoch=12, train loss <loss>=10.594027900695801\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:42 INFO 140374100940160] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:42 INFO 140374100940160] Epoch[13] Batch[0] avg_epoch_loss=9.934601\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:42 INFO 140374100940160] #quality_metric: host=algo-1, epoch=13, batch=0 train loss <loss>=9.934600830078125\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:43 INFO 140374100940160] Epoch[13] Batch[5] avg_epoch_loss=10.279168\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:43 INFO 140374100940160] #quality_metric: host=algo-1, epoch=13, batch=5 train loss <loss>=10.279168287913004\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:43 INFO 140374100940160] Epoch[13] Batch [5]#011Speed: 1787.26 samples/sec#011loss=10.279168\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:43 INFO 140374100940160] Epoch[13] Batch[10] avg_epoch_loss=10.070333\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:43 INFO 140374100940160] #quality_metric: host=algo-1, epoch=13, batch=10 train loss <loss>=9.819729995727538\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:43 INFO 140374100940160] Epoch[13] Batch [10]#011Speed: 1825.64 samples/sec#011loss=9.819730\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:43 INFO 140374100940160] processed a total of 660 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459142.6899683, \"EndTime\": 1631459143.296683, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 606.2021255493164, \"count\": 1, \"min\": 606.2021255493164, \"max\": 606.2021255493164}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:43 INFO 140374100940160] #throughput_metric: host=algo-1, train throughput=1088.5334230928333 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:43 INFO 140374100940160] #progress_metric: host=algo-1, completed 7.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:43 INFO 140374100940160] #quality_metric: host=algo-1, epoch=13, train loss <loss>=10.070332700555975\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:43 INFO 140374100940160] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:43 INFO 140374100940160] Saved checkpoint to \"/opt/ml/model/state_c01249c7-7cd3-40ec-9332-fa3e62098602-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459143.2967644, \"EndTime\": 1631459143.315664, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 18.3870792388916, \"count\": 1, \"min\": 18.3870792388916, \"max\": 18.3870792388916}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:43 INFO 140374100940160] Epoch[14] Batch[0] avg_epoch_loss=10.690368\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:43 INFO 140374100940160] #quality_metric: host=algo-1, epoch=14, batch=0 train loss <loss>=10.690367698669434\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:43 INFO 140374100940160] Epoch[14] Batch[5] avg_epoch_loss=10.395874\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:43 INFO 140374100940160] #quality_metric: host=algo-1, epoch=14, batch=5 train loss <loss>=10.3958740234375\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:43 INFO 140374100940160] Epoch[14] Batch [5]#011Speed: 2089.33 samples/sec#011loss=10.395874\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:43 INFO 140374100940160] Epoch[14] Batch[10] avg_epoch_loss=10.369086\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:43 INFO 140374100940160] #quality_metric: host=algo-1, epoch=14, batch=10 train loss <loss>=10.336940574645997\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:43 INFO 140374100940160] Epoch[14] Batch [10]#011Speed: 1904.70 samples/sec#011loss=10.336941\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:43 INFO 140374100940160] processed a total of 676 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459143.3157306, \"EndTime\": 1631459143.8879309, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 572.1430778503418, \"count\": 1, \"min\": 572.1430778503418, \"max\": 572.1430778503418}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:43 INFO 140374100940160] #throughput_metric: host=algo-1, train throughput=1181.2834047432161 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:43 INFO 140374100940160] #progress_metric: host=algo-1, completed 7.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:43 INFO 140374100940160] #quality_metric: host=algo-1, epoch=14, train loss <loss>=10.369086092168635\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:43 INFO 140374100940160] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:44 INFO 140374100940160] Epoch[15] Batch[0] avg_epoch_loss=10.418064\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:44 INFO 140374100940160] #quality_metric: host=algo-1, epoch=15, batch=0 train loss <loss>=10.41806411743164\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:44 INFO 140374100940160] Epoch[15] Batch[5] avg_epoch_loss=10.486854\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:44 INFO 140374100940160] #quality_metric: host=algo-1, epoch=15, batch=5 train loss <loss>=10.486854394276937\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:44 INFO 140374100940160] Epoch[15] Batch [5]#011Speed: 1972.48 samples/sec#011loss=10.486854\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:44 INFO 140374100940160] Epoch[15] Batch[10] avg_epoch_loss=10.588365\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:44 INFO 140374100940160] #quality_metric: host=algo-1, epoch=15, batch=10 train loss <loss>=10.710178184509278\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:44 INFO 140374100940160] Epoch[15] Batch [10]#011Speed: 1865.80 samples/sec#011loss=10.710178\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:44 INFO 140374100940160] processed a total of 671 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459143.8880122, \"EndTime\": 1631459144.4814403, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 592.9543972015381, \"count\": 1, \"min\": 592.9543972015381, \"max\": 592.9543972015381}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:44 INFO 140374100940160] #throughput_metric: host=algo-1, train throughput=1131.404596085721 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:44 INFO 140374100940160] #progress_metric: host=algo-1, completed 8.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:44 INFO 140374100940160] #quality_metric: host=algo-1, epoch=15, train loss <loss>=10.58836520801891\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:44 INFO 140374100940160] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:44 INFO 140374100940160] Epoch[16] Batch[0] avg_epoch_loss=10.468578\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:44 INFO 140374100940160] #quality_metric: host=algo-1, epoch=16, batch=0 train loss <loss>=10.468578338623047\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:44 INFO 140374100940160] Epoch[16] Batch[5] avg_epoch_loss=10.409967\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:44 INFO 140374100940160] #quality_metric: host=algo-1, epoch=16, batch=5 train loss <loss>=10.409967422485352\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:44 INFO 140374100940160] Epoch[16] Batch [5]#011Speed: 1858.86 samples/sec#011loss=10.409967\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:45 INFO 140374100940160] Epoch[16] Batch[10] avg_epoch_loss=10.460461\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:45 INFO 140374100940160] #quality_metric: host=algo-1, epoch=16, batch=10 train loss <loss>=10.521053314208984\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:45 INFO 140374100940160] Epoch[16] Batch [10]#011Speed: 1679.52 samples/sec#011loss=10.521053\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:45 INFO 140374100940160] processed a total of 683 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459144.4815183, \"EndTime\": 1631459145.1273751, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 645.3890800476074, \"count\": 1, \"min\": 645.3890800476074, \"max\": 645.3890800476074}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:45 INFO 140374100940160] #throughput_metric: host=algo-1, train throughput=1058.0856024459952 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:45 INFO 140374100940160] #progress_metric: host=algo-1, completed 8.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:45 INFO 140374100940160] #quality_metric: host=algo-1, epoch=16, train loss <loss>=10.460461009632457\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:45 INFO 140374100940160] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:45 INFO 140374100940160] Epoch[17] Batch[0] avg_epoch_loss=10.874466\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:45 INFO 140374100940160] #quality_metric: host=algo-1, epoch=17, batch=0 train loss <loss>=10.874465942382812\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:45 INFO 140374100940160] Epoch[17] Batch[5] avg_epoch_loss=10.410257\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:45 INFO 140374100940160] #quality_metric: host=algo-1, epoch=17, batch=5 train loss <loss>=10.410257339477539\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:45 INFO 140374100940160] Epoch[17] Batch [5]#011Speed: 1728.39 samples/sec#011loss=10.410257\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:45 INFO 140374100940160] processed a total of 636 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459145.1274557, \"EndTime\": 1631459145.6922343, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 564.2979145050049, \"count\": 1, \"min\": 564.2979145050049, \"max\": 564.2979145050049}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:45 INFO 140374100940160] #throughput_metric: host=algo-1, train throughput=1126.8176272675623 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:45 INFO 140374100940160] #progress_metric: host=algo-1, completed 9.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:45 INFO 140374100940160] #quality_metric: host=algo-1, epoch=17, train loss <loss>=10.497855186462402\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:45 INFO 140374100940160] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:45 INFO 140374100940160] Epoch[18] Batch[0] avg_epoch_loss=9.518381\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:45 INFO 140374100940160] #quality_metric: host=algo-1, epoch=18, batch=0 train loss <loss>=9.518381118774414\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:46 INFO 140374100940160] Epoch[18] Batch[5] avg_epoch_loss=10.054484\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:46 INFO 140374100940160] #quality_metric: host=algo-1, epoch=18, batch=5 train loss <loss>=10.054484049479166\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:46 INFO 140374100940160] Epoch[18] Batch [5]#011Speed: 1766.04 samples/sec#011loss=10.054484\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:46 INFO 140374100940160] processed a total of 639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459145.6923194, \"EndTime\": 1631459146.2718632, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 579.0379047393799, \"count\": 1, \"min\": 579.0379047393799, \"max\": 579.0379047393799}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:46 INFO 140374100940160] #throughput_metric: host=algo-1, train throughput=1103.3111995445408 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:46 INFO 140374100940160] #progress_metric: host=algo-1, completed 9.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:46 INFO 140374100940160] #quality_metric: host=algo-1, epoch=18, train loss <loss>=9.963340091705323\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:46 INFO 140374100940160] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:46 INFO 140374100940160] Saved checkpoint to \"/opt/ml/model/state_9fd7b619-4a75-4159-9c31-3cce8d2a3359-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459146.27195, \"EndTime\": 1631459146.2846522, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 12.133598327636719, \"count\": 1, \"min\": 12.133598327636719, \"max\": 12.133598327636719}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:46 INFO 140374100940160] Epoch[19] Batch[0] avg_epoch_loss=9.112722\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:46 INFO 140374100940160] #quality_metric: host=algo-1, epoch=19, batch=0 train loss <loss>=9.112722396850586\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:46 INFO 140374100940160] Epoch[19] Batch[5] avg_epoch_loss=10.272367\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:46 INFO 140374100940160] #quality_metric: host=algo-1, epoch=19, batch=5 train loss <loss>=10.272366523742676\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:46 INFO 140374100940160] Epoch[19] Batch [5]#011Speed: 2106.73 samples/sec#011loss=10.272367\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:46 INFO 140374100940160] processed a total of 626 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459146.2847285, \"EndTime\": 1631459146.8450181, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 560.2240562438965, \"count\": 1, \"min\": 560.2240562438965, \"max\": 560.2240562438965}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:46 INFO 140374100940160] #throughput_metric: host=algo-1, train throughput=1117.2003909469486 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:46 INFO 140374100940160] #progress_metric: host=algo-1, completed 10.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:46 INFO 140374100940160] #quality_metric: host=algo-1, epoch=19, train loss <loss>=10.28594093322754\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:46 INFO 140374100940160] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:47 INFO 140374100940160] Epoch[20] Batch[0] avg_epoch_loss=10.730015\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:47 INFO 140374100940160] #quality_metric: host=algo-1, epoch=20, batch=0 train loss <loss>=10.73001480102539\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:47 INFO 140374100940160] Epoch[20] Batch[5] avg_epoch_loss=10.416559\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:47 INFO 140374100940160] #quality_metric: host=algo-1, epoch=20, batch=5 train loss <loss>=10.416558901468912\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:47 INFO 140374100940160] Epoch[20] Batch [5]#011Speed: 1702.26 samples/sec#011loss=10.416559\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:47 INFO 140374100940160] processed a total of 623 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459146.8450868, \"EndTime\": 1631459147.415195, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 569.5548057556152, \"count\": 1, \"min\": 569.5548057556152, \"max\": 569.5548057556152}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:47 INFO 140374100940160] #throughput_metric: host=algo-1, train throughput=1093.5831793223517 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:47 INFO 140374100940160] #progress_metric: host=algo-1, completed 10.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:47 INFO 140374100940160] #quality_metric: host=algo-1, epoch=20, train loss <loss>=10.292873573303222\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:47 INFO 140374100940160] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:47 INFO 140374100940160] Epoch[21] Batch[0] avg_epoch_loss=9.841816\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:47 INFO 140374100940160] #quality_metric: host=algo-1, epoch=21, batch=0 train loss <loss>=9.841815948486328\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:47 INFO 140374100940160] Epoch[21] Batch[5] avg_epoch_loss=10.074699\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:47 INFO 140374100940160] #quality_metric: host=algo-1, epoch=21, batch=5 train loss <loss>=10.07469908396403\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:47 INFO 140374100940160] Epoch[21] Batch [5]#011Speed: 2054.69 samples/sec#011loss=10.074699\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:48 INFO 140374100940160] Epoch[21] Batch[10] avg_epoch_loss=10.408146\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:48 INFO 140374100940160] #quality_metric: host=algo-1, epoch=21, batch=10 train loss <loss>=10.808281707763673\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:48 INFO 140374100940160] Epoch[21] Batch [10]#011Speed: 1824.58 samples/sec#011loss=10.808282\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:48 INFO 140374100940160] processed a total of 670 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459147.4152868, \"EndTime\": 1631459148.0232515, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 607.4409484863281, \"count\": 1, \"min\": 607.4409484863281, \"max\": 607.4409484863281}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:48 INFO 140374100940160] #throughput_metric: host=algo-1, train throughput=1102.7697253380297 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:48 INFO 140374100940160] #progress_metric: host=algo-1, completed 11.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:48 INFO 140374100940160] #quality_metric: host=algo-1, epoch=21, train loss <loss>=10.408145731145686\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:48 INFO 140374100940160] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:48 INFO 140374100940160] Epoch[22] Batch[0] avg_epoch_loss=10.421488\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:48 INFO 140374100940160] #quality_metric: host=algo-1, epoch=22, batch=0 train loss <loss>=10.421487808227539\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:48 INFO 140374100940160] Epoch[22] Batch[5] avg_epoch_loss=10.172617\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:48 INFO 140374100940160] #quality_metric: host=algo-1, epoch=22, batch=5 train loss <loss>=10.172617276509603\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:48 INFO 140374100940160] Epoch[22] Batch [5]#011Speed: 2163.56 samples/sec#011loss=10.172617\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:48 INFO 140374100940160] Epoch[22] Batch[10] avg_epoch_loss=10.535052\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:48 INFO 140374100940160] #quality_metric: host=algo-1, epoch=22, batch=10 train loss <loss>=10.969973754882812\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:48 INFO 140374100940160] Epoch[22] Batch [10]#011Speed: 2018.35 samples/sec#011loss=10.969974\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:48 INFO 140374100940160] processed a total of 647 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459148.0233355, \"EndTime\": 1631459148.6311212, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 607.3124408721924, \"count\": 1, \"min\": 607.3124408721924, \"max\": 607.3124408721924}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:48 INFO 140374100940160] #throughput_metric: host=algo-1, train throughput=1065.1395789626554 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:48 INFO 140374100940160] #progress_metric: host=algo-1, completed 11.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:48 INFO 140374100940160] #quality_metric: host=algo-1, epoch=22, train loss <loss>=10.535052039406516\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:48 INFO 140374100940160] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:48 INFO 140374100940160] Epoch[23] Batch[0] avg_epoch_loss=10.922272\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:48 INFO 140374100940160] #quality_metric: host=algo-1, epoch=23, batch=0 train loss <loss>=10.922271728515625\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:49 INFO 140374100940160] Epoch[23] Batch[5] avg_epoch_loss=10.409966\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:49 INFO 140374100940160] #quality_metric: host=algo-1, epoch=23, batch=5 train loss <loss>=10.409966309865316\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:49 INFO 140374100940160] Epoch[23] Batch [5]#011Speed: 1894.95 samples/sec#011loss=10.409966\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:49 INFO 140374100940160] Epoch[23] Batch[10] avg_epoch_loss=10.698575\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:49 INFO 140374100940160] #quality_metric: host=algo-1, epoch=23, batch=10 train loss <loss>=11.04490623474121\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:49 INFO 140374100940160] Epoch[23] Batch [10]#011Speed: 1631.95 samples/sec#011loss=11.044906\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:49 INFO 140374100940160] processed a total of 663 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459148.631205, \"EndTime\": 1631459149.2882233, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 656.5365791320801, \"count\": 1, \"min\": 656.5365791320801, \"max\": 656.5365791320801}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:49 INFO 140374100940160] #throughput_metric: host=algo-1, train throughput=1009.6665282114589 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:49 INFO 140374100940160] #progress_metric: host=algo-1, completed 12.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:49 INFO 140374100940160] #quality_metric: host=algo-1, epoch=23, train loss <loss>=10.698575366627086\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:49 INFO 140374100940160] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:49 INFO 140374100940160] Epoch[24] Batch[0] avg_epoch_loss=10.598270\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:49 INFO 140374100940160] #quality_metric: host=algo-1, epoch=24, batch=0 train loss <loss>=10.598270416259766\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:49 INFO 140374100940160] Epoch[24] Batch[5] avg_epoch_loss=10.240781\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:49 INFO 140374100940160] #quality_metric: host=algo-1, epoch=24, batch=5 train loss <loss>=10.24078114827474\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:49 INFO 140374100940160] Epoch[24] Batch [5]#011Speed: 1707.55 samples/sec#011loss=10.240781\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:49 INFO 140374100940160] Epoch[24] Batch[10] avg_epoch_loss=10.450296\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:49 INFO 140374100940160] #quality_metric: host=algo-1, epoch=24, batch=10 train loss <loss>=10.701713180541992\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:49 INFO 140374100940160] Epoch[24] Batch [10]#011Speed: 1543.69 samples/sec#011loss=10.701713\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:49 INFO 140374100940160] processed a total of 659 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459149.288299, \"EndTime\": 1631459149.9360452, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 647.2499370574951, \"count\": 1, \"min\": 647.2499370574951, \"max\": 647.2499370574951}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:49 INFO 140374100940160] #throughput_metric: host=algo-1, train throughput=1018.0173288394981 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:49 INFO 140374100940160] #progress_metric: host=algo-1, completed 12.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:49 INFO 140374100940160] #quality_metric: host=algo-1, epoch=24, train loss <loss>=10.450295708396219\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:49 INFO 140374100940160] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:50 INFO 140374100940160] Epoch[25] Batch[0] avg_epoch_loss=9.825950\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:50 INFO 140374100940160] #quality_metric: host=algo-1, epoch=25, batch=0 train loss <loss>=9.825949668884277\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:50 INFO 140374100940160] Epoch[25] Batch[5] avg_epoch_loss=10.269238\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:50 INFO 140374100940160] #quality_metric: host=algo-1, epoch=25, batch=5 train loss <loss>=10.269237518310547\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:50 INFO 140374100940160] Epoch[25] Batch [5]#011Speed: 1779.68 samples/sec#011loss=10.269238\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:50 INFO 140374100940160] processed a total of 594 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459149.9360998, \"EndTime\": 1631459150.5167475, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 580.2676677703857, \"count\": 1, \"min\": 580.2676677703857, \"max\": 580.2676677703857}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:50 INFO 140374100940160] #throughput_metric: host=algo-1, train throughput=1023.4468477890713 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:50 INFO 140374100940160] #progress_metric: host=algo-1, completed 13.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:50 INFO 140374100940160] #quality_metric: host=algo-1, epoch=25, train loss <loss>=10.061701011657714\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:50 INFO 140374100940160] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:50 INFO 140374100940160] Epoch[26] Batch[0] avg_epoch_loss=11.174282\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:50 INFO 140374100940160] #quality_metric: host=algo-1, epoch=26, batch=0 train loss <loss>=11.17428207397461\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:50 INFO 140374100940160] Epoch[26] Batch[5] avg_epoch_loss=10.607372\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:50 INFO 140374100940160] #quality_metric: host=algo-1, epoch=26, batch=5 train loss <loss>=10.607371807098389\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:50 INFO 140374100940160] Epoch[26] Batch [5]#011Speed: 2191.12 samples/sec#011loss=10.607372\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:51 INFO 140374100940160] processed a total of 623 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459150.5168352, \"EndTime\": 1631459151.0541627, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 536.7939472198486, \"count\": 1, \"min\": 536.7939472198486, \"max\": 536.7939472198486}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:51 INFO 140374100940160] #throughput_metric: host=algo-1, train throughput=1160.3381708669815 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:51 INFO 140374100940160] #progress_metric: host=algo-1, completed 13.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:51 INFO 140374100940160] #quality_metric: host=algo-1, epoch=26, train loss <loss>=10.1255521774292\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:51 INFO 140374100940160] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:51 INFO 140374100940160] Epoch[27] Batch[0] avg_epoch_loss=10.903444\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:51 INFO 140374100940160] #quality_metric: host=algo-1, epoch=27, batch=0 train loss <loss>=10.903444290161133\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:51 INFO 140374100940160] Epoch[27] Batch[5] avg_epoch_loss=10.181408\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:51 INFO 140374100940160] #quality_metric: host=algo-1, epoch=27, batch=5 train loss <loss>=10.181407928466797\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:51 INFO 140374100940160] Epoch[27] Batch [5]#011Speed: 2045.41 samples/sec#011loss=10.181408\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:51 INFO 140374100940160] Epoch[27] Batch[10] avg_epoch_loss=9.717927\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:51 INFO 140374100940160] #quality_metric: host=algo-1, epoch=27, batch=10 train loss <loss>=9.161750745773315\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:51 INFO 140374100940160] Epoch[27] Batch [10]#011Speed: 1532.79 samples/sec#011loss=9.161751\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:51 INFO 140374100940160] processed a total of 653 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459151.0542443, \"EndTime\": 1631459151.691281, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 636.5599632263184, \"count\": 1, \"min\": 636.5599632263184, \"max\": 636.5599632263184}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:51 INFO 140374100940160] #throughput_metric: host=algo-1, train throughput=1025.643905915254 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:51 INFO 140374100940160] #progress_metric: host=algo-1, completed 14.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:51 INFO 140374100940160] #quality_metric: host=algo-1, epoch=27, train loss <loss>=9.71792739087885\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:51 INFO 140374100940160] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:51 INFO 140374100940160] Saved checkpoint to \"/opt/ml/model/state_11d9103a-60bb-4b3b-9ae5-d64ae3b633dd-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459151.6913586, \"EndTime\": 1631459151.7030275, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 11.156797409057617, \"count\": 1, \"min\": 11.156797409057617, \"max\": 11.156797409057617}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:51 INFO 140374100940160] Epoch[28] Batch[0] avg_epoch_loss=9.206250\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:51 INFO 140374100940160] #quality_metric: host=algo-1, epoch=28, batch=0 train loss <loss>=9.206250190734863\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:52 INFO 140374100940160] Epoch[28] Batch[5] avg_epoch_loss=10.429557\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:52 INFO 140374100940160] #quality_metric: host=algo-1, epoch=28, batch=5 train loss <loss>=10.42955732345581\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:52 INFO 140374100940160] Epoch[28] Batch [5]#011Speed: 1781.38 samples/sec#011loss=10.429557\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:52 INFO 140374100940160] processed a total of 615 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459151.7030938, \"EndTime\": 1631459152.2592096, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 556.0569763183594, \"count\": 1, \"min\": 556.0569763183594, \"max\": 556.0569763183594}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:52 INFO 140374100940160] #throughput_metric: host=algo-1, train throughput=1105.7552323567638 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:52 INFO 140374100940160] #progress_metric: host=algo-1, completed 14.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:52 INFO 140374100940160] #quality_metric: host=algo-1, epoch=28, train loss <loss>=10.241635131835938\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:52 INFO 140374100940160] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:52 INFO 140374100940160] Epoch[29] Batch[0] avg_epoch_loss=10.447813\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:52 INFO 140374100940160] #quality_metric: host=algo-1, epoch=29, batch=0 train loss <loss>=10.447813034057617\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:52 INFO 140374100940160] Epoch[29] Batch[5] avg_epoch_loss=9.986852\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:52 INFO 140374100940160] #quality_metric: host=algo-1, epoch=29, batch=5 train loss <loss>=9.986852169036865\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:52 INFO 140374100940160] Epoch[29] Batch [5]#011Speed: 2001.48 samples/sec#011loss=9.986852\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:52 INFO 140374100940160] Epoch[29] Batch[10] avg_epoch_loss=10.098243\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:52 INFO 140374100940160] #quality_metric: host=algo-1, epoch=29, batch=10 train loss <loss>=10.231912422180176\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:52 INFO 140374100940160] Epoch[29] Batch [10]#011Speed: 2032.48 samples/sec#011loss=10.231912\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:52 INFO 140374100940160] processed a total of 661 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459152.2592974, \"EndTime\": 1631459152.8284125, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 568.6078071594238, \"count\": 1, \"min\": 568.6078071594238, \"max\": 568.6078071594238}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:52 INFO 140374100940160] #throughput_metric: host=algo-1, train throughput=1162.2705930045809 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:52 INFO 140374100940160] #progress_metric: host=algo-1, completed 15.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:52 INFO 140374100940160] #quality_metric: host=algo-1, epoch=29, train loss <loss>=10.098243193192916\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:52 INFO 140374100940160] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:53 INFO 140374100940160] Epoch[30] Batch[0] avg_epoch_loss=9.484995\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:53 INFO 140374100940160] #quality_metric: host=algo-1, epoch=30, batch=0 train loss <loss>=9.484994888305664\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:53 INFO 140374100940160] Epoch[30] Batch[5] avg_epoch_loss=10.077862\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:53 INFO 140374100940160] #quality_metric: host=algo-1, epoch=30, batch=5 train loss <loss>=10.077861626942953\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:53 INFO 140374100940160] Epoch[30] Batch [5]#011Speed: 2117.94 samples/sec#011loss=10.077862\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:53 INFO 140374100940160] Epoch[30] Batch[10] avg_epoch_loss=9.921914\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:53 INFO 140374100940160] #quality_metric: host=algo-1, epoch=30, batch=10 train loss <loss>=9.734777641296386\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:53 INFO 140374100940160] Epoch[30] Batch [10]#011Speed: 1832.97 samples/sec#011loss=9.734778\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:53 INFO 140374100940160] processed a total of 682 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459152.8284793, \"EndTime\": 1631459153.391339, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 562.3650550842285, \"count\": 1, \"min\": 562.3650550842285, \"max\": 562.3650550842285}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:53 INFO 140374100940160] #throughput_metric: host=algo-1, train throughput=1212.4876136608325 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:53 INFO 140374100940160] #progress_metric: host=algo-1, completed 15.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:53 INFO 140374100940160] #quality_metric: host=algo-1, epoch=30, train loss <loss>=9.921914360739969\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:53 INFO 140374100940160] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:53 INFO 140374100940160] Epoch[31] Batch[0] avg_epoch_loss=10.726032\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:53 INFO 140374100940160] #quality_metric: host=algo-1, epoch=31, batch=0 train loss <loss>=10.726032257080078\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:53 INFO 140374100940160] Epoch[31] Batch[5] avg_epoch_loss=10.196699\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:53 INFO 140374100940160] #quality_metric: host=algo-1, epoch=31, batch=5 train loss <loss>=10.196698983510336\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:53 INFO 140374100940160] Epoch[31] Batch [5]#011Speed: 2162.48 samples/sec#011loss=10.196699\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:53 INFO 140374100940160] Epoch[31] Batch[10] avg_epoch_loss=10.155676\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:53 INFO 140374100940160] #quality_metric: host=algo-1, epoch=31, batch=10 train loss <loss>=10.106448745727539\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:53 INFO 140374100940160] Epoch[31] Batch [10]#011Speed: 1936.73 samples/sec#011loss=10.106449\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:53 INFO 140374100940160] processed a total of 682 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459153.3914201, \"EndTime\": 1631459153.93479, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 542.8812503814697, \"count\": 1, \"min\": 542.8812503814697, \"max\": 542.8812503814697}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:53 INFO 140374100940160] #throughput_metric: host=algo-1, train throughput=1256.016190029994 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:53 INFO 140374100940160] #progress_metric: host=algo-1, completed 16.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:53 INFO 140374100940160] #quality_metric: host=algo-1, epoch=31, train loss <loss>=10.15567614815452\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:53 INFO 140374100940160] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:54 INFO 140374100940160] Epoch[32] Batch[0] avg_epoch_loss=9.729583\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:54 INFO 140374100940160] #quality_metric: host=algo-1, epoch=32, batch=0 train loss <loss>=9.729582786560059\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:54 INFO 140374100940160] Epoch[32] Batch[5] avg_epoch_loss=10.115027\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:54 INFO 140374100940160] #quality_metric: host=algo-1, epoch=32, batch=5 train loss <loss>=10.115027268727621\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:54 INFO 140374100940160] Epoch[32] Batch [5]#011Speed: 2138.14 samples/sec#011loss=10.115027\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:54 INFO 140374100940160] Epoch[32] Batch[10] avg_epoch_loss=9.309087\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:54 INFO 140374100940160] #quality_metric: host=algo-1, epoch=32, batch=10 train loss <loss>=8.341959238052368\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:54 INFO 140374100940160] Epoch[32] Batch [10]#011Speed: 2125.50 samples/sec#011loss=8.341959\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:54 INFO 140374100940160] processed a total of 642 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459153.934862, \"EndTime\": 1631459154.480281, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 544.9378490447998, \"count\": 1, \"min\": 544.9378490447998, \"max\": 544.9378490447998}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:54 INFO 140374100940160] #throughput_metric: host=algo-1, train throughput=1177.8593668872284 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:54 INFO 140374100940160] #progress_metric: host=algo-1, completed 16.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:54 INFO 140374100940160] #quality_metric: host=algo-1, epoch=32, train loss <loss>=9.309087254784323\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:54 INFO 140374100940160] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:54 INFO 140374100940160] Saved checkpoint to \"/opt/ml/model/state_bd1744bb-5829-41c8-b497-56954e4a2fcd-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459154.4803598, \"EndTime\": 1631459154.4941325, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 13.255834579467773, \"count\": 1, \"min\": 13.255834579467773, \"max\": 13.255834579467773}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:54 INFO 140374100940160] Epoch[33] Batch[0] avg_epoch_loss=10.374475\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:54 INFO 140374100940160] #quality_metric: host=algo-1, epoch=33, batch=0 train loss <loss>=10.37447452545166\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:54 INFO 140374100940160] Epoch[33] Batch[5] avg_epoch_loss=10.060957\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:54 INFO 140374100940160] #quality_metric: host=algo-1, epoch=33, batch=5 train loss <loss>=10.060956954956055\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:54 INFO 140374100940160] Epoch[33] Batch [5]#011Speed: 1930.02 samples/sec#011loss=10.060957\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:55 INFO 140374100940160] processed a total of 606 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459154.4941897, \"EndTime\": 1631459155.0117004, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 517.4477100372314, \"count\": 1, \"min\": 517.4477100372314, \"max\": 517.4477100372314}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:55 INFO 140374100940160] #throughput_metric: host=algo-1, train throughput=1170.819398649601 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:55 INFO 140374100940160] #progress_metric: host=algo-1, completed 17.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:55 INFO 140374100940160] #quality_metric: host=algo-1, epoch=33, train loss <loss>=9.698627853393555\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:55 INFO 140374100940160] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:55 INFO 140374100940160] Epoch[34] Batch[0] avg_epoch_loss=11.037276\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:55 INFO 140374100940160] #quality_metric: host=algo-1, epoch=34, batch=0 train loss <loss>=11.037276268005371\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:55 INFO 140374100940160] Epoch[34] Batch[5] avg_epoch_loss=10.627122\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:55 INFO 140374100940160] #quality_metric: host=algo-1, epoch=34, batch=5 train loss <loss>=10.627121607462565\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:55 INFO 140374100940160] Epoch[34] Batch [5]#011Speed: 2112.34 samples/sec#011loss=10.627122\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:55 INFO 140374100940160] processed a total of 610 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459155.0117908, \"EndTime\": 1631459155.5388224, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 526.5102386474609, \"count\": 1, \"min\": 526.5102386474609, \"max\": 526.5102386474609}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:55 INFO 140374100940160] #throughput_metric: host=algo-1, train throughput=1158.3044601408694 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:55 INFO 140374100940160] #progress_metric: host=algo-1, completed 17.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:55 INFO 140374100940160] #quality_metric: host=algo-1, epoch=34, train loss <loss>=10.421638870239258\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:55 INFO 140374100940160] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:55 INFO 140374100940160] Epoch[35] Batch[0] avg_epoch_loss=9.823550\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:55 INFO 140374100940160] #quality_metric: host=algo-1, epoch=35, batch=0 train loss <loss>=9.8235502243042\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:55 INFO 140374100940160] Epoch[35] Batch[5] avg_epoch_loss=10.312316\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:55 INFO 140374100940160] #quality_metric: host=algo-1, epoch=35, batch=5 train loss <loss>=10.312316258748373\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:55 INFO 140374100940160] Epoch[35] Batch [5]#011Speed: 2169.43 samples/sec#011loss=10.312316\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:56 INFO 140374100940160] Epoch[35] Batch[10] avg_epoch_loss=10.279713\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:56 INFO 140374100940160] #quality_metric: host=algo-1, epoch=35, batch=10 train loss <loss>=10.240588569641114\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:56 INFO 140374100940160] Epoch[35] Batch [10]#011Speed: 1916.03 samples/sec#011loss=10.240589\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:56 INFO 140374100940160] processed a total of 647 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459155.5389051, \"EndTime\": 1631459156.0927575, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 553.3177852630615, \"count\": 1, \"min\": 553.3177852630615, \"max\": 553.3177852630615}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:56 INFO 140374100940160] #throughput_metric: host=algo-1, train throughput=1169.0606157470152 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:56 INFO 140374100940160] #progress_metric: host=algo-1, completed 18.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:56 INFO 140374100940160] #quality_metric: host=algo-1, epoch=35, train loss <loss>=10.279712763699619\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:56 INFO 140374100940160] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:56 INFO 140374100940160] Epoch[36] Batch[0] avg_epoch_loss=10.200305\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:56 INFO 140374100940160] #quality_metric: host=algo-1, epoch=36, batch=0 train loss <loss>=10.200304985046387\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:56 INFO 140374100940160] Epoch[36] Batch[5] avg_epoch_loss=10.376738\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:56 INFO 140374100940160] #quality_metric: host=algo-1, epoch=36, batch=5 train loss <loss>=10.376737753550211\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:56 INFO 140374100940160] Epoch[36] Batch [5]#011Speed: 2145.11 samples/sec#011loss=10.376738\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:56 INFO 140374100940160] Epoch[36] Batch[10] avg_epoch_loss=10.792122\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:56 INFO 140374100940160] #quality_metric: host=algo-1, epoch=36, batch=10 train loss <loss>=11.290583610534668\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:56 INFO 140374100940160] Epoch[36] Batch [10]#011Speed: 1847.18 samples/sec#011loss=11.290584\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:56 INFO 140374100940160] processed a total of 645 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459156.0928392, \"EndTime\": 1631459156.6612163, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 567.857027053833, \"count\": 1, \"min\": 567.857027053833, \"max\": 567.857027053833}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:56 INFO 140374100940160] #throughput_metric: host=algo-1, train throughput=1135.5517843643502 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:56 INFO 140374100940160] #progress_metric: host=algo-1, completed 18.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:56 INFO 140374100940160] #quality_metric: host=algo-1, epoch=36, train loss <loss>=10.792122233997691\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:56 INFO 140374100940160] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:56 INFO 140374100940160] Epoch[37] Batch[0] avg_epoch_loss=10.704820\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:56 INFO 140374100940160] #quality_metric: host=algo-1, epoch=37, batch=0 train loss <loss>=10.704819679260254\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:57 INFO 140374100940160] Epoch[37] Batch[5] avg_epoch_loss=10.460879\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:57 INFO 140374100940160] #quality_metric: host=algo-1, epoch=37, batch=5 train loss <loss>=10.460878849029541\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:57 INFO 140374100940160] Epoch[37] Batch [5]#011Speed: 1783.70 samples/sec#011loss=10.460879\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:57 INFO 140374100940160] Epoch[37] Batch[10] avg_epoch_loss=10.617369\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:57 INFO 140374100940160] #quality_metric: host=algo-1, epoch=37, batch=10 train loss <loss>=10.805157089233399\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:57 INFO 140374100940160] Epoch[37] Batch [10]#011Speed: 1895.32 samples/sec#011loss=10.805157\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:57 INFO 140374100940160] processed a total of 657 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459156.661327, \"EndTime\": 1631459157.2623894, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 600.5470752716064, \"count\": 1, \"min\": 600.5470752716064, \"max\": 600.5470752716064}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:57 INFO 140374100940160] #throughput_metric: host=algo-1, train throughput=1093.7875502307718 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:57 INFO 140374100940160] #progress_metric: host=algo-1, completed 19.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:57 INFO 140374100940160] #quality_metric: host=algo-1, epoch=37, train loss <loss>=10.617368958213113\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:57 INFO 140374100940160] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:57 INFO 140374100940160] Epoch[38] Batch[0] avg_epoch_loss=10.090693\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:57 INFO 140374100940160] #quality_metric: host=algo-1, epoch=38, batch=0 train loss <loss>=10.090692520141602\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:57 INFO 140374100940160] Epoch[38] Batch[5] avg_epoch_loss=10.210559\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:57 INFO 140374100940160] #quality_metric: host=algo-1, epoch=38, batch=5 train loss <loss>=10.210558732350668\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:57 INFO 140374100940160] Epoch[38] Batch [5]#011Speed: 2095.44 samples/sec#011loss=10.210559\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:57 INFO 140374100940160] Epoch[38] Batch[10] avg_epoch_loss=10.064479\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:57 INFO 140374100940160] #quality_metric: host=algo-1, epoch=38, batch=10 train loss <loss>=9.88918342590332\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:57 INFO 140374100940160] Epoch[38] Batch [10]#011Speed: 1922.23 samples/sec#011loss=9.889183\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:57 INFO 140374100940160] processed a total of 671 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459157.2624712, \"EndTime\": 1631459157.8399525, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 577.0182609558105, \"count\": 1, \"min\": 577.0182609558105, \"max\": 577.0182609558105}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:57 INFO 140374100940160] #throughput_metric: host=algo-1, train throughput=1162.6331732276667 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:57 INFO 140374100940160] #progress_metric: host=algo-1, completed 19.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:57 INFO 140374100940160] #quality_metric: host=algo-1, epoch=38, train loss <loss>=10.064479047601873\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:57 INFO 140374100940160] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:58 INFO 140374100940160] Epoch[39] Batch[0] avg_epoch_loss=9.668612\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:58 INFO 140374100940160] #quality_metric: host=algo-1, epoch=39, batch=0 train loss <loss>=9.668611526489258\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:58 INFO 140374100940160] Epoch[39] Batch[5] avg_epoch_loss=9.907803\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:58 INFO 140374100940160] #quality_metric: host=algo-1, epoch=39, batch=5 train loss <loss>=9.907803058624268\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:58 INFO 140374100940160] Epoch[39] Batch [5]#011Speed: 1717.84 samples/sec#011loss=9.907803\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:58 INFO 140374100940160] processed a total of 630 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459157.8400364, \"EndTime\": 1631459158.414689, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 574.1786956787109, \"count\": 1, \"min\": 574.1786956787109, \"max\": 574.1786956787109}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:58 INFO 140374100940160] #throughput_metric: host=algo-1, train throughput=1096.9807090987892 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:58 INFO 140374100940160] #progress_metric: host=algo-1, completed 20.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:58 INFO 140374100940160] #quality_metric: host=algo-1, epoch=39, train loss <loss>=9.884140014648438\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:58 INFO 140374100940160] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:58 INFO 140374100940160] Epoch[40] Batch[0] avg_epoch_loss=9.789661\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:58 INFO 140374100940160] #quality_metric: host=algo-1, epoch=40, batch=0 train loss <loss>=9.789661407470703\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:58 INFO 140374100940160] Epoch[40] Batch[5] avg_epoch_loss=10.026851\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:58 INFO 140374100940160] #quality_metric: host=algo-1, epoch=40, batch=5 train loss <loss>=10.026850700378418\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:58 INFO 140374100940160] Epoch[40] Batch [5]#011Speed: 2121.82 samples/sec#011loss=10.026851\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:58 INFO 140374100940160] processed a total of 603 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459158.4147758, \"EndTime\": 1631459158.978877, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 563.5945796966553, \"count\": 1, \"min\": 563.5945796966553, \"max\": 563.5945796966553}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:58 INFO 140374100940160] #throughput_metric: host=algo-1, train throughput=1069.6958405128114 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:58 INFO 140374100940160] #progress_metric: host=algo-1, completed 20.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:58 INFO 140374100940160] #quality_metric: host=algo-1, epoch=40, train loss <loss>=10.162254810333252\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:58 INFO 140374100940160] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:59 INFO 140374100940160] Epoch[41] Batch[0] avg_epoch_loss=10.270334\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:59 INFO 140374100940160] #quality_metric: host=algo-1, epoch=41, batch=0 train loss <loss>=10.270334243774414\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:59 INFO 140374100940160] Epoch[41] Batch[5] avg_epoch_loss=10.073849\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:59 INFO 140374100940160] #quality_metric: host=algo-1, epoch=41, batch=5 train loss <loss>=10.073849360148111\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:59 INFO 140374100940160] Epoch[41] Batch [5]#011Speed: 2037.45 samples/sec#011loss=10.073849\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:59 INFO 140374100940160] processed a total of 636 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459158.9789512, \"EndTime\": 1631459159.5218806, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 542.3259735107422, \"count\": 1, \"min\": 542.3259735107422, \"max\": 542.3259735107422}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:59 INFO 140374100940160] #throughput_metric: host=algo-1, train throughput=1172.451182174512 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:59 INFO 140374100940160] #progress_metric: host=algo-1, completed 21.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:59 INFO 140374100940160] #quality_metric: host=algo-1, epoch=41, train loss <loss>=10.042732429504394\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:59 INFO 140374100940160] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:59 INFO 140374100940160] Epoch[42] Batch[0] avg_epoch_loss=9.688555\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:59 INFO 140374100940160] #quality_metric: host=algo-1, epoch=42, batch=0 train loss <loss>=9.688554763793945\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:59 INFO 140374100940160] Epoch[42] Batch[5] avg_epoch_loss=9.860297\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:59 INFO 140374100940160] #quality_metric: host=algo-1, epoch=42, batch=5 train loss <loss>=9.860297203063965\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:05:59 INFO 140374100940160] Epoch[42] Batch [5]#011Speed: 2042.10 samples/sec#011loss=9.860297\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:00 INFO 140374100940160] Epoch[42] Batch[10] avg_epoch_loss=10.051289\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:00 INFO 140374100940160] #quality_metric: host=algo-1, epoch=42, batch=10 train loss <loss>=10.280479621887206\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:00 INFO 140374100940160] Epoch[42] Batch [10]#011Speed: 1967.76 samples/sec#011loss=10.280480\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:00 INFO 140374100940160] processed a total of 653 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459159.5219684, \"EndTime\": 1631459160.1132753, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 590.7950401306152, \"count\": 1, \"min\": 590.7950401306152, \"max\": 590.7950401306152}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:00 INFO 140374100940160] #throughput_metric: host=algo-1, train throughput=1105.0191246446557 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:00 INFO 140374100940160] #progress_metric: host=algo-1, completed 21.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:00 INFO 140374100940160] #quality_metric: host=algo-1, epoch=42, train loss <loss>=10.051289211619984\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:00 INFO 140374100940160] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:00 INFO 140374100940160] Epoch[43] Batch[0] avg_epoch_loss=9.265793\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:00 INFO 140374100940160] #quality_metric: host=algo-1, epoch=43, batch=0 train loss <loss>=9.265792846679688\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:00 INFO 140374100940160] Epoch[43] Batch[5] avg_epoch_loss=10.112419\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:00 INFO 140374100940160] #quality_metric: host=algo-1, epoch=43, batch=5 train loss <loss>=10.112419128417969\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:00 INFO 140374100940160] Epoch[43] Batch [5]#011Speed: 2090.29 samples/sec#011loss=10.112419\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:00 INFO 140374100940160] Epoch[43] Batch[10] avg_epoch_loss=9.975724\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:00 INFO 140374100940160] #quality_metric: host=algo-1, epoch=43, batch=10 train loss <loss>=9.811689186096192\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:00 INFO 140374100940160] Epoch[43] Batch [10]#011Speed: 1945.43 samples/sec#011loss=9.811689\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:00 INFO 140374100940160] processed a total of 652 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459160.113384, \"EndTime\": 1631459160.7464707, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 632.603645324707, \"count\": 1, \"min\": 632.603645324707, \"max\": 632.603645324707}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:00 INFO 140374100940160] #throughput_metric: host=algo-1, train throughput=1030.4650240387787 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:00 INFO 140374100940160] #progress_metric: host=algo-1, completed 22.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:00 INFO 140374100940160] #quality_metric: host=algo-1, epoch=43, train loss <loss>=9.975723700089889\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:00 INFO 140374100940160] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:01 INFO 140374100940160] Epoch[44] Batch[0] avg_epoch_loss=10.416033\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:01 INFO 140374100940160] #quality_metric: host=algo-1, epoch=44, batch=0 train loss <loss>=10.416032791137695\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:01 INFO 140374100940160] Epoch[44] Batch[5] avg_epoch_loss=10.391164\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:01 INFO 140374100940160] #quality_metric: host=algo-1, epoch=44, batch=5 train loss <loss>=10.39116382598877\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:01 INFO 140374100940160] Epoch[44] Batch [5]#011Speed: 2175.48 samples/sec#011loss=10.391164\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:01 INFO 140374100940160] Epoch[44] Batch[10] avg_epoch_loss=10.258063\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:01 INFO 140374100940160] #quality_metric: host=algo-1, epoch=44, batch=10 train loss <loss>=10.098342132568359\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:01 INFO 140374100940160] Epoch[44] Batch [10]#011Speed: 1641.31 samples/sec#011loss=10.098342\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:01 INFO 140374100940160] processed a total of 671 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459160.7465546, \"EndTime\": 1631459161.3467503, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 599.7250080108643, \"count\": 1, \"min\": 599.7250080108643, \"max\": 599.7250080108643}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:01 INFO 140374100940160] #throughput_metric: host=algo-1, train throughput=1118.6246596185024 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:01 INFO 140374100940160] #progress_metric: host=algo-1, completed 22.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:01 INFO 140374100940160] #quality_metric: host=algo-1, epoch=44, train loss <loss>=10.258063056252219\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:01 INFO 140374100940160] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:01 INFO 140374100940160] Epoch[45] Batch[0] avg_epoch_loss=10.101319\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:01 INFO 140374100940160] #quality_metric: host=algo-1, epoch=45, batch=0 train loss <loss>=10.101319313049316\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:01 INFO 140374100940160] Epoch[45] Batch[5] avg_epoch_loss=10.223509\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:01 INFO 140374100940160] #quality_metric: host=algo-1, epoch=45, batch=5 train loss <loss>=10.223508675893148\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:01 INFO 140374100940160] Epoch[45] Batch [5]#011Speed: 1540.15 samples/sec#011loss=10.223509\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:02 INFO 140374100940160] Epoch[45] Batch[10] avg_epoch_loss=10.322626\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:02 INFO 140374100940160] #quality_metric: host=algo-1, epoch=45, batch=10 train loss <loss>=10.44156608581543\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:02 INFO 140374100940160] Epoch[45] Batch [10]#011Speed: 1509.13 samples/sec#011loss=10.441566\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:02 INFO 140374100940160] processed a total of 683 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459161.3468337, \"EndTime\": 1631459162.0355842, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 688.2386207580566, \"count\": 1, \"min\": 688.2386207580566, \"max\": 688.2386207580566}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:02 INFO 140374100940160] #throughput_metric: host=algo-1, train throughput=992.2020709165621 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:02 INFO 140374100940160] #progress_metric: host=algo-1, completed 23.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:02 INFO 140374100940160] #quality_metric: host=algo-1, epoch=45, train loss <loss>=10.322625680403275\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:02 INFO 140374100940160] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:02 INFO 140374100940160] Epoch[46] Batch[0] avg_epoch_loss=9.610225\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:02 INFO 140374100940160] #quality_metric: host=algo-1, epoch=46, batch=0 train loss <loss>=9.610224723815918\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:02 INFO 140374100940160] Epoch[46] Batch[5] avg_epoch_loss=9.879163\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:02 INFO 140374100940160] #quality_metric: host=algo-1, epoch=46, batch=5 train loss <loss>=9.879162947336832\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:02 INFO 140374100940160] Epoch[46] Batch [5]#011Speed: 2128.64 samples/sec#011loss=9.879163\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:02 INFO 140374100940160] Epoch[46] Batch[10] avg_epoch_loss=10.123614\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:02 INFO 140374100940160] #quality_metric: host=algo-1, epoch=46, batch=10 train loss <loss>=10.41695556640625\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:02 INFO 140374100940160] Epoch[46] Batch [10]#011Speed: 1817.78 samples/sec#011loss=10.416956\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:02 INFO 140374100940160] processed a total of 662 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459162.0356746, \"EndTime\": 1631459162.6039495, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 567.7638053894043, \"count\": 1, \"min\": 567.7638053894043, \"max\": 567.7638053894043}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:02 INFO 140374100940160] #throughput_metric: host=algo-1, train throughput=1165.7350098871896 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:02 INFO 140374100940160] #progress_metric: host=algo-1, completed 23.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:02 INFO 140374100940160] #quality_metric: host=algo-1, epoch=46, train loss <loss>=10.123614137822932\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:02 INFO 140374100940160] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:02 INFO 140374100940160] Epoch[47] Batch[0] avg_epoch_loss=10.707739\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:02 INFO 140374100940160] #quality_metric: host=algo-1, epoch=47, batch=0 train loss <loss>=10.707738876342773\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:03 INFO 140374100940160] Epoch[47] Batch[5] avg_epoch_loss=9.678459\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:03 INFO 140374100940160] #quality_metric: host=algo-1, epoch=47, batch=5 train loss <loss>=9.678459485371908\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:03 INFO 140374100940160] Epoch[47] Batch [5]#011Speed: 1816.41 samples/sec#011loss=9.678459\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:03 INFO 140374100940160] Epoch[47] Batch[10] avg_epoch_loss=9.034372\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:03 INFO 140374100940160] #quality_metric: host=algo-1, epoch=47, batch=10 train loss <loss>=8.261467456817627\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:03 INFO 140374100940160] Epoch[47] Batch [10]#011Speed: 2080.70 samples/sec#011loss=8.261467\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:03 INFO 140374100940160] processed a total of 648 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459162.604026, \"EndTime\": 1631459163.1724489, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 567.8613185882568, \"count\": 1, \"min\": 567.8613185882568, \"max\": 567.8613185882568}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:03 INFO 140374100940160] #throughput_metric: host=algo-1, train throughput=1140.9124381250463 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:03 INFO 140374100940160] #progress_metric: host=algo-1, completed 24.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:03 INFO 140374100940160] #quality_metric: host=algo-1, epoch=47, train loss <loss>=9.034372199665416\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:03 INFO 140374100940160] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:03 INFO 140374100940160] Saved checkpoint to \"/opt/ml/model/state_fb7038ff-ee12-4931-b3ce-a1b247f81eef-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459163.172522, \"EndTime\": 1631459163.1857126, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 12.688159942626953, \"count\": 1, \"min\": 12.688159942626953, \"max\": 12.688159942626953}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:03 INFO 140374100940160] Epoch[48] Batch[0] avg_epoch_loss=10.043139\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:03 INFO 140374100940160] #quality_metric: host=algo-1, epoch=48, batch=0 train loss <loss>=10.04313850402832\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:03 INFO 140374100940160] Epoch[48] Batch[5] avg_epoch_loss=9.822919\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:03 INFO 140374100940160] #quality_metric: host=algo-1, epoch=48, batch=5 train loss <loss>=9.822918574015299\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:03 INFO 140374100940160] Epoch[48] Batch [5]#011Speed: 2162.44 samples/sec#011loss=9.822919\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:03 INFO 140374100940160] Epoch[48] Batch[10] avg_epoch_loss=10.179743\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:03 INFO 140374100940160] #quality_metric: host=algo-1, epoch=48, batch=10 train loss <loss>=10.607933235168456\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:03 INFO 140374100940160] Epoch[48] Batch [10]#011Speed: 1950.62 samples/sec#011loss=10.607933\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:03 INFO 140374100940160] processed a total of 681 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459163.185782, \"EndTime\": 1631459163.733461, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 547.6148128509521, \"count\": 1, \"min\": 547.6148128509521, \"max\": 547.6148128509521}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:03 INFO 140374100940160] #throughput_metric: host=algo-1, train throughput=1243.2987695076072 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:03 INFO 140374100940160] #progress_metric: host=algo-1, completed 24.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:03 INFO 140374100940160] #quality_metric: host=algo-1, epoch=48, train loss <loss>=10.179743419994008\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:03 INFO 140374100940160] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:03 INFO 140374100940160] Epoch[49] Batch[0] avg_epoch_loss=9.426348\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:03 INFO 140374100940160] #quality_metric: host=algo-1, epoch=49, batch=0 train loss <loss>=9.426347732543945\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:04 INFO 140374100940160] Epoch[49] Batch[5] avg_epoch_loss=10.048268\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:04 INFO 140374100940160] #quality_metric: host=algo-1, epoch=49, batch=5 train loss <loss>=10.048268477121988\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:04 INFO 140374100940160] Epoch[49] Batch [5]#011Speed: 1911.98 samples/sec#011loss=10.048268\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:04 INFO 140374100940160] processed a total of 621 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459163.7335463, \"EndTime\": 1631459164.2975335, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 563.4603500366211, \"count\": 1, \"min\": 563.4603500366211, \"max\": 563.4603500366211}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:04 INFO 140374100940160] #throughput_metric: host=algo-1, train throughput=1101.8899050432901 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:04 INFO 140374100940160] #progress_metric: host=algo-1, completed 25.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:04 INFO 140374100940160] #quality_metric: host=algo-1, epoch=49, train loss <loss>=10.272284889221192\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:04 INFO 140374100940160] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:04 INFO 140374100940160] Epoch[50] Batch[0] avg_epoch_loss=11.100423\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:04 INFO 140374100940160] #quality_metric: host=algo-1, epoch=50, batch=0 train loss <loss>=11.100422859191895\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:04 INFO 140374100940160] Epoch[50] Batch[5] avg_epoch_loss=10.564736\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:04 INFO 140374100940160] #quality_metric: host=algo-1, epoch=50, batch=5 train loss <loss>=10.564735730489096\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:04 INFO 140374100940160] Epoch[50] Batch [5]#011Speed: 1701.86 samples/sec#011loss=10.564736\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:04 INFO 140374100940160] processed a total of 605 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459164.2976086, \"EndTime\": 1631459164.8933086, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 595.0863361358643, \"count\": 1, \"min\": 595.0863361358643, \"max\": 595.0863361358643}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:04 INFO 140374100940160] #throughput_metric: host=algo-1, train throughput=1016.3981776918658 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:04 INFO 140374100940160] #progress_metric: host=algo-1, completed 25.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:04 INFO 140374100940160] #quality_metric: host=algo-1, epoch=50, train loss <loss>=10.059276580810547\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:04 INFO 140374100940160] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:05 INFO 140374100940160] Epoch[51] Batch[0] avg_epoch_loss=9.642342\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:05 INFO 140374100940160] #quality_metric: host=algo-1, epoch=51, batch=0 train loss <loss>=9.642341613769531\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:05 INFO 140374100940160] Epoch[51] Batch[5] avg_epoch_loss=10.248011\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:05 INFO 140374100940160] #quality_metric: host=algo-1, epoch=51, batch=5 train loss <loss>=10.248010635375977\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:05 INFO 140374100940160] Epoch[51] Batch [5]#011Speed: 1736.59 samples/sec#011loss=10.248011\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:05 INFO 140374100940160] Epoch[51] Batch[10] avg_epoch_loss=10.263760\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:05 INFO 140374100940160] #quality_metric: host=algo-1, epoch=51, batch=10 train loss <loss>=10.28265838623047\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:05 INFO 140374100940160] Epoch[51] Batch [10]#011Speed: 1565.81 samples/sec#011loss=10.282658\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:05 INFO 140374100940160] processed a total of 656 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459164.8934, \"EndTime\": 1631459165.5395353, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 645.6174850463867, \"count\": 1, \"min\": 645.6174850463867, \"max\": 645.6174850463867}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:05 INFO 140374100940160] #throughput_metric: host=algo-1, train throughput=1015.8725687869143 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:05 INFO 140374100940160] #progress_metric: host=algo-1, completed 26.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:05 INFO 140374100940160] #quality_metric: host=algo-1, epoch=51, train loss <loss>=10.26375961303711\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:05 INFO 140374100940160] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:05 INFO 140374100940160] Epoch[52] Batch[0] avg_epoch_loss=8.709549\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:05 INFO 140374100940160] #quality_metric: host=algo-1, epoch=52, batch=0 train loss <loss>=8.709548950195312\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:06 INFO 140374100940160] Epoch[52] Batch[5] avg_epoch_loss=9.623806\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:06 INFO 140374100940160] #quality_metric: host=algo-1, epoch=52, batch=5 train loss <loss>=9.623806317647299\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:06 INFO 140374100940160] Epoch[52] Batch [5]#011Speed: 1553.26 samples/sec#011loss=9.623806\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:06 INFO 140374100940160] Epoch[52] Batch[10] avg_epoch_loss=9.998454\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:06 INFO 140374100940160] #quality_metric: host=algo-1, epoch=52, batch=10 train loss <loss>=10.448032188415528\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:06 INFO 140374100940160] Epoch[52] Batch [10]#011Speed: 1487.14 samples/sec#011loss=10.448032\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:06 INFO 140374100940160] processed a total of 660 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459165.5396252, \"EndTime\": 1631459166.244112, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 703.9284706115723, \"count\": 1, \"min\": 703.9284706115723, \"max\": 703.9284706115723}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:06 INFO 140374100940160] #throughput_metric: host=algo-1, train throughput=937.4492084187 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:06 INFO 140374100940160] #progress_metric: host=algo-1, completed 26.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:06 INFO 140374100940160] #quality_metric: host=algo-1, epoch=52, train loss <loss>=9.998454440723766\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:06 INFO 140374100940160] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:06 INFO 140374100940160] Epoch[53] Batch[0] avg_epoch_loss=9.879906\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:06 INFO 140374100940160] #quality_metric: host=algo-1, epoch=53, batch=0 train loss <loss>=9.879905700683594\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:06 INFO 140374100940160] Epoch[53] Batch[5] avg_epoch_loss=9.984289\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:06 INFO 140374100940160] #quality_metric: host=algo-1, epoch=53, batch=5 train loss <loss>=9.984288533528646\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:06 INFO 140374100940160] Epoch[53] Batch [5]#011Speed: 1561.05 samples/sec#011loss=9.984289\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:06 INFO 140374100940160] processed a total of 640 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459166.244185, \"EndTime\": 1631459166.9356058, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 690.983772277832, \"count\": 1, \"min\": 690.983772277832, \"max\": 690.983772277832}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:06 INFO 140374100940160] #throughput_metric: host=algo-1, train throughput=926.0670909775238 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:06 INFO 140374100940160] #progress_metric: host=algo-1, completed 27.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:06 INFO 140374100940160] #quality_metric: host=algo-1, epoch=53, train loss <loss>=9.986470317840576\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:06 INFO 140374100940160] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:07 INFO 140374100940160] Epoch[54] Batch[0] avg_epoch_loss=10.650781\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:07 INFO 140374100940160] #quality_metric: host=algo-1, epoch=54, batch=0 train loss <loss>=10.65078067779541\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:07 INFO 140374100940160] Epoch[54] Batch[5] avg_epoch_loss=9.853809\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:07 INFO 140374100940160] #quality_metric: host=algo-1, epoch=54, batch=5 train loss <loss>=9.853809356689453\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:07 INFO 140374100940160] Epoch[54] Batch [5]#011Speed: 1954.63 samples/sec#011loss=9.853809\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:07 INFO 140374100940160] processed a total of 618 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459166.9356782, \"EndTime\": 1631459167.4795623, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 543.3835983276367, \"count\": 1, \"min\": 543.3835983276367, \"max\": 543.3835983276367}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:07 INFO 140374100940160] #throughput_metric: host=algo-1, train throughput=1137.0576696826427 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:07 INFO 140374100940160] #progress_metric: host=algo-1, completed 27.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:07 INFO 140374100940160] #quality_metric: host=algo-1, epoch=54, train loss <loss>=9.774107074737548\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:07 INFO 140374100940160] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:07 INFO 140374100940160] Epoch[55] Batch[0] avg_epoch_loss=9.801609\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:07 INFO 140374100940160] #quality_metric: host=algo-1, epoch=55, batch=0 train loss <loss>=9.80160903930664\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:07 INFO 140374100940160] Epoch[55] Batch[5] avg_epoch_loss=9.657429\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:07 INFO 140374100940160] #quality_metric: host=algo-1, epoch=55, batch=5 train loss <loss>=9.657428900400797\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:07 INFO 140374100940160] Epoch[55] Batch [5]#011Speed: 2159.63 samples/sec#011loss=9.657429\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:08 INFO 140374100940160] processed a total of 613 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459167.4796517, \"EndTime\": 1631459168.0134425, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 533.2217216491699, \"count\": 1, \"min\": 533.2217216491699, \"max\": 533.2217216491699}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:08 INFO 140374100940160] #throughput_metric: host=algo-1, train throughput=1149.329341172821 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:08 INFO 140374100940160] #progress_metric: host=algo-1, completed 28.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:08 INFO 140374100940160] #quality_metric: host=algo-1, epoch=55, train loss <loss>=9.591488456726074\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:08 INFO 140374100940160] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:08 INFO 140374100940160] Epoch[56] Batch[0] avg_epoch_loss=10.116004\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:08 INFO 140374100940160] #quality_metric: host=algo-1, epoch=56, batch=0 train loss <loss>=10.11600399017334\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:08 INFO 140374100940160] Epoch[56] Batch[5] avg_epoch_loss=10.060155\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:08 INFO 140374100940160] #quality_metric: host=algo-1, epoch=56, batch=5 train loss <loss>=10.060155391693115\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:08 INFO 140374100940160] Epoch[56] Batch [5]#011Speed: 1895.55 samples/sec#011loss=10.060155\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:08 INFO 140374100940160] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459168.0135326, \"EndTime\": 1631459168.5598443, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 545.778751373291, \"count\": 1, \"min\": 545.778751373291, \"max\": 545.778751373291}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:08 INFO 140374100940160] #throughput_metric: host=algo-1, train throughput=1166.900538995148 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:08 INFO 140374100940160] #progress_metric: host=algo-1, completed 28.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:08 INFO 140374100940160] #quality_metric: host=algo-1, epoch=56, train loss <loss>=10.209857177734374\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:08 INFO 140374100940160] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:08 INFO 140374100940160] Epoch[57] Batch[0] avg_epoch_loss=9.961946\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:08 INFO 140374100940160] #quality_metric: host=algo-1, epoch=57, batch=0 train loss <loss>=9.961946487426758\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:08 INFO 140374100940160] Epoch[57] Batch[5] avg_epoch_loss=10.342868\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:08 INFO 140374100940160] #quality_metric: host=algo-1, epoch=57, batch=5 train loss <loss>=10.342868010203043\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:08 INFO 140374100940160] Epoch[57] Batch [5]#011Speed: 2165.33 samples/sec#011loss=10.342868\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:09 INFO 140374100940160] processed a total of 618 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459168.5599232, \"EndTime\": 1631459169.0681517, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 507.6727867126465, \"count\": 1, \"min\": 507.6727867126465, \"max\": 507.6727867126465}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:09 INFO 140374100940160] #throughput_metric: host=algo-1, train throughput=1217.0160418618082 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:09 INFO 140374100940160] #progress_metric: host=algo-1, completed 29.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:09 INFO 140374100940160] #quality_metric: host=algo-1, epoch=57, train loss <loss>=9.85072979927063\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:09 INFO 140374100940160] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:09 INFO 140374100940160] Epoch[58] Batch[0] avg_epoch_loss=9.790997\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:09 INFO 140374100940160] #quality_metric: host=algo-1, epoch=58, batch=0 train loss <loss>=9.790996551513672\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:09 INFO 140374100940160] Epoch[58] Batch[5] avg_epoch_loss=10.405488\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:09 INFO 140374100940160] #quality_metric: host=algo-1, epoch=58, batch=5 train loss <loss>=10.40548849105835\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:09 INFO 140374100940160] Epoch[58] Batch [5]#011Speed: 1633.51 samples/sec#011loss=10.405488\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:09 INFO 140374100940160] processed a total of 638 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459169.068243, \"EndTime\": 1631459169.647944, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 579.1535377502441, \"count\": 1, \"min\": 579.1535377502441, \"max\": 579.1535377502441}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:09 INFO 140374100940160] #throughput_metric: host=algo-1, train throughput=1101.3696718609228 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:09 INFO 140374100940160] #progress_metric: host=algo-1, completed 29.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:09 INFO 140374100940160] #quality_metric: host=algo-1, epoch=58, train loss <loss>=10.28662052154541\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:09 INFO 140374100940160] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:09 INFO 140374100940160] Epoch[59] Batch[0] avg_epoch_loss=10.280340\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:09 INFO 140374100940160] #quality_metric: host=algo-1, epoch=59, batch=0 train loss <loss>=10.280340194702148\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:10 INFO 140374100940160] Epoch[59] Batch[5] avg_epoch_loss=10.017666\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:10 INFO 140374100940160] #quality_metric: host=algo-1, epoch=59, batch=5 train loss <loss>=10.01766554514567\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:10 INFO 140374100940160] Epoch[59] Batch [5]#011Speed: 2151.74 samples/sec#011loss=10.017666\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:10 INFO 140374100940160] processed a total of 594 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459169.6480293, \"EndTime\": 1631459170.1674736, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 518.9254283905029, \"count\": 1, \"min\": 518.9254283905029, \"max\": 518.9254283905029}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:10 INFO 140374100940160] #throughput_metric: host=algo-1, train throughput=1144.396050082267 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:10 INFO 140374100940160] #progress_metric: host=algo-1, completed 30.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:10 INFO 140374100940160] #quality_metric: host=algo-1, epoch=59, train loss <loss>=10.540978050231933\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:10 INFO 140374100940160] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:10 INFO 140374100940160] Epoch[60] Batch[0] avg_epoch_loss=9.626917\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:10 INFO 140374100940160] #quality_metric: host=algo-1, epoch=60, batch=0 train loss <loss>=9.626916885375977\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:10 INFO 140374100940160] Epoch[60] Batch[5] avg_epoch_loss=10.111447\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:10 INFO 140374100940160] #quality_metric: host=algo-1, epoch=60, batch=5 train loss <loss>=10.11144733428955\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:10 INFO 140374100940160] Epoch[60] Batch [5]#011Speed: 2010.91 samples/sec#011loss=10.111447\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:10 INFO 140374100940160] processed a total of 625 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459170.16756, \"EndTime\": 1631459170.7287421, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 560.6799125671387, \"count\": 1, \"min\": 560.6799125671387, \"max\": 560.6799125671387}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:10 INFO 140374100940160] #throughput_metric: host=algo-1, train throughput=1114.448368378786 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:10 INFO 140374100940160] #progress_metric: host=algo-1, completed 30.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:10 INFO 140374100940160] #quality_metric: host=algo-1, epoch=60, train loss <loss>=10.304143905639648\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:10 INFO 140374100940160] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:11 INFO 140374100940160] Epoch[61] Batch[0] avg_epoch_loss=9.762789\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:11 INFO 140374100940160] #quality_metric: host=algo-1, epoch=61, batch=0 train loss <loss>=9.762788772583008\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:11 INFO 140374100940160] Epoch[61] Batch[5] avg_epoch_loss=9.642181\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:11 INFO 140374100940160] #quality_metric: host=algo-1, epoch=61, batch=5 train loss <loss>=9.642180760701498\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:11 INFO 140374100940160] Epoch[61] Batch [5]#011Speed: 2044.21 samples/sec#011loss=9.642181\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:11 INFO 140374100940160] processed a total of 629 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459170.7288396, \"EndTime\": 1631459171.3207757, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 591.4077758789062, \"count\": 1, \"min\": 591.4077758789062, \"max\": 591.4077758789062}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:11 INFO 140374100940160] #throughput_metric: host=algo-1, train throughput=1063.3221995979216 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:11 INFO 140374100940160] #progress_metric: host=algo-1, completed 31.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:11 INFO 140374100940160] #quality_metric: host=algo-1, epoch=61, train loss <loss>=9.785400009155273\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:11 INFO 140374100940160] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:11 INFO 140374100940160] Epoch[62] Batch[0] avg_epoch_loss=10.470248\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:11 INFO 140374100940160] #quality_metric: host=algo-1, epoch=62, batch=0 train loss <loss>=10.470248222351074\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:11 INFO 140374100940160] Epoch[62] Batch[5] avg_epoch_loss=9.979641\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:11 INFO 140374100940160] #quality_metric: host=algo-1, epoch=62, batch=5 train loss <loss>=9.979641278584799\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:11 INFO 140374100940160] Epoch[62] Batch [5]#011Speed: 2139.15 samples/sec#011loss=9.979641\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:11 INFO 140374100940160] Epoch[62] Batch[10] avg_epoch_loss=10.064549\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:11 INFO 140374100940160] #quality_metric: host=algo-1, epoch=62, batch=10 train loss <loss>=10.166438484191895\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:11 INFO 140374100940160] Epoch[62] Batch [10]#011Speed: 1902.02 samples/sec#011loss=10.166438\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:11 INFO 140374100940160] processed a total of 658 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459171.3208723, \"EndTime\": 1631459171.9233172, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 601.9201278686523, \"count\": 1, \"min\": 601.9201278686523, \"max\": 601.9201278686523}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:11 INFO 140374100940160] #throughput_metric: host=algo-1, train throughput=1092.9423156887608 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:11 INFO 140374100940160] #progress_metric: host=algo-1, completed 31.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:11 INFO 140374100940160] #quality_metric: host=algo-1, epoch=62, train loss <loss>=10.064549099315297\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:11 INFO 140374100940160] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:12 INFO 140374100940160] Epoch[63] Batch[0] avg_epoch_loss=9.797945\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:12 INFO 140374100940160] #quality_metric: host=algo-1, epoch=63, batch=0 train loss <loss>=9.797945022583008\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:12 INFO 140374100940160] Epoch[63] Batch[5] avg_epoch_loss=9.820798\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:12 INFO 140374100940160] #quality_metric: host=algo-1, epoch=63, batch=5 train loss <loss>=9.820798397064209\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:12 INFO 140374100940160] Epoch[63] Batch [5]#011Speed: 1992.83 samples/sec#011loss=9.820798\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:12 INFO 140374100940160] Epoch[63] Batch[10] avg_epoch_loss=9.945509\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:12 INFO 140374100940160] #quality_metric: host=algo-1, epoch=63, batch=10 train loss <loss>=10.095161437988281\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:12 INFO 140374100940160] Epoch[63] Batch [10]#011Speed: 1816.84 samples/sec#011loss=10.095161\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:12 INFO 140374100940160] processed a total of 677 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459171.9234054, \"EndTime\": 1631459172.5251944, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 601.3176441192627, \"count\": 1, \"min\": 601.3176441192627, \"max\": 601.3176441192627}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:12 INFO 140374100940160] #throughput_metric: host=algo-1, train throughput=1125.5930858454146 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:12 INFO 140374100940160] #progress_metric: host=algo-1, completed 32.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:12 INFO 140374100940160] #quality_metric: host=algo-1, epoch=63, train loss <loss>=9.945508870211514\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:12 INFO 140374100940160] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:12 INFO 140374100940160] Epoch[64] Batch[0] avg_epoch_loss=9.709129\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:12 INFO 140374100940160] #quality_metric: host=algo-1, epoch=64, batch=0 train loss <loss>=9.709129333496094\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:12 INFO 140374100940160] Epoch[64] Batch[5] avg_epoch_loss=9.685596\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:12 INFO 140374100940160] #quality_metric: host=algo-1, epoch=64, batch=5 train loss <loss>=9.685596307118734\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:12 INFO 140374100940160] Epoch[64] Batch [5]#011Speed: 2165.55 samples/sec#011loss=9.685596\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:13 INFO 140374100940160] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459172.52528, \"EndTime\": 1631459173.0528677, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 527.0631313323975, \"count\": 1, \"min\": 527.0631313323975, \"max\": 527.0631313323975}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:13 INFO 140374100940160] #throughput_metric: host=algo-1, train throughput=1202.6085282816807 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:13 INFO 140374100940160] #progress_metric: host=algo-1, completed 32.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:13 INFO 140374100940160] #quality_metric: host=algo-1, epoch=64, train loss <loss>=9.682740211486816\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:13 INFO 140374100940160] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:13 INFO 140374100940160] Epoch[65] Batch[0] avg_epoch_loss=9.217613\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:13 INFO 140374100940160] #quality_metric: host=algo-1, epoch=65, batch=0 train loss <loss>=9.217613220214844\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:13 INFO 140374100940160] Epoch[65] Batch[5] avg_epoch_loss=10.024702\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:13 INFO 140374100940160] #quality_metric: host=algo-1, epoch=65, batch=5 train loss <loss>=10.024702390034994\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:13 INFO 140374100940160] Epoch[65] Batch [5]#011Speed: 1788.38 samples/sec#011loss=10.024702\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:13 INFO 140374100940160] processed a total of 621 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459173.0529535, \"EndTime\": 1631459173.6471221, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 593.6367511749268, \"count\": 1, \"min\": 593.6367511749268, \"max\": 593.6367511749268}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:13 INFO 140374100940160] #throughput_metric: host=algo-1, train throughput=1045.867019162122 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:13 INFO 140374100940160] #progress_metric: host=algo-1, completed 33.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:13 INFO 140374100940160] #quality_metric: host=algo-1, epoch=65, train loss <loss>=10.374413776397706\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:13 INFO 140374100940160] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:13 INFO 140374100940160] Epoch[66] Batch[0] avg_epoch_loss=10.344555\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:13 INFO 140374100940160] #quality_metric: host=algo-1, epoch=66, batch=0 train loss <loss>=10.344554901123047\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:14 INFO 140374100940160] Epoch[66] Batch[5] avg_epoch_loss=10.104613\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:14 INFO 140374100940160] #quality_metric: host=algo-1, epoch=66, batch=5 train loss <loss>=10.104613304138184\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:14 INFO 140374100940160] Epoch[66] Batch [5]#011Speed: 2178.85 samples/sec#011loss=10.104613\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:14 INFO 140374100940160] Epoch[66] Batch[10] avg_epoch_loss=10.353184\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:14 INFO 140374100940160] #quality_metric: host=algo-1, epoch=66, batch=10 train loss <loss>=10.651469039916993\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:14 INFO 140374100940160] Epoch[66] Batch [10]#011Speed: 1862.66 samples/sec#011loss=10.651469\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:14 INFO 140374100940160] processed a total of 656 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459173.647213, \"EndTime\": 1631459174.2471662, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 599.4429588317871, \"count\": 1, \"min\": 599.4429588317871, \"max\": 599.4429588317871}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:14 INFO 140374100940160] #throughput_metric: host=algo-1, train throughput=1094.1326131009757 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:14 INFO 140374100940160] #progress_metric: host=algo-1, completed 33.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:14 INFO 140374100940160] #quality_metric: host=algo-1, epoch=66, train loss <loss>=10.35318409312855\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:14 INFO 140374100940160] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:14 INFO 140374100940160] Epoch[67] Batch[0] avg_epoch_loss=9.666780\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:14 INFO 140374100940160] #quality_metric: host=algo-1, epoch=67, batch=0 train loss <loss>=9.666779518127441\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:14 INFO 140374100940160] Epoch[67] Batch[5] avg_epoch_loss=10.096874\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:14 INFO 140374100940160] #quality_metric: host=algo-1, epoch=67, batch=5 train loss <loss>=10.09687360127767\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:14 INFO 140374100940160] Epoch[67] Batch [5]#011Speed: 1855.43 samples/sec#011loss=10.096874\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:14 INFO 140374100940160] processed a total of 636 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459174.247249, \"EndTime\": 1631459174.7991357, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 551.4090061187744, \"count\": 1, \"min\": 551.4090061187744, \"max\": 551.4090061187744}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:14 INFO 140374100940160] #throughput_metric: host=algo-1, train throughput=1153.1510236053005 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:14 INFO 140374100940160] #progress_metric: host=algo-1, completed 34.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:14 INFO 140374100940160] #quality_metric: host=algo-1, epoch=67, train loss <loss>=9.93536148071289\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:14 INFO 140374100940160] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:15 INFO 140374100940160] Epoch[68] Batch[0] avg_epoch_loss=9.705398\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:15 INFO 140374100940160] #quality_metric: host=algo-1, epoch=68, batch=0 train loss <loss>=9.705397605895996\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:15 INFO 140374100940160] Epoch[68] Batch[5] avg_epoch_loss=9.948615\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:15 INFO 140374100940160] #quality_metric: host=algo-1, epoch=68, batch=5 train loss <loss>=9.948615392049154\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:15 INFO 140374100940160] Epoch[68] Batch [5]#011Speed: 2071.98 samples/sec#011loss=9.948615\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:15 INFO 140374100940160] Epoch[68] Batch[10] avg_epoch_loss=9.924714\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:15 INFO 140374100940160] #quality_metric: host=algo-1, epoch=68, batch=10 train loss <loss>=9.896032333374023\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:15 INFO 140374100940160] Epoch[68] Batch [10]#011Speed: 1754.07 samples/sec#011loss=9.896032\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:15 INFO 140374100940160] processed a total of 647 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459174.799222, \"EndTime\": 1631459175.3862355, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 586.5304470062256, \"count\": 1, \"min\": 586.5304470062256, \"max\": 586.5304470062256}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:15 INFO 140374100940160] #throughput_metric: host=algo-1, train throughput=1102.8679564886259 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:15 INFO 140374100940160] #progress_metric: host=algo-1, completed 34.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:15 INFO 140374100940160] #quality_metric: host=algo-1, epoch=68, train loss <loss>=9.924714001742275\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:15 INFO 140374100940160] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:15 INFO 140374100940160] Epoch[69] Batch[0] avg_epoch_loss=10.034850\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:15 INFO 140374100940160] #quality_metric: host=algo-1, epoch=69, batch=0 train loss <loss>=10.034850120544434\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:15 INFO 140374100940160] Epoch[69] Batch[5] avg_epoch_loss=9.972578\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:15 INFO 140374100940160] #quality_metric: host=algo-1, epoch=69, batch=5 train loss <loss>=9.972577571868896\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:15 INFO 140374100940160] Epoch[69] Batch [5]#011Speed: 2167.42 samples/sec#011loss=9.972578\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:15 INFO 140374100940160] Epoch[69] Batch[10] avg_epoch_loss=9.685341\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:15 INFO 140374100940160] #quality_metric: host=algo-1, epoch=69, batch=10 train loss <loss>=9.340658187866211\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:15 INFO 140374100940160] Epoch[69] Batch [10]#011Speed: 1927.56 samples/sec#011loss=9.340658\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:15 INFO 140374100940160] processed a total of 655 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459175.3863213, \"EndTime\": 1631459175.9906075, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 603.811502456665, \"count\": 1, \"min\": 603.811502456665, \"max\": 603.811502456665}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:15 INFO 140374100940160] #throughput_metric: host=algo-1, train throughput=1084.564499408226 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:15 INFO 140374100940160] #progress_metric: host=algo-1, completed 35.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:15 INFO 140374100940160] #quality_metric: host=algo-1, epoch=69, train loss <loss>=9.685341488231312\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:15 INFO 140374100940160] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:16 INFO 140374100940160] Epoch[70] Batch[0] avg_epoch_loss=10.185713\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:16 INFO 140374100940160] #quality_metric: host=algo-1, epoch=70, batch=0 train loss <loss>=10.185712814331055\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:16 INFO 140374100940160] Epoch[70] Batch[5] avg_epoch_loss=10.036390\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:16 INFO 140374100940160] #quality_metric: host=algo-1, epoch=70, batch=5 train loss <loss>=10.03639030456543\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:16 INFO 140374100940160] Epoch[70] Batch [5]#011Speed: 2098.78 samples/sec#011loss=10.036390\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:16 INFO 140374100940160] Epoch[70] Batch[10] avg_epoch_loss=10.358374\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:16 INFO 140374100940160] #quality_metric: host=algo-1, epoch=70, batch=10 train loss <loss>=10.744753646850587\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:16 INFO 140374100940160] Epoch[70] Batch [10]#011Speed: 1722.68 samples/sec#011loss=10.744754\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:16 INFO 140374100940160] processed a total of 673 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459175.990689, \"EndTime\": 1631459176.626515, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 635.361909866333, \"count\": 1, \"min\": 635.361909866333, \"max\": 635.361909866333}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:16 INFO 140374100940160] #throughput_metric: host=algo-1, train throughput=1059.042105563301 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:16 INFO 140374100940160] #progress_metric: host=algo-1, completed 35.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:16 INFO 140374100940160] #quality_metric: host=algo-1, epoch=70, train loss <loss>=10.358373641967773\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:16 INFO 140374100940160] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:16 INFO 140374100940160] Epoch[71] Batch[0] avg_epoch_loss=9.648983\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:16 INFO 140374100940160] #quality_metric: host=algo-1, epoch=71, batch=0 train loss <loss>=9.648983001708984\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:17 INFO 140374100940160] Epoch[71] Batch[5] avg_epoch_loss=9.654065\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:17 INFO 140374100940160] #quality_metric: host=algo-1, epoch=71, batch=5 train loss <loss>=9.654064814249674\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:17 INFO 140374100940160] Epoch[71] Batch [5]#011Speed: 1861.52 samples/sec#011loss=9.654065\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:17 INFO 140374100940160] Epoch[71] Batch[10] avg_epoch_loss=9.772130\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:17 INFO 140374100940160] #quality_metric: host=algo-1, epoch=71, batch=10 train loss <loss>=9.913808250427246\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:17 INFO 140374100940160] Epoch[71] Batch [10]#011Speed: 1822.49 samples/sec#011loss=9.913808\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:17 INFO 140374100940160] processed a total of 653 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459176.6265967, \"EndTime\": 1631459177.2635903, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 636.5218162536621, \"count\": 1, \"min\": 636.5218162536621, \"max\": 636.5218162536621}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:17 INFO 140374100940160] #throughput_metric: host=algo-1, train throughput=1025.6976797399207 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:17 INFO 140374100940160] #progress_metric: host=algo-1, completed 36.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:17 INFO 140374100940160] #quality_metric: host=algo-1, epoch=71, train loss <loss>=9.772130012512207\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:17 INFO 140374100940160] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:17 INFO 140374100940160] Epoch[72] Batch[0] avg_epoch_loss=9.274571\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:17 INFO 140374100940160] #quality_metric: host=algo-1, epoch=72, batch=0 train loss <loss>=9.274571418762207\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:17 INFO 140374100940160] Epoch[72] Batch[5] avg_epoch_loss=9.833201\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:17 INFO 140374100940160] #quality_metric: host=algo-1, epoch=72, batch=5 train loss <loss>=9.833200931549072\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:17 INFO 140374100940160] Epoch[72] Batch [5]#011Speed: 2031.23 samples/sec#011loss=9.833201\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:17 INFO 140374100940160] Epoch[72] Batch[10] avg_epoch_loss=9.749430\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:17 INFO 140374100940160] #quality_metric: host=algo-1, epoch=72, batch=10 train loss <loss>=9.648905563354493\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:17 INFO 140374100940160] Epoch[72] Batch [10]#011Speed: 1874.92 samples/sec#011loss=9.648906\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:17 INFO 140374100940160] processed a total of 676 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459177.2636719, \"EndTime\": 1631459177.8577464, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 593.6095714569092, \"count\": 1, \"min\": 593.6095714569092, \"max\": 593.6095714569092}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:17 INFO 140374100940160] #throughput_metric: host=algo-1, train throughput=1138.5724920259554 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:17 INFO 140374100940160] #progress_metric: host=algo-1, completed 36.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:17 INFO 140374100940160] #quality_metric: host=algo-1, epoch=72, train loss <loss>=9.749430309642445\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:17 INFO 140374100940160] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:18 INFO 140374100940160] Epoch[73] Batch[0] avg_epoch_loss=9.051925\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:18 INFO 140374100940160] #quality_metric: host=algo-1, epoch=73, batch=0 train loss <loss>=9.051924705505371\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:18 INFO 140374100940160] Epoch[73] Batch[5] avg_epoch_loss=9.640517\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:18 INFO 140374100940160] #quality_metric: host=algo-1, epoch=73, batch=5 train loss <loss>=9.640516599019369\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:18 INFO 140374100940160] Epoch[73] Batch [5]#011Speed: 1860.86 samples/sec#011loss=9.640517\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:18 INFO 140374100940160] Epoch[73] Batch[10] avg_epoch_loss=9.793848\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:18 INFO 140374100940160] #quality_metric: host=algo-1, epoch=73, batch=10 train loss <loss>=9.977845573425293\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:18 INFO 140374100940160] Epoch[73] Batch [10]#011Speed: 1954.75 samples/sec#011loss=9.977846\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:18 INFO 140374100940160] processed a total of 665 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459177.857824, \"EndTime\": 1631459178.443048, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 584.7206115722656, \"count\": 1, \"min\": 584.7206115722656, \"max\": 584.7206115722656}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:18 INFO 140374100940160] #throughput_metric: host=algo-1, train throughput=1137.0653308286371 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:18 INFO 140374100940160] #progress_metric: host=algo-1, completed 37.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:18 INFO 140374100940160] #quality_metric: host=algo-1, epoch=73, train loss <loss>=9.79384795102206\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:18 INFO 140374100940160] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:18 INFO 140374100940160] Epoch[74] Batch[0] avg_epoch_loss=10.681407\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:18 INFO 140374100940160] #quality_metric: host=algo-1, epoch=74, batch=0 train loss <loss>=10.68140697479248\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:18 INFO 140374100940160] Epoch[74] Batch[5] avg_epoch_loss=9.835126\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:18 INFO 140374100940160] #quality_metric: host=algo-1, epoch=74, batch=5 train loss <loss>=9.83512576421102\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:18 INFO 140374100940160] Epoch[74] Batch [5]#011Speed: 2171.84 samples/sec#011loss=9.835126\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:19 INFO 140374100940160] Epoch[74] Batch[10] avg_epoch_loss=9.118668\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:19 INFO 140374100940160] #quality_metric: host=algo-1, epoch=74, batch=10 train loss <loss>=8.258919286727906\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:19 INFO 140374100940160] Epoch[74] Batch [10]#011Speed: 2087.33 samples/sec#011loss=8.258919\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:19 INFO 140374100940160] processed a total of 645 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459178.443131, \"EndTime\": 1631459179.0183716, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 574.7621059417725, \"count\": 1, \"min\": 574.7621059417725, \"max\": 574.7621059417725}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:19 INFO 140374100940160] #throughput_metric: host=algo-1, train throughput=1121.9655960962791 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:19 INFO 140374100940160] #progress_metric: host=algo-1, completed 37.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:19 INFO 140374100940160] #quality_metric: host=algo-1, epoch=74, train loss <loss>=9.118668274445968\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:19 INFO 140374100940160] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:19 INFO 140374100940160] Epoch[75] Batch[0] avg_epoch_loss=10.219114\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:19 INFO 140374100940160] #quality_metric: host=algo-1, epoch=75, batch=0 train loss <loss>=10.219114303588867\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:19 INFO 140374100940160] Epoch[75] Batch[5] avg_epoch_loss=10.243311\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:19 INFO 140374100940160] #quality_metric: host=algo-1, epoch=75, batch=5 train loss <loss>=10.243310769399008\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:19 INFO 140374100940160] Epoch[75] Batch [5]#011Speed: 1733.81 samples/sec#011loss=10.243311\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:19 INFO 140374100940160] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459179.0184553, \"EndTime\": 1631459179.6014183, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 582.470178604126, \"count\": 1, \"min\": 582.470178604126, \"max\": 582.470178604126}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:19 INFO 140374100940160] #throughput_metric: host=algo-1, train throughput=1093.378783364817 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:19 INFO 140374100940160] #progress_metric: host=algo-1, completed 38.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:19 INFO 140374100940160] #quality_metric: host=algo-1, epoch=75, train loss <loss>=10.132392501831054\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:19 INFO 140374100940160] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:19 INFO 140374100940160] Epoch[76] Batch[0] avg_epoch_loss=9.270669\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:19 INFO 140374100940160] #quality_metric: host=algo-1, epoch=76, batch=0 train loss <loss>=9.270668983459473\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:19 INFO 140374100940160] Epoch[76] Batch[5] avg_epoch_loss=9.391495\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:19 INFO 140374100940160] #quality_metric: host=algo-1, epoch=76, batch=5 train loss <loss>=9.39149522781372\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:19 INFO 140374100940160] Epoch[76] Batch [5]#011Speed: 2171.50 samples/sec#011loss=9.391495\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:20 INFO 140374100940160] Epoch[76] Batch[10] avg_epoch_loss=9.614901\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:20 INFO 140374100940160] #quality_metric: host=algo-1, epoch=76, batch=10 train loss <loss>=9.882988739013673\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:20 INFO 140374100940160] Epoch[76] Batch [10]#011Speed: 1453.76 samples/sec#011loss=9.882989\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:20 INFO 140374100940160] processed a total of 671 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459179.6015065, \"EndTime\": 1631459180.2199695, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 617.9540157318115, \"count\": 1, \"min\": 617.9540157318115, \"max\": 617.9540157318115}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:20 INFO 140374100940160] #throughput_metric: host=algo-1, train throughput=1085.6243465045006 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:20 INFO 140374100940160] #progress_metric: host=algo-1, completed 38.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:20 INFO 140374100940160] #quality_metric: host=algo-1, epoch=76, train loss <loss>=9.614901369268244\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:20 INFO 140374100940160] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:20 INFO 140374100940160] Epoch[77] Batch[0] avg_epoch_loss=9.683620\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:20 INFO 140374100940160] #quality_metric: host=algo-1, epoch=77, batch=0 train loss <loss>=9.68362045288086\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:20 INFO 140374100940160] Epoch[77] Batch[5] avg_epoch_loss=10.129072\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:20 INFO 140374100940160] #quality_metric: host=algo-1, epoch=77, batch=5 train loss <loss>=10.129071871439615\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:20 INFO 140374100940160] Epoch[77] Batch [5]#011Speed: 1715.95 samples/sec#011loss=10.129072\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:20 INFO 140374100940160] processed a total of 638 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459180.220055, \"EndTime\": 1631459180.8924577, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 671.8919277191162, \"count\": 1, \"min\": 671.8919277191162, \"max\": 671.8919277191162}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:20 INFO 140374100940160] #throughput_metric: host=algo-1, train throughput=949.390711888893 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:20 INFO 140374100940160] #progress_metric: host=algo-1, completed 39.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:20 INFO 140374100940160] #quality_metric: host=algo-1, epoch=77, train loss <loss>=10.003535747528076\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:20 INFO 140374100940160] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:21 INFO 140374100940160] Epoch[78] Batch[0] avg_epoch_loss=10.049747\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:21 INFO 140374100940160] #quality_metric: host=algo-1, epoch=78, batch=0 train loss <loss>=10.0497465133667\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:21 INFO 140374100940160] Epoch[78] Batch[5] avg_epoch_loss=9.983493\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:21 INFO 140374100940160] #quality_metric: host=algo-1, epoch=78, batch=5 train loss <loss>=9.983493328094482\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:21 INFO 140374100940160] Epoch[78] Batch [5]#011Speed: 1783.22 samples/sec#011loss=9.983493\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:21 INFO 140374100940160] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459180.8925343, \"EndTime\": 1631459181.4512691, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 558.2118034362793, \"count\": 1, \"min\": 558.2118034362793, \"max\": 558.2118034362793}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:21 INFO 140374100940160] #throughput_metric: host=algo-1, train throughput=1135.522743645004 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:21 INFO 140374100940160] #progress_metric: host=algo-1, completed 39.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:21 INFO 140374100940160] #quality_metric: host=algo-1, epoch=78, train loss <loss>=9.9392578125\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:21 INFO 140374100940160] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:21 INFO 140374100940160] Epoch[79] Batch[0] avg_epoch_loss=9.636804\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:21 INFO 140374100940160] #quality_metric: host=algo-1, epoch=79, batch=0 train loss <loss>=9.63680362701416\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:21 INFO 140374100940160] Epoch[79] Batch[5] avg_epoch_loss=10.105265\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:21 INFO 140374100940160] #quality_metric: host=algo-1, epoch=79, batch=5 train loss <loss>=10.10526450475057\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:21 INFO 140374100940160] Epoch[79] Batch [5]#011Speed: 2148.61 samples/sec#011loss=10.105265\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:21 INFO 140374100940160] processed a total of 606 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459181.4513495, \"EndTime\": 1631459181.9926338, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 540.7326221466064, \"count\": 1, \"min\": 540.7326221466064, \"max\": 540.7326221466064}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:21 INFO 140374100940160] #throughput_metric: host=algo-1, train throughput=1120.4335052500728 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:21 INFO 140374100940160] #progress_metric: host=algo-1, completed 40.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:21 INFO 140374100940160] #quality_metric: host=algo-1, epoch=79, train loss <loss>=10.018005180358887\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:21 INFO 140374100940160] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:22 INFO 140374100940160] Epoch[80] Batch[0] avg_epoch_loss=9.221955\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:22 INFO 140374100940160] #quality_metric: host=algo-1, epoch=80, batch=0 train loss <loss>=9.221955299377441\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:22 INFO 140374100940160] Epoch[80] Batch[5] avg_epoch_loss=9.878658\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:22 INFO 140374100940160] #quality_metric: host=algo-1, epoch=80, batch=5 train loss <loss>=9.878657817840576\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:22 INFO 140374100940160] Epoch[80] Batch [5]#011Speed: 2167.95 samples/sec#011loss=9.878658\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:22 INFO 140374100940160] Epoch[80] Batch[10] avg_epoch_loss=9.752834\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:22 INFO 140374100940160] #quality_metric: host=algo-1, epoch=80, batch=10 train loss <loss>=9.601846504211426\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:22 INFO 140374100940160] Epoch[80] Batch [10]#011Speed: 1857.61 samples/sec#011loss=9.601847\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:22 INFO 140374100940160] processed a total of 649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459181.9927225, \"EndTime\": 1631459182.5822382, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 589.0152454376221, \"count\": 1, \"min\": 589.0152454376221, \"max\": 589.0152454376221}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:22 INFO 140374100940160] #throughput_metric: host=algo-1, train throughput=1101.6446323167856 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:22 INFO 140374100940160] #progress_metric: host=algo-1, completed 40.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:22 INFO 140374100940160] #quality_metric: host=algo-1, epoch=80, train loss <loss>=9.75283449346369\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:22 INFO 140374100940160] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:22 INFO 140374100940160] Epoch[81] Batch[0] avg_epoch_loss=10.498813\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:22 INFO 140374100940160] #quality_metric: host=algo-1, epoch=81, batch=0 train loss <loss>=10.498812675476074\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:22 INFO 140374100940160] Epoch[81] Batch[5] avg_epoch_loss=9.966754\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:22 INFO 140374100940160] #quality_metric: host=algo-1, epoch=81, batch=5 train loss <loss>=9.96675411860148\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:22 INFO 140374100940160] Epoch[81] Batch [5]#011Speed: 2164.01 samples/sec#011loss=9.966754\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:23 INFO 140374100940160] Epoch[81] Batch[10] avg_epoch_loss=10.157014\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:23 INFO 140374100940160] #quality_metric: host=algo-1, epoch=81, batch=10 train loss <loss>=10.385326194763184\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:23 INFO 140374100940160] Epoch[81] Batch [10]#011Speed: 2032.75 samples/sec#011loss=10.385326\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:23 INFO 140374100940160] processed a total of 657 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459182.5823097, \"EndTime\": 1631459183.1244388, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 541.6016578674316, \"count\": 1, \"min\": 541.6016578674316, \"max\": 541.6016578674316}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:23 INFO 140374100940160] #throughput_metric: host=algo-1, train throughput=1212.7953158104601 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:23 INFO 140374100940160] #progress_metric: host=algo-1, completed 41.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:23 INFO 140374100940160] #quality_metric: host=algo-1, epoch=81, train loss <loss>=10.157014153220437\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:23 INFO 140374100940160] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:23 INFO 140374100940160] Epoch[82] Batch[0] avg_epoch_loss=9.050155\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:23 INFO 140374100940160] #quality_metric: host=algo-1, epoch=82, batch=0 train loss <loss>=9.050154685974121\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:23 INFO 140374100940160] Epoch[82] Batch[5] avg_epoch_loss=9.669027\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:23 INFO 140374100940160] #quality_metric: host=algo-1, epoch=82, batch=5 train loss <loss>=9.669027328491211\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:23 INFO 140374100940160] Epoch[82] Batch [5]#011Speed: 2088.47 samples/sec#011loss=9.669027\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:23 INFO 140374100940160] Epoch[82] Batch[10] avg_epoch_loss=10.020620\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:23 INFO 140374100940160] #quality_metric: host=algo-1, epoch=82, batch=10 train loss <loss>=10.442531394958497\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:23 INFO 140374100940160] Epoch[82] Batch [10]#011Speed: 1540.63 samples/sec#011loss=10.442531\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:23 INFO 140374100940160] processed a total of 665 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459183.1245217, \"EndTime\": 1631459183.7197354, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 594.7086811065674, \"count\": 1, \"min\": 594.7086811065674, \"max\": 594.7086811065674}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:23 INFO 140374100940160] #throughput_metric: host=algo-1, train throughput=1118.007636665937 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:23 INFO 140374100940160] #progress_metric: host=algo-1, completed 41.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:23 INFO 140374100940160] #quality_metric: host=algo-1, epoch=82, train loss <loss>=10.02062008597634\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:23 INFO 140374100940160] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:23 INFO 140374100940160] Epoch[83] Batch[0] avg_epoch_loss=10.252203\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:23 INFO 140374100940160] #quality_metric: host=algo-1, epoch=83, batch=0 train loss <loss>=10.252202987670898\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:24 INFO 140374100940160] Epoch[83] Batch[5] avg_epoch_loss=10.494744\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:24 INFO 140374100940160] #quality_metric: host=algo-1, epoch=83, batch=5 train loss <loss>=10.494743824005127\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:24 INFO 140374100940160] Epoch[83] Batch [5]#011Speed: 1682.45 samples/sec#011loss=10.494744\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:24 INFO 140374100940160] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459183.7198021, \"EndTime\": 1631459184.27966, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 559.4162940979004, \"count\": 1, \"min\": 559.4162940979004, \"max\": 559.4162940979004}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:24 INFO 140374100940160] #throughput_metric: host=algo-1, train throughput=1131.2964693249703 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:24 INFO 140374100940160] #progress_metric: host=algo-1, completed 42.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:24 INFO 140374100940160] #quality_metric: host=algo-1, epoch=83, train loss <loss>=10.447286224365234\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:24 INFO 140374100940160] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:24 INFO 140374100940160] Epoch[84] Batch[0] avg_epoch_loss=10.144048\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:24 INFO 140374100940160] #quality_metric: host=algo-1, epoch=84, batch=0 train loss <loss>=10.144047737121582\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:24 INFO 140374100940160] Epoch[84] Batch[5] avg_epoch_loss=9.729590\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:24 INFO 140374100940160] #quality_metric: host=algo-1, epoch=84, batch=5 train loss <loss>=9.729589780171713\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:24 INFO 140374100940160] Epoch[84] Batch [5]#011Speed: 2101.54 samples/sec#011loss=9.729590\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:24 INFO 140374100940160] processed a total of 590 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459184.2797441, \"EndTime\": 1631459184.7868912, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 506.5875053405762, \"count\": 1, \"min\": 506.5875053405762, \"max\": 506.5875053405762}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:24 INFO 140374100940160] #throughput_metric: host=algo-1, train throughput=1164.3356297221278 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:24 INFO 140374100940160] #progress_metric: host=algo-1, completed 42.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:24 INFO 140374100940160] #quality_metric: host=algo-1, epoch=84, train loss <loss>=10.4234938621521\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:24 INFO 140374100940160] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:25 INFO 140374100940160] Epoch[85] Batch[0] avg_epoch_loss=10.598854\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:25 INFO 140374100940160] #quality_metric: host=algo-1, epoch=85, batch=0 train loss <loss>=10.598854064941406\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:25 INFO 140374100940160] Epoch[85] Batch[5] avg_epoch_loss=10.257767\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:25 INFO 140374100940160] #quality_metric: host=algo-1, epoch=85, batch=5 train loss <loss>=10.257766723632812\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:25 INFO 140374100940160] Epoch[85] Batch [5]#011Speed: 2165.83 samples/sec#011loss=10.257767\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:25 INFO 140374100940160] processed a total of 626 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459184.7869906, \"EndTime\": 1631459185.2925396, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 505.0504207611084, \"count\": 1, \"min\": 505.0504207611084, \"max\": 505.0504207611084}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:25 INFO 140374100940160] #throughput_metric: host=algo-1, train throughput=1239.2169587759022 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:25 INFO 140374100940160] #progress_metric: host=algo-1, completed 43.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:25 INFO 140374100940160] #quality_metric: host=algo-1, epoch=85, train loss <loss>=10.269993114471436\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:25 INFO 140374100940160] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:25 INFO 140374100940160] Epoch[86] Batch[0] avg_epoch_loss=10.034683\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:25 INFO 140374100940160] #quality_metric: host=algo-1, epoch=86, batch=0 train loss <loss>=10.034683227539062\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:25 INFO 140374100940160] Epoch[86] Batch[5] avg_epoch_loss=10.155276\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:25 INFO 140374100940160] #quality_metric: host=algo-1, epoch=86, batch=5 train loss <loss>=10.155276457468668\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:25 INFO 140374100940160] Epoch[86] Batch [5]#011Speed: 1792.54 samples/sec#011loss=10.155276\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:25 INFO 140374100940160] processed a total of 613 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459185.2926154, \"EndTime\": 1631459185.8297884, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 536.6687774658203, \"count\": 1, \"min\": 536.6687774658203, \"max\": 536.6687774658203}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:25 INFO 140374100940160] #throughput_metric: host=algo-1, train throughput=1141.9626503787283 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:25 INFO 140374100940160] #progress_metric: host=algo-1, completed 43.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:25 INFO 140374100940160] #quality_metric: host=algo-1, epoch=86, train loss <loss>=10.538503265380859\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:25 INFO 140374100940160] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:26 INFO 140374100940160] Epoch[87] Batch[0] avg_epoch_loss=9.987471\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:26 INFO 140374100940160] #quality_metric: host=algo-1, epoch=87, batch=0 train loss <loss>=9.987470626831055\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:26 INFO 140374100940160] Epoch[87] Batch[5] avg_epoch_loss=9.914793\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:26 INFO 140374100940160] #quality_metric: host=algo-1, epoch=87, batch=5 train loss <loss>=9.914793491363525\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:26 INFO 140374100940160] Epoch[87] Batch [5]#011Speed: 2102.01 samples/sec#011loss=9.914793\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:26 INFO 140374100940160] processed a total of 571 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459185.8298752, \"EndTime\": 1631459186.3139472, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 483.5681915283203, \"count\": 1, \"min\": 483.5681915283203, \"max\": 483.5681915283203}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:26 INFO 140374100940160] #throughput_metric: host=algo-1, train throughput=1180.5011369041604 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:26 INFO 140374100940160] #progress_metric: host=algo-1, completed 44.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:26 INFO 140374100940160] #quality_metric: host=algo-1, epoch=87, train loss <loss>=9.912001291910807\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:26 INFO 140374100940160] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:26 INFO 140374100940160] Loading parameters from best epoch (47)\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459186.3140328, \"EndTime\": 1631459186.3234036, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.deserialize.time\": {\"sum\": 8.785247802734375, \"count\": 1, \"min\": 8.785247802734375, \"max\": 8.785247802734375}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:26 INFO 140374100940160] stopping training now\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:26 INFO 140374100940160] #progress_metric: host=algo-1, completed 100 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:26 INFO 140374100940160] Final loss: 9.034372199665416 (occurred at epoch 47)\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:26 INFO 140374100940160] #quality_metric: host=algo-1, train final_loss <loss>=9.034372199665416\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:26 INFO 140374100940160] Worker algo-1 finished training.\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:26 WARNING 140374100940160] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:26 INFO 140374100940160] All workers finished. Serializing model for prediction.\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459186.3234742, \"EndTime\": 1631459186.4320683, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"get_graph.time\": {\"sum\": 107.89322853088379, \"count\": 1, \"min\": 107.89322853088379, \"max\": 107.89322853088379}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:26 INFO 140374100940160] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459186.4321268, \"EndTime\": 1631459186.4642777, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"finalize.time\": {\"sum\": 140.14053344726562, \"count\": 1, \"min\": 140.14053344726562, \"max\": 140.14053344726562}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:26 INFO 140374100940160] Serializing to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:26 INFO 140374100940160] Saved checkpoint to \"/opt/ml/model/model_algo-1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459186.4643538, \"EndTime\": 1631459186.470011, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"model.serialize.time\": {\"sum\": 5.620002746582031, \"count\": 1, \"min\": 5.620002746582031, \"max\": 5.620002746582031}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:26 INFO 140374100940160] Successfully serialized the model for prediction.\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:06:26 INFO 140374100940160] No test data passed, skipping evaluation.\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459186.4700584, \"EndTime\": 1631459186.4753852, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"setuptime\": {\"sum\": 7.967233657836914, \"count\": 1, \"min\": 7.967233657836914, \"max\": 7.967233657836914}, \"totaltime\": {\"sum\": 52283.87546539307, \"count\": 1, \"min\": 52283.87546539307, \"max\": 52283.87546539307}}}\n",
      "\u001b[0m\n",
      "\n",
      "2021-09-12 15:06:35 Uploading - Uploading generated training model\n",
      "2021-09-12 15:06:35 Completed - Training job completed\n",
      "Training seconds: 97\n",
      "Billable seconds: 97\n",
      "-------------!Overwriting existing file\n",
      "Uploading file to s3://sagemaker-us-east-2-222195595958/deepar-covid-demo-notebook/data/train/train.json\n",
      "2021-09-12 15:13:30 Starting - Starting the training job...\n",
      "2021-09-12 15:13:56 Starting - Launching requested ML instancesProfilerReport-1631459610: InProgress\n",
      "...\n",
      "2021-09-12 15:14:21 Starting - Preparing the instances for training............\n",
      "2021-09-12 15:16:16 Downloading - Downloading input data\n",
      "2021-09-12 15:16:16 Training - Downloading the training image..\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:40 INFO 140650192094592] Reading default configuration from /opt/amazon/lib/python3.6/site-packages/algorithm/resources/default-input.json: {'_kvstore': 'auto', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_tuning_objective_metric': '', 'cardinality': 'auto', 'dropout_rate': '0.10', 'early_stopping_patience': '', 'embedding_dimension': '10', 'learning_rate': '0.001', 'likelihood': 'student-t', 'mini_batch_size': '128', 'num_cells': '40', 'num_dynamic_feat': 'auto', 'num_eval_samples': '100', 'num_layers': '2', 'test_quantiles': '[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]'}\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:40 INFO 140650192094592] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {'prediction_length': '14', 'time_freq': 'D', 'context_length': '14', 'epochs': '200', 'learning_rate': '5E-4', 'early_stopping_patience': '40', 'mini_batch_size': '64'}\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:40 INFO 140650192094592] Final configuration: {'_kvstore': 'auto', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_tuning_objective_metric': '', 'cardinality': 'auto', 'dropout_rate': '0.10', 'early_stopping_patience': '40', 'embedding_dimension': '10', 'learning_rate': '5E-4', 'likelihood': 'student-t', 'mini_batch_size': '64', 'num_cells': '40', 'num_dynamic_feat': 'auto', 'num_eval_samples': '100', 'num_layers': '2', 'test_quantiles': '[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', 'prediction_length': '14', 'time_freq': 'D', 'context_length': '14', 'epochs': '200'}\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:40 INFO 140650192094592] Detected entry point for worker worker\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:40 INFO 140650192094592] Using early stopping with patience 40\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:40 INFO 140650192094592] random_seed is None\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:40 INFO 140650192094592] [cardinality=auto] `cat` field was NOT found in the file `/opt/ml/input/data/train/train.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:40 INFO 140650192094592] [num_dynamic_feat=auto] `dynamic_feat` field was NOT found in the file `/opt/ml/input/data/train/train.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:40 INFO 140650192094592] Training set statistics:\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:40 INFO 140650192094592] Integer time series\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:40 INFO 140650192094592] number of time series: 41\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:40 INFO 140650192094592] number of observations: 13492\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:40 INFO 140650192094592] mean target length: 329.0731707317073\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:40 INFO 140650192094592] min/mean/max target: 0.0/15262025.537948415/350627200.0\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:40 INFO 140650192094592] mean abs(target): 15262025.537948415\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:40 INFO 140650192094592] contains missing values: no\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:40 INFO 140650192094592] Small number of time series. Doing 16 passes over dataset with prob 0.975609756097561 per epoch.\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:40 INFO 140650192094592] No test channel found not running evaluations\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:40 INFO 140650192094592] #memory_usage::<batchbuffer> = 2.2568130493164062 mb\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:40 INFO 140650192094592] nvidia-smi took: 0.0252535343170166 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:40 INFO 140650192094592] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:40 INFO 140650192094592] Create Store: local\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459800.9450562, \"EndTime\": 1631459801.0316565, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"get_graph.time\": {\"sum\": 84.14816856384277, \"count\": 1, \"min\": 84.14816856384277, \"max\": 84.14816856384277}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:41 INFO 140650192094592] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:41 INFO 140650192094592] #memory_usage::<model> = 12 mb\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459801.031738, \"EndTime\": 1631459801.1457574, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"initialize.time\": {\"sum\": 200.5748748779297, \"count\": 1, \"min\": 200.5748748779297, \"max\": 200.5748748779297}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:41 INFO 140650192094592] Epoch[0] Batch[0] avg_epoch_loss=14.995230\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:41 INFO 140650192094592] #quality_metric: host=algo-1, epoch=0, batch=0 train loss <loss>=14.995229721069336\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:41 INFO 140650192094592] Epoch[0] Batch[5] avg_epoch_loss=14.239697\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:41 INFO 140650192094592] #quality_metric: host=algo-1, epoch=0, batch=5 train loss <loss>=14.239696502685547\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:41 INFO 140650192094592] Epoch[0] Batch [5]#011Speed: 2101.72 samples/sec#011loss=14.239697\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:41 INFO 140650192094592] Epoch[0] Batch[10] avg_epoch_loss=13.356511\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:41 INFO 140650192094592] #quality_metric: host=algo-1, epoch=0, batch=10 train loss <loss>=12.296689224243163\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:41 INFO 140650192094592] Epoch[0] Batch [10]#011Speed: 1863.72 samples/sec#011loss=12.296689\u001b[0m\n",
      "\u001b[34m/opt/amazon/python3.6/lib/python3.6/contextlib.py:99: DeprecationWarning: generator 'local_timer' raised StopIteration\n",
      "  self.gen.throw(type, value, traceback)\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:41 INFO 140650192094592] processed a total of 645 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459801.1458263, \"EndTime\": 1631459801.8511348, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"epochs\": {\"sum\": 200.0, \"count\": 1, \"min\": 200, \"max\": 200}, \"update.time\": {\"sum\": 705.1811218261719, \"count\": 1, \"min\": 705.1811218261719, \"max\": 705.1811218261719}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:41 INFO 140650192094592] #throughput_metric: host=algo-1, train throughput=914.4255226807658 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:41 INFO 140650192094592] #progress_metric: host=algo-1, completed 0.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:41 INFO 140650192094592] #quality_metric: host=algo-1, epoch=0, train loss <loss>=13.356511376120828\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:41 INFO 140650192094592] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:41 INFO 140650192094592] Saved checkpoint to \"/opt/ml/model/state_a285b7bc-57a4-49f0-bae0-3c1157d43dc5-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459801.851242, \"EndTime\": 1631459801.8989353, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 46.646833419799805, \"count\": 1, \"min\": 46.646833419799805, \"max\": 46.646833419799805}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:42 INFO 140650192094592] Epoch[1] Batch[0] avg_epoch_loss=12.448077\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:42 INFO 140650192094592] #quality_metric: host=algo-1, epoch=1, batch=0 train loss <loss>=12.448077201843262\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:42 INFO 140650192094592] Epoch[1] Batch[5] avg_epoch_loss=12.801442\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:42 INFO 140650192094592] #quality_metric: host=algo-1, epoch=1, batch=5 train loss <loss>=12.801442305246988\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:42 INFO 140650192094592] Epoch[1] Batch [5]#011Speed: 1702.27 samples/sec#011loss=12.801442\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:42 INFO 140650192094592] processed a total of 640 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459801.899018, \"EndTime\": 1631459802.5319972, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 632.9169273376465, \"count\": 1, \"min\": 632.9169273376465, \"max\": 632.9169273376465}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:42 INFO 140650192094592] #throughput_metric: host=algo-1, train throughput=1010.9946436413296 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:42 INFO 140650192094592] #progress_metric: host=algo-1, completed 1.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:42 INFO 140650192094592] #quality_metric: host=algo-1, epoch=1, train loss <loss>=12.77134437561035\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:42 INFO 140650192094592] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:42 INFO 140650192094592] Saved checkpoint to \"/opt/ml/model/state_207a5a82-f226-43c9-b3d9-cbfa8c601c75-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459802.5320845, \"EndTime\": 1631459802.5503876, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 17.820119857788086, \"count\": 1, \"min\": 17.820119857788086, \"max\": 17.820119857788086}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:42 INFO 140650192094592] Epoch[2] Batch[0] avg_epoch_loss=11.989712\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:42 INFO 140650192094592] #quality_metric: host=algo-1, epoch=2, batch=0 train loss <loss>=11.98971176147461\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:43 INFO 140650192094592] Epoch[2] Batch[5] avg_epoch_loss=12.206716\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:43 INFO 140650192094592] #quality_metric: host=algo-1, epoch=2, batch=5 train loss <loss>=12.206715742746988\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:43 INFO 140650192094592] Epoch[2] Batch [5]#011Speed: 1470.07 samples/sec#011loss=12.206716\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:43 INFO 140650192094592] processed a total of 640 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459802.5504518, \"EndTime\": 1631459803.2388291, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 688.3230209350586, \"count\": 1, \"min\": 688.3230209350586, \"max\": 688.3230209350586}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:43 INFO 140650192094592] #throughput_metric: host=algo-1, train throughput=929.6343667165593 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:43 INFO 140650192094592] #progress_metric: host=algo-1, completed 1.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:43 INFO 140650192094592] #quality_metric: host=algo-1, epoch=2, train loss <loss>=12.252535247802735\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:43 INFO 140650192094592] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:43 INFO 140650192094592] Saved checkpoint to \"/opt/ml/model/state_e2a00d3e-7d4b-48ee-908d-a479e01cfddc-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459803.2389133, \"EndTime\": 1631459803.2583826, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 18.95618438720703, \"count\": 1, \"min\": 18.95618438720703, \"max\": 18.95618438720703}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:43 INFO 140650192094592] Epoch[3] Batch[0] avg_epoch_loss=11.826653\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:43 INFO 140650192094592] #quality_metric: host=algo-1, epoch=3, batch=0 train loss <loss>=11.826652526855469\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:43 INFO 140650192094592] Epoch[3] Batch[5] avg_epoch_loss=12.154560\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:43 INFO 140650192094592] #quality_metric: host=algo-1, epoch=3, batch=5 train loss <loss>=12.154559771219889\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:43 INFO 140650192094592] Epoch[3] Batch [5]#011Speed: 1895.24 samples/sec#011loss=12.154560\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:43 INFO 140650192094592] Epoch[3] Batch[10] avg_epoch_loss=12.697657\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:43 INFO 140650192094592] #quality_metric: host=algo-1, epoch=3, batch=10 train loss <loss>=13.349372863769531\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:43 INFO 140650192094592] Epoch[3] Batch [10]#011Speed: 1845.35 samples/sec#011loss=13.349373\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:43 INFO 140650192094592] processed a total of 665 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459803.25847, \"EndTime\": 1631459803.874736, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 616.2025928497314, \"count\": 1, \"min\": 616.2025928497314, \"max\": 616.2025928497314}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:43 INFO 140650192094592] #throughput_metric: host=algo-1, train throughput=1078.9776139813102 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:43 INFO 140650192094592] #progress_metric: host=algo-1, completed 2.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:43 INFO 140650192094592] #quality_metric: host=algo-1, epoch=3, train loss <loss>=12.697656631469727\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:43 INFO 140650192094592] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:44 INFO 140650192094592] Epoch[4] Batch[0] avg_epoch_loss=11.783833\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:44 INFO 140650192094592] #quality_metric: host=algo-1, epoch=4, batch=0 train loss <loss>=11.783832550048828\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:44 INFO 140650192094592] Epoch[4] Batch[5] avg_epoch_loss=11.770147\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:44 INFO 140650192094592] #quality_metric: host=algo-1, epoch=4, batch=5 train loss <loss>=11.770147482554117\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:44 INFO 140650192094592] Epoch[4] Batch [5]#011Speed: 1841.33 samples/sec#011loss=11.770147\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:44 INFO 140650192094592] processed a total of 640 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459803.874822, \"EndTime\": 1631459804.4362319, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 560.9521865844727, \"count\": 1, \"min\": 560.9521865844727, \"max\": 560.9521865844727}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:44 INFO 140650192094592] #throughput_metric: host=algo-1, train throughput=1140.660254770466 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:44 INFO 140650192094592] #progress_metric: host=algo-1, completed 2.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:44 INFO 140650192094592] #quality_metric: host=algo-1, epoch=4, train loss <loss>=11.756581211090088\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:44 INFO 140650192094592] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:44 INFO 140650192094592] Saved checkpoint to \"/opt/ml/model/state_154344a8-0531-444d-b903-470e164ea89e-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459804.4363186, \"EndTime\": 1631459804.4482083, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 11.324167251586914, \"count\": 1, \"min\": 11.324167251586914, \"max\": 11.324167251586914}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:44 INFO 140650192094592] Epoch[5] Batch[0] avg_epoch_loss=11.569039\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:44 INFO 140650192094592] #quality_metric: host=algo-1, epoch=5, batch=0 train loss <loss>=11.569039344787598\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:44 INFO 140650192094592] Epoch[5] Batch[5] avg_epoch_loss=11.599869\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:44 INFO 140650192094592] #quality_metric: host=algo-1, epoch=5, batch=5 train loss <loss>=11.59986925125122\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:44 INFO 140650192094592] Epoch[5] Batch [5]#011Speed: 1906.29 samples/sec#011loss=11.599869\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:45 INFO 140650192094592] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459804.4482834, \"EndTime\": 1631459805.038976, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 590.6317234039307, \"count\": 1, \"min\": 590.6317234039307, \"max\": 590.6317234039307}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:45 INFO 140650192094592] #throughput_metric: host=algo-1, train throughput=1073.2012285042033 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:45 INFO 140650192094592] #progress_metric: host=algo-1, completed 3.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:45 INFO 140650192094592] #quality_metric: host=algo-1, epoch=5, train loss <loss>=11.547297191619872\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:45 INFO 140650192094592] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:45 INFO 140650192094592] Saved checkpoint to \"/opt/ml/model/state_3fb1f064-ddd8-42ca-9319-8862f393f02b-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459805.0390623, \"EndTime\": 1631459805.0582252, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 18.671751022338867, \"count\": 1, \"min\": 18.671751022338867, \"max\": 18.671751022338867}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:45 INFO 140650192094592] Epoch[6] Batch[0] avg_epoch_loss=11.405149\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:45 INFO 140650192094592] #quality_metric: host=algo-1, epoch=6, batch=0 train loss <loss>=11.40514850616455\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:45 INFO 140650192094592] Epoch[6] Batch[5] avg_epoch_loss=11.171829\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:45 INFO 140650192094592] #quality_metric: host=algo-1, epoch=6, batch=5 train loss <loss>=11.171829064687094\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:45 INFO 140650192094592] Epoch[6] Batch [5]#011Speed: 2084.43 samples/sec#011loss=11.171829\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:45 INFO 140650192094592] Epoch[6] Batch[10] avg_epoch_loss=11.117405\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:45 INFO 140650192094592] #quality_metric: host=algo-1, epoch=6, batch=10 train loss <loss>=11.05209560394287\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:45 INFO 140650192094592] Epoch[6] Batch [10]#011Speed: 1872.83 samples/sec#011loss=11.052096\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:45 INFO 140650192094592] processed a total of 677 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459805.0582986, \"EndTime\": 1631459805.6670437, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 608.6866855621338, \"count\": 1, \"min\": 608.6866855621338, \"max\": 608.6866855621338}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:45 INFO 140650192094592] #throughput_metric: host=algo-1, train throughput=1111.9802255019597 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:45 INFO 140650192094592] #progress_metric: host=algo-1, completed 3.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:45 INFO 140650192094592] #quality_metric: host=algo-1, epoch=6, train loss <loss>=11.11740476434881\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:45 INFO 140650192094592] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:45 INFO 140650192094592] Saved checkpoint to \"/opt/ml/model/state_5b47180e-f19e-4d10-a376-8dbb619e71dd-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459805.6671462, \"EndTime\": 1631459805.6858783, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 18.230438232421875, \"count\": 1, \"min\": 18.230438232421875, \"max\": 18.230438232421875}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:45 INFO 140650192094592] Epoch[7] Batch[0] avg_epoch_loss=9.942765\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:45 INFO 140650192094592] #quality_metric: host=algo-1, epoch=7, batch=0 train loss <loss>=9.942765235900879\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:46 INFO 140650192094592] Epoch[7] Batch[5] avg_epoch_loss=11.030123\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:46 INFO 140650192094592] #quality_metric: host=algo-1, epoch=7, batch=5 train loss <loss>=11.030122598012289\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:46 INFO 140650192094592] Epoch[7] Batch [5]#011Speed: 2148.55 samples/sec#011loss=11.030123\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:46 INFO 140650192094592] processed a total of 602 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459805.6859455, \"EndTime\": 1631459806.2605655, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 574.5635032653809, \"count\": 1, \"min\": 574.5635032653809, \"max\": 574.5635032653809}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:46 INFO 140650192094592] #throughput_metric: host=algo-1, train throughput=1047.523691109305 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:46 INFO 140650192094592] #progress_metric: host=algo-1, completed 4.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:46 INFO 140650192094592] #quality_metric: host=algo-1, epoch=7, train loss <loss>=11.158267402648926\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:46 INFO 140650192094592] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:46 INFO 140650192094592] Epoch[8] Batch[0] avg_epoch_loss=10.619458\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:46 INFO 140650192094592] #quality_metric: host=algo-1, epoch=8, batch=0 train loss <loss>=10.619458198547363\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:46 INFO 140650192094592] Epoch[8] Batch[5] avg_epoch_loss=10.495861\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:46 INFO 140650192094592] #quality_metric: host=algo-1, epoch=8, batch=5 train loss <loss>=10.495860735575357\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:46 INFO 140650192094592] Epoch[8] Batch [5]#011Speed: 2163.64 samples/sec#011loss=10.495861\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:46 INFO 140650192094592] Epoch[8] Batch[10] avg_epoch_loss=10.605988\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:46 INFO 140650192094592] #quality_metric: host=algo-1, epoch=8, batch=10 train loss <loss>=10.738141441345215\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:46 INFO 140650192094592] Epoch[8] Batch [10]#011Speed: 1776.61 samples/sec#011loss=10.738141\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:46 INFO 140650192094592] processed a total of 644 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459806.260655, \"EndTime\": 1631459806.8518782, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 590.693473815918, \"count\": 1, \"min\": 590.693473815918, \"max\": 590.693473815918}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:46 INFO 140650192094592] #throughput_metric: host=algo-1, train throughput=1090.0301513215718 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:46 INFO 140650192094592] #progress_metric: host=algo-1, completed 4.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:46 INFO 140650192094592] #quality_metric: host=algo-1, epoch=8, train loss <loss>=10.605988329107111\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:46 INFO 140650192094592] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:46 INFO 140650192094592] Saved checkpoint to \"/opt/ml/model/state_42e5470e-74a8-4a43-a151-3d9b3d7a074a-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459806.851958, \"EndTime\": 1631459806.8701115, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 17.67897605895996, \"count\": 1, \"min\": 17.67897605895996, \"max\": 17.67897605895996}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:47 INFO 140650192094592] Epoch[9] Batch[0] avg_epoch_loss=11.162806\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:47 INFO 140650192094592] #quality_metric: host=algo-1, epoch=9, batch=0 train loss <loss>=11.162805557250977\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:47 INFO 140650192094592] Epoch[9] Batch[5] avg_epoch_loss=10.524711\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:47 INFO 140650192094592] #quality_metric: host=algo-1, epoch=9, batch=5 train loss <loss>=10.524710973103842\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:47 INFO 140650192094592] Epoch[9] Batch [5]#011Speed: 2038.88 samples/sec#011loss=10.524711\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:47 INFO 140650192094592] processed a total of 610 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459806.8701758, \"EndTime\": 1631459807.412559, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 542.3262119293213, \"count\": 1, \"min\": 542.3262119293213, \"max\": 542.3262119293213}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:47 INFO 140650192094592] #throughput_metric: host=algo-1, train throughput=1124.5118915345777 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:47 INFO 140650192094592] #progress_metric: host=algo-1, completed 5.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:47 INFO 140650192094592] #quality_metric: host=algo-1, epoch=9, train loss <loss>=10.615612983703613\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:47 INFO 140650192094592] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:47 INFO 140650192094592] Epoch[10] Batch[0] avg_epoch_loss=10.058696\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:47 INFO 140650192094592] #quality_metric: host=algo-1, epoch=10, batch=0 train loss <loss>=10.058695793151855\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:47 INFO 140650192094592] Epoch[10] Batch[5] avg_epoch_loss=10.787800\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:47 INFO 140650192094592] #quality_metric: host=algo-1, epoch=10, batch=5 train loss <loss>=10.787800312042236\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:47 INFO 140650192094592] Epoch[10] Batch [5]#011Speed: 1878.74 samples/sec#011loss=10.787800\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:48 INFO 140650192094592] Epoch[10] Batch[10] avg_epoch_loss=10.802239\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:48 INFO 140650192094592] #quality_metric: host=algo-1, epoch=10, batch=10 train loss <loss>=10.819565582275391\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:48 INFO 140650192094592] Epoch[10] Batch [10]#011Speed: 1681.76 samples/sec#011loss=10.819566\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:48 INFO 140650192094592] processed a total of 644 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459807.412653, \"EndTime\": 1631459808.0363777, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 623.1861114501953, \"count\": 1, \"min\": 623.1861114501953, \"max\": 623.1861114501953}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:48 INFO 140650192094592] #throughput_metric: host=algo-1, train throughput=1033.1959794151196 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:48 INFO 140650192094592] #progress_metric: host=algo-1, completed 5.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:48 INFO 140650192094592] #quality_metric: host=algo-1, epoch=10, train loss <loss>=10.802239071239125\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:48 INFO 140650192094592] loss did not improve\u001b[0m\n",
      "\n",
      "2021-09-12 15:16:56 Training - Training image download completed. Training in progress.\u001b[34m[09/12/2021 15:16:48 INFO 140650192094592] Epoch[11] Batch[0] avg_epoch_loss=10.505910\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:48 INFO 140650192094592] #quality_metric: host=algo-1, epoch=11, batch=0 train loss <loss>=10.50590991973877\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:48 INFO 140650192094592] Epoch[11] Batch[5] avg_epoch_loss=10.693348\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:48 INFO 140650192094592] #quality_metric: host=algo-1, epoch=11, batch=5 train loss <loss>=10.693348407745361\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:48 INFO 140650192094592] Epoch[11] Batch [5]#011Speed: 1911.48 samples/sec#011loss=10.693348\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:48 INFO 140650192094592] Epoch[11] Batch[10] avg_epoch_loss=11.283073\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:48 INFO 140650192094592] #quality_metric: host=algo-1, epoch=11, batch=10 train loss <loss>=11.990742492675782\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:48 INFO 140650192094592] Epoch[11] Batch [10]#011Speed: 1518.64 samples/sec#011loss=11.990742\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:48 INFO 140650192094592] processed a total of 644 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459808.0364625, \"EndTime\": 1631459808.6787546, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 641.7667865753174, \"count\": 1, \"min\": 641.7667865753174, \"max\": 641.7667865753174}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:48 INFO 140650192094592] #throughput_metric: host=algo-1, train throughput=1003.2885816397861 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:48 INFO 140650192094592] #progress_metric: host=algo-1, completed 6.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:48 INFO 140650192094592] #quality_metric: host=algo-1, epoch=11, train loss <loss>=11.283072991804643\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:48 INFO 140650192094592] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:48 INFO 140650192094592] Epoch[12] Batch[0] avg_epoch_loss=11.464705\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:48 INFO 140650192094592] #quality_metric: host=algo-1, epoch=12, batch=0 train loss <loss>=11.464705467224121\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:49 INFO 140650192094592] Epoch[12] Batch[5] avg_epoch_loss=10.571960\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:49 INFO 140650192094592] #quality_metric: host=algo-1, epoch=12, batch=5 train loss <loss>=10.57196044921875\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:49 INFO 140650192094592] Epoch[12] Batch [5]#011Speed: 1973.38 samples/sec#011loss=10.571960\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:49 INFO 140650192094592] processed a total of 588 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459808.678839, \"EndTime\": 1631459809.3111885, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 631.8485736846924, \"count\": 1, \"min\": 631.8485736846924, \"max\": 631.8485736846924}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:49 INFO 140650192094592] #throughput_metric: host=algo-1, train throughput=930.3829505985194 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:49 INFO 140650192094592] #progress_metric: host=algo-1, completed 6.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:49 INFO 140650192094592] #quality_metric: host=algo-1, epoch=12, train loss <loss>=10.154528665542603\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:49 INFO 140650192094592] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:49 INFO 140650192094592] Saved checkpoint to \"/opt/ml/model/state_323a5959-2f9d-46e8-a66f-205eba6038da-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459809.3112993, \"EndTime\": 1631459809.323477, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 11.6424560546875, \"count\": 1, \"min\": 11.6424560546875, \"max\": 11.6424560546875}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:49 INFO 140650192094592] Epoch[13] Batch[0] avg_epoch_loss=10.266189\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:49 INFO 140650192094592] #quality_metric: host=algo-1, epoch=13, batch=0 train loss <loss>=10.266188621520996\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:49 INFO 140650192094592] Epoch[13] Batch[5] avg_epoch_loss=10.835131\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:49 INFO 140650192094592] #quality_metric: host=algo-1, epoch=13, batch=5 train loss <loss>=10.835131327311197\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:49 INFO 140650192094592] Epoch[13] Batch [5]#011Speed: 2078.25 samples/sec#011loss=10.835131\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:49 INFO 140650192094592] Epoch[13] Batch[10] avg_epoch_loss=10.869645\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:49 INFO 140650192094592] #quality_metric: host=algo-1, epoch=13, batch=10 train loss <loss>=10.911060333251953\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:49 INFO 140650192094592] Epoch[13] Batch [10]#011Speed: 2025.92 samples/sec#011loss=10.911060\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:49 INFO 140650192094592] processed a total of 649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459809.3235457, \"EndTime\": 1631459809.8874733, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 563.8666152954102, \"count\": 1, \"min\": 563.8666152954102, \"max\": 563.8666152954102}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:49 INFO 140650192094592] #throughput_metric: host=algo-1, train throughput=1150.7386286438498 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:49 INFO 140650192094592] #progress_metric: host=algo-1, completed 7.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:49 INFO 140650192094592] #quality_metric: host=algo-1, epoch=13, train loss <loss>=10.869644511829723\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:49 INFO 140650192094592] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:50 INFO 140650192094592] Epoch[14] Batch[0] avg_epoch_loss=11.112280\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:50 INFO 140650192094592] #quality_metric: host=algo-1, epoch=14, batch=0 train loss <loss>=11.112279891967773\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:50 INFO 140650192094592] Epoch[14] Batch[5] avg_epoch_loss=10.538873\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:50 INFO 140650192094592] #quality_metric: host=algo-1, epoch=14, batch=5 train loss <loss>=10.538873036702475\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:50 INFO 140650192094592] Epoch[14] Batch [5]#011Speed: 1945.41 samples/sec#011loss=10.538873\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:50 INFO 140650192094592] Epoch[14] Batch[10] avg_epoch_loss=10.344939\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:50 INFO 140650192094592] #quality_metric: host=algo-1, epoch=14, batch=10 train loss <loss>=10.112218284606934\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:50 INFO 140650192094592] Epoch[14] Batch [10]#011Speed: 1956.09 samples/sec#011loss=10.112218\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:50 INFO 140650192094592] processed a total of 675 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459809.8875577, \"EndTime\": 1631459810.4804924, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 592.4458503723145, \"count\": 1, \"min\": 592.4458503723145, \"max\": 592.4458503723145}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:50 INFO 140650192094592] #throughput_metric: host=algo-1, train throughput=1139.1241335828458 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:50 INFO 140650192094592] #progress_metric: host=algo-1, completed 7.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:50 INFO 140650192094592] #quality_metric: host=algo-1, epoch=14, train loss <loss>=10.344939058477228\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:50 INFO 140650192094592] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:50 INFO 140650192094592] Epoch[15] Batch[0] avg_epoch_loss=9.880992\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:50 INFO 140650192094592] #quality_metric: host=algo-1, epoch=15, batch=0 train loss <loss>=9.88099193572998\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:50 INFO 140650192094592] Epoch[15] Batch[5] avg_epoch_loss=10.257837\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:50 INFO 140650192094592] #quality_metric: host=algo-1, epoch=15, batch=5 train loss <loss>=10.257837136586508\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:50 INFO 140650192094592] Epoch[15] Batch [5]#011Speed: 2065.08 samples/sec#011loss=10.257837\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:50 INFO 140650192094592] processed a total of 576 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459810.4805691, \"EndTime\": 1631459810.9585652, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 477.5094985961914, \"count\": 1, \"min\": 477.5094985961914, \"max\": 477.5094985961914}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:50 INFO 140650192094592] #throughput_metric: host=algo-1, train throughput=1205.931777715771 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:50 INFO 140650192094592] #progress_metric: host=algo-1, completed 8.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:50 INFO 140650192094592] #quality_metric: host=algo-1, epoch=15, train loss <loss>=10.393656412760416\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:50 INFO 140650192094592] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:51 INFO 140650192094592] Epoch[16] Batch[0] avg_epoch_loss=11.204432\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:51 INFO 140650192094592] #quality_metric: host=algo-1, epoch=16, batch=0 train loss <loss>=11.204432487487793\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:51 INFO 140650192094592] Epoch[16] Batch[5] avg_epoch_loss=10.389320\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:51 INFO 140650192094592] #quality_metric: host=algo-1, epoch=16, batch=5 train loss <loss>=10.38931973775228\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:51 INFO 140650192094592] Epoch[16] Batch [5]#011Speed: 2096.05 samples/sec#011loss=10.389320\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:51 INFO 140650192094592] Epoch[16] Batch[10] avg_epoch_loss=10.164146\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:51 INFO 140650192094592] #quality_metric: host=algo-1, epoch=16, batch=10 train loss <loss>=9.893936729431152\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:51 INFO 140650192094592] Epoch[16] Batch [10]#011Speed: 2018.37 samples/sec#011loss=9.893937\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:51 INFO 140650192094592] processed a total of 661 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459810.9586506, \"EndTime\": 1631459811.5275571, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 568.3426856994629, \"count\": 1, \"min\": 568.3426856994629, \"max\": 568.3426856994629}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:51 INFO 140650192094592] #throughput_metric: host=algo-1, train throughput=1162.7770678624784 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:51 INFO 140650192094592] #progress_metric: host=algo-1, completed 8.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:51 INFO 140650192094592] #quality_metric: host=algo-1, epoch=16, train loss <loss>=10.164145643060857\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:51 INFO 140650192094592] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:51 INFO 140650192094592] Epoch[17] Batch[0] avg_epoch_loss=10.729756\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:51 INFO 140650192094592] #quality_metric: host=algo-1, epoch=17, batch=0 train loss <loss>=10.729756355285645\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:51 INFO 140650192094592] Epoch[17] Batch[5] avg_epoch_loss=10.247021\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:51 INFO 140650192094592] #quality_metric: host=algo-1, epoch=17, batch=5 train loss <loss>=10.247020880381266\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:51 INFO 140650192094592] Epoch[17] Batch [5]#011Speed: 2116.90 samples/sec#011loss=10.247021\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:52 INFO 140650192094592] Epoch[17] Batch[10] avg_epoch_loss=10.521555\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:52 INFO 140650192094592] #quality_metric: host=algo-1, epoch=17, batch=10 train loss <loss>=10.850996971130371\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:52 INFO 140650192094592] Epoch[17] Batch [10]#011Speed: 1776.09 samples/sec#011loss=10.850997\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:52 INFO 140650192094592] processed a total of 644 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459811.527647, \"EndTime\": 1631459812.10085, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 572.697639465332, \"count\": 1, \"min\": 572.697639465332, \"max\": 572.697639465332}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:52 INFO 140650192094592] #throughput_metric: host=algo-1, train throughput=1124.267670036669 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:52 INFO 140650192094592] #progress_metric: host=algo-1, completed 9.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:52 INFO 140650192094592] #quality_metric: host=algo-1, epoch=17, train loss <loss>=10.521555467085404\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:52 INFO 140650192094592] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:52 INFO 140650192094592] Epoch[18] Batch[0] avg_epoch_loss=10.691600\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:52 INFO 140650192094592] #quality_metric: host=algo-1, epoch=18, batch=0 train loss <loss>=10.69159984588623\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:52 INFO 140650192094592] Epoch[18] Batch[5] avg_epoch_loss=10.614453\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:52 INFO 140650192094592] #quality_metric: host=algo-1, epoch=18, batch=5 train loss <loss>=10.614452521006266\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:52 INFO 140650192094592] Epoch[18] Batch [5]#011Speed: 2011.67 samples/sec#011loss=10.614453\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:52 INFO 140650192094592] Epoch[18] Batch[10] avg_epoch_loss=10.529595\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:52 INFO 140650192094592] #quality_metric: host=algo-1, epoch=18, batch=10 train loss <loss>=10.427765846252441\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:52 INFO 140650192094592] Epoch[18] Batch [10]#011Speed: 2033.27 samples/sec#011loss=10.427766\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:52 INFO 140650192094592] processed a total of 655 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459812.1009324, \"EndTime\": 1631459812.6742275, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 572.7889537811279, \"count\": 1, \"min\": 572.7889537811279, \"max\": 572.7889537811279}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:52 INFO 140650192094592] #throughput_metric: host=algo-1, train throughput=1143.2863662203001 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:52 INFO 140650192094592] #progress_metric: host=algo-1, completed 9.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:52 INFO 140650192094592] #quality_metric: host=algo-1, epoch=18, train loss <loss>=10.529594941572709\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:52 INFO 140650192094592] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:52 INFO 140650192094592] Epoch[19] Batch[0] avg_epoch_loss=10.307876\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:52 INFO 140650192094592] #quality_metric: host=algo-1, epoch=19, batch=0 train loss <loss>=10.307875633239746\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:53 INFO 140650192094592] Epoch[19] Batch[5] avg_epoch_loss=10.429548\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:53 INFO 140650192094592] #quality_metric: host=algo-1, epoch=19, batch=5 train loss <loss>=10.429547627766928\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:53 INFO 140650192094592] Epoch[19] Batch [5]#011Speed: 1701.27 samples/sec#011loss=10.429548\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:53 INFO 140650192094592] Epoch[19] Batch[10] avg_epoch_loss=10.473208\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:53 INFO 140650192094592] #quality_metric: host=algo-1, epoch=19, batch=10 train loss <loss>=10.52560043334961\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:53 INFO 140650192094592] Epoch[19] Batch [10]#011Speed: 2023.37 samples/sec#011loss=10.525600\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:53 INFO 140650192094592] processed a total of 648 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459812.674311, \"EndTime\": 1631459813.2910979, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 616.3175106048584, \"count\": 1, \"min\": 616.3175106048584, \"max\": 616.3175106048584}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:53 INFO 140650192094592] #throughput_metric: host=algo-1, train throughput=1051.2109218206283 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:53 INFO 140650192094592] #progress_metric: host=algo-1, completed 10.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:53 INFO 140650192094592] #quality_metric: host=algo-1, epoch=19, train loss <loss>=10.473207993940873\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:53 INFO 140650192094592] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:53 INFO 140650192094592] Epoch[20] Batch[0] avg_epoch_loss=10.123663\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:53 INFO 140650192094592] #quality_metric: host=algo-1, epoch=20, batch=0 train loss <loss>=10.123662948608398\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:53 INFO 140650192094592] Epoch[20] Batch[5] avg_epoch_loss=10.185908\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:53 INFO 140650192094592] #quality_metric: host=algo-1, epoch=20, batch=5 train loss <loss>=10.18590752283732\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:53 INFO 140650192094592] Epoch[20] Batch [5]#011Speed: 2040.70 samples/sec#011loss=10.185908\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:53 INFO 140650192094592] processed a total of 605 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459813.2911775, \"EndTime\": 1631459813.8075507, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 515.9103870391846, \"count\": 1, \"min\": 515.9103870391846, \"max\": 515.9103870391846}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:53 INFO 140650192094592] #throughput_metric: host=algo-1, train throughput=1172.4101236791319 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:53 INFO 140650192094592] #progress_metric: host=algo-1, completed 10.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:53 INFO 140650192094592] #quality_metric: host=algo-1, epoch=20, train loss <loss>=9.952366065979003\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:53 INFO 140650192094592] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:53 INFO 140650192094592] Saved checkpoint to \"/opt/ml/model/state_bb8a7fce-56fd-480a-beb8-7124348f59cd-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459813.807637, \"EndTime\": 1631459813.8207393, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 12.543916702270508, \"count\": 1, \"min\": 12.543916702270508, \"max\": 12.543916702270508}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:54 INFO 140650192094592] Epoch[21] Batch[0] avg_epoch_loss=10.552511\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:54 INFO 140650192094592] #quality_metric: host=algo-1, epoch=21, batch=0 train loss <loss>=10.552511215209961\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:54 INFO 140650192094592] Epoch[21] Batch[5] avg_epoch_loss=10.426974\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:54 INFO 140650192094592] #quality_metric: host=algo-1, epoch=21, batch=5 train loss <loss>=10.426973660786947\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:54 INFO 140650192094592] Epoch[21] Batch [5]#011Speed: 2119.35 samples/sec#011loss=10.426974\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:54 INFO 140650192094592] Epoch[21] Batch[10] avg_epoch_loss=10.273226\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:54 INFO 140650192094592] #quality_metric: host=algo-1, epoch=21, batch=10 train loss <loss>=10.088728713989259\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:54 INFO 140650192094592] Epoch[21] Batch [10]#011Speed: 1729.38 samples/sec#011loss=10.088729\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:54 INFO 140650192094592] processed a total of 694 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459813.8208091, \"EndTime\": 1631459814.3945067, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 573.6448764801025, \"count\": 1, \"min\": 573.6448764801025, \"max\": 573.6448764801025}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:54 INFO 140650192094592] #throughput_metric: host=algo-1, train throughput=1209.5804874562277 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:54 INFO 140650192094592] #progress_metric: host=algo-1, completed 11.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:54 INFO 140650192094592] #quality_metric: host=algo-1, epoch=21, train loss <loss>=10.273225957697088\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:54 INFO 140650192094592] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:54 INFO 140650192094592] Epoch[22] Batch[0] avg_epoch_loss=10.966107\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:54 INFO 140650192094592] #quality_metric: host=algo-1, epoch=22, batch=0 train loss <loss>=10.966107368469238\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:54 INFO 140650192094592] Epoch[22] Batch[5] avg_epoch_loss=10.593420\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:54 INFO 140650192094592] #quality_metric: host=algo-1, epoch=22, batch=5 train loss <loss>=10.593420028686523\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:54 INFO 140650192094592] Epoch[22] Batch [5]#011Speed: 2023.15 samples/sec#011loss=10.593420\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:54 INFO 140650192094592] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459814.394585, \"EndTime\": 1631459814.9277754, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 532.6962471008301, \"count\": 1, \"min\": 532.6962471008301, \"max\": 532.6962471008301}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:54 INFO 140650192094592] #throughput_metric: host=algo-1, train throughput=1188.0312566980717 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:54 INFO 140650192094592] #progress_metric: host=algo-1, completed 11.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:54 INFO 140650192094592] #quality_metric: host=algo-1, epoch=22, train loss <loss>=10.438053703308105\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:54 INFO 140650192094592] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:55 INFO 140650192094592] Epoch[23] Batch[0] avg_epoch_loss=9.533178\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:55 INFO 140650192094592] #quality_metric: host=algo-1, epoch=23, batch=0 train loss <loss>=9.533178329467773\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:55 INFO 140650192094592] Epoch[23] Batch[5] avg_epoch_loss=9.877605\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:55 INFO 140650192094592] #quality_metric: host=algo-1, epoch=23, batch=5 train loss <loss>=9.877605120340982\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:55 INFO 140650192094592] Epoch[23] Batch [5]#011Speed: 2136.92 samples/sec#011loss=9.877605\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:55 INFO 140650192094592] processed a total of 630 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459814.927851, \"EndTime\": 1631459815.4545858, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 526.2651443481445, \"count\": 1, \"min\": 526.2651443481445, \"max\": 526.2651443481445}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:55 INFO 140650192094592] #throughput_metric: host=algo-1, train throughput=1196.8961281321906 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:55 INFO 140650192094592] #progress_metric: host=algo-1, completed 12.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:55 INFO 140650192094592] #quality_metric: host=algo-1, epoch=23, train loss <loss>=9.747599601745605\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:55 INFO 140650192094592] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:55 INFO 140650192094592] Saved checkpoint to \"/opt/ml/model/state_0de02485-8290-41ba-b8bc-e2667ef85644-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459815.4546504, \"EndTime\": 1631459815.4676347, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 12.414216995239258, \"count\": 1, \"min\": 12.414216995239258, \"max\": 12.414216995239258}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:55 INFO 140650192094592] Epoch[24] Batch[0] avg_epoch_loss=9.611933\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:55 INFO 140650192094592] #quality_metric: host=algo-1, epoch=24, batch=0 train loss <loss>=9.611932754516602\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:55 INFO 140650192094592] Epoch[24] Batch[5] avg_epoch_loss=10.273383\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:55 INFO 140650192094592] #quality_metric: host=algo-1, epoch=24, batch=5 train loss <loss>=10.273382981618246\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:55 INFO 140650192094592] Epoch[24] Batch [5]#011Speed: 2136.53 samples/sec#011loss=10.273383\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:55 INFO 140650192094592] processed a total of 603 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459815.467702, \"EndTime\": 1631459815.9782076, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 510.4484558105469, \"count\": 1, \"min\": 510.4484558105469, \"max\": 510.4484558105469}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:55 INFO 140650192094592] #throughput_metric: host=algo-1, train throughput=1181.0648371086957 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:55 INFO 140650192094592] #progress_metric: host=algo-1, completed 12.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:55 INFO 140650192094592] #quality_metric: host=algo-1, epoch=24, train loss <loss>=10.186165142059327\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:55 INFO 140650192094592] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:56 INFO 140650192094592] Epoch[25] Batch[0] avg_epoch_loss=11.273740\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:56 INFO 140650192094592] #quality_metric: host=algo-1, epoch=25, batch=0 train loss <loss>=11.2737398147583\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:56 INFO 140650192094592] Epoch[25] Batch[5] avg_epoch_loss=10.718177\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:56 INFO 140650192094592] #quality_metric: host=algo-1, epoch=25, batch=5 train loss <loss>=10.718177477518717\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:56 INFO 140650192094592] Epoch[25] Batch [5]#011Speed: 2046.48 samples/sec#011loss=10.718177\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:56 INFO 140650192094592] processed a total of 632 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459815.9782872, \"EndTime\": 1631459816.5046604, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 525.7837772369385, \"count\": 1, \"min\": 525.7837772369385, \"max\": 525.7837772369385}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:56 INFO 140650192094592] #throughput_metric: host=algo-1, train throughput=1201.6935264589708 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:56 INFO 140650192094592] #progress_metric: host=algo-1, completed 13.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:56 INFO 140650192094592] #quality_metric: host=algo-1, epoch=25, train loss <loss>=10.70869951248169\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:56 INFO 140650192094592] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:56 INFO 140650192094592] Epoch[26] Batch[0] avg_epoch_loss=10.309958\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:56 INFO 140650192094592] #quality_metric: host=algo-1, epoch=26, batch=0 train loss <loss>=10.309957504272461\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:56 INFO 140650192094592] Epoch[26] Batch[5] avg_epoch_loss=10.447516\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:56 INFO 140650192094592] #quality_metric: host=algo-1, epoch=26, batch=5 train loss <loss>=10.447516282399496\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:56 INFO 140650192094592] Epoch[26] Batch [5]#011Speed: 2032.47 samples/sec#011loss=10.447516\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:57 INFO 140650192094592] processed a total of 624 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459816.5047607, \"EndTime\": 1631459817.038522, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 533.1923961639404, \"count\": 1, \"min\": 533.1923961639404, \"max\": 533.1923961639404}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:57 INFO 140650192094592] #throughput_metric: host=algo-1, train throughput=1169.9749647967599 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:57 INFO 140650192094592] #progress_metric: host=algo-1, completed 13.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:57 INFO 140650192094592] #quality_metric: host=algo-1, epoch=26, train loss <loss>=10.450645256042481\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:57 INFO 140650192094592] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:57 INFO 140650192094592] Epoch[27] Batch[0] avg_epoch_loss=10.005763\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:57 INFO 140650192094592] #quality_metric: host=algo-1, epoch=27, batch=0 train loss <loss>=10.005763053894043\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:57 INFO 140650192094592] Epoch[27] Batch[5] avg_epoch_loss=9.937721\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:57 INFO 140650192094592] #quality_metric: host=algo-1, epoch=27, batch=5 train loss <loss>=9.93772061665853\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:57 INFO 140650192094592] Epoch[27] Batch [5]#011Speed: 2131.40 samples/sec#011loss=9.937721\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:57 INFO 140650192094592] Epoch[27] Batch[10] avg_epoch_loss=10.030104\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:57 INFO 140650192094592] #quality_metric: host=algo-1, epoch=27, batch=10 train loss <loss>=10.140963363647462\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:57 INFO 140650192094592] Epoch[27] Batch [10]#011Speed: 1474.25 samples/sec#011loss=10.140963\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:57 INFO 140650192094592] processed a total of 696 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459817.0386307, \"EndTime\": 1631459817.6624196, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 623.2423782348633, \"count\": 1, \"min\": 623.2423782348633, \"max\": 623.2423782348633}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:57 INFO 140650192094592] #throughput_metric: host=algo-1, train throughput=1116.528638809377 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:57 INFO 140650192094592] #progress_metric: host=algo-1, completed 14.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:57 INFO 140650192094592] #quality_metric: host=algo-1, epoch=27, train loss <loss>=10.03010368347168\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:57 INFO 140650192094592] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:57 INFO 140650192094592] Epoch[28] Batch[0] avg_epoch_loss=10.833770\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:57 INFO 140650192094592] #quality_metric: host=algo-1, epoch=28, batch=0 train loss <loss>=10.833769798278809\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:58 INFO 140650192094592] Epoch[28] Batch[5] avg_epoch_loss=10.571537\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:58 INFO 140650192094592] #quality_metric: host=algo-1, epoch=28, batch=5 train loss <loss>=10.571537176767984\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:58 INFO 140650192094592] Epoch[28] Batch [5]#011Speed: 1730.15 samples/sec#011loss=10.571537\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:58 INFO 140650192094592] processed a total of 622 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459817.6625004, \"EndTime\": 1631459818.2753434, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 612.3435497283936, \"count\": 1, \"min\": 612.3435497283936, \"max\": 612.3435497283936}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:58 INFO 140650192094592] #throughput_metric: host=algo-1, train throughput=1015.5600924136347 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:58 INFO 140650192094592] #progress_metric: host=algo-1, completed 14.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:58 INFO 140650192094592] #quality_metric: host=algo-1, epoch=28, train loss <loss>=10.576037788391114\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:58 INFO 140650192094592] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:58 INFO 140650192094592] Epoch[29] Batch[0] avg_epoch_loss=9.901408\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:58 INFO 140650192094592] #quality_metric: host=algo-1, epoch=29, batch=0 train loss <loss>=9.901408195495605\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:58 INFO 140650192094592] Epoch[29] Batch[5] avg_epoch_loss=9.911369\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:58 INFO 140650192094592] #quality_metric: host=algo-1, epoch=29, batch=5 train loss <loss>=9.911369482676188\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:58 INFO 140650192094592] Epoch[29] Batch [5]#011Speed: 2158.47 samples/sec#011loss=9.911369\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:58 INFO 140650192094592] processed a total of 639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459818.2754297, \"EndTime\": 1631459818.82859, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 552.6869297027588, \"count\": 1, \"min\": 552.6869297027588, \"max\": 552.6869297027588}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:58 INFO 140650192094592] #throughput_metric: host=algo-1, train throughput=1155.8891861819122 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:58 INFO 140650192094592] #progress_metric: host=algo-1, completed 15.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:58 INFO 140650192094592] #quality_metric: host=algo-1, epoch=29, train loss <loss>=10.105382537841797\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:58 INFO 140650192094592] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:59 INFO 140650192094592] Epoch[30] Batch[0] avg_epoch_loss=10.312374\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:59 INFO 140650192094592] #quality_metric: host=algo-1, epoch=30, batch=0 train loss <loss>=10.312374114990234\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:59 INFO 140650192094592] Epoch[30] Batch[5] avg_epoch_loss=10.002101\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:59 INFO 140650192094592] #quality_metric: host=algo-1, epoch=30, batch=5 train loss <loss>=10.002100785573324\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:59 INFO 140650192094592] Epoch[30] Batch [5]#011Speed: 2064.53 samples/sec#011loss=10.002101\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:59 INFO 140650192094592] processed a total of 632 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459818.8286846, \"EndTime\": 1631459819.357281, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 528.0284881591797, \"count\": 1, \"min\": 528.0284881591797, \"max\": 528.0284881591797}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:59 INFO 140650192094592] #throughput_metric: host=algo-1, train throughput=1196.599310786276 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:59 INFO 140650192094592] #progress_metric: host=algo-1, completed 15.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:59 INFO 140650192094592] #quality_metric: host=algo-1, epoch=30, train loss <loss>=10.142196655273438\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:59 INFO 140650192094592] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:59 INFO 140650192094592] Epoch[31] Batch[0] avg_epoch_loss=10.485149\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:59 INFO 140650192094592] #quality_metric: host=algo-1, epoch=31, batch=0 train loss <loss>=10.485149383544922\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:59 INFO 140650192094592] Epoch[31] Batch[5] avg_epoch_loss=10.346072\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:59 INFO 140650192094592] #quality_metric: host=algo-1, epoch=31, batch=5 train loss <loss>=10.346072355906168\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:59 INFO 140650192094592] Epoch[31] Batch [5]#011Speed: 1949.30 samples/sec#011loss=10.346072\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:59 INFO 140650192094592] Epoch[31] Batch[10] avg_epoch_loss=10.267895\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:59 INFO 140650192094592] #quality_metric: host=algo-1, epoch=31, batch=10 train loss <loss>=10.174082565307618\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:59 INFO 140650192094592] Epoch[31] Batch [10]#011Speed: 1903.30 samples/sec#011loss=10.174083\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:59 INFO 140650192094592] processed a total of 668 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459819.357373, \"EndTime\": 1631459819.945131, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 587.1527194976807, \"count\": 1, \"min\": 587.1527194976807, \"max\": 587.1527194976807}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:59 INFO 140650192094592] #throughput_metric: host=algo-1, train throughput=1137.4309847170382 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:59 INFO 140650192094592] #progress_metric: host=algo-1, completed 16.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:59 INFO 140650192094592] #quality_metric: host=algo-1, epoch=31, train loss <loss>=10.267895178361373\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:16:59 INFO 140650192094592] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:00 INFO 140650192094592] Epoch[32] Batch[0] avg_epoch_loss=10.079673\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:00 INFO 140650192094592] #quality_metric: host=algo-1, epoch=32, batch=0 train loss <loss>=10.079672813415527\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:00 INFO 140650192094592] Epoch[32] Batch[5] avg_epoch_loss=10.151982\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:00 INFO 140650192094592] #quality_metric: host=algo-1, epoch=32, batch=5 train loss <loss>=10.1519824663798\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:00 INFO 140650192094592] Epoch[32] Batch [5]#011Speed: 1894.91 samples/sec#011loss=10.151982\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:00 INFO 140650192094592] processed a total of 610 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459819.945211, \"EndTime\": 1631459820.5062656, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 560.5206489562988, \"count\": 1, \"min\": 560.5206489562988, \"max\": 560.5206489562988}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:00 INFO 140650192094592] #throughput_metric: host=algo-1, train throughput=1088.0166018158236 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:00 INFO 140650192094592] #progress_metric: host=algo-1, completed 16.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:00 INFO 140650192094592] #quality_metric: host=algo-1, epoch=32, train loss <loss>=10.155450916290283\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:00 INFO 140650192094592] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:00 INFO 140650192094592] Epoch[33] Batch[0] avg_epoch_loss=10.411628\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:00 INFO 140650192094592] #quality_metric: host=algo-1, epoch=33, batch=0 train loss <loss>=10.411627769470215\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:00 INFO 140650192094592] Epoch[33] Batch[5] avg_epoch_loss=10.274577\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:00 INFO 140650192094592] #quality_metric: host=algo-1, epoch=33, batch=5 train loss <loss>=10.274577140808105\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:00 INFO 140650192094592] Epoch[33] Batch [5]#011Speed: 2121.26 samples/sec#011loss=10.274577\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:01 INFO 140650192094592] processed a total of 638 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459820.5063589, \"EndTime\": 1631459821.088168, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 581.2942981719971, \"count\": 1, \"min\": 581.2942981719971, \"max\": 581.2942981719971}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:01 INFO 140650192094592] #throughput_metric: host=algo-1, train throughput=1097.3041289258545 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:01 INFO 140650192094592] #progress_metric: host=algo-1, completed 17.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:01 INFO 140650192094592] #quality_metric: host=algo-1, epoch=33, train loss <loss>=10.25877275466919\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:01 INFO 140650192094592] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:01 INFO 140650192094592] Epoch[34] Batch[0] avg_epoch_loss=10.715241\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:01 INFO 140650192094592] #quality_metric: host=algo-1, epoch=34, batch=0 train loss <loss>=10.715241432189941\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:01 INFO 140650192094592] Epoch[34] Batch[5] avg_epoch_loss=10.461844\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:01 INFO 140650192094592] #quality_metric: host=algo-1, epoch=34, batch=5 train loss <loss>=10.461844444274902\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:01 INFO 140650192094592] Epoch[34] Batch [5]#011Speed: 1838.50 samples/sec#011loss=10.461844\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:01 INFO 140650192094592] processed a total of 626 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459821.0882597, \"EndTime\": 1631459821.6710951, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 582.3187828063965, \"count\": 1, \"min\": 582.3187828063965, \"max\": 582.3187828063965}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:01 INFO 140650192094592] #throughput_metric: host=algo-1, train throughput=1074.8396026715081 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:01 INFO 140650192094592] #progress_metric: host=algo-1, completed 17.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:01 INFO 140650192094592] #quality_metric: host=algo-1, epoch=34, train loss <loss>=10.17521619796753\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:01 INFO 140650192094592] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:01 INFO 140650192094592] Epoch[35] Batch[0] avg_epoch_loss=9.704660\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:01 INFO 140650192094592] #quality_metric: host=algo-1, epoch=35, batch=0 train loss <loss>=9.704660415649414\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:02 INFO 140650192094592] Epoch[35] Batch[5] avg_epoch_loss=9.971275\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:02 INFO 140650192094592] #quality_metric: host=algo-1, epoch=35, batch=5 train loss <loss>=9.971275170644125\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:02 INFO 140650192094592] Epoch[35] Batch [5]#011Speed: 1991.16 samples/sec#011loss=9.971275\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:02 INFO 140650192094592] Epoch[35] Batch[10] avg_epoch_loss=10.374417\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:02 INFO 140650192094592] #quality_metric: host=algo-1, epoch=35, batch=10 train loss <loss>=10.858186721801758\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:02 INFO 140650192094592] Epoch[35] Batch [10]#011Speed: 1913.83 samples/sec#011loss=10.858187\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:02 INFO 140650192094592] processed a total of 655 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459821.6711566, \"EndTime\": 1631459822.244554, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 572.8797912597656, \"count\": 1, \"min\": 572.8797912597656, \"max\": 572.8797912597656}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:02 INFO 140650192094592] #throughput_metric: host=algo-1, train throughput=1143.1070241976254 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:02 INFO 140650192094592] #progress_metric: host=algo-1, completed 18.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:02 INFO 140650192094592] #quality_metric: host=algo-1, epoch=35, train loss <loss>=10.374416784806685\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:02 INFO 140650192094592] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:02 INFO 140650192094592] Epoch[36] Batch[0] avg_epoch_loss=10.525272\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:02 INFO 140650192094592] #quality_metric: host=algo-1, epoch=36, batch=0 train loss <loss>=10.525272369384766\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:02 INFO 140650192094592] Epoch[36] Batch[5] avg_epoch_loss=10.301587\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:02 INFO 140650192094592] #quality_metric: host=algo-1, epoch=36, batch=5 train loss <loss>=10.301586786905924\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:02 INFO 140650192094592] Epoch[36] Batch [5]#011Speed: 1686.82 samples/sec#011loss=10.301587\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:02 INFO 140650192094592] processed a total of 631 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459822.2446373, \"EndTime\": 1631459822.8956237, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 650.4166126251221, \"count\": 1, \"min\": 650.4166126251221, \"max\": 650.4166126251221}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:02 INFO 140650192094592] #throughput_metric: host=algo-1, train throughput=969.9554252478849 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:02 INFO 140650192094592] #progress_metric: host=algo-1, completed 18.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:02 INFO 140650192094592] #quality_metric: host=algo-1, epoch=36, train loss <loss>=10.297134399414062\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:02 INFO 140650192094592] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:03 INFO 140650192094592] Epoch[37] Batch[0] avg_epoch_loss=9.568629\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:03 INFO 140650192094592] #quality_metric: host=algo-1, epoch=37, batch=0 train loss <loss>=9.568629264831543\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:03 INFO 140650192094592] Epoch[37] Batch[5] avg_epoch_loss=9.857082\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:03 INFO 140650192094592] #quality_metric: host=algo-1, epoch=37, batch=5 train loss <loss>=9.857081731160482\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:03 INFO 140650192094592] Epoch[37] Batch [5]#011Speed: 1826.20 samples/sec#011loss=9.857082\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:03 INFO 140650192094592] processed a total of 638 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459822.8957143, \"EndTime\": 1631459823.4778216, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 581.5906524658203, \"count\": 1, \"min\": 581.5906524658203, \"max\": 581.5906524658203}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:03 INFO 140650192094592] #throughput_metric: host=algo-1, train throughput=1096.729831349003 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:03 INFO 140650192094592] #progress_metric: host=algo-1, completed 19.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:03 INFO 140650192094592] #quality_metric: host=algo-1, epoch=37, train loss <loss>=9.974939727783203\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:03 INFO 140650192094592] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:03 INFO 140650192094592] Epoch[38] Batch[0] avg_epoch_loss=9.477259\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:03 INFO 140650192094592] #quality_metric: host=algo-1, epoch=38, batch=0 train loss <loss>=9.477258682250977\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:03 INFO 140650192094592] Epoch[38] Batch[5] avg_epoch_loss=10.066339\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:03 INFO 140650192094592] #quality_metric: host=algo-1, epoch=38, batch=5 train loss <loss>=10.066339333852133\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:03 INFO 140650192094592] Epoch[38] Batch [5]#011Speed: 1651.21 samples/sec#011loss=10.066339\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:04 INFO 140650192094592] Epoch[38] Batch[10] avg_epoch_loss=10.325136\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:04 INFO 140650192094592] #quality_metric: host=algo-1, epoch=38, batch=10 train loss <loss>=10.635691070556641\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:04 INFO 140650192094592] Epoch[38] Batch [10]#011Speed: 1486.12 samples/sec#011loss=10.635691\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:04 INFO 140650192094592] processed a total of 659 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459823.477918, \"EndTime\": 1631459824.1757836, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 697.2856521606445, \"count\": 1, \"min\": 697.2856521606445, \"max\": 697.2856521606445}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:04 INFO 140650192094592] #throughput_metric: host=algo-1, train throughput=944.9324015444121 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:04 INFO 140650192094592] #progress_metric: host=algo-1, completed 19.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:04 INFO 140650192094592] #quality_metric: host=algo-1, epoch=38, train loss <loss>=10.325135577808727\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:04 INFO 140650192094592] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:04 INFO 140650192094592] Epoch[39] Batch[0] avg_epoch_loss=10.396589\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:04 INFO 140650192094592] #quality_metric: host=algo-1, epoch=39, batch=0 train loss <loss>=10.396589279174805\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:04 INFO 140650192094592] Epoch[39] Batch[5] avg_epoch_loss=10.724191\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:04 INFO 140650192094592] #quality_metric: host=algo-1, epoch=39, batch=5 train loss <loss>=10.724191188812256\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:04 INFO 140650192094592] Epoch[39] Batch [5]#011Speed: 1532.88 samples/sec#011loss=10.724191\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:04 INFO 140650192094592] processed a total of 625 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459824.175866, \"EndTime\": 1631459824.86722, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 690.8717155456543, \"count\": 1, \"min\": 690.8717155456543, \"max\": 690.8717155456543}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:04 INFO 140650192094592] #throughput_metric: host=algo-1, train throughput=904.4775336646549 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:04 INFO 140650192094592] #progress_metric: host=algo-1, completed 20.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:04 INFO 140650192094592] #quality_metric: host=algo-1, epoch=39, train loss <loss>=10.398866176605225\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:04 INFO 140650192094592] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:05 INFO 140650192094592] Epoch[40] Batch[0] avg_epoch_loss=10.472036\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:05 INFO 140650192094592] #quality_metric: host=algo-1, epoch=40, batch=0 train loss <loss>=10.472036361694336\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:05 INFO 140650192094592] Epoch[40] Batch[5] avg_epoch_loss=10.151624\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:05 INFO 140650192094592] #quality_metric: host=algo-1, epoch=40, batch=5 train loss <loss>=10.151624202728271\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:05 INFO 140650192094592] Epoch[40] Batch [5]#011Speed: 1893.68 samples/sec#011loss=10.151624\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:05 INFO 140650192094592] processed a total of 630 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459824.8673115, \"EndTime\": 1631459825.4439628, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 576.0915279388428, \"count\": 1, \"min\": 576.0915279388428, \"max\": 576.0915279388428}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:05 INFO 140650192094592] #throughput_metric: host=algo-1, train throughput=1093.3151501108869 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:05 INFO 140650192094592] #progress_metric: host=algo-1, completed 20.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:05 INFO 140650192094592] #quality_metric: host=algo-1, epoch=40, train loss <loss>=10.266918659210205\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:05 INFO 140650192094592] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:05 INFO 140650192094592] Epoch[41] Batch[0] avg_epoch_loss=9.820752\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:05 INFO 140650192094592] #quality_metric: host=algo-1, epoch=41, batch=0 train loss <loss>=9.820752143859863\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:05 INFO 140650192094592] Epoch[41] Batch[5] avg_epoch_loss=9.809598\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:05 INFO 140650192094592] #quality_metric: host=algo-1, epoch=41, batch=5 train loss <loss>=9.809598286946615\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:05 INFO 140650192094592] Epoch[41] Batch [5]#011Speed: 1869.47 samples/sec#011loss=9.809598\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:06 INFO 140650192094592] processed a total of 627 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459825.4440613, \"EndTime\": 1631459826.000831, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 556.2560558319092, \"count\": 1, \"min\": 556.2560558319092, \"max\": 556.2560558319092}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:06 INFO 140650192094592] #throughput_metric: host=algo-1, train throughput=1126.9077726566074 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:06 INFO 140650192094592] #progress_metric: host=algo-1, completed 21.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:06 INFO 140650192094592] #quality_metric: host=algo-1, epoch=41, train loss <loss>=9.990221977233887\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:06 INFO 140650192094592] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:06 INFO 140650192094592] Epoch[42] Batch[0] avg_epoch_loss=10.337097\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:06 INFO 140650192094592] #quality_metric: host=algo-1, epoch=42, batch=0 train loss <loss>=10.33709716796875\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:06 INFO 140650192094592] Epoch[42] Batch[5] avg_epoch_loss=10.174836\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:06 INFO 140650192094592] #quality_metric: host=algo-1, epoch=42, batch=5 train loss <loss>=10.174836158752441\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:06 INFO 140650192094592] Epoch[42] Batch [5]#011Speed: 1894.87 samples/sec#011loss=10.174836\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:06 INFO 140650192094592] processed a total of 626 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459826.0009253, \"EndTime\": 1631459826.5560577, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 554.5856952667236, \"count\": 1, \"min\": 554.5856952667236, \"max\": 554.5856952667236}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:06 INFO 140650192094592] #throughput_metric: host=algo-1, train throughput=1128.51050851508 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:06 INFO 140650192094592] #progress_metric: host=algo-1, completed 21.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:06 INFO 140650192094592] #quality_metric: host=algo-1, epoch=42, train loss <loss>=10.158233547210694\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:06 INFO 140650192094592] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:06 INFO 140650192094592] Epoch[43] Batch[0] avg_epoch_loss=10.535303\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:06 INFO 140650192094592] #quality_metric: host=algo-1, epoch=43, batch=0 train loss <loss>=10.535303115844727\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:06 INFO 140650192094592] Epoch[43] Batch[5] avg_epoch_loss=10.150913\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:06 INFO 140650192094592] #quality_metric: host=algo-1, epoch=43, batch=5 train loss <loss>=10.150912602742514\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:06 INFO 140650192094592] Epoch[43] Batch [5]#011Speed: 2089.68 samples/sec#011loss=10.150913\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:07 INFO 140650192094592] Epoch[43] Batch[10] avg_epoch_loss=9.816210\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:07 INFO 140650192094592] #quality_metric: host=algo-1, epoch=43, batch=10 train loss <loss>=9.414567375183106\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:07 INFO 140650192094592] Epoch[43] Batch [10]#011Speed: 1889.40 samples/sec#011loss=9.414567\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:07 INFO 140650192094592] processed a total of 683 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459826.5561461, \"EndTime\": 1631459827.1244662, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 567.7506923675537, \"count\": 1, \"min\": 567.7506923675537, \"max\": 567.7506923675537}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:07 INFO 140650192094592] #throughput_metric: host=algo-1, train throughput=1202.7614810286605 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:07 INFO 140650192094592] #progress_metric: host=algo-1, completed 22.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:07 INFO 140650192094592] #quality_metric: host=algo-1, epoch=43, train loss <loss>=9.816210226579146\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:07 INFO 140650192094592] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:07 INFO 140650192094592] Epoch[44] Batch[0] avg_epoch_loss=9.988308\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:07 INFO 140650192094592] #quality_metric: host=algo-1, epoch=44, batch=0 train loss <loss>=9.98830795288086\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:07 INFO 140650192094592] Epoch[44] Batch[5] avg_epoch_loss=9.943275\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:07 INFO 140650192094592] #quality_metric: host=algo-1, epoch=44, batch=5 train loss <loss>=9.943275292714437\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:07 INFO 140650192094592] Epoch[44] Batch [5]#011Speed: 1928.14 samples/sec#011loss=9.943275\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:07 INFO 140650192094592] processed a total of 628 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459827.1245348, \"EndTime\": 1631459827.6666603, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 541.6324138641357, \"count\": 1, \"min\": 541.6324138641357, \"max\": 541.6324138641357}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:07 INFO 140650192094592] #throughput_metric: host=algo-1, train throughput=1159.1829072617732 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:07 INFO 140650192094592] #progress_metric: host=algo-1, completed 22.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:07 INFO 140650192094592] #quality_metric: host=algo-1, epoch=44, train loss <loss>=9.857620429992675\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:07 INFO 140650192094592] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:07 INFO 140650192094592] Epoch[45] Batch[0] avg_epoch_loss=10.148737\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:07 INFO 140650192094592] #quality_metric: host=algo-1, epoch=45, batch=0 train loss <loss>=10.148736953735352\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:08 INFO 140650192094592] Epoch[45] Batch[5] avg_epoch_loss=10.318914\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:08 INFO 140650192094592] #quality_metric: host=algo-1, epoch=45, batch=5 train loss <loss>=10.31891393661499\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:08 INFO 140650192094592] Epoch[45] Batch [5]#011Speed: 2076.02 samples/sec#011loss=10.318914\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:08 INFO 140650192094592] processed a total of 639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459827.666749, \"EndTime\": 1631459828.2207227, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 553.4634590148926, \"count\": 1, \"min\": 553.4634590148926, \"max\": 553.4634590148926}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:08 INFO 140650192094592] #throughput_metric: host=algo-1, train throughput=1154.275778755791 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:08 INFO 140650192094592] #progress_metric: host=algo-1, completed 23.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:08 INFO 140650192094592] #quality_metric: host=algo-1, epoch=45, train loss <loss>=9.897504615783692\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:08 INFO 140650192094592] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:08 INFO 140650192094592] Epoch[46] Batch[0] avg_epoch_loss=9.410398\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:08 INFO 140650192094592] #quality_metric: host=algo-1, epoch=46, batch=0 train loss <loss>=9.410398483276367\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:08 INFO 140650192094592] Epoch[46] Batch[5] avg_epoch_loss=9.864044\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:08 INFO 140650192094592] #quality_metric: host=algo-1, epoch=46, batch=5 train loss <loss>=9.864044348398844\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:08 INFO 140650192094592] Epoch[46] Batch [5]#011Speed: 1905.89 samples/sec#011loss=9.864044\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:08 INFO 140650192094592] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459828.220815, \"EndTime\": 1631459828.8530421, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 631.6812038421631, \"count\": 1, \"min\": 631.6812038421631, \"max\": 631.6812038421631}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:08 INFO 140650192094592] #throughput_metric: host=algo-1, train throughput=1001.8423493044822 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:08 INFO 140650192094592] #progress_metric: host=algo-1, completed 23.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:08 INFO 140650192094592] #quality_metric: host=algo-1, epoch=46, train loss <loss>=9.981799983978272\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:08 INFO 140650192094592] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:09 INFO 140650192094592] Epoch[47] Batch[0] avg_epoch_loss=9.727430\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:09 INFO 140650192094592] #quality_metric: host=algo-1, epoch=47, batch=0 train loss <loss>=9.72743034362793\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:09 INFO 140650192094592] Epoch[47] Batch[5] avg_epoch_loss=10.317690\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:09 INFO 140650192094592] #quality_metric: host=algo-1, epoch=47, batch=5 train loss <loss>=10.317690372467041\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:09 INFO 140650192094592] Epoch[47] Batch [5]#011Speed: 1738.51 samples/sec#011loss=10.317690\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:09 INFO 140650192094592] Epoch[47] Batch[10] avg_epoch_loss=10.078733\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:09 INFO 140650192094592] #quality_metric: host=algo-1, epoch=47, batch=10 train loss <loss>=9.79198341369629\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:09 INFO 140650192094592] Epoch[47] Batch [10]#011Speed: 1768.08 samples/sec#011loss=9.791983\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:09 INFO 140650192094592] processed a total of 682 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459828.8531544, \"EndTime\": 1631459829.4727025, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 619.0080642700195, \"count\": 1, \"min\": 619.0080642700195, \"max\": 619.0080642700195}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:09 INFO 140650192094592] #throughput_metric: host=algo-1, train throughput=1101.57050571847 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:09 INFO 140650192094592] #progress_metric: host=algo-1, completed 24.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:09 INFO 140650192094592] #quality_metric: host=algo-1, epoch=47, train loss <loss>=10.07873266393488\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:09 INFO 140650192094592] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:09 INFO 140650192094592] Epoch[48] Batch[0] avg_epoch_loss=10.811315\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:09 INFO 140650192094592] #quality_metric: host=algo-1, epoch=48, batch=0 train loss <loss>=10.811314582824707\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:09 INFO 140650192094592] Epoch[48] Batch[5] avg_epoch_loss=10.261817\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:09 INFO 140650192094592] #quality_metric: host=algo-1, epoch=48, batch=5 train loss <loss>=10.26181666056315\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:09 INFO 140650192094592] Epoch[48] Batch [5]#011Speed: 2009.40 samples/sec#011loss=10.261817\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:10 INFO 140650192094592] Epoch[48] Batch[10] avg_epoch_loss=10.008192\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:10 INFO 140650192094592] #quality_metric: host=algo-1, epoch=48, batch=10 train loss <loss>=9.703841590881348\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:10 INFO 140650192094592] Epoch[48] Batch [10]#011Speed: 1947.25 samples/sec#011loss=9.703842\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:10 INFO 140650192094592] processed a total of 658 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459829.47278, \"EndTime\": 1631459830.0557265, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 582.4069976806641, \"count\": 1, \"min\": 582.4069976806641, \"max\": 582.4069976806641}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:10 INFO 140650192094592] #throughput_metric: host=algo-1, train throughput=1129.559678123717 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:10 INFO 140650192094592] #progress_metric: host=algo-1, completed 24.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:10 INFO 140650192094592] #quality_metric: host=algo-1, epoch=48, train loss <loss>=10.008191628889604\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:10 INFO 140650192094592] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:10 INFO 140650192094592] Epoch[49] Batch[0] avg_epoch_loss=10.423081\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:10 INFO 140650192094592] #quality_metric: host=algo-1, epoch=49, batch=0 train loss <loss>=10.423081398010254\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:10 INFO 140650192094592] Epoch[49] Batch[5] avg_epoch_loss=10.041001\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:10 INFO 140650192094592] #quality_metric: host=algo-1, epoch=49, batch=5 train loss <loss>=10.041000525156656\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:10 INFO 140650192094592] Epoch[49] Batch [5]#011Speed: 1973.69 samples/sec#011loss=10.041001\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:10 INFO 140650192094592] Epoch[49] Batch[10] avg_epoch_loss=9.195889\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:10 INFO 140650192094592] #quality_metric: host=algo-1, epoch=49, batch=10 train loss <loss>=8.181756138801575\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:10 INFO 140650192094592] Epoch[49] Batch [10]#011Speed: 1661.45 samples/sec#011loss=8.181756\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:10 INFO 140650192094592] processed a total of 646 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459830.0558097, \"EndTime\": 1631459830.6819415, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 625.591516494751, \"count\": 1, \"min\": 625.591516494751, \"max\": 625.591516494751}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:10 INFO 140650192094592] #throughput_metric: host=algo-1, train throughput=1032.42203128983 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:10 INFO 140650192094592] #progress_metric: host=algo-1, completed 25.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:10 INFO 140650192094592] #quality_metric: host=algo-1, epoch=49, train loss <loss>=9.195889440449802\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:10 INFO 140650192094592] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:10 INFO 140650192094592] Saved checkpoint to \"/opt/ml/model/state_914ea84d-7b15-4ded-b574-5f980824c52c-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459830.6820216, \"EndTime\": 1631459830.6939995, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 11.433839797973633, \"count\": 1, \"min\": 11.433839797973633, \"max\": 11.433839797973633}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:10 INFO 140650192094592] Epoch[50] Batch[0] avg_epoch_loss=10.339420\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:10 INFO 140650192094592] #quality_metric: host=algo-1, epoch=50, batch=0 train loss <loss>=10.339420318603516\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:11 INFO 140650192094592] Epoch[50] Batch[5] avg_epoch_loss=10.019577\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:11 INFO 140650192094592] #quality_metric: host=algo-1, epoch=50, batch=5 train loss <loss>=10.01957654953003\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:11 INFO 140650192094592] Epoch[50] Batch [5]#011Speed: 1929.54 samples/sec#011loss=10.019577\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:11 INFO 140650192094592] processed a total of 609 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459830.6940606, \"EndTime\": 1631459831.2366245, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 542.499303817749, \"count\": 1, \"min\": 542.499303817749, \"max\": 542.499303817749}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:11 INFO 140650192094592] #throughput_metric: host=algo-1, train throughput=1122.3221481818014 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:11 INFO 140650192094592] #progress_metric: host=algo-1, completed 25.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:11 INFO 140650192094592] #quality_metric: host=algo-1, epoch=50, train loss <loss>=9.895972728729248\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:11 INFO 140650192094592] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:11 INFO 140650192094592] Epoch[51] Batch[0] avg_epoch_loss=9.459160\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:11 INFO 140650192094592] #quality_metric: host=algo-1, epoch=51, batch=0 train loss <loss>=9.459159851074219\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:11 INFO 140650192094592] Epoch[51] Batch[5] avg_epoch_loss=9.752040\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:11 INFO 140650192094592] #quality_metric: host=algo-1, epoch=51, batch=5 train loss <loss>=9.752040386199951\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:11 INFO 140650192094592] Epoch[51] Batch [5]#011Speed: 2137.58 samples/sec#011loss=9.752040\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:11 INFO 140650192094592] processed a total of 617 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459831.236713, \"EndTime\": 1631459831.8147662, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 577.4788856506348, \"count\": 1, \"min\": 577.4788856506348, \"max\": 577.4788856506348}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:11 INFO 140650192094592] #throughput_metric: host=algo-1, train throughput=1068.1978597948953 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:11 INFO 140650192094592] #progress_metric: host=algo-1, completed 26.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:11 INFO 140650192094592] #quality_metric: host=algo-1, epoch=51, train loss <loss>=9.86041088104248\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:11 INFO 140650192094592] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:12 INFO 140650192094592] Epoch[52] Batch[0] avg_epoch_loss=10.132246\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:12 INFO 140650192094592] #quality_metric: host=algo-1, epoch=52, batch=0 train loss <loss>=10.132246017456055\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:12 INFO 140650192094592] Epoch[52] Batch[5] avg_epoch_loss=9.884766\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:12 INFO 140650192094592] #quality_metric: host=algo-1, epoch=52, batch=5 train loss <loss>=9.884766419728598\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:12 INFO 140650192094592] Epoch[52] Batch [5]#011Speed: 1849.75 samples/sec#011loss=9.884766\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:12 INFO 140650192094592] processed a total of 612 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459831.8148575, \"EndTime\": 1631459832.3830283, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 567.6546096801758, \"count\": 1, \"min\": 567.6546096801758, \"max\": 567.6546096801758}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:12 INFO 140650192094592] #throughput_metric: host=algo-1, train throughput=1077.856898473146 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:12 INFO 140650192094592] #progress_metric: host=algo-1, completed 26.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:12 INFO 140650192094592] #quality_metric: host=algo-1, epoch=52, train loss <loss>=10.135651683807373\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:12 INFO 140650192094592] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:12 INFO 140650192094592] Epoch[53] Batch[0] avg_epoch_loss=10.253538\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:12 INFO 140650192094592] #quality_metric: host=algo-1, epoch=53, batch=0 train loss <loss>=10.253538131713867\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:12 INFO 140650192094592] Epoch[53] Batch[5] avg_epoch_loss=9.916831\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:12 INFO 140650192094592] #quality_metric: host=algo-1, epoch=53, batch=5 train loss <loss>=9.916831493377686\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:12 INFO 140650192094592] Epoch[53] Batch [5]#011Speed: 1974.98 samples/sec#011loss=9.916831\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:12 INFO 140650192094592] processed a total of 622 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459832.3831296, \"EndTime\": 1631459832.9269998, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 543.3557033538818, \"count\": 1, \"min\": 543.3557033538818, \"max\": 543.3557033538818}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:12 INFO 140650192094592] #throughput_metric: host=algo-1, train throughput=1144.462951630302 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:12 INFO 140650192094592] #progress_metric: host=algo-1, completed 27.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:12 INFO 140650192094592] #quality_metric: host=algo-1, epoch=53, train loss <loss>=9.590027093887329\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:12 INFO 140650192094592] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:13 INFO 140650192094592] Epoch[54] Batch[0] avg_epoch_loss=9.622981\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:13 INFO 140650192094592] #quality_metric: host=algo-1, epoch=54, batch=0 train loss <loss>=9.622981071472168\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:13 INFO 140650192094592] Epoch[54] Batch[5] avg_epoch_loss=10.258526\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:13 INFO 140650192094592] #quality_metric: host=algo-1, epoch=54, batch=5 train loss <loss>=10.258525530497232\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:13 INFO 140650192094592] Epoch[54] Batch [5]#011Speed: 2091.31 samples/sec#011loss=10.258526\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:13 INFO 140650192094592] processed a total of 615 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459832.9270937, \"EndTime\": 1631459833.4815733, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 553.9991855621338, \"count\": 1, \"min\": 553.9991855621338, \"max\": 553.9991855621338}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:13 INFO 140650192094592] #throughput_metric: host=algo-1, train throughput=1109.8534449598956 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:13 INFO 140650192094592] #progress_metric: host=algo-1, completed 27.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:13 INFO 140650192094592] #quality_metric: host=algo-1, epoch=54, train loss <loss>=10.179281997680665\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:13 INFO 140650192094592] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:13 INFO 140650192094592] Epoch[55] Batch[0] avg_epoch_loss=10.027800\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:13 INFO 140650192094592] #quality_metric: host=algo-1, epoch=55, batch=0 train loss <loss>=10.027799606323242\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:13 INFO 140650192094592] Epoch[55] Batch[5] avg_epoch_loss=10.153062\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:13 INFO 140650192094592] #quality_metric: host=algo-1, epoch=55, batch=5 train loss <loss>=10.153061548868815\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:13 INFO 140650192094592] Epoch[55] Batch [5]#011Speed: 2111.18 samples/sec#011loss=10.153062\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:14 INFO 140650192094592] Epoch[55] Batch[10] avg_epoch_loss=9.952034\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:14 INFO 140650192094592] #quality_metric: host=algo-1, epoch=55, batch=10 train loss <loss>=9.7108003616333\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:14 INFO 140650192094592] Epoch[55] Batch [10]#011Speed: 2001.71 samples/sec#011loss=9.710800\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:14 INFO 140650192094592] processed a total of 660 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459833.4816644, \"EndTime\": 1631459834.0969262, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 614.7336959838867, \"count\": 1, \"min\": 614.7336959838867, \"max\": 614.7336959838867}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:14 INFO 140650192094592] #throughput_metric: host=algo-1, train throughput=1073.4278886291293 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:14 INFO 140650192094592] #progress_metric: host=algo-1, completed 28.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:14 INFO 140650192094592] #quality_metric: host=algo-1, epoch=55, train loss <loss>=9.952033736489035\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:14 INFO 140650192094592] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:14 INFO 140650192094592] Epoch[56] Batch[0] avg_epoch_loss=10.212973\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:14 INFO 140650192094592] #quality_metric: host=algo-1, epoch=56, batch=0 train loss <loss>=10.212972640991211\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:14 INFO 140650192094592] Epoch[56] Batch[5] avg_epoch_loss=10.312030\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:14 INFO 140650192094592] #quality_metric: host=algo-1, epoch=56, batch=5 train loss <loss>=10.312029520670572\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:14 INFO 140650192094592] Epoch[56] Batch [5]#011Speed: 2007.92 samples/sec#011loss=10.312030\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:14 INFO 140650192094592] processed a total of 609 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459834.0970087, \"EndTime\": 1631459834.652065, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 554.5613765716553, \"count\": 1, \"min\": 554.5613765716553, \"max\": 554.5613765716553}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:14 INFO 140650192094592] #throughput_metric: host=algo-1, train throughput=1097.91218958666 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:14 INFO 140650192094592] #progress_metric: host=algo-1, completed 28.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:14 INFO 140650192094592] #quality_metric: host=algo-1, epoch=56, train loss <loss>=9.952969360351563\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:14 INFO 140650192094592] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:14 INFO 140650192094592] Epoch[57] Batch[0] avg_epoch_loss=9.940937\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:14 INFO 140650192094592] #quality_metric: host=algo-1, epoch=57, batch=0 train loss <loss>=9.940937042236328\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:15 INFO 140650192094592] Epoch[57] Batch[5] avg_epoch_loss=9.878073\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:15 INFO 140650192094592] #quality_metric: host=algo-1, epoch=57, batch=5 train loss <loss>=9.878073374430338\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:15 INFO 140650192094592] Epoch[57] Batch [5]#011Speed: 2012.22 samples/sec#011loss=9.878073\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:15 INFO 140650192094592] processed a total of 615 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459834.652155, \"EndTime\": 1631459835.1991282, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 546.4611053466797, \"count\": 1, \"min\": 546.4611053466797, \"max\": 546.4611053466797}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:15 INFO 140650192094592] #throughput_metric: host=algo-1, train throughput=1125.1610471796134 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:15 INFO 140650192094592] #progress_metric: host=algo-1, completed 29.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:15 INFO 140650192094592] #quality_metric: host=algo-1, epoch=57, train loss <loss>=9.74446611404419\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:15 INFO 140650192094592] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:15 INFO 140650192094592] Epoch[58] Batch[0] avg_epoch_loss=9.261750\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:15 INFO 140650192094592] #quality_metric: host=algo-1, epoch=58, batch=0 train loss <loss>=9.261750221252441\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:15 INFO 140650192094592] Epoch[58] Batch[5] avg_epoch_loss=9.748260\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:15 INFO 140650192094592] #quality_metric: host=algo-1, epoch=58, batch=5 train loss <loss>=9.748260339101156\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:15 INFO 140650192094592] Epoch[58] Batch [5]#011Speed: 2031.95 samples/sec#011loss=9.748260\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:15 INFO 140650192094592] processed a total of 629 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459835.1992185, \"EndTime\": 1631459835.7513542, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 551.6173839569092, \"count\": 1, \"min\": 551.6173839569092, \"max\": 551.6173839569092}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:15 INFO 140650192094592] #throughput_metric: host=algo-1, train throughput=1140.0234623265799 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:15 INFO 140650192094592] #progress_metric: host=algo-1, completed 29.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:15 INFO 140650192094592] #quality_metric: host=algo-1, epoch=58, train loss <loss>=9.743031215667724\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:15 INFO 140650192094592] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:16 INFO 140650192094592] Epoch[59] Batch[0] avg_epoch_loss=10.092523\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:16 INFO 140650192094592] #quality_metric: host=algo-1, epoch=59, batch=0 train loss <loss>=10.092522621154785\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:16 INFO 140650192094592] Epoch[59] Batch[5] avg_epoch_loss=10.161465\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:16 INFO 140650192094592] #quality_metric: host=algo-1, epoch=59, batch=5 train loss <loss>=10.161465485890707\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:16 INFO 140650192094592] Epoch[59] Batch [5]#011Speed: 1983.73 samples/sec#011loss=10.161465\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:16 INFO 140650192094592] Epoch[59] Batch[10] avg_epoch_loss=10.248254\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:16 INFO 140650192094592] #quality_metric: host=algo-1, epoch=59, batch=10 train loss <loss>=10.352400588989259\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:16 INFO 140650192094592] Epoch[59] Batch [10]#011Speed: 1811.93 samples/sec#011loss=10.352401\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:16 INFO 140650192094592] processed a total of 680 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459835.7514417, \"EndTime\": 1631459836.3427806, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 590.8346176147461, \"count\": 1, \"min\": 590.8346176147461, \"max\": 590.8346176147461}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:16 INFO 140650192094592] #throughput_metric: host=algo-1, train throughput=1150.7206491519057 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:16 INFO 140650192094592] #progress_metric: host=algo-1, completed 30.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:16 INFO 140650192094592] #quality_metric: host=algo-1, epoch=59, train loss <loss>=10.24825416911732\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:16 INFO 140650192094592] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:16 INFO 140650192094592] Epoch[60] Batch[0] avg_epoch_loss=9.621758\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:16 INFO 140650192094592] #quality_metric: host=algo-1, epoch=60, batch=0 train loss <loss>=9.621757507324219\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:16 INFO 140650192094592] Epoch[60] Batch[5] avg_epoch_loss=9.809418\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:16 INFO 140650192094592] #quality_metric: host=algo-1, epoch=60, batch=5 train loss <loss>=9.809417565663656\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:16 INFO 140650192094592] Epoch[60] Batch [5]#011Speed: 2042.97 samples/sec#011loss=9.809418\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:16 INFO 140650192094592] Epoch[60] Batch[10] avg_epoch_loss=10.157490\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:16 INFO 140650192094592] #quality_metric: host=algo-1, epoch=60, batch=10 train loss <loss>=10.575176811218261\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:16 INFO 140650192094592] Epoch[60] Batch [10]#011Speed: 1749.85 samples/sec#011loss=10.575177\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:16 INFO 140650192094592] processed a total of 644 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459836.3428478, \"EndTime\": 1631459836.9366906, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 593.3241844177246, \"count\": 1, \"min\": 593.3241844177246, \"max\": 593.3241844177246}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:16 INFO 140650192094592] #throughput_metric: host=algo-1, train throughput=1085.1849850829497 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:16 INFO 140650192094592] #progress_metric: host=algo-1, completed 30.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:16 INFO 140650192094592] #quality_metric: host=algo-1, epoch=60, train loss <loss>=10.157489950006658\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:16 INFO 140650192094592] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:17 INFO 140650192094592] Epoch[61] Batch[0] avg_epoch_loss=9.353548\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:17 INFO 140650192094592] #quality_metric: host=algo-1, epoch=61, batch=0 train loss <loss>=9.353548049926758\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:17 INFO 140650192094592] Epoch[61] Batch[5] avg_epoch_loss=9.957697\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:17 INFO 140650192094592] #quality_metric: host=algo-1, epoch=61, batch=5 train loss <loss>=9.95769707361857\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:17 INFO 140650192094592] Epoch[61] Batch [5]#011Speed: 1828.30 samples/sec#011loss=9.957697\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:17 INFO 140650192094592] processed a total of 617 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459836.9367747, \"EndTime\": 1631459837.5342326, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 596.9483852386475, \"count\": 1, \"min\": 596.9483852386475, \"max\": 596.9483852386475}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:17 INFO 140650192094592] #throughput_metric: host=algo-1, train throughput=1033.3656778847171 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:17 INFO 140650192094592] #progress_metric: host=algo-1, completed 31.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:17 INFO 140650192094592] #quality_metric: host=algo-1, epoch=61, train loss <loss>=10.025362777709962\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:17 INFO 140650192094592] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:17 INFO 140650192094592] Epoch[62] Batch[0] avg_epoch_loss=11.292860\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:17 INFO 140650192094592] #quality_metric: host=algo-1, epoch=62, batch=0 train loss <loss>=11.29286003112793\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:17 INFO 140650192094592] Epoch[62] Batch[5] avg_epoch_loss=10.517880\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:17 INFO 140650192094592] #quality_metric: host=algo-1, epoch=62, batch=5 train loss <loss>=10.5178804397583\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:17 INFO 140650192094592] Epoch[62] Batch [5]#011Speed: 1970.99 samples/sec#011loss=10.517880\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:18 INFO 140650192094592] Epoch[62] Batch[10] avg_epoch_loss=10.336282\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:18 INFO 140650192094592] #quality_metric: host=algo-1, epoch=62, batch=10 train loss <loss>=10.11836280822754\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:18 INFO 140650192094592] Epoch[62] Batch [10]#011Speed: 2086.34 samples/sec#011loss=10.118363\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:18 INFO 140650192094592] processed a total of 645 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459837.5343232, \"EndTime\": 1631459838.1059387, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 571.1057186126709, \"count\": 1, \"min\": 571.1057186126709, \"max\": 571.1057186126709}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:18 INFO 140650192094592] #throughput_metric: host=algo-1, train throughput=1129.1528742972341 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:18 INFO 140650192094592] #progress_metric: host=algo-1, completed 31.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:18 INFO 140650192094592] #quality_metric: host=algo-1, epoch=62, train loss <loss>=10.336281516335227\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:18 INFO 140650192094592] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:18 INFO 140650192094592] Epoch[63] Batch[0] avg_epoch_loss=10.342625\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:18 INFO 140650192094592] #quality_metric: host=algo-1, epoch=63, batch=0 train loss <loss>=10.34262466430664\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:18 INFO 140650192094592] Epoch[63] Batch[5] avg_epoch_loss=10.128976\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:18 INFO 140650192094592] #quality_metric: host=algo-1, epoch=63, batch=5 train loss <loss>=10.128976027170816\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:18 INFO 140650192094592] Epoch[63] Batch [5]#011Speed: 1625.92 samples/sec#011loss=10.128976\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:18 INFO 140650192094592] Epoch[63] Batch[10] avg_epoch_loss=9.889489\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:18 INFO 140650192094592] #quality_metric: host=algo-1, epoch=63, batch=10 train loss <loss>=9.602105522155762\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:18 INFO 140650192094592] Epoch[63] Batch [10]#011Speed: 1985.45 samples/sec#011loss=9.602106\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:18 INFO 140650192094592] processed a total of 656 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459838.1060214, \"EndTime\": 1631459838.7442586, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 637.7582550048828, \"count\": 1, \"min\": 637.7582550048828, \"max\": 637.7582550048828}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:18 INFO 140650192094592] #throughput_metric: host=algo-1, train throughput=1028.4053035994477 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:18 INFO 140650192094592] #progress_metric: host=algo-1, completed 32.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:18 INFO 140650192094592] #quality_metric: host=algo-1, epoch=63, train loss <loss>=9.889489433982156\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:18 INFO 140650192094592] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:19 INFO 140650192094592] Epoch[64] Batch[0] avg_epoch_loss=10.597888\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:19 INFO 140650192094592] #quality_metric: host=algo-1, epoch=64, batch=0 train loss <loss>=10.597887992858887\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:19 INFO 140650192094592] Epoch[64] Batch[5] avg_epoch_loss=10.131580\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:19 INFO 140650192094592] #quality_metric: host=algo-1, epoch=64, batch=5 train loss <loss>=10.131580193837484\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:19 INFO 140650192094592] Epoch[64] Batch [5]#011Speed: 1616.07 samples/sec#011loss=10.131580\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:19 INFO 140650192094592] Epoch[64] Batch[10] avg_epoch_loss=10.456569\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:19 INFO 140650192094592] #quality_metric: host=algo-1, epoch=64, batch=10 train loss <loss>=10.84655475616455\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:19 INFO 140650192094592] Epoch[64] Batch [10]#011Speed: 1617.70 samples/sec#011loss=10.846555\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:19 INFO 140650192094592] processed a total of 661 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459838.744344, \"EndTime\": 1631459839.42207, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 677.2265434265137, \"count\": 1, \"min\": 677.2265434265137, \"max\": 677.2265434265137}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:19 INFO 140650192094592] #throughput_metric: host=algo-1, train throughput=975.8675900520768 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:19 INFO 140650192094592] #progress_metric: host=algo-1, completed 32.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:19 INFO 140650192094592] #quality_metric: host=algo-1, epoch=64, train loss <loss>=10.456568631258877\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:19 INFO 140650192094592] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:19 INFO 140650192094592] Epoch[65] Batch[0] avg_epoch_loss=9.715574\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:19 INFO 140650192094592] #quality_metric: host=algo-1, epoch=65, batch=0 train loss <loss>=9.715574264526367\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:19 INFO 140650192094592] Epoch[65] Batch[5] avg_epoch_loss=9.708075\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:19 INFO 140650192094592] #quality_metric: host=algo-1, epoch=65, batch=5 train loss <loss>=9.708074887593588\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:19 INFO 140650192094592] Epoch[65] Batch [5]#011Speed: 2092.89 samples/sec#011loss=9.708075\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:20 INFO 140650192094592] Epoch[65] Batch[10] avg_epoch_loss=9.542641\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:20 INFO 140650192094592] #quality_metric: host=algo-1, epoch=65, batch=10 train loss <loss>=9.344120025634766\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:20 INFO 140650192094592] Epoch[65] Batch [10]#011Speed: 1856.18 samples/sec#011loss=9.344120\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:20 INFO 140650192094592] processed a total of 673 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459839.4221528, \"EndTime\": 1631459840.0166552, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 594.0337181091309, \"count\": 1, \"min\": 594.0337181091309, \"max\": 594.0337181091309}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:20 INFO 140650192094592] #throughput_metric: host=algo-1, train throughput=1132.6995598036974 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:20 INFO 140650192094592] #progress_metric: host=algo-1, completed 33.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:20 INFO 140650192094592] #quality_metric: host=algo-1, epoch=65, train loss <loss>=9.542640859430486\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:20 INFO 140650192094592] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:20 INFO 140650192094592] Epoch[66] Batch[0] avg_epoch_loss=9.398625\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:20 INFO 140650192094592] #quality_metric: host=algo-1, epoch=66, batch=0 train loss <loss>=9.398625373840332\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:20 INFO 140650192094592] Epoch[66] Batch[5] avg_epoch_loss=10.025144\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:20 INFO 140650192094592] #quality_metric: host=algo-1, epoch=66, batch=5 train loss <loss>=10.02514394124349\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:20 INFO 140650192094592] Epoch[66] Batch [5]#011Speed: 1914.45 samples/sec#011loss=10.025144\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:20 INFO 140650192094592] Epoch[66] Batch[10] avg_epoch_loss=9.913108\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:20 INFO 140650192094592] #quality_metric: host=algo-1, epoch=66, batch=10 train loss <loss>=9.778665161132812\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:20 INFO 140650192094592] Epoch[66] Batch [10]#011Speed: 1802.78 samples/sec#011loss=9.778665\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:20 INFO 140650192094592] processed a total of 678 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459840.0167396, \"EndTime\": 1631459840.6471927, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 629.9481391906738, \"count\": 1, \"min\": 629.9481391906738, \"max\": 629.9481391906738}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:20 INFO 140650192094592] #throughput_metric: host=algo-1, train throughput=1076.0738124447062 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:20 INFO 140650192094592] #progress_metric: host=algo-1, completed 33.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:20 INFO 140650192094592] #quality_metric: host=algo-1, epoch=66, train loss <loss>=9.913108132102273\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:20 INFO 140650192094592] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:20 INFO 140650192094592] Epoch[67] Batch[0] avg_epoch_loss=11.182284\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:20 INFO 140650192094592] #quality_metric: host=algo-1, epoch=67, batch=0 train loss <loss>=11.182284355163574\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:21 INFO 140650192094592] Epoch[67] Batch[5] avg_epoch_loss=10.098004\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:21 INFO 140650192094592] #quality_metric: host=algo-1, epoch=67, batch=5 train loss <loss>=10.09800354639689\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:21 INFO 140650192094592] Epoch[67] Batch [5]#011Speed: 2130.44 samples/sec#011loss=10.098004\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:21 INFO 140650192094592] processed a total of 606 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459840.6472769, \"EndTime\": 1631459841.1722825, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 524.5165824890137, \"count\": 1, \"min\": 524.5165824890137, \"max\": 524.5165824890137}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:21 INFO 140650192094592] #throughput_metric: host=algo-1, train throughput=1155.0670859615 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:21 INFO 140650192094592] #progress_metric: host=algo-1, completed 34.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:21 INFO 140650192094592] #quality_metric: host=algo-1, epoch=67, train loss <loss>=10.409167766571045\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:21 INFO 140650192094592] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:21 INFO 140650192094592] Epoch[68] Batch[0] avg_epoch_loss=10.061582\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:21 INFO 140650192094592] #quality_metric: host=algo-1, epoch=68, batch=0 train loss <loss>=10.0615816116333\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:21 INFO 140650192094592] Epoch[68] Batch[5] avg_epoch_loss=9.884869\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:21 INFO 140650192094592] #quality_metric: host=algo-1, epoch=68, batch=5 train loss <loss>=9.88486878077189\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:21 INFO 140650192094592] Epoch[68] Batch [5]#011Speed: 2149.64 samples/sec#011loss=9.884869\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:21 INFO 140650192094592] processed a total of 635 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459841.172371, \"EndTime\": 1631459841.6952348, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 522.3383903503418, \"count\": 1, \"min\": 522.3383903503418, \"max\": 522.3383903503418}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:21 INFO 140650192094592] #throughput_metric: host=algo-1, train throughput=1215.4096340787808 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:21 INFO 140650192094592] #progress_metric: host=algo-1, completed 34.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:21 INFO 140650192094592] #quality_metric: host=algo-1, epoch=68, train loss <loss>=9.746240139007568\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:21 INFO 140650192094592] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:21 INFO 140650192094592] Epoch[69] Batch[0] avg_epoch_loss=8.998522\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:21 INFO 140650192094592] #quality_metric: host=algo-1, epoch=69, batch=0 train loss <loss>=8.99852180480957\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:22 INFO 140650192094592] Epoch[69] Batch[5] avg_epoch_loss=9.889406\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:22 INFO 140650192094592] #quality_metric: host=algo-1, epoch=69, batch=5 train loss <loss>=9.889406045277914\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:22 INFO 140650192094592] Epoch[69] Batch [5]#011Speed: 2130.95 samples/sec#011loss=9.889406\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:22 INFO 140650192094592] processed a total of 635 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459841.6953168, \"EndTime\": 1631459842.2479138, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 552.0365238189697, \"count\": 1, \"min\": 552.0365238189697, \"max\": 552.0365238189697}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:22 INFO 140650192094592] #throughput_metric: host=algo-1, train throughput=1150.0045941754147 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:22 INFO 140650192094592] #progress_metric: host=algo-1, completed 35.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:22 INFO 140650192094592] #quality_metric: host=algo-1, epoch=69, train loss <loss>=9.829223537445069\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:22 INFO 140650192094592] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:22 INFO 140650192094592] Epoch[70] Batch[0] avg_epoch_loss=10.036464\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:22 INFO 140650192094592] #quality_metric: host=algo-1, epoch=70, batch=0 train loss <loss>=10.036463737487793\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:22 INFO 140650192094592] Epoch[70] Batch[5] avg_epoch_loss=10.210212\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:22 INFO 140650192094592] #quality_metric: host=algo-1, epoch=70, batch=5 train loss <loss>=10.210212071736654\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:22 INFO 140650192094592] Epoch[70] Batch [5]#011Speed: 2034.33 samples/sec#011loss=10.210212\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:22 INFO 140650192094592] processed a total of 622 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459842.2480068, \"EndTime\": 1631459842.788843, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 540.2507781982422, \"count\": 1, \"min\": 540.2507781982422, \"max\": 540.2507781982422}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:22 INFO 140650192094592] #throughput_metric: host=algo-1, train throughput=1151.0245906557625 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:22 INFO 140650192094592] #progress_metric: host=algo-1, completed 35.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:22 INFO 140650192094592] #quality_metric: host=algo-1, epoch=70, train loss <loss>=10.31108455657959\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:22 INFO 140650192094592] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:23 INFO 140650192094592] Epoch[71] Batch[0] avg_epoch_loss=9.478346\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:23 INFO 140650192094592] #quality_metric: host=algo-1, epoch=71, batch=0 train loss <loss>=9.47834587097168\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:23 INFO 140650192094592] Epoch[71] Batch[5] avg_epoch_loss=9.841008\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:23 INFO 140650192094592] #quality_metric: host=algo-1, epoch=71, batch=5 train loss <loss>=9.841007709503174\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:23 INFO 140650192094592] Epoch[71] Batch [5]#011Speed: 2132.99 samples/sec#011loss=9.841008\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:23 INFO 140650192094592] Epoch[71] Batch[10] avg_epoch_loss=9.607423\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:23 INFO 140650192094592] #quality_metric: host=algo-1, epoch=71, batch=10 train loss <loss>=9.327122402191161\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:23 INFO 140650192094592] Epoch[71] Batch [10]#011Speed: 1964.35 samples/sec#011loss=9.327122\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:23 INFO 140650192094592] processed a total of 660 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459842.7889385, \"EndTime\": 1631459843.346111, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 556.6232204437256, \"count\": 1, \"min\": 556.6232204437256, \"max\": 556.6232204437256}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:23 INFO 140650192094592] #throughput_metric: host=algo-1, train throughput=1185.4633375514688 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:23 INFO 140650192094592] #progress_metric: host=algo-1, completed 36.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:23 INFO 140650192094592] #quality_metric: host=algo-1, epoch=71, train loss <loss>=9.607423478906805\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:23 INFO 140650192094592] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:23 INFO 140650192094592] Epoch[72] Batch[0] avg_epoch_loss=9.599543\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:23 INFO 140650192094592] #quality_metric: host=algo-1, epoch=72, batch=0 train loss <loss>=9.599542617797852\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:23 INFO 140650192094592] Epoch[72] Batch[5] avg_epoch_loss=9.964185\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:23 INFO 140650192094592] #quality_metric: host=algo-1, epoch=72, batch=5 train loss <loss>=9.964184761047363\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:23 INFO 140650192094592] Epoch[72] Batch [5]#011Speed: 2131.83 samples/sec#011loss=9.964185\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:23 INFO 140650192094592] processed a total of 625 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459843.3461943, \"EndTime\": 1631459843.8924818, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 545.8240509033203, \"count\": 1, \"min\": 545.8240509033203, \"max\": 545.8240509033203}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:23 INFO 140650192094592] #throughput_metric: host=algo-1, train throughput=1144.7976131416954 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:23 INFO 140650192094592] #progress_metric: host=algo-1, completed 36.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:23 INFO 140650192094592] #quality_metric: host=algo-1, epoch=72, train loss <loss>=9.871052837371826\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:23 INFO 140650192094592] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:24 INFO 140650192094592] Epoch[73] Batch[0] avg_epoch_loss=11.039769\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:24 INFO 140650192094592] #quality_metric: host=algo-1, epoch=73, batch=0 train loss <loss>=11.039769172668457\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:24 INFO 140650192094592] Epoch[73] Batch[5] avg_epoch_loss=10.361435\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:24 INFO 140650192094592] #quality_metric: host=algo-1, epoch=73, batch=5 train loss <loss>=10.361434777577719\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:24 INFO 140650192094592] Epoch[73] Batch [5]#011Speed: 1714.02 samples/sec#011loss=10.361435\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:24 INFO 140650192094592] processed a total of 631 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459843.8925745, \"EndTime\": 1631459844.4697924, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 576.7064094543457, \"count\": 1, \"min\": 576.7064094543457, \"max\": 576.7064094543457}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:24 INFO 140650192094592] #throughput_metric: host=algo-1, train throughput=1093.8964235377755 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:24 INFO 140650192094592] #progress_metric: host=algo-1, completed 37.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:24 INFO 140650192094592] #quality_metric: host=algo-1, epoch=73, train loss <loss>=10.165387821197509\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:24 INFO 140650192094592] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:24 INFO 140650192094592] Epoch[74] Batch[0] avg_epoch_loss=9.529246\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:24 INFO 140650192094592] #quality_metric: host=algo-1, epoch=74, batch=0 train loss <loss>=9.52924633026123\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:24 INFO 140650192094592] Epoch[74] Batch[5] avg_epoch_loss=9.838466\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:24 INFO 140650192094592] #quality_metric: host=algo-1, epoch=74, batch=5 train loss <loss>=9.838465531667074\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:24 INFO 140650192094592] Epoch[74] Batch [5]#011Speed: 2124.09 samples/sec#011loss=9.838466\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:25 INFO 140650192094592] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459844.4698842, \"EndTime\": 1631459845.0585198, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 588.1297588348389, \"count\": 1, \"min\": 588.1297588348389, \"max\": 588.1297588348389}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:25 INFO 140650192094592] #throughput_metric: host=algo-1, train throughput=1082.8625388731025 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:25 INFO 140650192094592] #progress_metric: host=algo-1, completed 37.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:25 INFO 140650192094592] #quality_metric: host=algo-1, epoch=74, train loss <loss>=9.953633689880371\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:25 INFO 140650192094592] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:25 INFO 140650192094592] Epoch[75] Batch[0] avg_epoch_loss=10.144457\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:25 INFO 140650192094592] #quality_metric: host=algo-1, epoch=75, batch=0 train loss <loss>=10.14445686340332\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:25 INFO 140650192094592] Epoch[75] Batch[5] avg_epoch_loss=9.939851\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:25 INFO 140650192094592] #quality_metric: host=algo-1, epoch=75, batch=5 train loss <loss>=9.93985112508138\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:25 INFO 140650192094592] Epoch[75] Batch [5]#011Speed: 2049.92 samples/sec#011loss=9.939851\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:25 INFO 140650192094592] processed a total of 630 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459845.0586073, \"EndTime\": 1631459845.6449847, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 585.8633518218994, \"count\": 1, \"min\": 585.8633518218994, \"max\": 585.8633518218994}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:25 INFO 140650192094592] #throughput_metric: host=algo-1, train throughput=1075.0832001894325 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:25 INFO 140650192094592] #progress_metric: host=algo-1, completed 38.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:25 INFO 140650192094592] #quality_metric: host=algo-1, epoch=75, train loss <loss>=9.877957630157471\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:25 INFO 140650192094592] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:25 INFO 140650192094592] Epoch[76] Batch[0] avg_epoch_loss=8.190951\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:25 INFO 140650192094592] #quality_metric: host=algo-1, epoch=76, batch=0 train loss <loss>=8.190951347351074\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:26 INFO 140650192094592] Epoch[76] Batch[5] avg_epoch_loss=9.770948\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:26 INFO 140650192094592] #quality_metric: host=algo-1, epoch=76, batch=5 train loss <loss>=9.770947615305582\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:26 INFO 140650192094592] Epoch[76] Batch [5]#011Speed: 2097.27 samples/sec#011loss=9.770948\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:26 INFO 140650192094592] processed a total of 618 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459845.6450837, \"EndTime\": 1631459846.173268, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 527.6520252227783, \"count\": 1, \"min\": 527.6520252227783, \"max\": 527.6520252227783}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:26 INFO 140650192094592] #throughput_metric: host=algo-1, train throughput=1171.004734472016 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:26 INFO 140650192094592] #progress_metric: host=algo-1, completed 38.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:26 INFO 140650192094592] #quality_metric: host=algo-1, epoch=76, train loss <loss>=9.611307525634766\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:26 INFO 140650192094592] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:26 INFO 140650192094592] Epoch[77] Batch[0] avg_epoch_loss=10.126904\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:26 INFO 140650192094592] #quality_metric: host=algo-1, epoch=77, batch=0 train loss <loss>=10.126903533935547\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:26 INFO 140650192094592] Epoch[77] Batch[5] avg_epoch_loss=10.202304\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:26 INFO 140650192094592] #quality_metric: host=algo-1, epoch=77, batch=5 train loss <loss>=10.202303568522135\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:26 INFO 140650192094592] Epoch[77] Batch [5]#011Speed: 1934.81 samples/sec#011loss=10.202304\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:26 INFO 140650192094592] processed a total of 617 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459846.173336, \"EndTime\": 1631459846.71675, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 542.8450107574463, \"count\": 1, \"min\": 542.8450107574463, \"max\": 542.8450107574463}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:26 INFO 140650192094592] #throughput_metric: host=algo-1, train throughput=1136.3368412917753 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:26 INFO 140650192094592] #progress_metric: host=algo-1, completed 39.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:26 INFO 140650192094592] #quality_metric: host=algo-1, epoch=77, train loss <loss>=10.073166847229004\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:26 INFO 140650192094592] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:27 INFO 140650192094592] Epoch[78] Batch[0] avg_epoch_loss=10.910173\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:27 INFO 140650192094592] #quality_metric: host=algo-1, epoch=78, batch=0 train loss <loss>=10.910173416137695\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:27 INFO 140650192094592] Epoch[78] Batch[5] avg_epoch_loss=9.996672\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:27 INFO 140650192094592] #quality_metric: host=algo-1, epoch=78, batch=5 train loss <loss>=9.9966721534729\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:27 INFO 140650192094592] Epoch[78] Batch [5]#011Speed: 2011.94 samples/sec#011loss=9.996672\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:27 INFO 140650192094592] processed a total of 624 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459846.7168386, \"EndTime\": 1631459847.3058434, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 588.4580612182617, \"count\": 1, \"min\": 588.4580612182617, \"max\": 588.4580612182617}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:27 INFO 140650192094592] #throughput_metric: host=algo-1, train throughput=1060.1664738535721 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:27 INFO 140650192094592] #progress_metric: host=algo-1, completed 39.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:27 INFO 140650192094592] #quality_metric: host=algo-1, epoch=78, train loss <loss>=9.957435894012452\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:27 INFO 140650192094592] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:27 INFO 140650192094592] Epoch[79] Batch[0] avg_epoch_loss=10.072476\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:27 INFO 140650192094592] #quality_metric: host=algo-1, epoch=79, batch=0 train loss <loss>=10.072476387023926\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:27 INFO 140650192094592] Epoch[79] Batch[5] avg_epoch_loss=9.953280\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:27 INFO 140650192094592] #quality_metric: host=algo-1, epoch=79, batch=5 train loss <loss>=9.953280131022135\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:27 INFO 140650192094592] Epoch[79] Batch [5]#011Speed: 2019.78 samples/sec#011loss=9.953280\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:27 INFO 140650192094592] processed a total of 608 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459847.3059332, \"EndTime\": 1631459847.8487225, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 542.2799587249756, \"count\": 1, \"min\": 542.2799587249756, \"max\": 542.2799587249756}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:27 INFO 140650192094592] #throughput_metric: host=algo-1, train throughput=1120.936803215817 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:27 INFO 140650192094592] #progress_metric: host=algo-1, completed 40.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:27 INFO 140650192094592] #quality_metric: host=algo-1, epoch=79, train loss <loss>=9.806919479370118\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:27 INFO 140650192094592] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:28 INFO 140650192094592] Epoch[80] Batch[0] avg_epoch_loss=9.218312\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:28 INFO 140650192094592] #quality_metric: host=algo-1, epoch=80, batch=0 train loss <loss>=9.21831226348877\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:28 INFO 140650192094592] Epoch[80] Batch[5] avg_epoch_loss=9.707932\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:28 INFO 140650192094592] #quality_metric: host=algo-1, epoch=80, batch=5 train loss <loss>=9.707932154337565\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:28 INFO 140650192094592] Epoch[80] Batch [5]#011Speed: 2017.50 samples/sec#011loss=9.707932\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:28 INFO 140650192094592] Epoch[80] Batch[10] avg_epoch_loss=9.497760\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:28 INFO 140650192094592] #quality_metric: host=algo-1, epoch=80, batch=10 train loss <loss>=9.245553398132325\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:28 INFO 140650192094592] Epoch[80] Batch [10]#011Speed: 1917.61 samples/sec#011loss=9.245553\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:28 INFO 140650192094592] processed a total of 651 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459847.8488076, \"EndTime\": 1631459848.437514, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 588.1657600402832, \"count\": 1, \"min\": 588.1657600402832, \"max\": 588.1657600402832}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:28 INFO 140650192094592] #throughput_metric: host=algo-1, train throughput=1106.6020646366135 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:28 INFO 140650192094592] #progress_metric: host=algo-1, completed 40.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:28 INFO 140650192094592] #quality_metric: host=algo-1, epoch=80, train loss <loss>=9.497759992426092\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:28 INFO 140650192094592] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:28 INFO 140650192094592] Epoch[81] Batch[0] avg_epoch_loss=10.224624\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:28 INFO 140650192094592] #quality_metric: host=algo-1, epoch=81, batch=0 train loss <loss>=10.224623680114746\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:28 INFO 140650192094592] Epoch[81] Batch[5] avg_epoch_loss=10.100674\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:28 INFO 140650192094592] #quality_metric: host=algo-1, epoch=81, batch=5 train loss <loss>=10.100674470265707\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:28 INFO 140650192094592] Epoch[81] Batch [5]#011Speed: 2115.07 samples/sec#011loss=10.100674\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:28 INFO 140650192094592] processed a total of 636 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459848.4375994, \"EndTime\": 1631459848.9734783, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 535.33935546875, \"count\": 1, \"min\": 535.33935546875, \"max\": 535.33935546875}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:28 INFO 140650192094592] #throughput_metric: host=algo-1, train throughput=1187.7643312357154 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:28 INFO 140650192094592] #progress_metric: host=algo-1, completed 41.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:28 INFO 140650192094592] #quality_metric: host=algo-1, epoch=81, train loss <loss>=10.00298900604248\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:28 INFO 140650192094592] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:29 INFO 140650192094592] Epoch[82] Batch[0] avg_epoch_loss=10.763310\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:29 INFO 140650192094592] #quality_metric: host=algo-1, epoch=82, batch=0 train loss <loss>=10.763310432434082\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:29 INFO 140650192094592] Epoch[82] Batch[5] avg_epoch_loss=9.974651\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:29 INFO 140650192094592] #quality_metric: host=algo-1, epoch=82, batch=5 train loss <loss>=9.974651336669922\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:29 INFO 140650192094592] Epoch[82] Batch [5]#011Speed: 2035.89 samples/sec#011loss=9.974651\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:29 INFO 140650192094592] processed a total of 624 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459848.97356, \"EndTime\": 1631459849.4944942, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 520.435094833374, \"count\": 1, \"min\": 520.435094833374, \"max\": 520.435094833374}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:29 INFO 140650192094592] #throughput_metric: host=algo-1, train throughput=1198.593191603949 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:29 INFO 140650192094592] #progress_metric: host=algo-1, completed 41.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:29 INFO 140650192094592] #quality_metric: host=algo-1, epoch=82, train loss <loss>=10.08801851272583\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:29 INFO 140650192094592] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:29 INFO 140650192094592] Epoch[83] Batch[0] avg_epoch_loss=9.595561\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:29 INFO 140650192094592] #quality_metric: host=algo-1, epoch=83, batch=0 train loss <loss>=9.595561027526855\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:29 INFO 140650192094592] Epoch[83] Batch[5] avg_epoch_loss=9.657182\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:29 INFO 140650192094592] #quality_metric: host=algo-1, epoch=83, batch=5 train loss <loss>=9.65718158086141\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:29 INFO 140650192094592] Epoch[83] Batch [5]#011Speed: 1931.15 samples/sec#011loss=9.657182\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:30 INFO 140650192094592] Epoch[83] Batch[10] avg_epoch_loss=9.402611\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:30 INFO 140650192094592] #quality_metric: host=algo-1, epoch=83, batch=10 train loss <loss>=9.097125625610351\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:30 INFO 140650192094592] Epoch[83] Batch [10]#011Speed: 1888.71 samples/sec#011loss=9.097126\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:30 INFO 140650192094592] processed a total of 660 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459849.494631, \"EndTime\": 1631459850.0960903, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 600.8877754211426, \"count\": 1, \"min\": 600.8877754211426, \"max\": 600.8877754211426}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:30 INFO 140650192094592] #throughput_metric: host=algo-1, train throughput=1098.1499844098473 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:30 INFO 140650192094592] #progress_metric: host=algo-1, completed 42.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:30 INFO 140650192094592] #quality_metric: host=algo-1, epoch=83, train loss <loss>=9.402610692110928\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:30 INFO 140650192094592] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:30 INFO 140650192094592] Epoch[84] Batch[0] avg_epoch_loss=10.145989\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:30 INFO 140650192094592] #quality_metric: host=algo-1, epoch=84, batch=0 train loss <loss>=10.145989418029785\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:30 INFO 140650192094592] Epoch[84] Batch[5] avg_epoch_loss=10.026516\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:30 INFO 140650192094592] #quality_metric: host=algo-1, epoch=84, batch=5 train loss <loss>=10.026516119639078\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:30 INFO 140650192094592] Epoch[84] Batch [5]#011Speed: 2091.63 samples/sec#011loss=10.026516\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:30 INFO 140650192094592] Epoch[84] Batch[10] avg_epoch_loss=9.769527\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:30 INFO 140650192094592] #quality_metric: host=algo-1, epoch=84, batch=10 train loss <loss>=9.461139678955078\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:30 INFO 140650192094592] Epoch[84] Batch [10]#011Speed: 1872.07 samples/sec#011loss=9.461140\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:30 INFO 140650192094592] processed a total of 673 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459850.096172, \"EndTime\": 1631459850.670402, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 573.7483501434326, \"count\": 1, \"min\": 573.7483501434326, \"max\": 573.7483501434326}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:30 INFO 140650192094592] #throughput_metric: host=algo-1, train throughput=1172.7371497181127 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:30 INFO 140650192094592] #progress_metric: host=algo-1, completed 42.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:30 INFO 140650192094592] #quality_metric: host=algo-1, epoch=84, train loss <loss>=9.769526828419078\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:30 INFO 140650192094592] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:30 INFO 140650192094592] Epoch[85] Batch[0] avg_epoch_loss=10.034805\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:30 INFO 140650192094592] #quality_metric: host=algo-1, epoch=85, batch=0 train loss <loss>=10.034805297851562\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:31 INFO 140650192094592] Epoch[85] Batch[5] avg_epoch_loss=9.907522\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:31 INFO 140650192094592] #quality_metric: host=algo-1, epoch=85, batch=5 train loss <loss>=9.907522201538086\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:31 INFO 140650192094592] Epoch[85] Batch [5]#011Speed: 2077.58 samples/sec#011loss=9.907522\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:31 INFO 140650192094592] processed a total of 594 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459850.6704865, \"EndTime\": 1631459851.2314658, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 560.4851245880127, \"count\": 1, \"min\": 560.4851245880127, \"max\": 560.4851245880127}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:31 INFO 140650192094592] #throughput_metric: host=algo-1, train throughput=1059.5577632482114 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:31 INFO 140650192094592] #progress_metric: host=algo-1, completed 43.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:31 INFO 140650192094592] #quality_metric: host=algo-1, epoch=85, train loss <loss>=9.650752639770507\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:31 INFO 140650192094592] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:31 INFO 140650192094592] Epoch[86] Batch[0] avg_epoch_loss=10.268645\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:31 INFO 140650192094592] #quality_metric: host=algo-1, epoch=86, batch=0 train loss <loss>=10.268645286560059\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:31 INFO 140650192094592] Epoch[86] Batch[5] avg_epoch_loss=10.100062\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:31 INFO 140650192094592] #quality_metric: host=algo-1, epoch=86, batch=5 train loss <loss>=10.100061575571695\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:31 INFO 140650192094592] Epoch[86] Batch [5]#011Speed: 2017.31 samples/sec#011loss=10.100062\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:31 INFO 140650192094592] Epoch[86] Batch[10] avg_epoch_loss=10.166432\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:31 INFO 140650192094592] #quality_metric: host=algo-1, epoch=86, batch=10 train loss <loss>=10.246075630187988\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:31 INFO 140650192094592] Epoch[86] Batch [10]#011Speed: 1974.64 samples/sec#011loss=10.246076\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:31 INFO 140650192094592] processed a total of 658 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459851.231554, \"EndTime\": 1631459851.820852, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 588.7999534606934, \"count\": 1, \"min\": 588.7999534606934, \"max\": 588.7999534606934}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:31 INFO 140650192094592] #throughput_metric: host=algo-1, train throughput=1117.2996948717648 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:31 INFO 140650192094592] #progress_metric: host=algo-1, completed 43.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:31 INFO 140650192094592] #quality_metric: host=algo-1, epoch=86, train loss <loss>=10.166431600397283\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:31 INFO 140650192094592] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:32 INFO 140650192094592] Epoch[87] Batch[0] avg_epoch_loss=10.802514\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:32 INFO 140650192094592] #quality_metric: host=algo-1, epoch=87, batch=0 train loss <loss>=10.80251407623291\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:32 INFO 140650192094592] Epoch[87] Batch[5] avg_epoch_loss=9.954930\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:32 INFO 140650192094592] #quality_metric: host=algo-1, epoch=87, batch=5 train loss <loss>=9.954930305480957\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:32 INFO 140650192094592] Epoch[87] Batch [5]#011Speed: 1993.03 samples/sec#011loss=9.954930\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:32 INFO 140650192094592] Epoch[87] Batch[10] avg_epoch_loss=9.787735\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:32 INFO 140650192094592] #quality_metric: host=algo-1, epoch=87, batch=10 train loss <loss>=9.587100601196289\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:32 INFO 140650192094592] Epoch[87] Batch [10]#011Speed: 1915.94 samples/sec#011loss=9.587101\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:32 INFO 140650192094592] processed a total of 653 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459851.8209348, \"EndTime\": 1631459852.4634075, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 641.9677734375, \"count\": 1, \"min\": 641.9677734375, \"max\": 641.9677734375}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:32 INFO 140650192094592] #throughput_metric: host=algo-1, train throughput=1017.0029638816378 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:32 INFO 140650192094592] #progress_metric: host=algo-1, completed 44.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:32 INFO 140650192094592] #quality_metric: host=algo-1, epoch=87, train loss <loss>=9.787734985351562\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:32 INFO 140650192094592] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:32 INFO 140650192094592] Epoch[88] Batch[0] avg_epoch_loss=9.691046\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:32 INFO 140650192094592] #quality_metric: host=algo-1, epoch=88, batch=0 train loss <loss>=9.691045761108398\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:32 INFO 140650192094592] Epoch[88] Batch[5] avg_epoch_loss=10.249430\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:32 INFO 140650192094592] #quality_metric: host=algo-1, epoch=88, batch=5 train loss <loss>=10.249430020650228\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:32 INFO 140650192094592] Epoch[88] Batch [5]#011Speed: 2045.60 samples/sec#011loss=10.249430\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:33 INFO 140650192094592] processed a total of 622 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459852.4634876, \"EndTime\": 1631459853.0457842, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 581.8307399749756, \"count\": 1, \"min\": 581.8307399749756, \"max\": 581.8307399749756}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:33 INFO 140650192094592] #throughput_metric: host=algo-1, train throughput=1068.7889118444605 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:33 INFO 140650192094592] #progress_metric: host=algo-1, completed 44.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:33 INFO 140650192094592] #quality_metric: host=algo-1, epoch=88, train loss <loss>=10.445395278930665\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:33 INFO 140650192094592] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:33 INFO 140650192094592] Epoch[89] Batch[0] avg_epoch_loss=10.057184\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:33 INFO 140650192094592] #quality_metric: host=algo-1, epoch=89, batch=0 train loss <loss>=10.057184219360352\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:33 INFO 140650192094592] Epoch[89] Batch[5] avg_epoch_loss=9.726496\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:33 INFO 140650192094592] #quality_metric: host=algo-1, epoch=89, batch=5 train loss <loss>=9.726496060689291\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:33 INFO 140650192094592] Epoch[89] Batch [5]#011Speed: 2038.00 samples/sec#011loss=9.726496\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:33 INFO 140650192094592] Epoch[89] Batch[10] avg_epoch_loss=10.363719\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:33 INFO 140650192094592] #quality_metric: host=algo-1, epoch=89, batch=10 train loss <loss>=11.128385543823242\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:33 INFO 140650192094592] Epoch[89] Batch [10]#011Speed: 1992.80 samples/sec#011loss=11.128386\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:33 INFO 140650192094592] processed a total of 643 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459853.045872, \"EndTime\": 1631459853.6339843, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 587.6021385192871, \"count\": 1, \"min\": 587.6021385192871, \"max\": 587.6021385192871}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:33 INFO 140650192094592] #throughput_metric: host=algo-1, train throughput=1094.0403526485761 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:33 INFO 140650192094592] #progress_metric: host=algo-1, completed 45.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:33 INFO 140650192094592] #quality_metric: host=algo-1, epoch=89, train loss <loss>=10.363718553022904\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:33 INFO 140650192094592] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:33 INFO 140650192094592] Loading parameters from best epoch (49)\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459853.6340718, \"EndTime\": 1631459853.6407928, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.deserialize.time\": {\"sum\": 6.159305572509766, \"count\": 1, \"min\": 6.159305572509766, \"max\": 6.159305572509766}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:33 INFO 140650192094592] stopping training now\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:33 INFO 140650192094592] #progress_metric: host=algo-1, completed 100 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:33 INFO 140650192094592] Final loss: 9.195889440449802 (occurred at epoch 49)\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:33 INFO 140650192094592] #quality_metric: host=algo-1, train final_loss <loss>=9.195889440449802\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:33 INFO 140650192094592] Worker algo-1 finished training.\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:33 WARNING 140650192094592] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:33 INFO 140650192094592] All workers finished. Serializing model for prediction.\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459853.6408482, \"EndTime\": 1631459853.7719307, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"get_graph.time\": {\"sum\": 130.3093433380127, \"count\": 1, \"min\": 130.3093433380127, \"max\": 130.3093433380127}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:33 INFO 140650192094592] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459853.772013, \"EndTime\": 1631459853.8058949, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"finalize.time\": {\"sum\": 164.31546211242676, \"count\": 1, \"min\": 164.31546211242676, \"max\": 164.31546211242676}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:33 INFO 140650192094592] Serializing to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:33 INFO 140650192094592] Saved checkpoint to \"/opt/ml/model/model_algo-1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459853.8059611, \"EndTime\": 1631459853.8113441, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"model.serialize.time\": {\"sum\": 5.341291427612305, \"count\": 1, \"min\": 5.341291427612305, \"max\": 5.341291427612305}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:33 INFO 140650192094592] Successfully serialized the model for prediction.\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:17:33 INFO 140650192094592] No test data passed, skipping evaluation.\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631459853.8113933, \"EndTime\": 1631459853.8167708, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"setuptime\": {\"sum\": 7.126092910766602, \"count\": 1, \"min\": 7.126092910766602, \"max\": 7.126092910766602}, \"totaltime\": {\"sum\": 53181.917667388916, \"count\": 1, \"min\": 53181.917667388916, \"max\": 53181.917667388916}}}\n",
      "\u001b[0m\n",
      "\n",
      "2021-09-12 15:17:57 Uploading - Uploading generated training model\n",
      "2021-09-12 15:17:57 Completed - Training job completed\n",
      "Training seconds: 102\n",
      "Billable seconds: 102\n",
      "-----------!Overwriting existing file\n",
      "Uploading file to s3://sagemaker-us-east-2-222195595958/deepar-covid-demo-notebook/data/train/train.json\n",
      "2021-09-12 15:23:46 Starting - Starting the training job...\n",
      "2021-09-12 15:24:09 Starting - Launching requested ML instancesProfilerReport-1631460226: InProgress\n",
      "......\n",
      "2021-09-12 15:25:10 Starting - Preparing the instances for training......\n",
      "2021-09-12 15:26:10 Downloading - Downloading input data...\n",
      "2021-09-12 15:26:30 Training - Downloading the training image..\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:26:58 INFO 139940971021696] Reading default configuration from /opt/amazon/lib/python3.6/site-packages/algorithm/resources/default-input.json: {'_kvstore': 'auto', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_tuning_objective_metric': '', 'cardinality': 'auto', 'dropout_rate': '0.10', 'early_stopping_patience': '', 'embedding_dimension': '10', 'learning_rate': '0.001', 'likelihood': 'student-t', 'mini_batch_size': '128', 'num_cells': '40', 'num_dynamic_feat': 'auto', 'num_eval_samples': '100', 'num_layers': '2', 'test_quantiles': '[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]'}\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:26:58 INFO 139940971021696] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {'prediction_length': '14', 'time_freq': 'D', 'context_length': '14', 'epochs': '200', 'learning_rate': '5E-4', 'early_stopping_patience': '40', 'mini_batch_size': '64'}\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:26:58 INFO 139940971021696] Final configuration: {'_kvstore': 'auto', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_tuning_objective_metric': '', 'cardinality': 'auto', 'dropout_rate': '0.10', 'early_stopping_patience': '40', 'embedding_dimension': '10', 'learning_rate': '5E-4', 'likelihood': 'student-t', 'mini_batch_size': '64', 'num_cells': '40', 'num_dynamic_feat': 'auto', 'num_eval_samples': '100', 'num_layers': '2', 'test_quantiles': '[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', 'prediction_length': '14', 'time_freq': 'D', 'context_length': '14', 'epochs': '200'}\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:26:58 INFO 139940971021696] Detected entry point for worker worker\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:26:59 INFO 139940971021696] Using early stopping with patience 40\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:26:59 INFO 139940971021696] random_seed is None\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:26:59 INFO 139940971021696] [cardinality=auto] `cat` field was NOT found in the file `/opt/ml/input/data/train/train.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:26:59 INFO 139940971021696] [num_dynamic_feat=auto] `dynamic_feat` field was NOT found in the file `/opt/ml/input/data/train/train.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:26:59 INFO 139940971021696] Training set statistics:\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:26:59 INFO 139940971021696] Integer time series\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:26:59 INFO 139940971021696] number of time series: 41\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:26:59 INFO 139940971021696] number of observations: 13495\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:26:59 INFO 139940971021696] mean target length: 329.1463414634146\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:26:59 INFO 139940971021696] min/mean/max target: 0.0/15312645.518488329/351400928.0\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:26:59 INFO 139940971021696] mean abs(target): 15312645.518488329\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:26:59 INFO 139940971021696] contains missing values: no\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:26:59 INFO 139940971021696] Small number of time series. Doing 16 passes over dataset with prob 0.975609756097561 per epoch.\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:26:59 INFO 139940971021696] No test channel found not running evaluations\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:26:59 INFO 139940971021696] #memory_usage::<batchbuffer> = 2.2568130493164062 mb\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:26:59 INFO 139940971021696] nvidia-smi took: 0.02521038055419922 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:26:59 INFO 139940971021696] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:26:59 INFO 139940971021696] Create Store: local\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631460419.1379848, \"EndTime\": 1631460419.224023, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"get_graph.time\": {\"sum\": 84.00988578796387, \"count\": 1, \"min\": 84.00988578796387, \"max\": 84.00988578796387}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:26:59 INFO 139940971021696] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:26:59 INFO 139940971021696] #memory_usage::<model> = 12 mb\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631460419.2240934, \"EndTime\": 1631460419.331182, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"initialize.time\": {\"sum\": 193.05825233459473, \"count\": 1, \"min\": 193.05825233459473, \"max\": 193.05825233459473}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:26:59 INFO 139940971021696] Epoch[0] Batch[0] avg_epoch_loss=15.049043\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:26:59 INFO 139940971021696] #quality_metric: host=algo-1, epoch=0, batch=0 train loss <loss>=15.049042701721191\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:26:59 INFO 139940971021696] Epoch[0] Batch[5] avg_epoch_loss=13.776680\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:26:59 INFO 139940971021696] #quality_metric: host=algo-1, epoch=0, batch=5 train loss <loss>=13.776679833730062\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:26:59 INFO 139940971021696] Epoch[0] Batch [5]#011Speed: 2107.24 samples/sec#011loss=13.776680\u001b[0m\n",
      "\u001b[34m/opt/amazon/python3.6/lib/python3.6/contextlib.py:99: DeprecationWarning: generator 'local_timer' raised StopIteration\n",
      "  self.gen.throw(type, value, traceback)\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:26:59 INFO 139940971021696] processed a total of 627 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631460419.3312492, \"EndTime\": 1631460419.990081, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"epochs\": {\"sum\": 200.0, \"count\": 1, \"min\": 200, \"max\": 200}, \"update.time\": {\"sum\": 658.710241317749, \"count\": 1, \"min\": 658.710241317749, \"max\": 658.710241317749}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:26:59 INFO 139940971021696] #throughput_metric: host=algo-1, train throughput=951.5979956520355 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:26:59 INFO 139940971021696] #progress_metric: host=algo-1, completed 0.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:26:59 INFO 139940971021696] #quality_metric: host=algo-1, epoch=0, train loss <loss>=13.506579685211182\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:26:59 INFO 139940971021696] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:00 INFO 139940971021696] Saved checkpoint to \"/opt/ml/model/state_446e2dd7-2509-4429-992e-151c993f46fd-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631460419.9901874, \"EndTime\": 1631460420.0110745, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 20.299911499023438, \"count\": 1, \"min\": 20.299911499023438, \"max\": 20.299911499023438}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:00 INFO 139940971021696] Epoch[1] Batch[0] avg_epoch_loss=13.416768\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:00 INFO 139940971021696] #quality_metric: host=algo-1, epoch=1, batch=0 train loss <loss>=13.416768074035645\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:00 INFO 139940971021696] Epoch[1] Batch[5] avg_epoch_loss=13.163088\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:00 INFO 139940971021696] #quality_metric: host=algo-1, epoch=1, batch=5 train loss <loss>=13.163088162740072\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:00 INFO 139940971021696] Epoch[1] Batch [5]#011Speed: 1990.69 samples/sec#011loss=13.163088\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:00 INFO 139940971021696] Epoch[1] Batch[10] avg_epoch_loss=13.146617\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:00 INFO 139940971021696] #quality_metric: host=algo-1, epoch=1, batch=10 train loss <loss>=13.126850509643555\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:00 INFO 139940971021696] Epoch[1] Batch [10]#011Speed: 1697.94 samples/sec#011loss=13.126851\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:00 INFO 139940971021696] processed a total of 658 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631460420.011147, \"EndTime\": 1631460420.6429825, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 631.7689418792725, \"count\": 1, \"min\": 631.7689418792725, \"max\": 631.7689418792725}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:00 INFO 139940971021696] #throughput_metric: host=algo-1, train throughput=1041.310957027396 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:00 INFO 139940971021696] #progress_metric: host=algo-1, completed 1.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:00 INFO 139940971021696] #quality_metric: host=algo-1, epoch=1, train loss <loss>=13.146616502241654\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:00 INFO 139940971021696] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:00 INFO 139940971021696] Saved checkpoint to \"/opt/ml/model/state_49fb178c-47ab-441e-af40-b6c03874e7b3-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631460420.6430705, \"EndTime\": 1631460420.655331, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 11.747121810913086, \"count\": 1, \"min\": 11.747121810913086, \"max\": 11.747121810913086}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:00 INFO 139940971021696] Epoch[2] Batch[0] avg_epoch_loss=11.459707\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:00 INFO 139940971021696] #quality_metric: host=algo-1, epoch=2, batch=0 train loss <loss>=11.459707260131836\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:01 INFO 139940971021696] Epoch[2] Batch[5] avg_epoch_loss=11.964536\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:01 INFO 139940971021696] #quality_metric: host=algo-1, epoch=2, batch=5 train loss <loss>=11.9645357131958\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:01 INFO 139940971021696] Epoch[2] Batch [5]#011Speed: 1555.54 samples/sec#011loss=11.964536\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:01 INFO 139940971021696] processed a total of 630 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631460420.6554065, \"EndTime\": 1631460421.2504551, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 594.9742794036865, \"count\": 1, \"min\": 594.9742794036865, \"max\": 594.9742794036865}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:01 INFO 139940971021696] #throughput_metric: host=algo-1, train throughput=1058.6283483122074 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:01 INFO 139940971021696] #progress_metric: host=algo-1, completed 1.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:01 INFO 139940971021696] #quality_metric: host=algo-1, epoch=2, train loss <loss>=11.902169704437256\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:01 INFO 139940971021696] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:01 INFO 139940971021696] Saved checkpoint to \"/opt/ml/model/state_a426b7bc-1011-4e2c-ab94-bea70cf79eaf-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631460421.250543, \"EndTime\": 1631460421.269509, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 18.45550537109375, \"count\": 1, \"min\": 18.45550537109375, \"max\": 18.45550537109375}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:01 INFO 139940971021696] Epoch[3] Batch[0] avg_epoch_loss=11.360671\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:01 INFO 139940971021696] #quality_metric: host=algo-1, epoch=3, batch=0 train loss <loss>=11.360671043395996\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:01 INFO 139940971021696] Epoch[3] Batch[5] avg_epoch_loss=11.838071\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:01 INFO 139940971021696] #quality_metric: host=algo-1, epoch=3, batch=5 train loss <loss>=11.838071346282959\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:01 INFO 139940971021696] Epoch[3] Batch [5]#011Speed: 1553.53 samples/sec#011loss=11.838071\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:01 INFO 139940971021696] Epoch[3] Batch[10] avg_epoch_loss=12.046649\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:01 INFO 139940971021696] #quality_metric: host=algo-1, epoch=3, batch=10 train loss <loss>=12.296941566467286\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:01 INFO 139940971021696] Epoch[3] Batch [10]#011Speed: 1685.65 samples/sec#011loss=12.296942\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:01 INFO 139940971021696] processed a total of 684 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631460421.2695749, \"EndTime\": 1631460421.968552, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 698.9233493804932, \"count\": 1, \"min\": 698.9233493804932, \"max\": 698.9233493804932}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:01 INFO 139940971021696] #throughput_metric: host=algo-1, train throughput=978.4855360738667 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:01 INFO 139940971021696] #progress_metric: host=algo-1, completed 2.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:01 INFO 139940971021696] #quality_metric: host=algo-1, epoch=3, train loss <loss>=12.046648719094016\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:01 INFO 139940971021696] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:02 INFO 139940971021696] Epoch[4] Batch[0] avg_epoch_loss=12.162571\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:02 INFO 139940971021696] #quality_metric: host=algo-1, epoch=4, batch=0 train loss <loss>=12.16257095336914\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:02 INFO 139940971021696] Epoch[4] Batch[5] avg_epoch_loss=11.750048\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:02 INFO 139940971021696] #quality_metric: host=algo-1, epoch=4, batch=5 train loss <loss>=11.75004800160726\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:02 INFO 139940971021696] Epoch[4] Batch [5]#011Speed: 2038.65 samples/sec#011loss=11.750048\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:02 INFO 139940971021696] Epoch[4] Batch[10] avg_epoch_loss=11.363774\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:02 INFO 139940971021696] #quality_metric: host=algo-1, epoch=4, batch=10 train loss <loss>=10.900246047973633\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:02 INFO 139940971021696] Epoch[4] Batch [10]#011Speed: 1985.46 samples/sec#011loss=10.900246\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:02 INFO 139940971021696] processed a total of 657 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631460421.9686372, \"EndTime\": 1631460422.5473497, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 578.1958103179932, \"count\": 1, \"min\": 578.1958103179932, \"max\": 578.1958103179932}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:02 INFO 139940971021696] #throughput_metric: host=algo-1, train throughput=1136.0440424280227 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:02 INFO 139940971021696] #progress_metric: host=algo-1, completed 2.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:02 INFO 139940971021696] #quality_metric: host=algo-1, epoch=4, train loss <loss>=11.363774386319248\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:02 INFO 139940971021696] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:02 INFO 139940971021696] Saved checkpoint to \"/opt/ml/model/state_cb19fba5-a270-4ed2-ad73-91e09d6c2a43-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631460422.5474381, \"EndTime\": 1631460422.566941, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 19.004106521606445, \"count\": 1, \"min\": 19.004106521606445, \"max\": 19.004106521606445}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:02 INFO 139940971021696] Epoch[5] Batch[0] avg_epoch_loss=10.521025\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:02 INFO 139940971021696] #quality_metric: host=algo-1, epoch=5, batch=0 train loss <loss>=10.521024703979492\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:03 INFO 139940971021696] Epoch[5] Batch[5] avg_epoch_loss=11.255079\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:03 INFO 139940971021696] #quality_metric: host=algo-1, epoch=5, batch=5 train loss <loss>=11.25507911046346\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:03 INFO 139940971021696] Epoch[5] Batch [5]#011Speed: 1518.21 samples/sec#011loss=11.255079\u001b[0m\n",
      "\n",
      "2021-09-12 15:27:10 Training - Training image download completed. Training in progress.\u001b[34m[09/12/2021 15:27:03 INFO 139940971021696] Epoch[5] Batch[10] avg_epoch_loss=10.949741\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:03 INFO 139940971021696] #quality_metric: host=algo-1, epoch=5, batch=10 train loss <loss>=10.583336067199706\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:03 INFO 139940971021696] Epoch[5] Batch [10]#011Speed: 1504.61 samples/sec#011loss=10.583336\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:03 INFO 139940971021696] processed a total of 641 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631460422.5670083, \"EndTime\": 1631460423.2661738, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 699.1064548492432, \"count\": 1, \"min\": 699.1064548492432, \"max\": 699.1064548492432}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:03 INFO 139940971021696] #throughput_metric: host=algo-1, train throughput=916.7164872649134 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:03 INFO 139940971021696] #progress_metric: host=algo-1, completed 3.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:03 INFO 139940971021696] #quality_metric: host=algo-1, epoch=5, train loss <loss>=10.94974136352539\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:03 INFO 139940971021696] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:03 INFO 139940971021696] Saved checkpoint to \"/opt/ml/model/state_ba4df301-4217-4a51-9f15-9d1389c92615-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631460423.2662659, \"EndTime\": 1631460423.286177, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 19.409656524658203, \"count\": 1, \"min\": 19.409656524658203, \"max\": 19.409656524658203}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:03 INFO 139940971021696] Epoch[6] Batch[0] avg_epoch_loss=11.101087\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:03 INFO 139940971021696] #quality_metric: host=algo-1, epoch=6, batch=0 train loss <loss>=11.101086616516113\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:03 INFO 139940971021696] Epoch[6] Batch[5] avg_epoch_loss=11.484727\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:03 INFO 139940971021696] #quality_metric: host=algo-1, epoch=6, batch=5 train loss <loss>=11.484726746877035\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:03 INFO 139940971021696] Epoch[6] Batch [5]#011Speed: 1531.67 samples/sec#011loss=11.484727\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:04 INFO 139940971021696] processed a total of 627 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631460423.286244, \"EndTime\": 1631460424.0173132, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 731.0049533843994, \"count\": 1, \"min\": 731.0049533843994, \"max\": 731.0049533843994}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:04 INFO 139940971021696] #throughput_metric: host=algo-1, train throughput=857.5935426497977 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:04 INFO 139940971021696] #progress_metric: host=algo-1, completed 3.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:04 INFO 139940971021696] #quality_metric: host=algo-1, epoch=6, train loss <loss>=11.307877922058106\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:04 INFO 139940971021696] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:04 INFO 139940971021696] Epoch[7] Batch[0] avg_epoch_loss=11.231376\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:04 INFO 139940971021696] #quality_metric: host=algo-1, epoch=7, batch=0 train loss <loss>=11.231375694274902\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:04 INFO 139940971021696] Epoch[7] Batch[5] avg_epoch_loss=10.868097\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:04 INFO 139940971021696] #quality_metric: host=algo-1, epoch=7, batch=5 train loss <loss>=10.868096828460693\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:04 INFO 139940971021696] Epoch[7] Batch [5]#011Speed: 1568.32 samples/sec#011loss=10.868097\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:04 INFO 139940971021696] Epoch[7] Batch[10] avg_epoch_loss=10.836721\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:04 INFO 139940971021696] #quality_metric: host=algo-1, epoch=7, batch=10 train loss <loss>=10.799070739746094\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:04 INFO 139940971021696] Epoch[7] Batch [10]#011Speed: 1780.11 samples/sec#011loss=10.799071\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:04 INFO 139940971021696] processed a total of 669 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631460424.0173852, \"EndTime\": 1631460424.7182133, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 700.2630233764648, \"count\": 1, \"min\": 700.2630233764648, \"max\": 700.2630233764648}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:04 INFO 139940971021696] #throughput_metric: host=algo-1, train throughput=955.1774236867877 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:04 INFO 139940971021696] #progress_metric: host=algo-1, completed 4.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:04 INFO 139940971021696] #quality_metric: host=algo-1, epoch=7, train loss <loss>=10.83672133359042\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:04 INFO 139940971021696] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:04 INFO 139940971021696] Saved checkpoint to \"/opt/ml/model/state_66926c1e-7091-4b62-ad2c-90df02b31be6-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631460424.7183034, \"EndTime\": 1631460424.738002, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 19.156217575073242, \"count\": 1, \"min\": 19.156217575073242, \"max\": 19.156217575073242}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:05 INFO 139940971021696] Epoch[8] Batch[0] avg_epoch_loss=10.473145\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:05 INFO 139940971021696] #quality_metric: host=algo-1, epoch=8, batch=0 train loss <loss>=10.47314453125\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:05 INFO 139940971021696] Epoch[8] Batch[5] avg_epoch_loss=10.368243\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:05 INFO 139940971021696] #quality_metric: host=algo-1, epoch=8, batch=5 train loss <loss>=10.368242899576822\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:05 INFO 139940971021696] Epoch[8] Batch [5]#011Speed: 1540.59 samples/sec#011loss=10.368243\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:05 INFO 139940971021696] Epoch[8] Batch[10] avg_epoch_loss=10.742416\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:05 INFO 139940971021696] #quality_metric: host=algo-1, epoch=8, batch=10 train loss <loss>=11.191422653198241\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:05 INFO 139940971021696] Epoch[8] Batch [10]#011Speed: 1531.99 samples/sec#011loss=11.191423\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:05 INFO 139940971021696] processed a total of 652 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631460424.7380888, \"EndTime\": 1631460425.4269588, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 688.8096332550049, \"count\": 1, \"min\": 688.8096332550049, \"max\": 688.8096332550049}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:05 INFO 139940971021696] #throughput_metric: host=algo-1, train throughput=946.3898466153423 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:05 INFO 139940971021696] #progress_metric: host=algo-1, completed 4.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:05 INFO 139940971021696] #quality_metric: host=algo-1, epoch=8, train loss <loss>=10.742415514859287\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:05 INFO 139940971021696] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:05 INFO 139940971021696] Saved checkpoint to \"/opt/ml/model/state_ec9de0f1-cb18-4dc5-92d1-a590a04c943f-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631460425.427046, \"EndTime\": 1631460425.4462557, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 18.719911575317383, \"count\": 1, \"min\": 18.719911575317383, \"max\": 18.719911575317383}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:05 INFO 139940971021696] Epoch[9] Batch[0] avg_epoch_loss=10.502746\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:05 INFO 139940971021696] #quality_metric: host=algo-1, epoch=9, batch=0 train loss <loss>=10.502745628356934\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:05 INFO 139940971021696] Epoch[9] Batch[5] avg_epoch_loss=10.264108\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:05 INFO 139940971021696] #quality_metric: host=algo-1, epoch=9, batch=5 train loss <loss>=10.264108022054037\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:05 INFO 139940971021696] Epoch[9] Batch [5]#011Speed: 1577.48 samples/sec#011loss=10.264108\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:06 INFO 139940971021696] Epoch[9] Batch[10] avg_epoch_loss=10.299413\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:06 INFO 139940971021696] #quality_metric: host=algo-1, epoch=9, batch=10 train loss <loss>=10.341778564453126\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:06 INFO 139940971021696] Epoch[9] Batch [10]#011Speed: 1386.86 samples/sec#011loss=10.341779\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:06 INFO 139940971021696] processed a total of 668 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631460425.4463267, \"EndTime\": 1631460426.2045665, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 758.173942565918, \"count\": 1, \"min\": 758.173942565918, \"max\": 758.173942565918}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:06 INFO 139940971021696] #throughput_metric: host=algo-1, train throughput=880.9130573943314 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:06 INFO 139940971021696] #progress_metric: host=algo-1, completed 5.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:06 INFO 139940971021696] #quality_metric: host=algo-1, epoch=9, train loss <loss>=10.299412814053623\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:06 INFO 139940971021696] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:06 INFO 139940971021696] Saved checkpoint to \"/opt/ml/model/state_f5b6ab53-5a70-47f1-b94f-b8dc5715c3dd-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631460426.204658, \"EndTime\": 1631460426.224701, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 19.52672004699707, \"count\": 1, \"min\": 19.52672004699707, \"max\": 19.52672004699707}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:06 INFO 139940971021696] Epoch[10] Batch[0] avg_epoch_loss=11.751358\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:06 INFO 139940971021696] #quality_metric: host=algo-1, epoch=10, batch=0 train loss <loss>=11.751358032226562\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:06 INFO 139940971021696] Epoch[10] Batch[5] avg_epoch_loss=10.829162\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:06 INFO 139940971021696] #quality_metric: host=algo-1, epoch=10, batch=5 train loss <loss>=10.829162120819092\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:06 INFO 139940971021696] Epoch[10] Batch [5]#011Speed: 1561.38 samples/sec#011loss=10.829162\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:06 INFO 139940971021696] processed a total of 617 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631460426.2247891, \"EndTime\": 1631460426.8946283, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 669.7659492492676, \"count\": 1, \"min\": 669.7659492492676, \"max\": 669.7659492492676}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:06 INFO 139940971021696] #throughput_metric: host=algo-1, train throughput=921.0320884258374 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:06 INFO 139940971021696] #progress_metric: host=algo-1, completed 5.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:06 INFO 139940971021696] #quality_metric: host=algo-1, epoch=10, train loss <loss>=10.57329249382019\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:06 INFO 139940971021696] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:07 INFO 139940971021696] Epoch[11] Batch[0] avg_epoch_loss=9.950850\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:07 INFO 139940971021696] #quality_metric: host=algo-1, epoch=11, batch=0 train loss <loss>=9.950849533081055\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:07 INFO 139940971021696] Epoch[11] Batch[5] avg_epoch_loss=10.611207\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:07 INFO 139940971021696] #quality_metric: host=algo-1, epoch=11, batch=5 train loss <loss>=10.611207485198975\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:07 INFO 139940971021696] Epoch[11] Batch [5]#011Speed: 1885.00 samples/sec#011loss=10.611207\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:07 INFO 139940971021696] Epoch[11] Batch[10] avg_epoch_loss=10.846096\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:07 INFO 139940971021696] #quality_metric: host=algo-1, epoch=11, batch=10 train loss <loss>=11.127961540222168\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:07 INFO 139940971021696] Epoch[11] Batch [10]#011Speed: 1595.43 samples/sec#011loss=11.127962\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:07 INFO 139940971021696] processed a total of 648 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631460426.8947227, \"EndTime\": 1631460427.5646162, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 669.3506240844727, \"count\": 1, \"min\": 669.3506240844727, \"max\": 669.3506240844727}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:07 INFO 139940971021696] #throughput_metric: host=algo-1, train throughput=967.9331982644997 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:07 INFO 139940971021696] #progress_metric: host=algo-1, completed 6.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:07 INFO 139940971021696] #quality_metric: host=algo-1, epoch=11, train loss <loss>=10.8460956920277\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:07 INFO 139940971021696] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:07 INFO 139940971021696] Epoch[12] Batch[0] avg_epoch_loss=10.300929\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:07 INFO 139940971021696] #quality_metric: host=algo-1, epoch=12, batch=0 train loss <loss>=10.300929069519043\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:07 INFO 139940971021696] Epoch[12] Batch[5] avg_epoch_loss=10.947984\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:07 INFO 139940971021696] #quality_metric: host=algo-1, epoch=12, batch=5 train loss <loss>=10.947983741760254\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:07 INFO 139940971021696] Epoch[12] Batch [5]#011Speed: 1991.46 samples/sec#011loss=10.947984\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:08 INFO 139940971021696] processed a total of 608 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631460427.5646906, \"EndTime\": 1631460428.1308053, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 565.6309127807617, \"count\": 1, \"min\": 565.6309127807617, \"max\": 565.6309127807617}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:08 INFO 139940971021696] #throughput_metric: host=algo-1, train throughput=1074.6789440392022 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:08 INFO 139940971021696] #progress_metric: host=algo-1, completed 6.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:08 INFO 139940971021696] #quality_metric: host=algo-1, epoch=12, train loss <loss>=10.734973049163818\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:08 INFO 139940971021696] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:08 INFO 139940971021696] Epoch[13] Batch[0] avg_epoch_loss=11.206287\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:08 INFO 139940971021696] #quality_metric: host=algo-1, epoch=13, batch=0 train loss <loss>=11.206287384033203\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:08 INFO 139940971021696] Epoch[13] Batch[5] avg_epoch_loss=10.425997\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:08 INFO 139940971021696] #quality_metric: host=algo-1, epoch=13, batch=5 train loss <loss>=10.425996939341227\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:08 INFO 139940971021696] Epoch[13] Batch [5]#011Speed: 2055.83 samples/sec#011loss=10.425997\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:08 INFO 139940971021696] Epoch[13] Batch[10] avg_epoch_loss=10.563571\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:08 INFO 139940971021696] #quality_metric: host=algo-1, epoch=13, batch=10 train loss <loss>=10.728660011291504\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:08 INFO 139940971021696] Epoch[13] Batch [10]#011Speed: 1943.22 samples/sec#011loss=10.728660\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:08 INFO 139940971021696] processed a total of 648 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631460428.1308913, \"EndTime\": 1631460428.715557, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 584.1240882873535, \"count\": 1, \"min\": 584.1240882873535, \"max\": 584.1240882873535}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:08 INFO 139940971021696] #throughput_metric: host=algo-1, train throughput=1109.1034373564096 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:08 INFO 139940971021696] #progress_metric: host=algo-1, completed 7.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:08 INFO 139940971021696] #quality_metric: host=algo-1, epoch=13, train loss <loss>=10.56357106295499\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:08 INFO 139940971021696] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:09 INFO 139940971021696] Epoch[14] Batch[0] avg_epoch_loss=10.760332\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:09 INFO 139940971021696] #quality_metric: host=algo-1, epoch=14, batch=0 train loss <loss>=10.760332107543945\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:09 INFO 139940971021696] Epoch[14] Batch[5] avg_epoch_loss=10.803139\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:09 INFO 139940971021696] #quality_metric: host=algo-1, epoch=14, batch=5 train loss <loss>=10.803138573964437\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:09 INFO 139940971021696] Epoch[14] Batch [5]#011Speed: 1716.83 samples/sec#011loss=10.803139\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:09 INFO 139940971021696] Epoch[14] Batch[10] avg_epoch_loss=10.628566\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:09 INFO 139940971021696] #quality_metric: host=algo-1, epoch=14, batch=10 train loss <loss>=10.419079780578613\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:09 INFO 139940971021696] Epoch[14] Batch [10]#011Speed: 1730.91 samples/sec#011loss=10.419080\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:09 INFO 139940971021696] processed a total of 690 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631460428.7156484, \"EndTime\": 1631460429.3748293, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 658.7011814117432, \"count\": 1, \"min\": 658.7011814117432, \"max\": 658.7011814117432}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:09 INFO 139940971021696] #throughput_metric: host=algo-1, train throughput=1047.3131528360007 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:09 INFO 139940971021696] #progress_metric: host=algo-1, completed 7.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:09 INFO 139940971021696] #quality_metric: host=algo-1, epoch=14, train loss <loss>=10.6285663951527\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:09 INFO 139940971021696] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:09 INFO 139940971021696] Epoch[15] Batch[0] avg_epoch_loss=10.539530\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:09 INFO 139940971021696] #quality_metric: host=algo-1, epoch=15, batch=0 train loss <loss>=10.539529800415039\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:09 INFO 139940971021696] Epoch[15] Batch[5] avg_epoch_loss=10.632648\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:09 INFO 139940971021696] #quality_metric: host=algo-1, epoch=15, batch=5 train loss <loss>=10.632647514343262\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:09 INFO 139940971021696] Epoch[15] Batch [5]#011Speed: 1722.95 samples/sec#011loss=10.632648\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:10 INFO 139940971021696] Epoch[15] Batch[10] avg_epoch_loss=10.469786\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:10 INFO 139940971021696] #quality_metric: host=algo-1, epoch=15, batch=10 train loss <loss>=10.274351501464844\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:10 INFO 139940971021696] Epoch[15] Batch [10]#011Speed: 2012.95 samples/sec#011loss=10.274352\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:10 INFO 139940971021696] processed a total of 647 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631460429.3749173, \"EndTime\": 1631460430.0039387, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 628.5452842712402, \"count\": 1, \"min\": 628.5452842712402, \"max\": 628.5452842712402}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:10 INFO 139940971021696] #throughput_metric: host=algo-1, train throughput=1029.151364489483 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:10 INFO 139940971021696] #progress_metric: host=algo-1, completed 8.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:10 INFO 139940971021696] #quality_metric: host=algo-1, epoch=15, train loss <loss>=10.469785690307617\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:10 INFO 139940971021696] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:10 INFO 139940971021696] Epoch[16] Batch[0] avg_epoch_loss=10.434819\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:10 INFO 139940971021696] #quality_metric: host=algo-1, epoch=16, batch=0 train loss <loss>=10.434819221496582\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:10 INFO 139940971021696] Epoch[16] Batch[5] avg_epoch_loss=10.281862\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:10 INFO 139940971021696] #quality_metric: host=algo-1, epoch=16, batch=5 train loss <loss>=10.281861623128256\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:10 INFO 139940971021696] Epoch[16] Batch [5]#011Speed: 1953.95 samples/sec#011loss=10.281862\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:10 INFO 139940971021696] Epoch[16] Batch[10] avg_epoch_loss=10.463388\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:10 INFO 139940971021696] #quality_metric: host=algo-1, epoch=16, batch=10 train loss <loss>=10.681219100952148\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:10 INFO 139940971021696] Epoch[16] Batch [10]#011Speed: 1782.86 samples/sec#011loss=10.681219\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:10 INFO 139940971021696] processed a total of 664 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631460430.0040276, \"EndTime\": 1631460430.6385405, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 634.033203125, \"count\": 1, \"min\": 634.033203125, \"max\": 634.033203125}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:10 INFO 139940971021696] #throughput_metric: host=algo-1, train throughput=1047.0487910329587 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:10 INFO 139940971021696] #progress_metric: host=algo-1, completed 8.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:10 INFO 139940971021696] #quality_metric: host=algo-1, epoch=16, train loss <loss>=10.463387749411844\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:10 INFO 139940971021696] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:10 INFO 139940971021696] Epoch[17] Batch[0] avg_epoch_loss=10.986233\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:10 INFO 139940971021696] #quality_metric: host=algo-1, epoch=17, batch=0 train loss <loss>=10.98623275756836\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:11 INFO 139940971021696] Epoch[17] Batch[5] avg_epoch_loss=10.617100\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:11 INFO 139940971021696] #quality_metric: host=algo-1, epoch=17, batch=5 train loss <loss>=10.61709992090861\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:11 INFO 139940971021696] Epoch[17] Batch [5]#011Speed: 1877.24 samples/sec#011loss=10.617100\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:11 INFO 139940971021696] processed a total of 598 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631460430.6386306, \"EndTime\": 1631460431.2373016, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 598.1826782226562, \"count\": 1, \"min\": 598.1826782226562, \"max\": 598.1826782226562}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:11 INFO 139940971021696] #throughput_metric: host=algo-1, train throughput=999.3309589768088 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:11 INFO 139940971021696] #progress_metric: host=algo-1, completed 9.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:11 INFO 139940971021696] #quality_metric: host=algo-1, epoch=17, train loss <loss>=10.521934413909912\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:11 INFO 139940971021696] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:11 INFO 139940971021696] Epoch[18] Batch[0] avg_epoch_loss=11.007030\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:11 INFO 139940971021696] #quality_metric: host=algo-1, epoch=18, batch=0 train loss <loss>=11.00702953338623\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:11 INFO 139940971021696] Epoch[18] Batch[5] avg_epoch_loss=10.250891\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:11 INFO 139940971021696] #quality_metric: host=algo-1, epoch=18, batch=5 train loss <loss>=10.250890731811523\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:11 INFO 139940971021696] Epoch[18] Batch [5]#011Speed: 1839.84 samples/sec#011loss=10.250891\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:11 INFO 139940971021696] Epoch[18] Batch[10] avg_epoch_loss=10.763071\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:11 INFO 139940971021696] #quality_metric: host=algo-1, epoch=18, batch=10 train loss <loss>=11.377687644958495\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:11 INFO 139940971021696] Epoch[18] Batch [10]#011Speed: 1627.82 samples/sec#011loss=11.377688\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:11 INFO 139940971021696] processed a total of 647 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631460431.2374756, \"EndTime\": 1631460431.929854, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 691.8399333953857, \"count\": 1, \"min\": 691.8399333953857, \"max\": 691.8399333953857}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:11 INFO 139940971021696] #throughput_metric: host=algo-1, train throughput=935.0182502777096 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:11 INFO 139940971021696] #progress_metric: host=algo-1, completed 9.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:11 INFO 139940971021696] #quality_metric: host=algo-1, epoch=18, train loss <loss>=10.76307114687833\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:11 INFO 139940971021696] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:12 INFO 139940971021696] Epoch[19] Batch[0] avg_epoch_loss=10.287827\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:12 INFO 139940971021696] #quality_metric: host=algo-1, epoch=19, batch=0 train loss <loss>=10.287826538085938\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:12 INFO 139940971021696] Epoch[19] Batch[5] avg_epoch_loss=10.674297\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:12 INFO 139940971021696] #quality_metric: host=algo-1, epoch=19, batch=5 train loss <loss>=10.674297332763672\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:12 INFO 139940971021696] Epoch[19] Batch [5]#011Speed: 1743.57 samples/sec#011loss=10.674297\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:12 INFO 139940971021696] Epoch[19] Batch[10] avg_epoch_loss=10.388923\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:12 INFO 139940971021696] #quality_metric: host=algo-1, epoch=19, batch=10 train loss <loss>=10.046474838256836\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:12 INFO 139940971021696] Epoch[19] Batch [10]#011Speed: 1629.73 samples/sec#011loss=10.046475\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:12 INFO 139940971021696] processed a total of 706 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631460431.9299405, \"EndTime\": 1631460432.6671276, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 736.7057800292969, \"count\": 1, \"min\": 736.7057800292969, \"max\": 736.7057800292969}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:12 INFO 139940971021696] #throughput_metric: host=algo-1, train throughput=958.1939486308296 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:12 INFO 139940971021696] #progress_metric: host=algo-1, completed 10.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:12 INFO 139940971021696] #quality_metric: host=algo-1, epoch=19, train loss <loss>=9.877729137738546\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:12 INFO 139940971021696] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:12 INFO 139940971021696] Saved checkpoint to \"/opt/ml/model/state_482f8ce0-047f-4be6-b172-8e9aa2b46a27-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631460432.6671922, \"EndTime\": 1631460432.6803455, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 12.585163116455078, \"count\": 1, \"min\": 12.585163116455078, \"max\": 12.585163116455078}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:12 INFO 139940971021696] Epoch[20] Batch[0] avg_epoch_loss=10.477185\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:12 INFO 139940971021696] #quality_metric: host=algo-1, epoch=20, batch=0 train loss <loss>=10.477185249328613\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:13 INFO 139940971021696] Epoch[20] Batch[5] avg_epoch_loss=10.599447\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:13 INFO 139940971021696] #quality_metric: host=algo-1, epoch=20, batch=5 train loss <loss>=10.599447091420492\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:13 INFO 139940971021696] Epoch[20] Batch [5]#011Speed: 2038.80 samples/sec#011loss=10.599447\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:13 INFO 139940971021696] processed a total of 622 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631460432.6804214, \"EndTime\": 1631460433.2394965, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 559.0128898620605, \"count\": 1, \"min\": 559.0128898620605, \"max\": 559.0128898620605}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:13 INFO 139940971021696] #throughput_metric: host=algo-1, train throughput=1112.4070618052938 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:13 INFO 139940971021696] #progress_metric: host=algo-1, completed 10.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:13 INFO 139940971021696] #quality_metric: host=algo-1, epoch=20, train loss <loss>=10.789569759368897\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:13 INFO 139940971021696] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:13 INFO 139940971021696] Epoch[21] Batch[0] avg_epoch_loss=9.875082\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:13 INFO 139940971021696] #quality_metric: host=algo-1, epoch=21, batch=0 train loss <loss>=9.875082015991211\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:13 INFO 139940971021696] Epoch[21] Batch[5] avg_epoch_loss=10.140537\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:13 INFO 139940971021696] #quality_metric: host=algo-1, epoch=21, batch=5 train loss <loss>=10.140536785125732\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:13 INFO 139940971021696] Epoch[21] Batch [5]#011Speed: 1943.23 samples/sec#011loss=10.140537\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:13 INFO 139940971021696] processed a total of 623 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631460433.2395966, \"EndTime\": 1631460433.8007412, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 560.5745315551758, \"count\": 1, \"min\": 560.5745315551758, \"max\": 560.5745315551758}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:13 INFO 139940971021696] #throughput_metric: host=algo-1, train throughput=1111.0748521466114 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:13 INFO 139940971021696] #progress_metric: host=algo-1, completed 11.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:13 INFO 139940971021696] #quality_metric: host=algo-1, epoch=21, train loss <loss>=10.226567363739013\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:13 INFO 139940971021696] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:14 INFO 139940971021696] Epoch[22] Batch[0] avg_epoch_loss=10.187443\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:14 INFO 139940971021696] #quality_metric: host=algo-1, epoch=22, batch=0 train loss <loss>=10.187442779541016\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:14 INFO 139940971021696] Epoch[22] Batch[5] avg_epoch_loss=10.156768\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:14 INFO 139940971021696] #quality_metric: host=algo-1, epoch=22, batch=5 train loss <loss>=10.156768480936686\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:14 INFO 139940971021696] Epoch[22] Batch [5]#011Speed: 1672.57 samples/sec#011loss=10.156768\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:14 INFO 139940971021696] processed a total of 639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631460433.8008401, \"EndTime\": 1631460434.4097986, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 608.386754989624, \"count\": 1, \"min\": 608.386754989624, \"max\": 608.386754989624}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:14 INFO 139940971021696] #throughput_metric: host=algo-1, train throughput=1050.1380598213775 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:14 INFO 139940971021696] #progress_metric: host=algo-1, completed 11.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:14 INFO 139940971021696] #quality_metric: host=algo-1, epoch=22, train loss <loss>=10.213001155853272\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:14 INFO 139940971021696] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:14 INFO 139940971021696] Epoch[23] Batch[0] avg_epoch_loss=9.322637\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:14 INFO 139940971021696] #quality_metric: host=algo-1, epoch=23, batch=0 train loss <loss>=9.322636604309082\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:14 INFO 139940971021696] Epoch[23] Batch[5] avg_epoch_loss=9.799615\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:14 INFO 139940971021696] #quality_metric: host=algo-1, epoch=23, batch=5 train loss <loss>=9.799615065256754\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:14 INFO 139940971021696] Epoch[23] Batch [5]#011Speed: 1997.17 samples/sec#011loss=9.799615\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:14 INFO 139940971021696] processed a total of 635 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631460434.4098692, \"EndTime\": 1631460434.9941897, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 583.7416648864746, \"count\": 1, \"min\": 583.7416648864746, \"max\": 583.7416648864746}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:14 INFO 139940971021696] #throughput_metric: host=algo-1, train throughput=1087.5572192102848 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:14 INFO 139940971021696] #progress_metric: host=algo-1, completed 12.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:14 INFO 139940971021696] #quality_metric: host=algo-1, epoch=23, train loss <loss>=10.102778339385987\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:14 INFO 139940971021696] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:15 INFO 139940971021696] Epoch[24] Batch[0] avg_epoch_loss=10.178377\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:15 INFO 139940971021696] #quality_metric: host=algo-1, epoch=24, batch=0 train loss <loss>=10.178377151489258\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:15 INFO 139940971021696] Epoch[24] Batch[5] avg_epoch_loss=10.572680\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:15 INFO 139940971021696] #quality_metric: host=algo-1, epoch=24, batch=5 train loss <loss>=10.572680473327637\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:15 INFO 139940971021696] Epoch[24] Batch [5]#011Speed: 1780.27 samples/sec#011loss=10.572680\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:15 INFO 139940971021696] processed a total of 626 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631460434.9942834, \"EndTime\": 1631460435.5677764, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 573.0056762695312, \"count\": 1, \"min\": 573.0056762695312, \"max\": 573.0056762695312}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:15 INFO 139940971021696] #throughput_metric: host=algo-1, train throughput=1092.2157780188622 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:15 INFO 139940971021696] #progress_metric: host=algo-1, completed 12.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:15 INFO 139940971021696] #quality_metric: host=algo-1, epoch=24, train loss <loss>=10.381821632385254\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:15 INFO 139940971021696] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:15 INFO 139940971021696] Epoch[25] Batch[0] avg_epoch_loss=9.675883\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:15 INFO 139940971021696] #quality_metric: host=algo-1, epoch=25, batch=0 train loss <loss>=9.675883293151855\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:15 INFO 139940971021696] Epoch[25] Batch[5] avg_epoch_loss=10.000639\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:15 INFO 139940971021696] #quality_metric: host=algo-1, epoch=25, batch=5 train loss <loss>=10.000639120737711\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:15 INFO 139940971021696] Epoch[25] Batch [5]#011Speed: 2072.79 samples/sec#011loss=10.000639\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:16 INFO 139940971021696] Epoch[25] Batch[10] avg_epoch_loss=9.430384\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:16 INFO 139940971021696] #quality_metric: host=algo-1, epoch=25, batch=10 train loss <loss>=8.746077680587769\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:16 INFO 139940971021696] Epoch[25] Batch [10]#011Speed: 1974.04 samples/sec#011loss=8.746078\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:16 INFO 139940971021696] processed a total of 652 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631460435.5678701, \"EndTime\": 1631460436.1462734, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 577.8944492340088, \"count\": 1, \"min\": 577.8944492340088, \"max\": 577.8944492340088}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:16 INFO 139940971021696] #throughput_metric: host=algo-1, train throughput=1127.97171456525 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:16 INFO 139940971021696] #progress_metric: host=algo-1, completed 13.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:16 INFO 139940971021696] #quality_metric: host=algo-1, epoch=25, train loss <loss>=9.430383920669556\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:16 INFO 139940971021696] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:16 INFO 139940971021696] Saved checkpoint to \"/opt/ml/model/state_b70ca4b7-678d-4b50-beb0-afc58199e9db-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631460436.146374, \"EndTime\": 1631460436.1605833, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 13.689279556274414, \"count\": 1, \"min\": 13.689279556274414, \"max\": 13.689279556274414}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:16 INFO 139940971021696] Epoch[26] Batch[0] avg_epoch_loss=10.800662\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:16 INFO 139940971021696] #quality_metric: host=algo-1, epoch=26, batch=0 train loss <loss>=10.80066204071045\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:16 INFO 139940971021696] Epoch[26] Batch[5] avg_epoch_loss=10.329294\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:16 INFO 139940971021696] #quality_metric: host=algo-1, epoch=26, batch=5 train loss <loss>=10.329294045766195\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:16 INFO 139940971021696] Epoch[26] Batch [5]#011Speed: 2073.07 samples/sec#011loss=10.329294\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:16 INFO 139940971021696] Epoch[26] Batch[10] avg_epoch_loss=10.260221\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:16 INFO 139940971021696] #quality_metric: host=algo-1, epoch=26, batch=10 train loss <loss>=10.177333068847656\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:16 INFO 139940971021696] Epoch[26] Batch [10]#011Speed: 1606.51 samples/sec#011loss=10.177333\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:16 INFO 139940971021696] processed a total of 690 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631460436.1606505, \"EndTime\": 1631460436.777955, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 617.2432899475098, \"count\": 1, \"min\": 617.2432899475098, \"max\": 617.2432899475098}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:16 INFO 139940971021696] #throughput_metric: host=algo-1, train throughput=1117.631019190042 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:16 INFO 139940971021696] #progress_metric: host=algo-1, completed 13.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:16 INFO 139940971021696] #quality_metric: host=algo-1, epoch=26, train loss <loss>=10.260220874439586\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:16 INFO 139940971021696] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:17 INFO 139940971021696] Epoch[27] Batch[0] avg_epoch_loss=9.566209\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:17 INFO 139940971021696] #quality_metric: host=algo-1, epoch=27, batch=0 train loss <loss>=9.566208839416504\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:17 INFO 139940971021696] Epoch[27] Batch[5] avg_epoch_loss=10.623991\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:17 INFO 139940971021696] #quality_metric: host=algo-1, epoch=27, batch=5 train loss <loss>=10.6239914894104\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:17 INFO 139940971021696] Epoch[27] Batch [5]#011Speed: 1905.77 samples/sec#011loss=10.623991\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:17 INFO 139940971021696] processed a total of 609 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631460436.7780533, \"EndTime\": 1631460437.3342354, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 555.6812286376953, \"count\": 1, \"min\": 555.6812286376953, \"max\": 555.6812286376953}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:17 INFO 139940971021696] #throughput_metric: host=algo-1, train throughput=1095.6803601807424 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:17 INFO 139940971021696] #progress_metric: host=algo-1, completed 14.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:17 INFO 139940971021696] #quality_metric: host=algo-1, epoch=27, train loss <loss>=10.425578784942626\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:17 INFO 139940971021696] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:17 INFO 139940971021696] Epoch[28] Batch[0] avg_epoch_loss=10.377567\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:17 INFO 139940971021696] #quality_metric: host=algo-1, epoch=28, batch=0 train loss <loss>=10.377567291259766\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:17 INFO 139940971021696] Epoch[28] Batch[5] avg_epoch_loss=10.258786\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:17 INFO 139940971021696] #quality_metric: host=algo-1, epoch=28, batch=5 train loss <loss>=10.25878620147705\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:17 INFO 139940971021696] Epoch[28] Batch [5]#011Speed: 2134.58 samples/sec#011loss=10.258786\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:17 INFO 139940971021696] processed a total of 626 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631460437.334332, \"EndTime\": 1631460437.882362, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 547.5125312805176, \"count\": 1, \"min\": 547.5125312805176, \"max\": 547.5125312805176}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:17 INFO 139940971021696] #throughput_metric: host=algo-1, train throughput=1143.1014511051446 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:17 INFO 139940971021696] #progress_metric: host=algo-1, completed 14.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:17 INFO 139940971021696] #quality_metric: host=algo-1, epoch=28, train loss <loss>=9.968684768676757\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:17 INFO 139940971021696] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:18 INFO 139940971021696] Epoch[29] Batch[0] avg_epoch_loss=9.507891\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:18 INFO 139940971021696] #quality_metric: host=algo-1, epoch=29, batch=0 train loss <loss>=9.507890701293945\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:18 INFO 139940971021696] Epoch[29] Batch[5] avg_epoch_loss=10.321945\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:18 INFO 139940971021696] #quality_metric: host=algo-1, epoch=29, batch=5 train loss <loss>=10.321945031483969\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:18 INFO 139940971021696] Epoch[29] Batch [5]#011Speed: 2071.76 samples/sec#011loss=10.321945\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:18 INFO 139940971021696] Epoch[29] Batch[10] avg_epoch_loss=10.185439\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:18 INFO 139940971021696] #quality_metric: host=algo-1, epoch=29, batch=10 train loss <loss>=10.021632766723632\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:18 INFO 139940971021696] Epoch[29] Batch [10]#011Speed: 1889.16 samples/sec#011loss=10.021633\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:18 INFO 139940971021696] processed a total of 665 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631460437.8824396, \"EndTime\": 1631460438.4656997, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 582.7040672302246, \"count\": 1, \"min\": 582.7040672302246, \"max\": 582.7040672302246}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:18 INFO 139940971021696] #throughput_metric: host=algo-1, train throughput=1140.9808661959098 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:18 INFO 139940971021696] #progress_metric: host=algo-1, completed 15.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:18 INFO 139940971021696] #quality_metric: host=algo-1, epoch=29, train loss <loss>=10.185439456592906\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:18 INFO 139940971021696] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:18 INFO 139940971021696] Epoch[30] Batch[0] avg_epoch_loss=9.567598\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:18 INFO 139940971021696] #quality_metric: host=algo-1, epoch=30, batch=0 train loss <loss>=9.567598342895508\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:18 INFO 139940971021696] Epoch[30] Batch[5] avg_epoch_loss=9.953226\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:18 INFO 139940971021696] #quality_metric: host=algo-1, epoch=30, batch=5 train loss <loss>=9.953226089477539\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:18 INFO 139940971021696] Epoch[30] Batch [5]#011Speed: 2121.93 samples/sec#011loss=9.953226\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:19 INFO 139940971021696] processed a total of 619 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631460438.4657884, \"EndTime\": 1631460439.0101986, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 543.8666343688965, \"count\": 1, \"min\": 543.8666343688965, \"max\": 543.8666343688965}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:19 INFO 139940971021696] #throughput_metric: host=algo-1, train throughput=1137.8648424184175 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:19 INFO 139940971021696] #progress_metric: host=algo-1, completed 15.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:19 INFO 139940971021696] #quality_metric: host=algo-1, epoch=30, train loss <loss>=10.012022018432617\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:19 INFO 139940971021696] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:19 INFO 139940971021696] Epoch[31] Batch[0] avg_epoch_loss=9.580347\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:19 INFO 139940971021696] #quality_metric: host=algo-1, epoch=31, batch=0 train loss <loss>=9.580347061157227\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:19 INFO 139940971021696] Epoch[31] Batch[5] avg_epoch_loss=10.070110\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:19 INFO 139940971021696] #quality_metric: host=algo-1, epoch=31, batch=5 train loss <loss>=10.070109685262045\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:19 INFO 139940971021696] Epoch[31] Batch [5]#011Speed: 1945.38 samples/sec#011loss=10.070110\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:19 INFO 139940971021696] Epoch[31] Batch[10] avg_epoch_loss=10.619558\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:19 INFO 139940971021696] #quality_metric: host=algo-1, epoch=31, batch=10 train loss <loss>=11.278895568847656\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:19 INFO 139940971021696] Epoch[31] Batch [10]#011Speed: 1774.71 samples/sec#011loss=11.278896\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:19 INFO 139940971021696] processed a total of 651 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631460439.0102918, \"EndTime\": 1631460439.635365, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 624.5613098144531, \"count\": 1, \"min\": 624.5613098144531, \"max\": 624.5613098144531}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:19 INFO 139940971021696] #throughput_metric: host=algo-1, train throughput=1042.1207741600608 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:19 INFO 139940971021696] #progress_metric: host=algo-1, completed 16.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:19 INFO 139940971021696] #quality_metric: host=algo-1, epoch=31, train loss <loss>=10.619557814164596\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:19 INFO 139940971021696] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:19 INFO 139940971021696] Epoch[32] Batch[0] avg_epoch_loss=9.571597\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:19 INFO 139940971021696] #quality_metric: host=algo-1, epoch=32, batch=0 train loss <loss>=9.5715970993042\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:20 INFO 139940971021696] Epoch[32] Batch[5] avg_epoch_loss=10.536942\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:20 INFO 139940971021696] #quality_metric: host=algo-1, epoch=32, batch=5 train loss <loss>=10.536941846211752\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:20 INFO 139940971021696] Epoch[32] Batch [5]#011Speed: 2132.07 samples/sec#011loss=10.536942\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:20 INFO 139940971021696] Epoch[32] Batch[10] avg_epoch_loss=10.018942\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:20 INFO 139940971021696] #quality_metric: host=algo-1, epoch=32, batch=10 train loss <loss>=9.397342967987061\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:20 INFO 139940971021696] Epoch[32] Batch [10]#011Speed: 1731.04 samples/sec#011loss=9.397343\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:20 INFO 139940971021696] processed a total of 689 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631460439.6354527, \"EndTime\": 1631460440.25267, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 616.7309284210205, \"count\": 1, \"min\": 616.7309284210205, \"max\": 616.7309284210205}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:20 INFO 139940971021696] #throughput_metric: host=algo-1, train throughput=1116.9585014640904 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:20 INFO 139940971021696] #progress_metric: host=algo-1, completed 16.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:20 INFO 139940971021696] #quality_metric: host=algo-1, epoch=32, train loss <loss>=10.01894235610962\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:20 INFO 139940971021696] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:20 INFO 139940971021696] Epoch[33] Batch[0] avg_epoch_loss=10.352300\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:20 INFO 139940971021696] #quality_metric: host=algo-1, epoch=33, batch=0 train loss <loss>=10.352299690246582\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:20 INFO 139940971021696] Epoch[33] Batch[5] avg_epoch_loss=10.407323\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:20 INFO 139940971021696] #quality_metric: host=algo-1, epoch=33, batch=5 train loss <loss>=10.407322724660238\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:20 INFO 139940971021696] Epoch[33] Batch [5]#011Speed: 2050.10 samples/sec#011loss=10.407323\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:20 INFO 139940971021696] Epoch[33] Batch[10] avg_epoch_loss=10.696516\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:20 INFO 139940971021696] #quality_metric: host=algo-1, epoch=33, batch=10 train loss <loss>=11.043547821044921\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:20 INFO 139940971021696] Epoch[33] Batch [10]#011Speed: 1876.40 samples/sec#011loss=11.043548\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:20 INFO 139940971021696] processed a total of 672 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631460440.2527559, \"EndTime\": 1631460440.862802, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 609.5850467681885, \"count\": 1, \"min\": 609.5850467681885, \"max\": 609.5850467681885}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:20 INFO 139940971021696] #throughput_metric: host=algo-1, train throughput=1102.1560364224194 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:20 INFO 139940971021696] #progress_metric: host=algo-1, completed 17.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:20 INFO 139940971021696] #quality_metric: host=algo-1, epoch=33, train loss <loss>=10.696515950289639\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:20 INFO 139940971021696] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:21 INFO 139940971021696] Epoch[34] Batch[0] avg_epoch_loss=10.546801\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:21 INFO 139940971021696] #quality_metric: host=algo-1, epoch=34, batch=0 train loss <loss>=10.54680061340332\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:21 INFO 139940971021696] Epoch[34] Batch[5] avg_epoch_loss=10.088786\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:21 INFO 139940971021696] #quality_metric: host=algo-1, epoch=34, batch=5 train loss <loss>=10.088785648345947\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:21 INFO 139940971021696] Epoch[34] Batch [5]#011Speed: 1882.80 samples/sec#011loss=10.088786\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:21 INFO 139940971021696] processed a total of 572 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631460440.8628922, \"EndTime\": 1631460441.42433, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 560.9545707702637, \"count\": 1, \"min\": 560.9545707702637, \"max\": 560.9545707702637}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:21 INFO 139940971021696] #throughput_metric: host=algo-1, train throughput=1019.4317473658447 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:21 INFO 139940971021696] #progress_metric: host=algo-1, completed 17.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:21 INFO 139940971021696] #quality_metric: host=algo-1, epoch=34, train loss <loss>=10.08809651268853\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:21 INFO 139940971021696] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:21 INFO 139940971021696] Epoch[35] Batch[0] avg_epoch_loss=11.113654\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:21 INFO 139940971021696] #quality_metric: host=algo-1, epoch=35, batch=0 train loss <loss>=11.113654136657715\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:21 INFO 139940971021696] Epoch[35] Batch[5] avg_epoch_loss=10.422438\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:21 INFO 139940971021696] #quality_metric: host=algo-1, epoch=35, batch=5 train loss <loss>=10.422438462575277\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:21 INFO 139940971021696] Epoch[35] Batch [5]#011Speed: 2077.81 samples/sec#011loss=10.422438\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:22 INFO 139940971021696] Epoch[35] Batch[10] avg_epoch_loss=10.250110\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:22 INFO 139940971021696] #quality_metric: host=algo-1, epoch=35, batch=10 train loss <loss>=10.043315315246582\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:22 INFO 139940971021696] Epoch[35] Batch [10]#011Speed: 1889.85 samples/sec#011loss=10.043315\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:22 INFO 139940971021696] processed a total of 673 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631460441.4244306, \"EndTime\": 1631460442.0667465, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 641.7930126190186, \"count\": 1, \"min\": 641.7930126190186, \"max\": 641.7930126190186}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:22 INFO 139940971021696] #throughput_metric: host=algo-1, train throughput=1048.419827507121 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:22 INFO 139940971021696] #progress_metric: host=algo-1, completed 18.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:22 INFO 139940971021696] #quality_metric: host=algo-1, epoch=35, train loss <loss>=10.250109759244053\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:22 INFO 139940971021696] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:22 INFO 139940971021696] Epoch[36] Batch[0] avg_epoch_loss=10.544926\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:22 INFO 139940971021696] #quality_metric: host=algo-1, epoch=36, batch=0 train loss <loss>=10.544925689697266\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:22 INFO 139940971021696] Epoch[36] Batch[5] avg_epoch_loss=10.154032\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:22 INFO 139940971021696] #quality_metric: host=algo-1, epoch=36, batch=5 train loss <loss>=10.154031912485758\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:22 INFO 139940971021696] Epoch[36] Batch [5]#011Speed: 2057.26 samples/sec#011loss=10.154032\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:22 INFO 139940971021696] Epoch[36] Batch[10] avg_epoch_loss=10.205272\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:22 INFO 139940971021696] #quality_metric: host=algo-1, epoch=36, batch=10 train loss <loss>=10.266759490966797\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:22 INFO 139940971021696] Epoch[36] Batch [10]#011Speed: 1660.54 samples/sec#011loss=10.266759\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:22 INFO 139940971021696] processed a total of 678 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631460442.0668328, \"EndTime\": 1631460442.6889126, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 621.6151714324951, \"count\": 1, \"min\": 621.6151714324951, \"max\": 621.6151714324951}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:22 INFO 139940971021696] #throughput_metric: host=algo-1, train throughput=1090.484041574123 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:22 INFO 139940971021696] #progress_metric: host=algo-1, completed 18.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:22 INFO 139940971021696] #quality_metric: host=algo-1, epoch=36, train loss <loss>=10.20527172088623\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:22 INFO 139940971021696] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:22 INFO 139940971021696] Epoch[37] Batch[0] avg_epoch_loss=9.433572\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:22 INFO 139940971021696] #quality_metric: host=algo-1, epoch=37, batch=0 train loss <loss>=9.433571815490723\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:23 INFO 139940971021696] Epoch[37] Batch[5] avg_epoch_loss=10.275144\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:23 INFO 139940971021696] #quality_metric: host=algo-1, epoch=37, batch=5 train loss <loss>=10.27514362335205\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:23 INFO 139940971021696] Epoch[37] Batch [5]#011Speed: 2113.15 samples/sec#011loss=10.275144\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:23 INFO 139940971021696] processed a total of 626 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631460442.6889994, \"EndTime\": 1631460443.2319775, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 542.4673557281494, \"count\": 1, \"min\": 542.4673557281494, \"max\": 542.4673557281494}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:23 INFO 139940971021696] #throughput_metric: host=algo-1, train throughput=1153.7400506819922 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:23 INFO 139940971021696] #progress_metric: host=algo-1, completed 19.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:23 INFO 139940971021696] #quality_metric: host=algo-1, epoch=37, train loss <loss>=10.105851459503175\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:23 INFO 139940971021696] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:23 INFO 139940971021696] Epoch[38] Batch[0] avg_epoch_loss=9.758585\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:23 INFO 139940971021696] #quality_metric: host=algo-1, epoch=38, batch=0 train loss <loss>=9.758584976196289\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:23 INFO 139940971021696] Epoch[38] Batch[5] avg_epoch_loss=9.679913\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:23 INFO 139940971021696] #quality_metric: host=algo-1, epoch=38, batch=5 train loss <loss>=9.67991304397583\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:23 INFO 139940971021696] Epoch[38] Batch [5]#011Speed: 2032.66 samples/sec#011loss=9.679913\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:23 INFO 139940971021696] Epoch[38] Batch[10] avg_epoch_loss=10.150804\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:23 INFO 139940971021696] #quality_metric: host=algo-1, epoch=38, batch=10 train loss <loss>=10.715872955322265\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:23 INFO 139940971021696] Epoch[38] Batch [10]#011Speed: 2006.61 samples/sec#011loss=10.715873\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:23 INFO 139940971021696] processed a total of 651 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631460443.2320576, \"EndTime\": 1631460443.8203642, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 587.7795219421387, \"count\": 1, \"min\": 587.7795219421387, \"max\": 587.7795219421387}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:23 INFO 139940971021696] #throughput_metric: host=algo-1, train throughput=1107.31830030902 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:23 INFO 139940971021696] #progress_metric: host=algo-1, completed 19.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:23 INFO 139940971021696] #quality_metric: host=algo-1, epoch=38, train loss <loss>=10.150803912769664\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:23 INFO 139940971021696] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:24 INFO 139940971021696] Epoch[39] Batch[0] avg_epoch_loss=8.844725\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:24 INFO 139940971021696] #quality_metric: host=algo-1, epoch=39, batch=0 train loss <loss>=8.844724655151367\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:24 INFO 139940971021696] Epoch[39] Batch[5] avg_epoch_loss=10.024714\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:24 INFO 139940971021696] #quality_metric: host=algo-1, epoch=39, batch=5 train loss <loss>=10.024714152018229\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:24 INFO 139940971021696] Epoch[39] Batch [5]#011Speed: 1933.40 samples/sec#011loss=10.024714\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:24 INFO 139940971021696] processed a total of 617 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631460443.8204522, \"EndTime\": 1631460444.4096155, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 588.6716842651367, \"count\": 1, \"min\": 588.6716842651367, \"max\": 588.6716842651367}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:24 INFO 139940971021696] #throughput_metric: host=algo-1, train throughput=1047.897499107142 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:24 INFO 139940971021696] #progress_metric: host=algo-1, completed 20.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:24 INFO 139940971021696] #quality_metric: host=algo-1, epoch=39, train loss <loss>=10.008408451080323\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:24 INFO 139940971021696] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:24 INFO 139940971021696] Epoch[40] Batch[0] avg_epoch_loss=10.425303\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:24 INFO 139940971021696] #quality_metric: host=algo-1, epoch=40, batch=0 train loss <loss>=10.425302505493164\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:24 INFO 139940971021696] Epoch[40] Batch[5] avg_epoch_loss=10.107244\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:24 INFO 139940971021696] #quality_metric: host=algo-1, epoch=40, batch=5 train loss <loss>=10.10724433263143\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:24 INFO 139940971021696] Epoch[40] Batch [5]#011Speed: 1622.21 samples/sec#011loss=10.107244\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:25 INFO 139940971021696] Epoch[40] Batch[10] avg_epoch_loss=10.017298\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:25 INFO 139940971021696] #quality_metric: host=algo-1, epoch=40, batch=10 train loss <loss>=9.909363174438477\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:25 INFO 139940971021696] Epoch[40] Batch [10]#011Speed: 1841.15 samples/sec#011loss=9.909363\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:25 INFO 139940971021696] processed a total of 671 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631460444.4096944, \"EndTime\": 1631460445.0360453, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 625.7836818695068, \"count\": 1, \"min\": 625.7836818695068, \"max\": 625.7836818695068}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:25 INFO 139940971021696] #throughput_metric: host=algo-1, train throughput=1072.0472033572587 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:25 INFO 139940971021696] #progress_metric: host=algo-1, completed 20.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:25 INFO 139940971021696] #quality_metric: host=algo-1, epoch=40, train loss <loss>=10.017298351634633\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:25 INFO 139940971021696] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:25 INFO 139940971021696] Epoch[41] Batch[0] avg_epoch_loss=10.042496\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:25 INFO 139940971021696] #quality_metric: host=algo-1, epoch=41, batch=0 train loss <loss>=10.042495727539062\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:25 INFO 139940971021696] Epoch[41] Batch[5] avg_epoch_loss=10.065228\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:25 INFO 139940971021696] #quality_metric: host=algo-1, epoch=41, batch=5 train loss <loss>=10.06522766749064\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:25 INFO 139940971021696] Epoch[41] Batch [5]#011Speed: 1745.24 samples/sec#011loss=10.065228\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:25 INFO 139940971021696] processed a total of 618 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631460445.0361261, \"EndTime\": 1631460445.5957975, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 559.1626167297363, \"count\": 1, \"min\": 559.1626167297363, \"max\": 559.1626167297363}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:25 INFO 139940971021696] #throughput_metric: host=algo-1, train throughput=1104.9583786258527 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:25 INFO 139940971021696] #progress_metric: host=algo-1, completed 21.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:25 INFO 139940971021696] #quality_metric: host=algo-1, epoch=41, train loss <loss>=10.441767311096191\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:25 INFO 139940971021696] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:25 INFO 139940971021696] Epoch[42] Batch[0] avg_epoch_loss=10.619664\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:25 INFO 139940971021696] #quality_metric: host=algo-1, epoch=42, batch=0 train loss <loss>=10.619664192199707\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:26 INFO 139940971021696] Epoch[42] Batch[5] avg_epoch_loss=9.904007\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:26 INFO 139940971021696] #quality_metric: host=algo-1, epoch=42, batch=5 train loss <loss>=9.904006640116373\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:26 INFO 139940971021696] Epoch[42] Batch [5]#011Speed: 2060.18 samples/sec#011loss=9.904007\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:26 INFO 139940971021696] processed a total of 595 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631460445.5958915, \"EndTime\": 1631460446.1399932, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 543.5733795166016, \"count\": 1, \"min\": 543.5733795166016, \"max\": 543.5733795166016}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:26 INFO 139940971021696] #throughput_metric: host=algo-1, train throughput=1094.3396279366727 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:26 INFO 139940971021696] #progress_metric: host=algo-1, completed 21.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:26 INFO 139940971021696] #quality_metric: host=algo-1, epoch=42, train loss <loss>=10.321866416931153\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:26 INFO 139940971021696] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:26 INFO 139940971021696] Epoch[43] Batch[0] avg_epoch_loss=9.577823\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:26 INFO 139940971021696] #quality_metric: host=algo-1, epoch=43, batch=0 train loss <loss>=9.5778226852417\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:26 INFO 139940971021696] Epoch[43] Batch[5] avg_epoch_loss=9.942068\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:26 INFO 139940971021696] #quality_metric: host=algo-1, epoch=43, batch=5 train loss <loss>=9.942067623138428\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:26 INFO 139940971021696] Epoch[43] Batch [5]#011Speed: 1756.44 samples/sec#011loss=9.942068\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:26 INFO 139940971021696] processed a total of 618 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631460446.140086, \"EndTime\": 1631460446.7136834, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 573.0690956115723, \"count\": 1, \"min\": 573.0690956115723, \"max\": 573.0690956115723}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:26 INFO 139940971021696] #throughput_metric: host=algo-1, train throughput=1078.153250269842 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:26 INFO 139940971021696] #progress_metric: host=algo-1, completed 22.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:26 INFO 139940971021696] #quality_metric: host=algo-1, epoch=43, train loss <loss>=9.761656475067138\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:26 INFO 139940971021696] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:26 INFO 139940971021696] Epoch[44] Batch[0] avg_epoch_loss=9.429372\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:26 INFO 139940971021696] #quality_metric: host=algo-1, epoch=44, batch=0 train loss <loss>=9.42937183380127\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:27 INFO 139940971021696] Epoch[44] Batch[5] avg_epoch_loss=10.314536\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:27 INFO 139940971021696] #quality_metric: host=algo-1, epoch=44, batch=5 train loss <loss>=10.314535935719809\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:27 INFO 139940971021696] Epoch[44] Batch [5]#011Speed: 1924.23 samples/sec#011loss=10.314536\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:27 INFO 139940971021696] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631460446.7137764, \"EndTime\": 1631460447.282729, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 568.4001445770264, \"count\": 1, \"min\": 568.4001445770264, \"max\": 568.4001445770264}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:27 INFO 139940971021696] #throughput_metric: host=algo-1, train throughput=1113.37513649882 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:27 INFO 139940971021696] #progress_metric: host=algo-1, completed 22.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:27 INFO 139940971021696] #quality_metric: host=algo-1, epoch=44, train loss <loss>=10.269951820373535\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:27 INFO 139940971021696] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:27 INFO 139940971021696] Epoch[45] Batch[0] avg_epoch_loss=10.165696\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:27 INFO 139940971021696] #quality_metric: host=algo-1, epoch=45, batch=0 train loss <loss>=10.165696144104004\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:27 INFO 139940971021696] Epoch[45] Batch[5] avg_epoch_loss=10.159822\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:27 INFO 139940971021696] #quality_metric: host=algo-1, epoch=45, batch=5 train loss <loss>=10.159822305043539\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:27 INFO 139940971021696] Epoch[45] Batch [5]#011Speed: 1922.42 samples/sec#011loss=10.159822\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:27 INFO 139940971021696] Epoch[45] Batch[10] avg_epoch_loss=9.890213\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:27 INFO 139940971021696] #quality_metric: host=algo-1, epoch=45, batch=10 train loss <loss>=9.566681861877441\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:27 INFO 139940971021696] Epoch[45] Batch [10]#011Speed: 1929.53 samples/sec#011loss=9.566682\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:27 INFO 139940971021696] processed a total of 660 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631460447.2828288, \"EndTime\": 1631460447.888221, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 604.8445701599121, \"count\": 1, \"min\": 604.8445701599121, \"max\": 604.8445701599121}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:27 INFO 139940971021696] #throughput_metric: host=algo-1, train throughput=1090.9804255399647 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:27 INFO 139940971021696] #progress_metric: host=algo-1, completed 23.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:27 INFO 139940971021696] #quality_metric: host=algo-1, epoch=45, train loss <loss>=9.890213012695312\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:27 INFO 139940971021696] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:28 INFO 139940971021696] Epoch[46] Batch[0] avg_epoch_loss=10.169827\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:28 INFO 139940971021696] #quality_metric: host=algo-1, epoch=46, batch=0 train loss <loss>=10.169827461242676\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:28 INFO 139940971021696] Epoch[46] Batch[5] avg_epoch_loss=10.030409\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:28 INFO 139940971021696] #quality_metric: host=algo-1, epoch=46, batch=5 train loss <loss>=10.03040885925293\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:28 INFO 139940971021696] Epoch[46] Batch [5]#011Speed: 1923.07 samples/sec#011loss=10.030409\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:28 INFO 139940971021696] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631460447.8882997, \"EndTime\": 1631460448.443673, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 554.8131465911865, \"count\": 1, \"min\": 554.8131465911865, \"max\": 554.8131465911865}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:28 INFO 139940971021696] #throughput_metric: host=algo-1, train throughput=1147.8482012858544 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:28 INFO 139940971021696] #progress_metric: host=algo-1, completed 23.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:28 INFO 139940971021696] #quality_metric: host=algo-1, epoch=46, train loss <loss>=9.96445837020874\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:28 INFO 139940971021696] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:28 INFO 139940971021696] Epoch[47] Batch[0] avg_epoch_loss=10.611091\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:28 INFO 139940971021696] #quality_metric: host=algo-1, epoch=47, batch=0 train loss <loss>=10.611090660095215\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:28 INFO 139940971021696] Epoch[47] Batch[5] avg_epoch_loss=9.809036\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:28 INFO 139940971021696] #quality_metric: host=algo-1, epoch=47, batch=5 train loss <loss>=9.809036413828531\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:28 INFO 139940971021696] Epoch[47] Batch [5]#011Speed: 2086.51 samples/sec#011loss=9.809036\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:29 INFO 139940971021696] processed a total of 623 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631460448.4437697, \"EndTime\": 1631460449.0110722, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 566.7760372161865, \"count\": 1, \"min\": 566.7760372161865, \"max\": 566.7760372161865}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:29 INFO 139940971021696] #throughput_metric: host=algo-1, train throughput=1098.9781731161495 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:29 INFO 139940971021696] #progress_metric: host=algo-1, completed 24.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:29 INFO 139940971021696] #quality_metric: host=algo-1, epoch=47, train loss <loss>=9.852566814422607\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:29 INFO 139940971021696] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:29 INFO 139940971021696] Epoch[48] Batch[0] avg_epoch_loss=10.567697\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:29 INFO 139940971021696] #quality_metric: host=algo-1, epoch=48, batch=0 train loss <loss>=10.567696571350098\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:29 INFO 139940971021696] Epoch[48] Batch[5] avg_epoch_loss=10.071024\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:29 INFO 139940971021696] #quality_metric: host=algo-1, epoch=48, batch=5 train loss <loss>=10.071024258931478\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:29 INFO 139940971021696] Epoch[48] Batch [5]#011Speed: 1981.92 samples/sec#011loss=10.071024\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:29 INFO 139940971021696] processed a total of 639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631460449.0111454, \"EndTime\": 1631460449.5753307, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 563.7719631195068, \"count\": 1, \"min\": 563.7719631195068, \"max\": 563.7719631195068}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:29 INFO 139940971021696] #throughput_metric: host=algo-1, train throughput=1133.159531711772 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:29 INFO 139940971021696] #progress_metric: host=algo-1, completed 24.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:29 INFO 139940971021696] #quality_metric: host=algo-1, epoch=48, train loss <loss>=10.123363971710205\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:29 INFO 139940971021696] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:29 INFO 139940971021696] Epoch[49] Batch[0] avg_epoch_loss=9.324324\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:29 INFO 139940971021696] #quality_metric: host=algo-1, epoch=49, batch=0 train loss <loss>=9.324323654174805\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:29 INFO 139940971021696] Epoch[49] Batch[5] avg_epoch_loss=9.755439\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:29 INFO 139940971021696] #quality_metric: host=algo-1, epoch=49, batch=5 train loss <loss>=9.755438645680746\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:29 INFO 139940971021696] Epoch[49] Batch [5]#011Speed: 1925.51 samples/sec#011loss=9.755439\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:30 INFO 139940971021696] processed a total of 579 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631460449.5754294, \"EndTime\": 1631460450.128406, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 552.4258613586426, \"count\": 1, \"min\": 552.4258613586426, \"max\": 552.4258613586426}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:30 INFO 139940971021696] #throughput_metric: host=algo-1, train throughput=1047.8358189120293 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:30 INFO 139940971021696] #progress_metric: host=algo-1, completed 25.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:30 INFO 139940971021696] #quality_metric: host=algo-1, epoch=49, train loss <loss>=9.66535563468933\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:30 INFO 139940971021696] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:30 INFO 139940971021696] Epoch[50] Batch[0] avg_epoch_loss=10.907867\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:30 INFO 139940971021696] #quality_metric: host=algo-1, epoch=50, batch=0 train loss <loss>=10.907867431640625\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:30 INFO 139940971021696] Epoch[50] Batch[5] avg_epoch_loss=10.496431\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:30 INFO 139940971021696] #quality_metric: host=algo-1, epoch=50, batch=5 train loss <loss>=10.49643071492513\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:30 INFO 139940971021696] Epoch[50] Batch [5]#011Speed: 2024.46 samples/sec#011loss=10.496431\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:30 INFO 139940971021696] Epoch[50] Batch[10] avg_epoch_loss=10.257089\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:30 INFO 139940971021696] #quality_metric: host=algo-1, epoch=50, batch=10 train loss <loss>=9.969878578186036\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:30 INFO 139940971021696] Epoch[50] Batch [10]#011Speed: 1728.84 samples/sec#011loss=9.969879\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:30 INFO 139940971021696] processed a total of 692 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631460450.1285052, \"EndTime\": 1631460450.7326229, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 603.5726070404053, \"count\": 1, \"min\": 603.5726070404053, \"max\": 603.5726070404053}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:30 INFO 139940971021696] #throughput_metric: host=algo-1, train throughput=1146.2852196001984 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:30 INFO 139940971021696] #progress_metric: host=algo-1, completed 25.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:30 INFO 139940971021696] #quality_metric: host=algo-1, epoch=50, train loss <loss>=10.257088834589178\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:30 INFO 139940971021696] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:30 INFO 139940971021696] Epoch[51] Batch[0] avg_epoch_loss=10.781137\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:30 INFO 139940971021696] #quality_metric: host=algo-1, epoch=51, batch=0 train loss <loss>=10.781137466430664\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:31 INFO 139940971021696] Epoch[51] Batch[5] avg_epoch_loss=10.245625\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:31 INFO 139940971021696] #quality_metric: host=algo-1, epoch=51, batch=5 train loss <loss>=10.245624542236328\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:31 INFO 139940971021696] Epoch[51] Batch [5]#011Speed: 2024.12 samples/sec#011loss=10.245625\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:31 INFO 139940971021696] processed a total of 607 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631460450.7326994, \"EndTime\": 1631460451.27097, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 537.7590656280518, \"count\": 1, \"min\": 537.7590656280518, \"max\": 537.7590656280518}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:31 INFO 139940971021696] #throughput_metric: host=algo-1, train throughput=1128.4916114745276 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:31 INFO 139940971021696] #progress_metric: host=algo-1, completed 26.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:31 INFO 139940971021696] #quality_metric: host=algo-1, epoch=51, train loss <loss>=10.481815624237061\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:31 INFO 139940971021696] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:31 INFO 139940971021696] Epoch[52] Batch[0] avg_epoch_loss=9.385182\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:31 INFO 139940971021696] #quality_metric: host=algo-1, epoch=52, batch=0 train loss <loss>=9.38518238067627\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:31 INFO 139940971021696] Epoch[52] Batch[5] avg_epoch_loss=9.917719\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:31 INFO 139940971021696] #quality_metric: host=algo-1, epoch=52, batch=5 train loss <loss>=9.917718887329102\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:31 INFO 139940971021696] Epoch[52] Batch [5]#011Speed: 1635.56 samples/sec#011loss=9.917719\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:31 INFO 139940971021696] processed a total of 623 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631460451.2710555, \"EndTime\": 1631460451.8608851, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 589.2922878265381, \"count\": 1, \"min\": 589.2922878265381, \"max\": 589.2922878265381}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:31 INFO 139940971021696] #throughput_metric: host=algo-1, train throughput=1056.955297292662 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:31 INFO 139940971021696] #progress_metric: host=algo-1, completed 26.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:31 INFO 139940971021696] #quality_metric: host=algo-1, epoch=52, train loss <loss>=9.97598991394043\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:31 INFO 139940971021696] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:32 INFO 139940971021696] Epoch[53] Batch[0] avg_epoch_loss=10.449670\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:32 INFO 139940971021696] #quality_metric: host=algo-1, epoch=53, batch=0 train loss <loss>=10.44966983795166\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:32 INFO 139940971021696] Epoch[53] Batch[5] avg_epoch_loss=9.937151\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:32 INFO 139940971021696] #quality_metric: host=algo-1, epoch=53, batch=5 train loss <loss>=9.937150637308756\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:32 INFO 139940971021696] Epoch[53] Batch [5]#011Speed: 2039.67 samples/sec#011loss=9.937151\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:32 INFO 139940971021696] processed a total of 630 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631460451.8609807, \"EndTime\": 1631460452.4165227, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 555.01389503479, \"count\": 1, \"min\": 555.01389503479, \"max\": 555.01389503479}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:32 INFO 139940971021696] #throughput_metric: host=algo-1, train throughput=1134.8152003762086 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:32 INFO 139940971021696] #progress_metric: host=algo-1, completed 27.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:32 INFO 139940971021696] #quality_metric: host=algo-1, epoch=53, train loss <loss>=9.90327558517456\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:32 INFO 139940971021696] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:32 INFO 139940971021696] Epoch[54] Batch[0] avg_epoch_loss=10.561111\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:32 INFO 139940971021696] #quality_metric: host=algo-1, epoch=54, batch=0 train loss <loss>=10.561111450195312\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:32 INFO 139940971021696] Epoch[54] Batch[5] avg_epoch_loss=10.137008\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:32 INFO 139940971021696] #quality_metric: host=algo-1, epoch=54, batch=5 train loss <loss>=10.13700819015503\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:32 INFO 139940971021696] Epoch[54] Batch [5]#011Speed: 2017.31 samples/sec#011loss=10.137008\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:33 INFO 139940971021696] Epoch[54] Batch[10] avg_epoch_loss=10.326307\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:33 INFO 139940971021696] #quality_metric: host=algo-1, epoch=54, batch=10 train loss <loss>=10.553465461730957\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:33 INFO 139940971021696] Epoch[54] Batch [10]#011Speed: 1898.02 samples/sec#011loss=10.553465\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:33 INFO 139940971021696] processed a total of 650 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631460452.4166226, \"EndTime\": 1631460453.0242789, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 607.1093082427979, \"count\": 1, \"min\": 607.1093082427979, \"max\": 607.1093082427979}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:33 INFO 139940971021696] #throughput_metric: host=algo-1, train throughput=1070.416195678676 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:33 INFO 139940971021696] #progress_metric: host=algo-1, completed 27.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:33 INFO 139940971021696] #quality_metric: host=algo-1, epoch=54, train loss <loss>=10.32630694996227\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:33 INFO 139940971021696] loss did not improve\u001b[0m\n",
      "\n",
      "2021-09-12 15:27:41 Uploading - Uploading generated training model\u001b[34m[09/12/2021 15:27:33 INFO 139940971021696] Epoch[55] Batch[0] avg_epoch_loss=9.604989\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:33 INFO 139940971021696] #quality_metric: host=algo-1, epoch=55, batch=0 train loss <loss>=9.604989051818848\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:33 INFO 139940971021696] Epoch[55] Batch[5] avg_epoch_loss=9.853861\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:33 INFO 139940971021696] #quality_metric: host=algo-1, epoch=55, batch=5 train loss <loss>=9.853861172993978\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:33 INFO 139940971021696] Epoch[55] Batch [5]#011Speed: 1908.80 samples/sec#011loss=9.853861\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:33 INFO 139940971021696] processed a total of 627 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631460453.0243673, \"EndTime\": 1631460453.5878258, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 562.9425048828125, \"count\": 1, \"min\": 562.9425048828125, \"max\": 562.9425048828125}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:33 INFO 139940971021696] #throughput_metric: host=algo-1, train throughput=1113.511761440597 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:33 INFO 139940971021696] #progress_metric: host=algo-1, completed 28.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:33 INFO 139940971021696] #quality_metric: host=algo-1, epoch=55, train loss <loss>=9.680038356781006\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:33 INFO 139940971021696] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:33 INFO 139940971021696] Epoch[56] Batch[0] avg_epoch_loss=10.371727\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:33 INFO 139940971021696] #quality_metric: host=algo-1, epoch=56, batch=0 train loss <loss>=10.371726989746094\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:34 INFO 139940971021696] Epoch[56] Batch[5] avg_epoch_loss=10.128420\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:34 INFO 139940971021696] #quality_metric: host=algo-1, epoch=56, batch=5 train loss <loss>=10.128420193990072\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:34 INFO 139940971021696] Epoch[56] Batch [5]#011Speed: 2085.33 samples/sec#011loss=10.128420\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:34 INFO 139940971021696] processed a total of 626 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631460453.5879242, \"EndTime\": 1631460454.133962, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 545.4967021942139, \"count\": 1, \"min\": 545.4967021942139, \"max\": 545.4967021942139}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:34 INFO 139940971021696] #throughput_metric: host=algo-1, train throughput=1147.2540516685783 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:34 INFO 139940971021696] #progress_metric: host=algo-1, completed 28.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:34 INFO 139940971021696] #quality_metric: host=algo-1, epoch=56, train loss <loss>=10.000306606292725\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:34 INFO 139940971021696] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:34 INFO 139940971021696] Epoch[57] Batch[0] avg_epoch_loss=10.011633\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:34 INFO 139940971021696] #quality_metric: host=algo-1, epoch=57, batch=0 train loss <loss>=10.011632919311523\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:34 INFO 139940971021696] Epoch[57] Batch[5] avg_epoch_loss=10.032240\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:34 INFO 139940971021696] #quality_metric: host=algo-1, epoch=57, batch=5 train loss <loss>=10.032240231831869\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:34 INFO 139940971021696] Epoch[57] Batch [5]#011Speed: 1585.09 samples/sec#011loss=10.032240\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:34 INFO 139940971021696] Epoch[57] Batch[10] avg_epoch_loss=10.262264\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:34 INFO 139940971021696] #quality_metric: host=algo-1, epoch=57, batch=10 train loss <loss>=10.538291931152344\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:34 INFO 139940971021696] Epoch[57] Batch [10]#011Speed: 1907.43 samples/sec#011loss=10.538292\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:34 INFO 139940971021696] processed a total of 646 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631460454.1340754, \"EndTime\": 1631460454.7711678, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 636.5392208099365, \"count\": 1, \"min\": 636.5392208099365, \"max\": 636.5392208099365}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:34 INFO 139940971021696] #throughput_metric: host=algo-1, train throughput=1014.6606535196795 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:34 INFO 139940971021696] #progress_metric: host=algo-1, completed 29.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:34 INFO 139940971021696] #quality_metric: host=algo-1, epoch=57, train loss <loss>=10.262263731522994\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:34 INFO 139940971021696] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:35 INFO 139940971021696] Epoch[58] Batch[0] avg_epoch_loss=9.648558\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:35 INFO 139940971021696] #quality_metric: host=algo-1, epoch=58, batch=0 train loss <loss>=9.648557662963867\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:35 INFO 139940971021696] Epoch[58] Batch[5] avg_epoch_loss=9.476554\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:35 INFO 139940971021696] #quality_metric: host=algo-1, epoch=58, batch=5 train loss <loss>=9.476553757985434\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:35 INFO 139940971021696] Epoch[58] Batch [5]#011Speed: 1852.14 samples/sec#011loss=9.476554\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:35 INFO 139940971021696] Epoch[58] Batch[10] avg_epoch_loss=9.557746\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:35 INFO 139940971021696] #quality_metric: host=algo-1, epoch=58, batch=10 train loss <loss>=9.655176544189453\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:35 INFO 139940971021696] Epoch[58] Batch [10]#011Speed: 1683.47 samples/sec#011loss=9.655177\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:35 INFO 139940971021696] processed a total of 668 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631460454.7712545, \"EndTime\": 1631460455.3878217, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 615.9937381744385, \"count\": 1, \"min\": 615.9937381744385, \"max\": 615.9937381744385}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:35 INFO 139940971021696] #throughput_metric: host=algo-1, train throughput=1084.1928719746213 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:35 INFO 139940971021696] #progress_metric: host=algo-1, completed 29.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:35 INFO 139940971021696] #quality_metric: host=algo-1, epoch=58, train loss <loss>=9.557745933532715\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:35 INFO 139940971021696] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:35 INFO 139940971021696] Epoch[59] Batch[0] avg_epoch_loss=10.390606\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:35 INFO 139940971021696] #quality_metric: host=algo-1, epoch=59, batch=0 train loss <loss>=10.390605926513672\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:35 INFO 139940971021696] Epoch[59] Batch[5] avg_epoch_loss=9.878969\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:35 INFO 139940971021696] #quality_metric: host=algo-1, epoch=59, batch=5 train loss <loss>=9.878969192504883\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:35 INFO 139940971021696] Epoch[59] Batch [5]#011Speed: 2013.26 samples/sec#011loss=9.878969\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:35 INFO 139940971021696] processed a total of 626 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631460455.3879118, \"EndTime\": 1631460455.979347, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 590.9526348114014, \"count\": 1, \"min\": 590.9526348114014, \"max\": 590.9526348114014}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:35 INFO 139940971021696] #throughput_metric: host=algo-1, train throughput=1059.0638620556688 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:35 INFO 139940971021696] #progress_metric: host=algo-1, completed 30.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:35 INFO 139940971021696] #quality_metric: host=algo-1, epoch=59, train loss <loss>=9.888445281982422\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:35 INFO 139940971021696] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:36 INFO 139940971021696] Epoch[60] Batch[0] avg_epoch_loss=10.847204\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:36 INFO 139940971021696] #quality_metric: host=algo-1, epoch=60, batch=0 train loss <loss>=10.847204208374023\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:36 INFO 139940971021696] Epoch[60] Batch[5] avg_epoch_loss=10.076617\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:36 INFO 139940971021696] #quality_metric: host=algo-1, epoch=60, batch=5 train loss <loss>=10.076616923014322\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:36 INFO 139940971021696] Epoch[60] Batch [5]#011Speed: 2086.35 samples/sec#011loss=10.076617\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:36 INFO 139940971021696] processed a total of 639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631460455.979442, \"EndTime\": 1631460456.5440083, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 564.0404224395752, \"count\": 1, \"min\": 564.0404224395752, \"max\": 564.0404224395752}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:36 INFO 139940971021696] #throughput_metric: host=algo-1, train throughput=1132.6231996382585 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:36 INFO 139940971021696] #progress_metric: host=algo-1, completed 30.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:36 INFO 139940971021696] #quality_metric: host=algo-1, epoch=60, train loss <loss>=10.159080123901367\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:36 INFO 139940971021696] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:36 INFO 139940971021696] Epoch[61] Batch[0] avg_epoch_loss=9.756624\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:36 INFO 139940971021696] #quality_metric: host=algo-1, epoch=61, batch=0 train loss <loss>=9.756624221801758\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:37 INFO 139940971021696] Epoch[61] Batch[5] avg_epoch_loss=9.825951\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:37 INFO 139940971021696] #quality_metric: host=algo-1, epoch=61, batch=5 train loss <loss>=9.82595125834147\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:37 INFO 139940971021696] Epoch[61] Batch [5]#011Speed: 2023.21 samples/sec#011loss=9.825951\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:37 INFO 139940971021696] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631460456.5441024, \"EndTime\": 1631460457.1310887, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 586.4615440368652, \"count\": 1, \"min\": 586.4615440368652, \"max\": 586.4615440368652}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:37 INFO 139940971021696] #throughput_metric: host=algo-1, train throughput=1079.1155672196926 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:37 INFO 139940971021696] #progress_metric: host=algo-1, completed 31.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:37 INFO 139940971021696] #quality_metric: host=algo-1, epoch=61, train loss <loss>=10.034099960327149\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:37 INFO 139940971021696] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:37 INFO 139940971021696] Epoch[62] Batch[0] avg_epoch_loss=10.278923\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:37 INFO 139940971021696] #quality_metric: host=algo-1, epoch=62, batch=0 train loss <loss>=10.278923034667969\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:37 INFO 139940971021696] Epoch[62] Batch[5] avg_epoch_loss=10.109135\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:37 INFO 139940971021696] #quality_metric: host=algo-1, epoch=62, batch=5 train loss <loss>=10.109135309855143\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:37 INFO 139940971021696] Epoch[62] Batch [5]#011Speed: 1776.04 samples/sec#011loss=10.109135\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:37 INFO 139940971021696] processed a total of 636 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631460457.1311793, \"EndTime\": 1631460457.7386017, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 606.9250106811523, \"count\": 1, \"min\": 606.9250106811523, \"max\": 606.9250106811523}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:37 INFO 139940971021696] #throughput_metric: host=algo-1, train throughput=1047.6736992629383 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:37 INFO 139940971021696] #progress_metric: host=algo-1, completed 31.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:37 INFO 139940971021696] #quality_metric: host=algo-1, epoch=62, train loss <loss>=9.931065273284911\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:37 INFO 139940971021696] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:37 INFO 139940971021696] Epoch[63] Batch[0] avg_epoch_loss=9.230184\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:38 INFO 139940971021696] #quality_metric: host=algo-1, epoch=63, batch=0 train loss <loss>=9.230183601379395\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:38 INFO 139940971021696] Epoch[63] Batch[5] avg_epoch_loss=9.891781\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:38 INFO 139940971021696] #quality_metric: host=algo-1, epoch=63, batch=5 train loss <loss>=9.891781012217203\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:38 INFO 139940971021696] Epoch[63] Batch [5]#011Speed: 1909.00 samples/sec#011loss=9.891781\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:38 INFO 139940971021696] Epoch[63] Batch[10] avg_epoch_loss=10.101083\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:38 INFO 139940971021696] #quality_metric: host=algo-1, epoch=63, batch=10 train loss <loss>=10.352245330810547\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:38 INFO 139940971021696] Epoch[63] Batch [10]#011Speed: 1903.45 samples/sec#011loss=10.352245\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:38 INFO 139940971021696] processed a total of 662 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631460457.738695, \"EndTime\": 1631460458.3362844, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 597.069263458252, \"count\": 1, \"min\": 597.069263458252, \"max\": 597.069263458252}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:38 INFO 139940971021696] #throughput_metric: host=algo-1, train throughput=1108.5162534892781 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:38 INFO 139940971021696] #progress_metric: host=algo-1, completed 32.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:38 INFO 139940971021696] #quality_metric: host=algo-1, epoch=63, train loss <loss>=10.101082975214178\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:38 INFO 139940971021696] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:38 INFO 139940971021696] Epoch[64] Batch[0] avg_epoch_loss=9.865070\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:38 INFO 139940971021696] #quality_metric: host=algo-1, epoch=64, batch=0 train loss <loss>=9.865070343017578\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:38 INFO 139940971021696] Epoch[64] Batch[5] avg_epoch_loss=10.013855\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:38 INFO 139940971021696] #quality_metric: host=algo-1, epoch=64, batch=5 train loss <loss>=10.013855139414469\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:38 INFO 139940971021696] Epoch[64] Batch [5]#011Speed: 2086.00 samples/sec#011loss=10.013855\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:38 INFO 139940971021696] Epoch[64] Batch[10] avg_epoch_loss=10.200946\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:38 INFO 139940971021696] #quality_metric: host=algo-1, epoch=64, batch=10 train loss <loss>=10.425455665588379\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:38 INFO 139940971021696] Epoch[64] Batch [10]#011Speed: 1865.36 samples/sec#011loss=10.425456\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:38 INFO 139940971021696] processed a total of 645 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631460458.336372, \"EndTime\": 1631460458.940238, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 603.379487991333, \"count\": 1, \"min\": 603.379487991333, \"max\": 603.379487991333}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:38 INFO 139940971021696] #throughput_metric: host=algo-1, train throughput=1068.7441971814628 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:38 INFO 139940971021696] #progress_metric: host=algo-1, completed 32.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:38 INFO 139940971021696] #quality_metric: host=algo-1, epoch=64, train loss <loss>=10.200946287675338\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:38 INFO 139940971021696] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:39 INFO 139940971021696] Epoch[65] Batch[0] avg_epoch_loss=9.107023\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:39 INFO 139940971021696] #quality_metric: host=algo-1, epoch=65, batch=0 train loss <loss>=9.107023239135742\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:39 INFO 139940971021696] Epoch[65] Batch[5] avg_epoch_loss=9.827850\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:39 INFO 139940971021696] #quality_metric: host=algo-1, epoch=65, batch=5 train loss <loss>=9.827850341796875\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:39 INFO 139940971021696] Epoch[65] Batch [5]#011Speed: 2034.92 samples/sec#011loss=9.827850\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:39 INFO 139940971021696] Epoch[65] Batch[10] avg_epoch_loss=9.698823\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:39 INFO 139940971021696] #quality_metric: host=algo-1, epoch=65, batch=10 train loss <loss>=9.543990325927734\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:39 INFO 139940971021696] Epoch[65] Batch [10]#011Speed: 1842.44 samples/sec#011loss=9.543990\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:39 INFO 139940971021696] processed a total of 641 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631460458.940327, \"EndTime\": 1631460459.5374734, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 596.6277122497559, \"count\": 1, \"min\": 596.6277122497559, \"max\": 596.6277122497559}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:39 INFO 139940971021696] #throughput_metric: host=algo-1, train throughput=1074.1730751635803 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:39 INFO 139940971021696] #progress_metric: host=algo-1, completed 33.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:39 INFO 139940971021696] #quality_metric: host=algo-1, epoch=65, train loss <loss>=9.698823061856357\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:39 INFO 139940971021696] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:39 INFO 139940971021696] Loading parameters from best epoch (25)\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631460459.5375426, \"EndTime\": 1631460459.5443022, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.deserialize.time\": {\"sum\": 6.115436553955078, \"count\": 1, \"min\": 6.115436553955078, \"max\": 6.115436553955078}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:39 INFO 139940971021696] stopping training now\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:39 INFO 139940971021696] #progress_metric: host=algo-1, completed 100 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:39 INFO 139940971021696] Final loss: 9.430383920669556 (occurred at epoch 25)\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:39 INFO 139940971021696] #quality_metric: host=algo-1, train final_loss <loss>=9.430383920669556\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:39 INFO 139940971021696] Worker algo-1 finished training.\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:39 WARNING 139940971021696] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:39 INFO 139940971021696] All workers finished. Serializing model for prediction.\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631460459.5443592, \"EndTime\": 1631460459.6555033, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"get_graph.time\": {\"sum\": 110.30173301696777, \"count\": 1, \"min\": 110.30173301696777, \"max\": 110.30173301696777}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:39 INFO 139940971021696] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631460459.6555696, \"EndTime\": 1631460459.688457, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"finalize.time\": {\"sum\": 143.30196380615234, \"count\": 1, \"min\": 143.30196380615234, \"max\": 143.30196380615234}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:39 INFO 139940971021696] Serializing to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:39 INFO 139940971021696] Saved checkpoint to \"/opt/ml/model/model_algo-1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631460459.688526, \"EndTime\": 1631460459.6942027, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"model.serialize.time\": {\"sum\": 5.6362152099609375, \"count\": 1, \"min\": 5.6362152099609375, \"max\": 5.6362152099609375}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:39 INFO 139940971021696] Successfully serialized the model for prediction.\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:27:39 INFO 139940971021696] No test data passed, skipping evaluation.\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631460459.694251, \"EndTime\": 1631460459.6995473, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"setuptime\": {\"sum\": 6.938457489013672, \"count\": 1, \"min\": 6.938457489013672, \"max\": 6.938457489013672}, \"totaltime\": {\"sum\": 40854.00176048279, \"count\": 1, \"min\": 40854.00176048279, \"max\": 40854.00176048279}}}\n",
      "\u001b[0m\n",
      "\n",
      "2021-09-12 15:28:10 Completed - Training job completed\n",
      "ProfilerReport-1631460226: NoIssuesFound\n",
      "Training seconds: 102\n",
      "Billable seconds: 102\n",
      "-------------!Overwriting existing file\n",
      "Uploading file to s3://sagemaker-us-east-2-222195595958/deepar-covid-demo-notebook/data/train/train.json\n",
      "2021-09-12 15:35:03 Starting - Starting the training job...\n",
      "2021-09-12 15:35:27 Starting - Launching requested ML instancesProfilerReport-1631460903: InProgress\n",
      "...\n",
      "2021-09-12 15:35:56 Starting - Preparing the instances for training.........\n",
      "2021-09-12 15:37:27 Downloading - Downloading input data\n",
      "2021-09-12 15:37:27 Training - Downloading the training image...\n",
      "2021-09-12 15:37:47 Training - Training image download completed. Training in progress.\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:48 INFO 140391921509760] Reading default configuration from /opt/amazon/lib/python3.6/site-packages/algorithm/resources/default-input.json: {'_kvstore': 'auto', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_tuning_objective_metric': '', 'cardinality': 'auto', 'dropout_rate': '0.10', 'early_stopping_patience': '', 'embedding_dimension': '10', 'learning_rate': '0.001', 'likelihood': 'student-t', 'mini_batch_size': '128', 'num_cells': '40', 'num_dynamic_feat': 'auto', 'num_eval_samples': '100', 'num_layers': '2', 'test_quantiles': '[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]'}\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:48 INFO 140391921509760] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {'prediction_length': '14', 'time_freq': 'D', 'context_length': '14', 'epochs': '200', 'learning_rate': '5E-4', 'early_stopping_patience': '40', 'mini_batch_size': '64'}\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:48 INFO 140391921509760] Final configuration: {'_kvstore': 'auto', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_tuning_objective_metric': '', 'cardinality': 'auto', 'dropout_rate': '0.10', 'early_stopping_patience': '40', 'embedding_dimension': '10', 'learning_rate': '5E-4', 'likelihood': 'student-t', 'mini_batch_size': '64', 'num_cells': '40', 'num_dynamic_feat': 'auto', 'num_eval_samples': '100', 'num_layers': '2', 'test_quantiles': '[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', 'prediction_length': '14', 'time_freq': 'D', 'context_length': '14', 'epochs': '200'}\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:48 INFO 140391921509760] Detected entry point for worker worker\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:49 INFO 140391921509760] Using early stopping with patience 40\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:49 INFO 140391921509760] random_seed is None\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:49 INFO 140391921509760] [cardinality=auto] `cat` field was NOT found in the file `/opt/ml/input/data/train/train.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:49 INFO 140391921509760] [num_dynamic_feat=auto] `dynamic_feat` field was NOT found in the file `/opt/ml/input/data/train/train.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:49 INFO 140391921509760] Training set statistics:\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:49 INFO 140391921509760] Integer time series\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:49 INFO 140391921509760] number of time series: 41\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:49 INFO 140391921509760] number of observations: 13492\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:49 INFO 140391921509760] mean target length: 329.0731707317073\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:49 INFO 140391921509760] min/mean/max target: 0.0/15369745.3476134/351933184.0\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:49 INFO 140391921509760] mean abs(target): 15369745.3476134\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:49 INFO 140391921509760] contains missing values: no\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:49 INFO 140391921509760] Small number of time series. Doing 16 passes over dataset with prob 0.975609756097561 per epoch.\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:49 INFO 140391921509760] No test channel found not running evaluations\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:49 INFO 140391921509760] #memory_usage::<batchbuffer> = 2.2568130493164062 mb\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:49 INFO 140391921509760] nvidia-smi took: 0.025274276733398438 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:49 INFO 140391921509760] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:49 INFO 140391921509760] Create Store: local\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461069.2207665, \"EndTime\": 1631461069.3012896, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"get_graph.time\": {\"sum\": 78.59158515930176, \"count\": 1, \"min\": 78.59158515930176, \"max\": 78.59158515930176}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:49 INFO 140391921509760] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:49 INFO 140391921509760] #memory_usage::<model> = 12 mb\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461069.3013606, \"EndTime\": 1631461069.404456, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"initialize.time\": {\"sum\": 183.579683303833, \"count\": 1, \"min\": 183.579683303833, \"max\": 183.579683303833}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:49 INFO 140391921509760] Epoch[0] Batch[0] avg_epoch_loss=14.167727\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:49 INFO 140391921509760] #quality_metric: host=algo-1, epoch=0, batch=0 train loss <loss>=14.167726516723633\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:49 INFO 140391921509760] Epoch[0] Batch[5] avg_epoch_loss=14.239325\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:49 INFO 140391921509760] #quality_metric: host=algo-1, epoch=0, batch=5 train loss <loss>=14.239325205485025\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:49 INFO 140391921509760] Epoch[0] Batch [5]#011Speed: 2101.25 samples/sec#011loss=14.239325\u001b[0m\n",
      "\u001b[34m/opt/amazon/python3.6/lib/python3.6/contextlib.py:99: DeprecationWarning: generator 'local_timer' raised StopIteration\n",
      "  self.gen.throw(type, value, traceback)\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:50 INFO 140391921509760] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461069.4045184, \"EndTime\": 1631461070.0127976, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"epochs\": {\"sum\": 200.0, \"count\": 1, \"min\": 200, \"max\": 200}, \"update.time\": {\"sum\": 608.1902980804443, \"count\": 1, \"min\": 608.1902980804443, \"max\": 608.1902980804443}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:50 INFO 140391921509760] #throughput_metric: host=algo-1, train throughput=1042.1852183240703 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:50 INFO 140391921509760] #progress_metric: host=algo-1, completed 0.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:50 INFO 140391921509760] #quality_metric: host=algo-1, epoch=0, train loss <loss>=13.903299808502197\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:50 INFO 140391921509760] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:50 INFO 140391921509760] Saved checkpoint to \"/opt/ml/model/state_37676c89-8216-47e6-ab16-c3a9c193fc47-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461070.0128982, \"EndTime\": 1631461070.0272195, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 13.729572296142578, \"count\": 1, \"min\": 13.729572296142578, \"max\": 13.729572296142578}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:50 INFO 140391921509760] Epoch[1] Batch[0] avg_epoch_loss=12.544996\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:50 INFO 140391921509760] #quality_metric: host=algo-1, epoch=1, batch=0 train loss <loss>=12.54499626159668\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:50 INFO 140391921509760] Epoch[1] Batch[5] avg_epoch_loss=12.730156\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:50 INFO 140391921509760] #quality_metric: host=algo-1, epoch=1, batch=5 train loss <loss>=12.730156262715658\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:50 INFO 140391921509760] Epoch[1] Batch [5]#011Speed: 2069.90 samples/sec#011loss=12.730156\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:50 INFO 140391921509760] processed a total of 604 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461070.0272787, \"EndTime\": 1631461070.574461, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 547.1212863922119, \"count\": 1, \"min\": 547.1212863922119, \"max\": 547.1212863922119}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:50 INFO 140391921509760] #throughput_metric: host=algo-1, train throughput=1103.7089096703503 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:50 INFO 140391921509760] #progress_metric: host=algo-1, completed 1.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:50 INFO 140391921509760] #quality_metric: host=algo-1, epoch=1, train loss <loss>=12.651425170898438\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:50 INFO 140391921509760] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:50 INFO 140391921509760] Saved checkpoint to \"/opt/ml/model/state_c8293e73-1b00-40be-98a0-299aa540bfaf-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461070.5745482, \"EndTime\": 1631461070.5937011, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 18.59760284423828, \"count\": 1, \"min\": 18.59760284423828, \"max\": 18.59760284423828}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:50 INFO 140391921509760] Epoch[2] Batch[0] avg_epoch_loss=12.925117\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:50 INFO 140391921509760] #quality_metric: host=algo-1, epoch=2, batch=0 train loss <loss>=12.925117492675781\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:51 INFO 140391921509760] Epoch[2] Batch[5] avg_epoch_loss=12.549348\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:51 INFO 140391921509760] #quality_metric: host=algo-1, epoch=2, batch=5 train loss <loss>=12.54934819539388\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:51 INFO 140391921509760] Epoch[2] Batch [5]#011Speed: 2084.60 samples/sec#011loss=12.549348\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:51 INFO 140391921509760] Epoch[2] Batch[10] avg_epoch_loss=12.003593\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:51 INFO 140391921509760] #quality_metric: host=algo-1, epoch=2, batch=10 train loss <loss>=11.348687171936035\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:51 INFO 140391921509760] Epoch[2] Batch [10]#011Speed: 1675.89 samples/sec#011loss=11.348687\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:51 INFO 140391921509760] processed a total of 659 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461070.5937672, \"EndTime\": 1631461071.2061946, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 612.3702526092529, \"count\": 1, \"min\": 612.3702526092529, \"max\": 612.3702526092529}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:51 INFO 140391921509760] #throughput_metric: host=algo-1, train throughput=1075.9402294859512 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:51 INFO 140391921509760] #progress_metric: host=algo-1, completed 1.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:51 INFO 140391921509760] #quality_metric: host=algo-1, epoch=2, train loss <loss>=12.003593184731223\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:51 INFO 140391921509760] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:51 INFO 140391921509760] Saved checkpoint to \"/opt/ml/model/state_47f768c2-f7e5-4ad5-95ce-911a23baf694-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461071.2062778, \"EndTime\": 1631461071.2250729, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 18.297910690307617, \"count\": 1, \"min\": 18.297910690307617, \"max\": 18.297910690307617}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:51 INFO 140391921509760] Epoch[3] Batch[0] avg_epoch_loss=12.045080\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:51 INFO 140391921509760] #quality_metric: host=algo-1, epoch=3, batch=0 train loss <loss>=12.045080184936523\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:51 INFO 140391921509760] Epoch[3] Batch[5] avg_epoch_loss=12.257322\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:51 INFO 140391921509760] #quality_metric: host=algo-1, epoch=3, batch=5 train loss <loss>=12.257321993509928\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:51 INFO 140391921509760] Epoch[3] Batch [5]#011Speed: 1891.82 samples/sec#011loss=12.257322\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:51 INFO 140391921509760] Epoch[3] Batch[10] avg_epoch_loss=11.979231\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:51 INFO 140391921509760] #quality_metric: host=algo-1, epoch=3, batch=10 train loss <loss>=11.645520782470703\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:51 INFO 140391921509760] Epoch[3] Batch [10]#011Speed: 1482.23 samples/sec#011loss=11.645521\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:51 INFO 140391921509760] processed a total of 647 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461071.2251415, \"EndTime\": 1631461071.8796349, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 654.4380187988281, \"count\": 1, \"min\": 654.4380187988281, \"max\": 654.4380187988281}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:51 INFO 140391921509760] #throughput_metric: host=algo-1, train throughput=988.4566027756914 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:51 INFO 140391921509760] #progress_metric: host=algo-1, completed 2.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:51 INFO 140391921509760] #quality_metric: host=algo-1, epoch=3, train loss <loss>=11.979230533946644\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:51 INFO 140391921509760] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:51 INFO 140391921509760] Saved checkpoint to \"/opt/ml/model/state_276b6fc5-a37f-4b22-8271-0f95b96e71d9-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461071.8797185, \"EndTime\": 1631461071.8989346, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 18.712282180786133, \"count\": 1, \"min\": 18.712282180786133, \"max\": 18.712282180786133}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:52 INFO 140391921509760] Epoch[4] Batch[0] avg_epoch_loss=11.969288\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:52 INFO 140391921509760] #quality_metric: host=algo-1, epoch=4, batch=0 train loss <loss>=11.969287872314453\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:52 INFO 140391921509760] Epoch[4] Batch[5] avg_epoch_loss=12.115577\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:52 INFO 140391921509760] #quality_metric: host=algo-1, epoch=4, batch=5 train loss <loss>=12.115577379862467\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:52 INFO 140391921509760] Epoch[4] Batch [5]#011Speed: 1863.42 samples/sec#011loss=12.115577\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:52 INFO 140391921509760] Epoch[4] Batch[10] avg_epoch_loss=11.953670\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:52 INFO 140391921509760] #quality_metric: host=algo-1, epoch=4, batch=10 train loss <loss>=11.759380340576172\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:52 INFO 140391921509760] Epoch[4] Batch [10]#011Speed: 2006.43 samples/sec#011loss=11.759380\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:52 INFO 140391921509760] processed a total of 650 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461071.8990035, \"EndTime\": 1631461072.494616, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 595.5536365509033, \"count\": 1, \"min\": 595.5536365509033, \"max\": 595.5536365509033}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:52 INFO 140391921509760] #throughput_metric: host=algo-1, train throughput=1091.2091206478976 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:52 INFO 140391921509760] #progress_metric: host=algo-1, completed 2.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:52 INFO 140391921509760] #quality_metric: host=algo-1, epoch=4, train loss <loss>=11.953669634732334\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:52 INFO 140391921509760] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:52 INFO 140391921509760] Saved checkpoint to \"/opt/ml/model/state_c5c97342-3335-4a21-b8e2-37a7eb8f1482-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461072.4946988, \"EndTime\": 1631461072.5109344, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 15.729665756225586, \"count\": 1, \"min\": 15.729665756225586, \"max\": 15.729665756225586}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:52 INFO 140391921509760] Epoch[5] Batch[0] avg_epoch_loss=11.317109\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:52 INFO 140391921509760] #quality_metric: host=algo-1, epoch=5, batch=0 train loss <loss>=11.317109107971191\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:52 INFO 140391921509760] Epoch[5] Batch[5] avg_epoch_loss=11.401425\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:52 INFO 140391921509760] #quality_metric: host=algo-1, epoch=5, batch=5 train loss <loss>=11.401424566904703\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:52 INFO 140391921509760] Epoch[5] Batch [5]#011Speed: 1524.92 samples/sec#011loss=11.401425\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:53 INFO 140391921509760] Epoch[5] Batch[10] avg_epoch_loss=11.648415\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:53 INFO 140391921509760] #quality_metric: host=algo-1, epoch=5, batch=10 train loss <loss>=11.944803237915039\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:53 INFO 140391921509760] Epoch[5] Batch [10]#011Speed: 1474.76 samples/sec#011loss=11.944803\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:53 INFO 140391921509760] processed a total of 643 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461072.5110116, \"EndTime\": 1631461073.1976047, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 686.5298748016357, \"count\": 1, \"min\": 686.5298748016357, \"max\": 686.5298748016357}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:53 INFO 140391921509760] #throughput_metric: host=algo-1, train throughput=936.3062545805254 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:53 INFO 140391921509760] #progress_metric: host=algo-1, completed 3.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:53 INFO 140391921509760] #quality_metric: host=algo-1, epoch=5, train loss <loss>=11.648414871909402\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:53 INFO 140391921509760] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:53 INFO 140391921509760] Saved checkpoint to \"/opt/ml/model/state_f899db4a-09b1-4f19-a3f1-1ed534a684cd-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461073.1977656, \"EndTime\": 1631461073.2177243, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 19.148826599121094, \"count\": 1, \"min\": 19.148826599121094, \"max\": 19.148826599121094}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:53 INFO 140391921509760] Epoch[6] Batch[0] avg_epoch_loss=11.858006\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:53 INFO 140391921509760] #quality_metric: host=algo-1, epoch=6, batch=0 train loss <loss>=11.85800552368164\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:53 INFO 140391921509760] Epoch[6] Batch[5] avg_epoch_loss=11.145684\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:53 INFO 140391921509760] #quality_metric: host=algo-1, epoch=6, batch=5 train loss <loss>=11.145684401194254\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:53 INFO 140391921509760] Epoch[6] Batch [5]#011Speed: 1864.26 samples/sec#011loss=11.145684\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:53 INFO 140391921509760] Epoch[6] Batch[10] avg_epoch_loss=10.359650\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:53 INFO 140391921509760] #quality_metric: host=algo-1, epoch=6, batch=10 train loss <loss>=9.416409730911255\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:53 INFO 140391921509760] Epoch[6] Batch [10]#011Speed: 1700.50 samples/sec#011loss=9.416410\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:53 INFO 140391921509760] processed a total of 647 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461073.2178023, \"EndTime\": 1631461073.8877668, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 669.9020862579346, \"count\": 1, \"min\": 669.9020862579346, \"max\": 669.9020862579346}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:53 INFO 140391921509760] #throughput_metric: host=algo-1, train throughput=965.6447292103014 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:53 INFO 140391921509760] #progress_metric: host=algo-1, completed 3.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:53 INFO 140391921509760] #quality_metric: host=algo-1, epoch=6, train loss <loss>=10.359650460156528\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:53 INFO 140391921509760] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:53 INFO 140391921509760] Saved checkpoint to \"/opt/ml/model/state_d7a98c2b-f7a0-4714-a249-2cb3c2bea6e5-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461073.8878486, \"EndTime\": 1631461073.9068816, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 18.54419708251953, \"count\": 1, \"min\": 18.54419708251953, \"max\": 18.54419708251953}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:54 INFO 140391921509760] Epoch[7] Batch[0] avg_epoch_loss=11.607087\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:54 INFO 140391921509760] #quality_metric: host=algo-1, epoch=7, batch=0 train loss <loss>=11.607087135314941\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:54 INFO 140391921509760] Epoch[7] Batch[5] avg_epoch_loss=11.372609\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:54 INFO 140391921509760] #quality_metric: host=algo-1, epoch=7, batch=5 train loss <loss>=11.372608502705893\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:54 INFO 140391921509760] Epoch[7] Batch [5]#011Speed: 2014.97 samples/sec#011loss=11.372609\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:54 INFO 140391921509760] processed a total of 625 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461073.906947, \"EndTime\": 1631461074.4519694, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 544.9669361114502, \"count\": 1, \"min\": 544.9669361114502, \"max\": 544.9669361114502}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:54 INFO 140391921509760] #throughput_metric: host=algo-1, train throughput=1146.6017340916576 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:54 INFO 140391921509760] #progress_metric: host=algo-1, completed 4.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:54 INFO 140391921509760] #quality_metric: host=algo-1, epoch=7, train loss <loss>=11.257083702087403\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:54 INFO 140391921509760] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:54 INFO 140391921509760] Epoch[8] Batch[0] avg_epoch_loss=11.618073\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:54 INFO 140391921509760] #quality_metric: host=algo-1, epoch=8, batch=0 train loss <loss>=11.618072509765625\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:54 INFO 140391921509760] Epoch[8] Batch[5] avg_epoch_loss=11.022213\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:54 INFO 140391921509760] #quality_metric: host=algo-1, epoch=8, batch=5 train loss <loss>=11.022212982177734\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:54 INFO 140391921509760] Epoch[8] Batch [5]#011Speed: 1826.59 samples/sec#011loss=11.022213\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:55 INFO 140391921509760] processed a total of 597 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461074.4520574, \"EndTime\": 1631461075.0522497, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 599.6778011322021, \"count\": 1, \"min\": 599.6778011322021, \"max\": 599.6778011322021}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:55 INFO 140391921509760] #throughput_metric: host=algo-1, train throughput=995.3220996530682 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:55 INFO 140391921509760] #progress_metric: host=algo-1, completed 4.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:55 INFO 140391921509760] #quality_metric: host=algo-1, epoch=8, train loss <loss>=10.721546649932861\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:55 INFO 140391921509760] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:55 INFO 140391921509760] Epoch[9] Batch[0] avg_epoch_loss=11.463380\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:55 INFO 140391921509760] #quality_metric: host=algo-1, epoch=9, batch=0 train loss <loss>=11.463379859924316\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:55 INFO 140391921509760] Epoch[9] Batch[5] avg_epoch_loss=10.841678\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:55 INFO 140391921509760] #quality_metric: host=algo-1, epoch=9, batch=5 train loss <loss>=10.841678460439047\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:55 INFO 140391921509760] Epoch[9] Batch [5]#011Speed: 2070.97 samples/sec#011loss=10.841678\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:55 INFO 140391921509760] processed a total of 563 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461075.0523388, \"EndTime\": 1631461075.5804834, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 527.6353359222412, \"count\": 1, \"min\": 527.6353359222412, \"max\": 527.6353359222412}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:55 INFO 140391921509760] #throughput_metric: host=algo-1, train throughput=1066.7794641421197 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:55 INFO 140391921509760] #progress_metric: host=algo-1, completed 5.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:55 INFO 140391921509760] #quality_metric: host=algo-1, epoch=9, train loss <loss>=10.961414125230577\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:55 INFO 140391921509760] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:55 INFO 140391921509760] Epoch[10] Batch[0] avg_epoch_loss=11.959420\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:55 INFO 140391921509760] #quality_metric: host=algo-1, epoch=10, batch=0 train loss <loss>=11.959420204162598\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:56 INFO 140391921509760] Epoch[10] Batch[5] avg_epoch_loss=10.867325\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:56 INFO 140391921509760] #quality_metric: host=algo-1, epoch=10, batch=5 train loss <loss>=10.867324829101562\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:56 INFO 140391921509760] Epoch[10] Batch [5]#011Speed: 2007.13 samples/sec#011loss=10.867325\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:56 INFO 140391921509760] processed a total of 621 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461075.5805671, \"EndTime\": 1631461076.1837826, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 602.6883125305176, \"count\": 1, \"min\": 602.6883125305176, \"max\": 602.6883125305176}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:56 INFO 140391921509760] #throughput_metric: host=algo-1, train throughput=1030.1579861818352 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:56 INFO 140391921509760] #progress_metric: host=algo-1, completed 5.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:56 INFO 140391921509760] #quality_metric: host=algo-1, epoch=10, train loss <loss>=10.664752960205078\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:56 INFO 140391921509760] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:56 INFO 140391921509760] Epoch[11] Batch[0] avg_epoch_loss=10.850500\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:56 INFO 140391921509760] #quality_metric: host=algo-1, epoch=11, batch=0 train loss <loss>=10.850500106811523\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:56 INFO 140391921509760] Epoch[11] Batch[5] avg_epoch_loss=10.901259\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:56 INFO 140391921509760] #quality_metric: host=algo-1, epoch=11, batch=5 train loss <loss>=10.901258627573648\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:56 INFO 140391921509760] Epoch[11] Batch [5]#011Speed: 1902.92 samples/sec#011loss=10.901259\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:56 INFO 140391921509760] processed a total of 628 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461076.1838763, \"EndTime\": 1631461076.8051972, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 620.7973957061768, \"count\": 1, \"min\": 620.7973957061768, \"max\": 620.7973957061768}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:56 INFO 140391921509760] #throughput_metric: host=algo-1, train throughput=1011.3337940736509 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:56 INFO 140391921509760] #progress_metric: host=algo-1, completed 6.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:56 INFO 140391921509760] #quality_metric: host=algo-1, epoch=11, train loss <loss>=10.952054977416992\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:56 INFO 140391921509760] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:57 INFO 140391921509760] Epoch[12] Batch[0] avg_epoch_loss=9.971716\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:57 INFO 140391921509760] #quality_metric: host=algo-1, epoch=12, batch=0 train loss <loss>=9.971715927124023\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:57 INFO 140391921509760] Epoch[12] Batch[5] avg_epoch_loss=10.574152\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:57 INFO 140391921509760] #quality_metric: host=algo-1, epoch=12, batch=5 train loss <loss>=10.57415246963501\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:57 INFO 140391921509760] Epoch[12] Batch [5]#011Speed: 1930.65 samples/sec#011loss=10.574152\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:57 INFO 140391921509760] Epoch[12] Batch[10] avg_epoch_loss=10.790989\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:57 INFO 140391921509760] #quality_metric: host=algo-1, epoch=12, batch=10 train loss <loss>=11.051192092895509\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:57 INFO 140391921509760] Epoch[12] Batch [10]#011Speed: 1795.91 samples/sec#011loss=11.051192\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:57 INFO 140391921509760] processed a total of 670 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461076.8053207, \"EndTime\": 1631461077.4509697, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 644.9050903320312, \"count\": 1, \"min\": 644.9050903320312, \"max\": 644.9050903320312}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:57 INFO 140391921509760] #throughput_metric: host=algo-1, train throughput=1038.7305398606195 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:57 INFO 140391921509760] #progress_metric: host=algo-1, completed 6.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:57 INFO 140391921509760] #quality_metric: host=algo-1, epoch=12, train loss <loss>=10.790988662026145\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:57 INFO 140391921509760] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:57 INFO 140391921509760] Epoch[13] Batch[0] avg_epoch_loss=10.201305\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:57 INFO 140391921509760] #quality_metric: host=algo-1, epoch=13, batch=0 train loss <loss>=10.201305389404297\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:57 INFO 140391921509760] Epoch[13] Batch[5] avg_epoch_loss=10.470851\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:57 INFO 140391921509760] #quality_metric: host=algo-1, epoch=13, batch=5 train loss <loss>=10.470851262410482\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:57 INFO 140391921509760] Epoch[13] Batch [5]#011Speed: 2014.07 samples/sec#011loss=10.470851\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:57 INFO 140391921509760] processed a total of 630 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461077.4510443, \"EndTime\": 1631461077.9793935, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 527.8396606445312, \"count\": 1, \"min\": 527.8396606445312, \"max\": 527.8396606445312}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:57 INFO 140391921509760] #throughput_metric: host=algo-1, train throughput=1193.2559117254584 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:57 INFO 140391921509760] #progress_metric: host=algo-1, completed 7.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:57 INFO 140391921509760] #quality_metric: host=algo-1, epoch=13, train loss <loss>=10.425532722473145\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:57 INFO 140391921509760] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:58 INFO 140391921509760] Epoch[14] Batch[0] avg_epoch_loss=10.246346\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:58 INFO 140391921509760] #quality_metric: host=algo-1, epoch=14, batch=0 train loss <loss>=10.246346473693848\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:58 INFO 140391921509760] Epoch[14] Batch[5] avg_epoch_loss=10.381170\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:58 INFO 140391921509760] #quality_metric: host=algo-1, epoch=14, batch=5 train loss <loss>=10.38117011388143\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:58 INFO 140391921509760] Epoch[14] Batch [5]#011Speed: 2030.01 samples/sec#011loss=10.381170\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:58 INFO 140391921509760] processed a total of 606 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461077.9794815, \"EndTime\": 1631461078.5163436, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 536.3516807556152, \"count\": 1, \"min\": 536.3516807556152, \"max\": 536.3516807556152}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:58 INFO 140391921509760] #throughput_metric: host=algo-1, train throughput=1129.589567282566 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:58 INFO 140391921509760] #progress_metric: host=algo-1, completed 7.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:58 INFO 140391921509760] #quality_metric: host=algo-1, epoch=14, train loss <loss>=10.43569736480713\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:58 INFO 140391921509760] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:58 INFO 140391921509760] Epoch[15] Batch[0] avg_epoch_loss=11.293495\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:58 INFO 140391921509760] #quality_metric: host=algo-1, epoch=15, batch=0 train loss <loss>=11.293495178222656\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:58 INFO 140391921509760] Epoch[15] Batch[5] avg_epoch_loss=10.497484\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:58 INFO 140391921509760] #quality_metric: host=algo-1, epoch=15, batch=5 train loss <loss>=10.497483730316162\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:58 INFO 140391921509760] Epoch[15] Batch [5]#011Speed: 2084.28 samples/sec#011loss=10.497484\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:59 INFO 140391921509760] processed a total of 639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461078.5164354, \"EndTime\": 1631461079.0428855, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 525.8750915527344, \"count\": 1, \"min\": 525.8750915527344, \"max\": 525.8750915527344}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:59 INFO 140391921509760] #throughput_metric: host=algo-1, train throughput=1214.7991105301373 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:59 INFO 140391921509760] #progress_metric: host=algo-1, completed 8.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:59 INFO 140391921509760] #quality_metric: host=algo-1, epoch=15, train loss <loss>=10.500802040100098\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:59 INFO 140391921509760] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:59 INFO 140391921509760] Epoch[16] Batch[0] avg_epoch_loss=10.591850\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:59 INFO 140391921509760] #quality_metric: host=algo-1, epoch=16, batch=0 train loss <loss>=10.591850280761719\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:59 INFO 140391921509760] Epoch[16] Batch[5] avg_epoch_loss=10.216042\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:59 INFO 140391921509760] #quality_metric: host=algo-1, epoch=16, batch=5 train loss <loss>=10.216042359670004\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:59 INFO 140391921509760] Epoch[16] Batch [5]#011Speed: 1966.53 samples/sec#011loss=10.216042\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:59 INFO 140391921509760] processed a total of 623 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461079.042982, \"EndTime\": 1631461079.5802696, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 536.745548248291, \"count\": 1, \"min\": 536.745548248291, \"max\": 536.745548248291}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:59 INFO 140391921509760] #throughput_metric: host=algo-1, train throughput=1160.423193974267 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:59 INFO 140391921509760] #progress_metric: host=algo-1, completed 8.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:59 INFO 140391921509760] #quality_metric: host=algo-1, epoch=16, train loss <loss>=10.149336051940917\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:59 INFO 140391921509760] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:59 INFO 140391921509760] Saved checkpoint to \"/opt/ml/model/state_53c9bdf9-89dd-4a56-857f-651e44b93960-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461079.5803585, \"EndTime\": 1631461079.5994594, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 18.5544490814209, \"count\": 1, \"min\": 18.5544490814209, \"max\": 18.5544490814209}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:59 INFO 140391921509760] Epoch[17] Batch[0] avg_epoch_loss=10.629800\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:37:59 INFO 140391921509760] #quality_metric: host=algo-1, epoch=17, batch=0 train loss <loss>=10.629799842834473\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:00 INFO 140391921509760] Epoch[17] Batch[5] avg_epoch_loss=10.563147\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:00 INFO 140391921509760] #quality_metric: host=algo-1, epoch=17, batch=5 train loss <loss>=10.563146909077963\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:00 INFO 140391921509760] Epoch[17] Batch [5]#011Speed: 1849.34 samples/sec#011loss=10.563147\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:00 INFO 140391921509760] processed a total of 639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461079.5995278, \"EndTime\": 1631461080.1435254, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 543.938398361206, \"count\": 1, \"min\": 543.938398361206, \"max\": 543.938398361206}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:00 INFO 140391921509760] #throughput_metric: host=algo-1, train throughput=1174.4904773360993 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:00 INFO 140391921509760] #progress_metric: host=algo-1, completed 9.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:00 INFO 140391921509760] #quality_metric: host=algo-1, epoch=17, train loss <loss>=10.492076396942139\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:00 INFO 140391921509760] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:00 INFO 140391921509760] Epoch[18] Batch[0] avg_epoch_loss=10.353322\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:00 INFO 140391921509760] #quality_metric: host=algo-1, epoch=18, batch=0 train loss <loss>=10.35332202911377\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:00 INFO 140391921509760] Epoch[18] Batch[5] avg_epoch_loss=10.582346\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:00 INFO 140391921509760] #quality_metric: host=algo-1, epoch=18, batch=5 train loss <loss>=10.582345962524414\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:00 INFO 140391921509760] Epoch[18] Batch [5]#011Speed: 2092.12 samples/sec#011loss=10.582346\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:00 INFO 140391921509760] Epoch[18] Batch[10] avg_epoch_loss=10.639121\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:00 INFO 140391921509760] #quality_metric: host=algo-1, epoch=18, batch=10 train loss <loss>=10.707251167297363\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:00 INFO 140391921509760] Epoch[18] Batch [10]#011Speed: 2009.19 samples/sec#011loss=10.707251\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:00 INFO 140391921509760] processed a total of 657 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461080.143617, \"EndTime\": 1631461080.7166739, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 572.5338459014893, \"count\": 1, \"min\": 572.5338459014893, \"max\": 572.5338459014893}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:00 INFO 140391921509760] #throughput_metric: host=algo-1, train throughput=1147.305860355201 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:00 INFO 140391921509760] #progress_metric: host=algo-1, completed 9.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:00 INFO 140391921509760] #quality_metric: host=algo-1, epoch=18, train loss <loss>=10.639121055603027\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:00 INFO 140391921509760] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:00 INFO 140391921509760] Epoch[19] Batch[0] avg_epoch_loss=10.297550\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:00 INFO 140391921509760] #quality_metric: host=algo-1, epoch=19, batch=0 train loss <loss>=10.297550201416016\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:01 INFO 140391921509760] Epoch[19] Batch[5] avg_epoch_loss=10.661713\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:01 INFO 140391921509760] #quality_metric: host=algo-1, epoch=19, batch=5 train loss <loss>=10.661712805430094\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:01 INFO 140391921509760] Epoch[19] Batch [5]#011Speed: 2097.95 samples/sec#011loss=10.661713\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:01 INFO 140391921509760] processed a total of 598 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461080.7167501, \"EndTime\": 1631461081.2538328, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 536.5695953369141, \"count\": 1, \"min\": 536.5695953369141, \"max\": 536.5695953369141}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:01 INFO 140391921509760] #throughput_metric: host=algo-1, train throughput=1114.2263863783467 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:01 INFO 140391921509760] #progress_metric: host=algo-1, completed 10.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:01 INFO 140391921509760] #quality_metric: host=algo-1, epoch=19, train loss <loss>=10.62725477218628\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:01 INFO 140391921509760] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:01 INFO 140391921509760] Epoch[20] Batch[0] avg_epoch_loss=10.029211\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:01 INFO 140391921509760] #quality_metric: host=algo-1, epoch=20, batch=0 train loss <loss>=10.029211044311523\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:01 INFO 140391921509760] Epoch[20] Batch[5] avg_epoch_loss=10.225057\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:01 INFO 140391921509760] #quality_metric: host=algo-1, epoch=20, batch=5 train loss <loss>=10.225057284037272\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:01 INFO 140391921509760] Epoch[20] Batch [5]#011Speed: 1566.78 samples/sec#011loss=10.225057\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:01 INFO 140391921509760] Epoch[20] Batch[10] avg_epoch_loss=10.192515\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:01 INFO 140391921509760] #quality_metric: host=algo-1, epoch=20, batch=10 train loss <loss>=10.153465270996094\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:01 INFO 140391921509760] Epoch[20] Batch [10]#011Speed: 1531.13 samples/sec#011loss=10.153465\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:01 INFO 140391921509760] processed a total of 645 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461081.2539198, \"EndTime\": 1631461081.9355817, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 681.1456680297852, \"count\": 1, \"min\": 681.1456680297852, \"max\": 681.1456680297852}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:01 INFO 140391921509760] #throughput_metric: host=algo-1, train throughput=946.7613570546918 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:01 INFO 140391921509760] #progress_metric: host=algo-1, completed 10.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:01 INFO 140391921509760] #quality_metric: host=algo-1, epoch=20, train loss <loss>=10.192515459927646\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:01 INFO 140391921509760] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:02 INFO 140391921509760] Epoch[21] Batch[0] avg_epoch_loss=9.204162\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:02 INFO 140391921509760] #quality_metric: host=algo-1, epoch=21, batch=0 train loss <loss>=9.204161643981934\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:02 INFO 140391921509760] Epoch[21] Batch[5] avg_epoch_loss=10.488222\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:02 INFO 140391921509760] #quality_metric: host=algo-1, epoch=21, batch=5 train loss <loss>=10.488221645355225\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:02 INFO 140391921509760] Epoch[21] Batch [5]#011Speed: 1931.60 samples/sec#011loss=10.488222\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:02 INFO 140391921509760] processed a total of 631 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461081.9356682, \"EndTime\": 1631461082.519392, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 583.2140445709229, \"count\": 1, \"min\": 583.2140445709229, \"max\": 583.2140445709229}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:02 INFO 140391921509760] #throughput_metric: host=algo-1, train throughput=1081.6901172666235 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:02 INFO 140391921509760] #progress_metric: host=algo-1, completed 11.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:02 INFO 140391921509760] #quality_metric: host=algo-1, epoch=21, train loss <loss>=10.32696304321289\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:02 INFO 140391921509760] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:02 INFO 140391921509760] Epoch[22] Batch[0] avg_epoch_loss=10.688367\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:02 INFO 140391921509760] #quality_metric: host=algo-1, epoch=22, batch=0 train loss <loss>=10.688366889953613\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:02 INFO 140391921509760] Epoch[22] Batch[5] avg_epoch_loss=11.008822\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:02 INFO 140391921509760] #quality_metric: host=algo-1, epoch=22, batch=5 train loss <loss>=11.008821964263916\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:02 INFO 140391921509760] Epoch[22] Batch [5]#011Speed: 1962.03 samples/sec#011loss=11.008822\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:03 INFO 140391921509760] Epoch[22] Batch[10] avg_epoch_loss=10.084714\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:03 INFO 140391921509760] #quality_metric: host=algo-1, epoch=22, batch=10 train loss <loss>=8.97578387260437\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:03 INFO 140391921509760] Epoch[22] Batch [10]#011Speed: 1553.41 samples/sec#011loss=8.975784\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:03 INFO 140391921509760] processed a total of 642 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461082.5194824, \"EndTime\": 1631461083.1470473, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 627.044677734375, \"count\": 1, \"min\": 627.044677734375, \"max\": 627.044677734375}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:03 INFO 140391921509760] #throughput_metric: host=algo-1, train throughput=1023.6667751383013 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:03 INFO 140391921509760] #progress_metric: host=algo-1, completed 11.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:03 INFO 140391921509760] #quality_metric: host=algo-1, epoch=22, train loss <loss>=10.084713740782304\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:03 INFO 140391921509760] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:03 INFO 140391921509760] Saved checkpoint to \"/opt/ml/model/state_c5b85c3a-6116-4589-9e67-a358b437ebbc-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461083.1471207, \"EndTime\": 1631461083.1589, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 11.265993118286133, \"count\": 1, \"min\": 11.265993118286133, \"max\": 11.265993118286133}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:03 INFO 140391921509760] Epoch[23] Batch[0] avg_epoch_loss=10.294958\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:03 INFO 140391921509760] #quality_metric: host=algo-1, epoch=23, batch=0 train loss <loss>=10.294958114624023\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:03 INFO 140391921509760] Epoch[23] Batch[5] avg_epoch_loss=10.653121\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:03 INFO 140391921509760] #quality_metric: host=algo-1, epoch=23, batch=5 train loss <loss>=10.653120835622152\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:03 INFO 140391921509760] Epoch[23] Batch [5]#011Speed: 1918.56 samples/sec#011loss=10.653121\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:03 INFO 140391921509760] Epoch[23] Batch[10] avg_epoch_loss=9.967654\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:03 INFO 140391921509760] #quality_metric: host=algo-1, epoch=23, batch=10 train loss <loss>=9.145093250274659\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:03 INFO 140391921509760] Epoch[23] Batch [10]#011Speed: 1820.15 samples/sec#011loss=9.145093\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:03 INFO 140391921509760] processed a total of 644 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461083.158955, \"EndTime\": 1631461083.7643569, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 605.3471565246582, \"count\": 1, \"min\": 605.3471565246582, \"max\": 605.3471565246582}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:03 INFO 140391921509760] #throughput_metric: host=algo-1, train throughput=1063.6378720089845 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:03 INFO 140391921509760] #progress_metric: host=algo-1, completed 12.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:03 INFO 140391921509760] #quality_metric: host=algo-1, epoch=23, train loss <loss>=9.967653751373291\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:03 INFO 140391921509760] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:03 INFO 140391921509760] Saved checkpoint to \"/opt/ml/model/state_72506b2c-1f1a-4d39-aec3-57cfae2f23fc-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461083.764442, \"EndTime\": 1631461083.7838755, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 18.930673599243164, \"count\": 1, \"min\": 18.930673599243164, \"max\": 18.930673599243164}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:04 INFO 140391921509760] Epoch[24] Batch[0] avg_epoch_loss=11.109097\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:04 INFO 140391921509760] #quality_metric: host=algo-1, epoch=24, batch=0 train loss <loss>=11.10909652709961\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:04 INFO 140391921509760] Epoch[24] Batch[5] avg_epoch_loss=10.165516\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:04 INFO 140391921509760] #quality_metric: host=algo-1, epoch=24, batch=5 train loss <loss>=10.165515899658203\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:04 INFO 140391921509760] Epoch[24] Batch [5]#011Speed: 1630.90 samples/sec#011loss=10.165516\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:04 INFO 140391921509760] processed a total of 638 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461083.7839465, \"EndTime\": 1631461084.4664772, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 682.4674606323242, \"count\": 1, \"min\": 682.4674606323242, \"max\": 682.4674606323242}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:04 INFO 140391921509760] #throughput_metric: host=algo-1, train throughput=934.6889821261161 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:04 INFO 140391921509760] #progress_metric: host=algo-1, completed 12.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:04 INFO 140391921509760] #quality_metric: host=algo-1, epoch=24, train loss <loss>=9.967399406433106\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:04 INFO 140391921509760] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:04 INFO 140391921509760] Saved checkpoint to \"/opt/ml/model/state_7525739b-b58c-44fb-b46e-149bbcc50ec4-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461084.46655, \"EndTime\": 1631461084.482783, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 15.664100646972656, \"count\": 1, \"min\": 15.664100646972656, \"max\": 15.664100646972656}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:04 INFO 140391921509760] Epoch[25] Batch[0] avg_epoch_loss=10.500690\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:04 INFO 140391921509760] #quality_metric: host=algo-1, epoch=25, batch=0 train loss <loss>=10.500690460205078\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:04 INFO 140391921509760] Epoch[25] Batch[5] avg_epoch_loss=10.355457\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:04 INFO 140391921509760] #quality_metric: host=algo-1, epoch=25, batch=5 train loss <loss>=10.355456829071045\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:04 INFO 140391921509760] Epoch[25] Batch [5]#011Speed: 1676.54 samples/sec#011loss=10.355457\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:05 INFO 140391921509760] processed a total of 631 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461084.4828644, \"EndTime\": 1631461085.108616, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 625.6856918334961, \"count\": 1, \"min\": 625.6856918334961, \"max\": 625.6856918334961}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:05 INFO 140391921509760] #throughput_metric: host=algo-1, train throughput=1008.2768827527353 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:05 INFO 140391921509760] #progress_metric: host=algo-1, completed 13.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:05 INFO 140391921509760] #quality_metric: host=algo-1, epoch=25, train loss <loss>=10.329957962036133\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:05 INFO 140391921509760] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:05 INFO 140391921509760] Epoch[26] Batch[0] avg_epoch_loss=10.000211\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:05 INFO 140391921509760] #quality_metric: host=algo-1, epoch=26, batch=0 train loss <loss>=10.000210762023926\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:05 INFO 140391921509760] Epoch[26] Batch[5] avg_epoch_loss=10.193289\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:05 INFO 140391921509760] #quality_metric: host=algo-1, epoch=26, batch=5 train loss <loss>=10.193288803100586\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:05 INFO 140391921509760] Epoch[26] Batch [5]#011Speed: 1672.20 samples/sec#011loss=10.193289\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:05 INFO 140391921509760] Epoch[26] Batch[10] avg_epoch_loss=10.258535\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:05 INFO 140391921509760] #quality_metric: host=algo-1, epoch=26, batch=10 train loss <loss>=10.336831092834473\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:05 INFO 140391921509760] Epoch[26] Batch [10]#011Speed: 1403.84 samples/sec#011loss=10.336831\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:05 INFO 140391921509760] processed a total of 671 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461085.1087117, \"EndTime\": 1631461085.8068445, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 697.57080078125, \"count\": 1, \"min\": 697.57080078125, \"max\": 697.57080078125}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:05 INFO 140391921509760] #throughput_metric: host=algo-1, train throughput=961.73761484882 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:05 INFO 140391921509760] #progress_metric: host=algo-1, completed 13.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:05 INFO 140391921509760] #quality_metric: host=algo-1, epoch=26, train loss <loss>=10.25853529843417\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:05 INFO 140391921509760] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:06 INFO 140391921509760] Epoch[27] Batch[0] avg_epoch_loss=10.358986\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:06 INFO 140391921509760] #quality_metric: host=algo-1, epoch=27, batch=0 train loss <loss>=10.358985900878906\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:06 INFO 140391921509760] Epoch[27] Batch[5] avg_epoch_loss=9.941507\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:06 INFO 140391921509760] #quality_metric: host=algo-1, epoch=27, batch=5 train loss <loss>=9.94150718053182\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:06 INFO 140391921509760] Epoch[27] Batch [5]#011Speed: 1487.00 samples/sec#011loss=9.941507\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:06 INFO 140391921509760] Epoch[27] Batch[10] avg_epoch_loss=10.261756\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:06 INFO 140391921509760] #quality_metric: host=algo-1, epoch=27, batch=10 train loss <loss>=10.646053695678711\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:06 INFO 140391921509760] Epoch[27] Batch [10]#011Speed: 1401.11 samples/sec#011loss=10.646054\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:06 INFO 140391921509760] processed a total of 661 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461085.8069296, \"EndTime\": 1631461086.5625124, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 755.0985813140869, \"count\": 1, \"min\": 755.0985813140869, \"max\": 755.0985813140869}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:06 INFO 140391921509760] #throughput_metric: host=algo-1, train throughput=875.235646429965 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:06 INFO 140391921509760] #progress_metric: host=algo-1, completed 14.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:06 INFO 140391921509760] #quality_metric: host=algo-1, epoch=27, train loss <loss>=10.26175559650768\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:06 INFO 140391921509760] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:06 INFO 140391921509760] Epoch[28] Batch[0] avg_epoch_loss=9.989321\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:06 INFO 140391921509760] #quality_metric: host=algo-1, epoch=28, batch=0 train loss <loss>=9.989320755004883\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:07 INFO 140391921509760] Epoch[28] Batch[5] avg_epoch_loss=10.049645\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:07 INFO 140391921509760] #quality_metric: host=algo-1, epoch=28, batch=5 train loss <loss>=10.04964542388916\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:07 INFO 140391921509760] Epoch[28] Batch [5]#011Speed: 1562.84 samples/sec#011loss=10.049645\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:07 INFO 140391921509760] Epoch[28] Batch[10] avg_epoch_loss=10.381005\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:07 INFO 140391921509760] #quality_metric: host=algo-1, epoch=28, batch=10 train loss <loss>=10.77863655090332\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:07 INFO 140391921509760] Epoch[28] Batch [10]#011Speed: 1849.43 samples/sec#011loss=10.778637\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:07 INFO 140391921509760] processed a total of 655 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461086.5626004, \"EndTime\": 1631461087.2034206, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 640.3124332427979, \"count\": 1, \"min\": 640.3124332427979, \"max\": 640.3124332427979}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:07 INFO 140391921509760] #throughput_metric: host=algo-1, train throughput=1022.7324869099715 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:07 INFO 140391921509760] #progress_metric: host=algo-1, completed 14.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:07 INFO 140391921509760] #quality_metric: host=algo-1, epoch=28, train loss <loss>=10.381005027077414\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:07 INFO 140391921509760] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:07 INFO 140391921509760] Epoch[29] Batch[0] avg_epoch_loss=10.145407\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:07 INFO 140391921509760] #quality_metric: host=algo-1, epoch=29, batch=0 train loss <loss>=10.145406723022461\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:07 INFO 140391921509760] Epoch[29] Batch[5] avg_epoch_loss=10.521860\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:07 INFO 140391921509760] #quality_metric: host=algo-1, epoch=29, batch=5 train loss <loss>=10.521859963734945\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:07 INFO 140391921509760] Epoch[29] Batch [5]#011Speed: 1645.73 samples/sec#011loss=10.521860\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:07 INFO 140391921509760] Epoch[29] Batch[10] avg_epoch_loss=9.782534\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:07 INFO 140391921509760] #quality_metric: host=algo-1, epoch=29, batch=10 train loss <loss>=8.895343780517578\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:07 INFO 140391921509760] Epoch[29] Batch [10]#011Speed: 1995.89 samples/sec#011loss=8.895344\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:07 INFO 140391921509760] processed a total of 653 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461087.2035105, \"EndTime\": 1631461087.8436947, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 639.6603584289551, \"count\": 1, \"min\": 639.6603584289551, \"max\": 639.6603584289551}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:07 INFO 140391921509760] #throughput_metric: host=algo-1, train throughput=1020.6598587410716 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:07 INFO 140391921509760] #progress_metric: host=algo-1, completed 15.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:07 INFO 140391921509760] #quality_metric: host=algo-1, epoch=29, train loss <loss>=9.78253442590887\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:07 INFO 140391921509760] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:07 INFO 140391921509760] Saved checkpoint to \"/opt/ml/model/state_97a491a5-6005-44a9-8da5-3548ed9f71ab-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461087.8437793, \"EndTime\": 1631461087.8628368, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 18.55015754699707, \"count\": 1, \"min\": 18.55015754699707, \"max\": 18.55015754699707}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:08 INFO 140391921509760] Epoch[30] Batch[0] avg_epoch_loss=10.343047\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:08 INFO 140391921509760] #quality_metric: host=algo-1, epoch=30, batch=0 train loss <loss>=10.343047142028809\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:08 INFO 140391921509760] Epoch[30] Batch[5] avg_epoch_loss=10.088745\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:08 INFO 140391921509760] #quality_metric: host=algo-1, epoch=30, batch=5 train loss <loss>=10.088744958241781\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:08 INFO 140391921509760] Epoch[30] Batch [5]#011Speed: 2143.17 samples/sec#011loss=10.088745\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:08 INFO 140391921509760] processed a total of 611 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461087.8629045, \"EndTime\": 1631461088.402261, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 539.2982959747314, \"count\": 1, \"min\": 539.2982959747314, \"max\": 539.2982959747314}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:08 INFO 140391921509760] #throughput_metric: host=algo-1, train throughput=1132.6852987181974 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:08 INFO 140391921509760] #progress_metric: host=algo-1, completed 15.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:08 INFO 140391921509760] #quality_metric: host=algo-1, epoch=30, train loss <loss>=10.099143123626709\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:08 INFO 140391921509760] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:08 INFO 140391921509760] Epoch[31] Batch[0] avg_epoch_loss=10.574306\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:08 INFO 140391921509760] #quality_metric: host=algo-1, epoch=31, batch=0 train loss <loss>=10.574305534362793\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:08 INFO 140391921509760] Epoch[31] Batch[5] avg_epoch_loss=10.011617\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:08 INFO 140391921509760] #quality_metric: host=algo-1, epoch=31, batch=5 train loss <loss>=10.011616547902426\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:08 INFO 140391921509760] Epoch[31] Batch [5]#011Speed: 1977.56 samples/sec#011loss=10.011617\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:09 INFO 140391921509760] Epoch[31] Batch[10] avg_epoch_loss=10.037431\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:09 INFO 140391921509760] #quality_metric: host=algo-1, epoch=31, batch=10 train loss <loss>=10.06840934753418\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:09 INFO 140391921509760] Epoch[31] Batch [10]#011Speed: 1543.44 samples/sec#011loss=10.068409\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:09 INFO 140391921509760] processed a total of 698 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461088.4023526, \"EndTime\": 1631461089.0466962, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 643.7959671020508, \"count\": 1, \"min\": 643.7959671020508, \"max\": 643.7959671020508}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:09 INFO 140391921509760] #throughput_metric: host=algo-1, train throughput=1084.0234102096417 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:09 INFO 140391921509760] #progress_metric: host=algo-1, completed 16.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:09 INFO 140391921509760] #quality_metric: host=algo-1, epoch=31, train loss <loss>=10.03743145682595\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:09 INFO 140391921509760] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:09 INFO 140391921509760] Epoch[32] Batch[0] avg_epoch_loss=10.386448\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:09 INFO 140391921509760] #quality_metric: host=algo-1, epoch=32, batch=0 train loss <loss>=10.38644790649414\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:09 INFO 140391921509760] Epoch[32] Batch[5] avg_epoch_loss=10.116024\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:09 INFO 140391921509760] #quality_metric: host=algo-1, epoch=32, batch=5 train loss <loss>=10.116023699442545\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:09 INFO 140391921509760] Epoch[32] Batch [5]#011Speed: 1985.82 samples/sec#011loss=10.116024\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:09 INFO 140391921509760] Epoch[32] Batch[10] avg_epoch_loss=10.460778\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:09 INFO 140391921509760] #quality_metric: host=algo-1, epoch=32, batch=10 train loss <loss>=10.874483871459962\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:09 INFO 140391921509760] Epoch[32] Batch [10]#011Speed: 1626.34 samples/sec#011loss=10.874484\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:09 INFO 140391921509760] processed a total of 681 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461089.0467627, \"EndTime\": 1631461089.6502657, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 602.989912033081, \"count\": 1, \"min\": 602.989912033081, \"max\": 602.989912033081}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:09 INFO 140391921509760] #throughput_metric: host=algo-1, train throughput=1129.141310225961 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:09 INFO 140391921509760] #progress_metric: host=algo-1, completed 16.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:09 INFO 140391921509760] #quality_metric: host=algo-1, epoch=32, train loss <loss>=10.460778323086826\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:09 INFO 140391921509760] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:09 INFO 140391921509760] Epoch[33] Batch[0] avg_epoch_loss=10.198336\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:09 INFO 140391921509760] #quality_metric: host=algo-1, epoch=33, batch=0 train loss <loss>=10.198335647583008\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:10 INFO 140391921509760] Epoch[33] Batch[5] avg_epoch_loss=10.052660\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:10 INFO 140391921509760] #quality_metric: host=algo-1, epoch=33, batch=5 train loss <loss>=10.052659511566162\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:10 INFO 140391921509760] Epoch[33] Batch [5]#011Speed: 2002.48 samples/sec#011loss=10.052660\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:10 INFO 140391921509760] processed a total of 629 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461089.650351, \"EndTime\": 1631461090.2065847, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 555.7410717010498, \"count\": 1, \"min\": 555.7410717010498, \"max\": 555.7410717010498}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:10 INFO 140391921509760] #throughput_metric: host=algo-1, train throughput=1131.5696151822008 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:10 INFO 140391921509760] #progress_metric: host=algo-1, completed 17.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:10 INFO 140391921509760] #quality_metric: host=algo-1, epoch=33, train loss <loss>=10.575355434417725\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:10 INFO 140391921509760] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:10 INFO 140391921509760] Epoch[34] Batch[0] avg_epoch_loss=9.019712\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:10 INFO 140391921509760] #quality_metric: host=algo-1, epoch=34, batch=0 train loss <loss>=9.019712448120117\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:10 INFO 140391921509760] Epoch[34] Batch[5] avg_epoch_loss=9.673677\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:10 INFO 140391921509760] #quality_metric: host=algo-1, epoch=34, batch=5 train loss <loss>=9.673677444458008\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:10 INFO 140391921509760] Epoch[34] Batch [5]#011Speed: 1899.86 samples/sec#011loss=9.673677\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:10 INFO 140391921509760] Epoch[34] Batch[10] avg_epoch_loss=9.177153\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:10 INFO 140391921509760] #quality_metric: host=algo-1, epoch=34, batch=10 train loss <loss>=8.581324529647826\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:10 INFO 140391921509760] Epoch[34] Batch [10]#011Speed: 1834.74 samples/sec#011loss=8.581325\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:10 INFO 140391921509760] processed a total of 651 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461090.2066703, \"EndTime\": 1631461090.816147, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 608.9839935302734, \"count\": 1, \"min\": 608.9839935302734, \"max\": 608.9839935302734}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:10 INFO 140391921509760] #throughput_metric: host=algo-1, train throughput=1068.7789595725967 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:10 INFO 140391921509760] #progress_metric: host=algo-1, completed 17.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:10 INFO 140391921509760] #quality_metric: host=algo-1, epoch=34, train loss <loss>=9.177153392271562\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:10 INFO 140391921509760] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:10 INFO 140391921509760] Saved checkpoint to \"/opt/ml/model/state_baa64939-d746-44f3-963a-19db8ca50a8b-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461090.8162324, \"EndTime\": 1631461090.8356483, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 18.90420913696289, \"count\": 1, \"min\": 18.90420913696289, \"max\": 18.90420913696289}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:11 INFO 140391921509760] Epoch[35] Batch[0] avg_epoch_loss=10.342342\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:11 INFO 140391921509760] #quality_metric: host=algo-1, epoch=35, batch=0 train loss <loss>=10.342342376708984\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:11 INFO 140391921509760] Epoch[35] Batch[5] avg_epoch_loss=10.166801\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:11 INFO 140391921509760] #quality_metric: host=algo-1, epoch=35, batch=5 train loss <loss>=10.166801293691\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:11 INFO 140391921509760] Epoch[35] Batch [5]#011Speed: 2018.55 samples/sec#011loss=10.166801\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:11 INFO 140391921509760] Epoch[35] Batch[10] avg_epoch_loss=10.514950\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:11 INFO 140391921509760] #quality_metric: host=algo-1, epoch=35, batch=10 train loss <loss>=10.932729148864746\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:11 INFO 140391921509760] Epoch[35] Batch [10]#011Speed: 2016.02 samples/sec#011loss=10.932729\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:11 INFO 140391921509760] processed a total of 643 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461090.8357198, \"EndTime\": 1631461091.443286, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 607.5022220611572, \"count\": 1, \"min\": 607.5022220611572, \"max\": 607.5022220611572}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:11 INFO 140391921509760] #throughput_metric: host=algo-1, train throughput=1058.220536389387 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:11 INFO 140391921509760] #progress_metric: host=algo-1, completed 18.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:11 INFO 140391921509760] #quality_metric: host=algo-1, epoch=35, train loss <loss>=10.514950318769975\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:11 INFO 140391921509760] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:11 INFO 140391921509760] Epoch[36] Batch[0] avg_epoch_loss=9.536668\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:11 INFO 140391921509760] #quality_metric: host=algo-1, epoch=36, batch=0 train loss <loss>=9.536667823791504\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:11 INFO 140391921509760] Epoch[36] Batch[5] avg_epoch_loss=10.070113\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:11 INFO 140391921509760] #quality_metric: host=algo-1, epoch=36, batch=5 train loss <loss>=10.070113023122152\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:11 INFO 140391921509760] Epoch[36] Batch [5]#011Speed: 2033.38 samples/sec#011loss=10.070113\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:12 INFO 140391921509760] processed a total of 630 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461091.4433708, \"EndTime\": 1631461092.0201838, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 576.2696266174316, \"count\": 1, \"min\": 576.2696266174316, \"max\": 576.2696266174316}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:12 INFO 140391921509760] #throughput_metric: host=algo-1, train throughput=1092.9994895717946 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:12 INFO 140391921509760] #progress_metric: host=algo-1, completed 18.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:12 INFO 140391921509760] #quality_metric: host=algo-1, epoch=36, train loss <loss>=10.098842430114747\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:12 INFO 140391921509760] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:12 INFO 140391921509760] Epoch[37] Batch[0] avg_epoch_loss=9.787041\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:12 INFO 140391921509760] #quality_metric: host=algo-1, epoch=37, batch=0 train loss <loss>=9.787040710449219\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:12 INFO 140391921509760] Epoch[37] Batch[5] avg_epoch_loss=9.982740\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:12 INFO 140391921509760] #quality_metric: host=algo-1, epoch=37, batch=5 train loss <loss>=9.98274008433024\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:12 INFO 140391921509760] Epoch[37] Batch [5]#011Speed: 2139.60 samples/sec#011loss=9.982740\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:12 INFO 140391921509760] processed a total of 585 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461092.0202708, \"EndTime\": 1631461092.5986125, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 577.7997970581055, \"count\": 1, \"min\": 577.7997970581055, \"max\": 577.7997970581055}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:12 INFO 140391921509760] #throughput_metric: host=algo-1, train throughput=1012.2149882552316 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:12 INFO 140391921509760] #progress_metric: host=algo-1, completed 19.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:12 INFO 140391921509760] #quality_metric: host=algo-1, epoch=37, train loss <loss>=10.109323406219483\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:12 INFO 140391921509760] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:12 INFO 140391921509760] Epoch[38] Batch[0] avg_epoch_loss=9.379631\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:12 INFO 140391921509760] #quality_metric: host=algo-1, epoch=38, batch=0 train loss <loss>=9.379631042480469\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:13 INFO 140391921509760] Epoch[38] Batch[5] avg_epoch_loss=9.918848\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:13 INFO 140391921509760] #quality_metric: host=algo-1, epoch=38, batch=5 train loss <loss>=9.918848196665445\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:13 INFO 140391921509760] Epoch[38] Batch [5]#011Speed: 2038.86 samples/sec#011loss=9.918848\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:13 INFO 140391921509760] processed a total of 623 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461092.5987127, \"EndTime\": 1631461093.1353903, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 536.1459255218506, \"count\": 1, \"min\": 536.1459255218506, \"max\": 536.1459255218506}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:13 INFO 140391921509760] #throughput_metric: host=algo-1, train throughput=1161.7067515041206 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:13 INFO 140391921509760] #progress_metric: host=algo-1, completed 19.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:13 INFO 140391921509760] #quality_metric: host=algo-1, epoch=38, train loss <loss>=9.891543579101562\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:13 INFO 140391921509760] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:13 INFO 140391921509760] Epoch[39] Batch[0] avg_epoch_loss=10.367806\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:13 INFO 140391921509760] #quality_metric: host=algo-1, epoch=39, batch=0 train loss <loss>=10.367806434631348\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:13 INFO 140391921509760] Epoch[39] Batch[5] avg_epoch_loss=10.048364\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:13 INFO 140391921509760] #quality_metric: host=algo-1, epoch=39, batch=5 train loss <loss>=10.048363844553629\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:13 INFO 140391921509760] Epoch[39] Batch [5]#011Speed: 1815.39 samples/sec#011loss=10.048364\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:13 INFO 140391921509760] Epoch[39] Batch[10] avg_epoch_loss=10.233469\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:13 INFO 140391921509760] #quality_metric: host=algo-1, epoch=39, batch=10 train loss <loss>=10.455594635009765\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:13 INFO 140391921509760] Epoch[39] Batch [10]#011Speed: 2006.73 samples/sec#011loss=10.455595\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:13 INFO 140391921509760] processed a total of 642 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461093.1354754, \"EndTime\": 1631461093.7488625, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 612.8530502319336, \"count\": 1, \"min\": 612.8530502319336, \"max\": 612.8530502319336}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:13 INFO 140391921509760] #throughput_metric: host=algo-1, train throughput=1047.3007237682152 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:13 INFO 140391921509760] #progress_metric: host=algo-1, completed 20.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:13 INFO 140391921509760] #quality_metric: host=algo-1, epoch=39, train loss <loss>=10.233468749306418\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:13 INFO 140391921509760] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:14 INFO 140391921509760] Epoch[40] Batch[0] avg_epoch_loss=10.478372\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:14 INFO 140391921509760] #quality_metric: host=algo-1, epoch=40, batch=0 train loss <loss>=10.478371620178223\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:14 INFO 140391921509760] Epoch[40] Batch[5] avg_epoch_loss=10.100573\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:14 INFO 140391921509760] #quality_metric: host=algo-1, epoch=40, batch=5 train loss <loss>=10.100573380788168\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:14 INFO 140391921509760] Epoch[40] Batch [5]#011Speed: 2144.50 samples/sec#011loss=10.100573\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:14 INFO 140391921509760] processed a total of 615 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461093.7489746, \"EndTime\": 1631461094.2820752, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 532.5925350189209, \"count\": 1, \"min\": 532.5925350189209, \"max\": 532.5925350189209}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:14 INFO 140391921509760] #throughput_metric: host=algo-1, train throughput=1154.4581111798848 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:14 INFO 140391921509760] #progress_metric: host=algo-1, completed 20.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:14 INFO 140391921509760] #quality_metric: host=algo-1, epoch=40, train loss <loss>=9.768037366867066\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:14 INFO 140391921509760] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:14 INFO 140391921509760] Epoch[41] Batch[0] avg_epoch_loss=9.897503\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:14 INFO 140391921509760] #quality_metric: host=algo-1, epoch=41, batch=0 train loss <loss>=9.897502899169922\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:14 INFO 140391921509760] Epoch[41] Batch[5] avg_epoch_loss=10.204395\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:14 INFO 140391921509760] #quality_metric: host=algo-1, epoch=41, batch=5 train loss <loss>=10.204395294189453\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:14 INFO 140391921509760] Epoch[41] Batch [5]#011Speed: 1878.55 samples/sec#011loss=10.204395\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:14 INFO 140391921509760] processed a total of 640 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461094.2821615, \"EndTime\": 1631461094.8726933, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 590.0201797485352, \"count\": 1, \"min\": 590.0201797485352, \"max\": 590.0201797485352}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:14 INFO 140391921509760] #throughput_metric: host=algo-1, train throughput=1084.5355556641923 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:14 INFO 140391921509760] #progress_metric: host=algo-1, completed 21.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:14 INFO 140391921509760] #quality_metric: host=algo-1, epoch=41, train loss <loss>=10.016715335845948\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:14 INFO 140391921509760] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:15 INFO 140391921509760] Epoch[42] Batch[0] avg_epoch_loss=9.900161\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:15 INFO 140391921509760] #quality_metric: host=algo-1, epoch=42, batch=0 train loss <loss>=9.900160789489746\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:15 INFO 140391921509760] Epoch[42] Batch[5] avg_epoch_loss=10.261806\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:15 INFO 140391921509760] #quality_metric: host=algo-1, epoch=42, batch=5 train loss <loss>=10.261805693308512\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:15 INFO 140391921509760] Epoch[42] Batch [5]#011Speed: 2047.88 samples/sec#011loss=10.261806\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:15 INFO 140391921509760] processed a total of 630 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461094.8727558, \"EndTime\": 1631461095.417571, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 544.2326068878174, \"count\": 1, \"min\": 544.2326068878174, \"max\": 544.2326068878174}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:15 INFO 140391921509760] #throughput_metric: host=algo-1, train throughput=1157.3210045138126 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:15 INFO 140391921509760] #progress_metric: host=algo-1, completed 21.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:15 INFO 140391921509760] #quality_metric: host=algo-1, epoch=42, train loss <loss>=10.101787567138672\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:15 INFO 140391921509760] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:15 INFO 140391921509760] Epoch[43] Batch[0] avg_epoch_loss=10.231109\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:15 INFO 140391921509760] #quality_metric: host=algo-1, epoch=43, batch=0 train loss <loss>=10.231108665466309\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:15 INFO 140391921509760] Epoch[43] Batch[5] avg_epoch_loss=9.779422\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:15 INFO 140391921509760] #quality_metric: host=algo-1, epoch=43, batch=5 train loss <loss>=9.779422442118326\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:15 INFO 140391921509760] Epoch[43] Batch [5]#011Speed: 2138.66 samples/sec#011loss=9.779422\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:15 INFO 140391921509760] processed a total of 618 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461095.4176598, \"EndTime\": 1631461095.9588373, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 540.6649112701416, \"count\": 1, \"min\": 540.6649112701416, \"max\": 540.6649112701416}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:15 INFO 140391921509760] #throughput_metric: host=algo-1, train throughput=1142.813753551343 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:15 INFO 140391921509760] #progress_metric: host=algo-1, completed 22.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:15 INFO 140391921509760] #quality_metric: host=algo-1, epoch=43, train loss <loss>=9.929623794555663\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:15 INFO 140391921509760] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:16 INFO 140391921509760] Epoch[44] Batch[0] avg_epoch_loss=10.389561\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:16 INFO 140391921509760] #quality_metric: host=algo-1, epoch=44, batch=0 train loss <loss>=10.38956069946289\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:16 INFO 140391921509760] Epoch[44] Batch[5] avg_epoch_loss=10.131190\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:16 INFO 140391921509760] #quality_metric: host=algo-1, epoch=44, batch=5 train loss <loss>=10.131190141042074\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:16 INFO 140391921509760] Epoch[44] Batch [5]#011Speed: 2102.00 samples/sec#011loss=10.131190\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:16 INFO 140391921509760] Epoch[44] Batch[10] avg_epoch_loss=10.429191\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:16 INFO 140391921509760] #quality_metric: host=algo-1, epoch=44, batch=10 train loss <loss>=10.786792373657226\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:16 INFO 140391921509760] Epoch[44] Batch [10]#011Speed: 1844.53 samples/sec#011loss=10.786792\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:16 INFO 140391921509760] processed a total of 655 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461095.9589074, \"EndTime\": 1631461096.5308645, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 571.3956356048584, \"count\": 1, \"min\": 571.3956356048584, \"max\": 571.3956356048584}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:16 INFO 140391921509760] #throughput_metric: host=algo-1, train throughput=1146.0798356014202 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:16 INFO 140391921509760] #progress_metric: host=algo-1, completed 22.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:16 INFO 140391921509760] #quality_metric: host=algo-1, epoch=44, train loss <loss>=10.429191155867143\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:16 INFO 140391921509760] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:16 INFO 140391921509760] Epoch[45] Batch[0] avg_epoch_loss=10.323128\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:16 INFO 140391921509760] #quality_metric: host=algo-1, epoch=45, batch=0 train loss <loss>=10.323127746582031\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:16 INFO 140391921509760] Epoch[45] Batch[5] avg_epoch_loss=10.317139\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:16 INFO 140391921509760] #quality_metric: host=algo-1, epoch=45, batch=5 train loss <loss>=10.317138512929281\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:16 INFO 140391921509760] Epoch[45] Batch [5]#011Speed: 2133.16 samples/sec#011loss=10.317139\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:17 INFO 140391921509760] Epoch[45] Batch[10] avg_epoch_loss=10.229013\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:17 INFO 140391921509760] #quality_metric: host=algo-1, epoch=45, batch=10 train loss <loss>=10.123263359069824\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:17 INFO 140391921509760] Epoch[45] Batch [10]#011Speed: 1912.74 samples/sec#011loss=10.123263\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:17 INFO 140391921509760] processed a total of 669 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461096.5309446, \"EndTime\": 1631461097.1110976, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 579.6921253204346, \"count\": 1, \"min\": 579.6921253204346, \"max\": 579.6921253204346}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:17 INFO 140391921509760] #throughput_metric: host=algo-1, train throughput=1153.8221741940895 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:17 INFO 140391921509760] #progress_metric: host=algo-1, completed 23.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:17 INFO 140391921509760] #quality_metric: host=algo-1, epoch=45, train loss <loss>=10.229013442993164\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:17 INFO 140391921509760] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:17 INFO 140391921509760] Epoch[46] Batch[0] avg_epoch_loss=9.652947\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:17 INFO 140391921509760] #quality_metric: host=algo-1, epoch=46, batch=0 train loss <loss>=9.652947425842285\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:17 INFO 140391921509760] Epoch[46] Batch[5] avg_epoch_loss=9.904517\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:17 INFO 140391921509760] #quality_metric: host=algo-1, epoch=46, batch=5 train loss <loss>=9.904516696929932\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:17 INFO 140391921509760] Epoch[46] Batch [5]#011Speed: 2064.37 samples/sec#011loss=9.904517\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:17 INFO 140391921509760] processed a total of 612 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461097.1111803, \"EndTime\": 1631461097.6876547, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 575.991153717041, \"count\": 1, \"min\": 575.991153717041, \"max\": 575.991153717041}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:17 INFO 140391921509760] #throughput_metric: host=algo-1, train throughput=1062.2670074973648 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:17 INFO 140391921509760] #progress_metric: host=algo-1, completed 23.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:17 INFO 140391921509760] #quality_metric: host=algo-1, epoch=46, train loss <loss>=10.09078664779663\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:17 INFO 140391921509760] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:17 INFO 140391921509760] Epoch[47] Batch[0] avg_epoch_loss=10.137726\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:17 INFO 140391921509760] #quality_metric: host=algo-1, epoch=47, batch=0 train loss <loss>=10.137725830078125\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:18 INFO 140391921509760] Epoch[47] Batch[5] avg_epoch_loss=10.177806\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:18 INFO 140391921509760] #quality_metric: host=algo-1, epoch=47, batch=5 train loss <loss>=10.177805582682291\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:18 INFO 140391921509760] Epoch[47] Batch [5]#011Speed: 1862.03 samples/sec#011loss=10.177806\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:18 INFO 140391921509760] Epoch[47] Batch[10] avg_epoch_loss=9.919887\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:18 INFO 140391921509760] #quality_metric: host=algo-1, epoch=47, batch=10 train loss <loss>=9.610383605957031\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:18 INFO 140391921509760] Epoch[47] Batch [10]#011Speed: 2007.77 samples/sec#011loss=9.610384\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:18 INFO 140391921509760] processed a total of 648 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461097.687751, \"EndTime\": 1631461098.2960346, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 607.7625751495361, \"count\": 1, \"min\": 607.7625751495361, \"max\": 607.7625751495361}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:18 INFO 140391921509760] #throughput_metric: host=algo-1, train throughput=1065.95785756475 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:18 INFO 140391921509760] #progress_metric: host=algo-1, completed 24.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:18 INFO 140391921509760] #quality_metric: host=algo-1, epoch=47, train loss <loss>=9.919886502352627\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:18 INFO 140391921509760] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:18 INFO 140391921509760] Epoch[48] Batch[0] avg_epoch_loss=9.471520\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:18 INFO 140391921509760] #quality_metric: host=algo-1, epoch=48, batch=0 train loss <loss>=9.47152042388916\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:18 INFO 140391921509760] Epoch[48] Batch[5] avg_epoch_loss=9.783527\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:18 INFO 140391921509760] #quality_metric: host=algo-1, epoch=48, batch=5 train loss <loss>=9.78352721532186\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:18 INFO 140391921509760] Epoch[48] Batch [5]#011Speed: 1889.79 samples/sec#011loss=9.783527\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:18 INFO 140391921509760] processed a total of 623 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461098.2961385, \"EndTime\": 1631461098.8717418, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 575.1185417175293, \"count\": 1, \"min\": 575.1185417175293, \"max\": 575.1185417175293}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:18 INFO 140391921509760] #throughput_metric: host=algo-1, train throughput=1083.0165342245973 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:18 INFO 140391921509760] #progress_metric: host=algo-1, completed 24.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:18 INFO 140391921509760] #quality_metric: host=algo-1, epoch=48, train loss <loss>=9.55748929977417\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:18 INFO 140391921509760] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:19 INFO 140391921509760] Epoch[49] Batch[0] avg_epoch_loss=10.126846\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:19 INFO 140391921509760] #quality_metric: host=algo-1, epoch=49, batch=0 train loss <loss>=10.126846313476562\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:19 INFO 140391921509760] Epoch[49] Batch[5] avg_epoch_loss=9.779695\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:19 INFO 140391921509760] #quality_metric: host=algo-1, epoch=49, batch=5 train loss <loss>=9.77969487508138\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:19 INFO 140391921509760] Epoch[49] Batch [5]#011Speed: 2140.95 samples/sec#011loss=9.779695\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:19 INFO 140391921509760] processed a total of 630 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461098.8718293, \"EndTime\": 1631461099.4582121, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 585.8640670776367, \"count\": 1, \"min\": 585.8640670776367, \"max\": 585.8640670776367}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:19 INFO 140391921509760] #throughput_metric: host=algo-1, train throughput=1075.1129445491679 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:19 INFO 140391921509760] #progress_metric: host=algo-1, completed 25.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:19 INFO 140391921509760] #quality_metric: host=algo-1, epoch=49, train loss <loss>=10.057441234588623\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:19 INFO 140391921509760] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:19 INFO 140391921509760] Epoch[50] Batch[0] avg_epoch_loss=10.094536\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:19 INFO 140391921509760] #quality_metric: host=algo-1, epoch=50, batch=0 train loss <loss>=10.094535827636719\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:19 INFO 140391921509760] Epoch[50] Batch[5] avg_epoch_loss=10.028439\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:19 INFO 140391921509760] #quality_metric: host=algo-1, epoch=50, batch=5 train loss <loss>=10.028438727060953\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:19 INFO 140391921509760] Epoch[50] Batch [5]#011Speed: 1919.27 samples/sec#011loss=10.028439\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:20 INFO 140391921509760] processed a total of 624 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461099.4582956, \"EndTime\": 1631461100.0225222, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 563.6963844299316, \"count\": 1, \"min\": 563.6963844299316, \"max\": 563.6963844299316}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:20 INFO 140391921509760] #throughput_metric: host=algo-1, train throughput=1106.724249828108 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:20 INFO 140391921509760] #progress_metric: host=algo-1, completed 25.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:20 INFO 140391921509760] #quality_metric: host=algo-1, epoch=50, train loss <loss>=9.957980537414551\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:20 INFO 140391921509760] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:20 INFO 140391921509760] Epoch[51] Batch[0] avg_epoch_loss=9.751469\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:20 INFO 140391921509760] #quality_metric: host=algo-1, epoch=51, batch=0 train loss <loss>=9.751468658447266\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:20 INFO 140391921509760] Epoch[51] Batch[5] avg_epoch_loss=10.268904\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:20 INFO 140391921509760] #quality_metric: host=algo-1, epoch=51, batch=5 train loss <loss>=10.268904050191244\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:20 INFO 140391921509760] Epoch[51] Batch [5]#011Speed: 2081.02 samples/sec#011loss=10.268904\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:20 INFO 140391921509760] processed a total of 622 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461100.0226138, \"EndTime\": 1631461100.574014, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 550.8687496185303, \"count\": 1, \"min\": 550.8687496185303, \"max\": 550.8687496185303}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:20 INFO 140391921509760] #throughput_metric: host=algo-1, train throughput=1128.8641974240236 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:20 INFO 140391921509760] #progress_metric: host=algo-1, completed 26.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:20 INFO 140391921509760] #quality_metric: host=algo-1, epoch=51, train loss <loss>=10.016654682159423\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:20 INFO 140391921509760] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:20 INFO 140391921509760] Epoch[52] Batch[0] avg_epoch_loss=9.613117\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:20 INFO 140391921509760] #quality_metric: host=algo-1, epoch=52, batch=0 train loss <loss>=9.613117218017578\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:21 INFO 140391921509760] Epoch[52] Batch[5] avg_epoch_loss=9.744245\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:21 INFO 140391921509760] #quality_metric: host=algo-1, epoch=52, batch=5 train loss <loss>=9.744244575500488\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:21 INFO 140391921509760] Epoch[52] Batch [5]#011Speed: 1957.01 samples/sec#011loss=9.744245\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:21 INFO 140391921509760] processed a total of 636 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461100.574102, \"EndTime\": 1631461101.176233, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 601.6178131103516, \"count\": 1, \"min\": 601.6178131103516, \"max\": 601.6178131103516}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:21 INFO 140391921509760] #throughput_metric: host=algo-1, train throughput=1056.9685965607418 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:21 INFO 140391921509760] #progress_metric: host=algo-1, completed 26.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:21 INFO 140391921509760] #quality_metric: host=algo-1, epoch=52, train loss <loss>=9.73910551071167\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:21 INFO 140391921509760] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:21 INFO 140391921509760] Epoch[53] Batch[0] avg_epoch_loss=10.481524\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:21 INFO 140391921509760] #quality_metric: host=algo-1, epoch=53, batch=0 train loss <loss>=10.481524467468262\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:21 INFO 140391921509760] Epoch[53] Batch[5] avg_epoch_loss=10.095287\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:21 INFO 140391921509760] #quality_metric: host=algo-1, epoch=53, batch=5 train loss <loss>=10.095287164052328\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:21 INFO 140391921509760] Epoch[53] Batch [5]#011Speed: 2010.42 samples/sec#011loss=10.095287\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:21 INFO 140391921509760] Epoch[53] Batch[10] avg_epoch_loss=9.894297\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:21 INFO 140391921509760] #quality_metric: host=algo-1, epoch=53, batch=10 train loss <loss>=9.653109169006347\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:21 INFO 140391921509760] Epoch[53] Batch [10]#011Speed: 1814.80 samples/sec#011loss=9.653109\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:21 INFO 140391921509760] processed a total of 674 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461101.1763005, \"EndTime\": 1631461101.7716637, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 594.8071479797363, \"count\": 1, \"min\": 594.8071479797363, \"max\": 594.8071479797363}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:21 INFO 140391921509760] #throughput_metric: host=algo-1, train throughput=1132.9133291280975 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:21 INFO 140391921509760] #progress_metric: host=algo-1, completed 27.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:21 INFO 140391921509760] #quality_metric: host=algo-1, epoch=53, train loss <loss>=9.894297166304154\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:21 INFO 140391921509760] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:22 INFO 140391921509760] Epoch[54] Batch[0] avg_epoch_loss=9.947419\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:22 INFO 140391921509760] #quality_metric: host=algo-1, epoch=54, batch=0 train loss <loss>=9.947419166564941\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:22 INFO 140391921509760] Epoch[54] Batch[5] avg_epoch_loss=9.893335\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:22 INFO 140391921509760] #quality_metric: host=algo-1, epoch=54, batch=5 train loss <loss>=9.893334547678629\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:22 INFO 140391921509760] Epoch[54] Batch [5]#011Speed: 1945.25 samples/sec#011loss=9.893335\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:22 INFO 140391921509760] Epoch[54] Batch[10] avg_epoch_loss=9.895393\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:22 INFO 140391921509760] #quality_metric: host=algo-1, epoch=54, batch=10 train loss <loss>=9.897863578796386\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:22 INFO 140391921509760] Epoch[54] Batch [10]#011Speed: 1507.25 samples/sec#011loss=9.897864\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:22 INFO 140391921509760] processed a total of 710 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461101.7717474, \"EndTime\": 1631461102.4514985, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 679.2910099029541, \"count\": 1, \"min\": 679.2910099029541, \"max\": 679.2910099029541}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:22 INFO 140391921509760] #throughput_metric: host=algo-1, train throughput=1045.0331902971195 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:22 INFO 140391921509760] #progress_metric: host=algo-1, completed 27.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:22 INFO 140391921509760] #quality_metric: host=algo-1, epoch=54, train loss <loss>=9.849213441212973\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:22 INFO 140391921509760] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:22 INFO 140391921509760] Epoch[55] Batch[0] avg_epoch_loss=9.053271\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:22 INFO 140391921509760] #quality_metric: host=algo-1, epoch=55, batch=0 train loss <loss>=9.053271293640137\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:22 INFO 140391921509760] Epoch[55] Batch[5] avg_epoch_loss=9.764862\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:22 INFO 140391921509760] #quality_metric: host=algo-1, epoch=55, batch=5 train loss <loss>=9.764861583709717\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:22 INFO 140391921509760] Epoch[55] Batch [5]#011Speed: 1876.21 samples/sec#011loss=9.764862\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:23 INFO 140391921509760] processed a total of 618 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461102.4515738, \"EndTime\": 1631461103.0110128, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 558.8803291320801, \"count\": 1, \"min\": 558.8803291320801, \"max\": 558.8803291320801}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:23 INFO 140391921509760] #throughput_metric: host=algo-1, train throughput=1105.525783291209 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:23 INFO 140391921509760] #progress_metric: host=algo-1, completed 28.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:23 INFO 140391921509760] #quality_metric: host=algo-1, epoch=55, train loss <loss>=9.496653699874878\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:23 INFO 140391921509760] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:23 INFO 140391921509760] Epoch[56] Batch[0] avg_epoch_loss=10.204462\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:23 INFO 140391921509760] #quality_metric: host=algo-1, epoch=56, batch=0 train loss <loss>=10.204462051391602\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:23 INFO 140391921509760] Epoch[56] Batch[5] avg_epoch_loss=10.132861\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:23 INFO 140391921509760] #quality_metric: host=algo-1, epoch=56, batch=5 train loss <loss>=10.132860660552979\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:23 INFO 140391921509760] Epoch[56] Batch [5]#011Speed: 2106.66 samples/sec#011loss=10.132861\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:23 INFO 140391921509760] processed a total of 620 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461103.0111032, \"EndTime\": 1631461103.5664847, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 554.7657012939453, \"count\": 1, \"min\": 554.7657012939453, \"max\": 554.7657012939453}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:23 INFO 140391921509760] #throughput_metric: host=algo-1, train throughput=1117.38050694155 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:23 INFO 140391921509760] #progress_metric: host=algo-1, completed 28.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:23 INFO 140391921509760] #quality_metric: host=algo-1, epoch=56, train loss <loss>=9.880435180664062\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:23 INFO 140391921509760] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:23 INFO 140391921509760] Epoch[57] Batch[0] avg_epoch_loss=9.671279\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:23 INFO 140391921509760] #quality_metric: host=algo-1, epoch=57, batch=0 train loss <loss>=9.671278953552246\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:23 INFO 140391921509760] Epoch[57] Batch[5] avg_epoch_loss=9.797271\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:23 INFO 140391921509760] #quality_metric: host=algo-1, epoch=57, batch=5 train loss <loss>=9.797271092732748\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:23 INFO 140391921509760] Epoch[57] Batch [5]#011Speed: 1943.82 samples/sec#011loss=9.797271\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:24 INFO 140391921509760] processed a total of 639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461103.5665536, \"EndTime\": 1631461104.11871, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 551.5427589416504, \"count\": 1, \"min\": 551.5427589416504, \"max\": 551.5427589416504}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:24 INFO 140391921509760] #throughput_metric: host=algo-1, train throughput=1158.2899890833373 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:24 INFO 140391921509760] #progress_metric: host=algo-1, completed 29.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:24 INFO 140391921509760] #quality_metric: host=algo-1, epoch=57, train loss <loss>=9.93292694091797\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:24 INFO 140391921509760] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:24 INFO 140391921509760] Epoch[58] Batch[0] avg_epoch_loss=9.904857\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:24 INFO 140391921509760] #quality_metric: host=algo-1, epoch=58, batch=0 train loss <loss>=9.90485668182373\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:24 INFO 140391921509760] Epoch[58] Batch[5] avg_epoch_loss=10.040822\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:24 INFO 140391921509760] #quality_metric: host=algo-1, epoch=58, batch=5 train loss <loss>=10.040821552276611\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:24 INFO 140391921509760] Epoch[58] Batch [5]#011Speed: 1871.26 samples/sec#011loss=10.040822\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:24 INFO 140391921509760] Epoch[58] Batch[10] avg_epoch_loss=9.805228\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:24 INFO 140391921509760] #quality_metric: host=algo-1, epoch=58, batch=10 train loss <loss>=9.522516822814941\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:24 INFO 140391921509760] Epoch[58] Batch [10]#011Speed: 1669.68 samples/sec#011loss=9.522517\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:24 INFO 140391921509760] processed a total of 696 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461104.1188002, \"EndTime\": 1631461104.7235358, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 604.1624546051025, \"count\": 1, \"min\": 604.1624546051025, \"max\": 604.1624546051025}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:24 INFO 140391921509760] #throughput_metric: host=algo-1, train throughput=1151.775316594287 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:24 INFO 140391921509760] #progress_metric: host=algo-1, completed 29.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:24 INFO 140391921509760] #quality_metric: host=algo-1, epoch=58, train loss <loss>=9.805228493430398\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:24 INFO 140391921509760] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:25 INFO 140391921509760] Epoch[59] Batch[0] avg_epoch_loss=10.376356\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:25 INFO 140391921509760] #quality_metric: host=algo-1, epoch=59, batch=0 train loss <loss>=10.37635612487793\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:25 INFO 140391921509760] Epoch[59] Batch[5] avg_epoch_loss=9.826344\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:25 INFO 140391921509760] #quality_metric: host=algo-1, epoch=59, batch=5 train loss <loss>=9.826343536376953\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:25 INFO 140391921509760] Epoch[59] Batch [5]#011Speed: 2100.05 samples/sec#011loss=9.826344\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:25 INFO 140391921509760] processed a total of 631 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461104.72362, \"EndTime\": 1631461105.282332, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 558.2256317138672, \"count\": 1, \"min\": 558.2256317138672, \"max\": 558.2256317138672}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:25 INFO 140391921509760] #throughput_metric: host=algo-1, train throughput=1130.1110013711095 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:25 INFO 140391921509760] #progress_metric: host=algo-1, completed 30.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:25 INFO 140391921509760] #quality_metric: host=algo-1, epoch=59, train loss <loss>=9.846443939208985\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:25 INFO 140391921509760] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:25 INFO 140391921509760] Epoch[60] Batch[0] avg_epoch_loss=10.300617\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:25 INFO 140391921509760] #quality_metric: host=algo-1, epoch=60, batch=0 train loss <loss>=10.300617218017578\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:25 INFO 140391921509760] Epoch[60] Batch[5] avg_epoch_loss=10.285276\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:25 INFO 140391921509760] #quality_metric: host=algo-1, epoch=60, batch=5 train loss <loss>=10.285276412963867\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:25 INFO 140391921509760] Epoch[60] Batch [5]#011Speed: 1846.30 samples/sec#011loss=10.285276\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:25 INFO 140391921509760] Epoch[60] Batch[10] avg_epoch_loss=10.056226\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:25 INFO 140391921509760] #quality_metric: host=algo-1, epoch=60, batch=10 train loss <loss>=9.781364631652831\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:25 INFO 140391921509760] Epoch[60] Batch [10]#011Speed: 1885.18 samples/sec#011loss=9.781365\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:25 INFO 140391921509760] processed a total of 665 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461105.2824194, \"EndTime\": 1631461105.8873446, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 604.4089794158936, \"count\": 1, \"min\": 604.4089794158936, \"max\": 604.4089794158936}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:25 INFO 140391921509760] #throughput_metric: host=algo-1, train throughput=1100.0288139089228 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:25 INFO 140391921509760] #progress_metric: host=algo-1, completed 30.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:25 INFO 140391921509760] #quality_metric: host=algo-1, epoch=60, train loss <loss>=10.056225603277033\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:25 INFO 140391921509760] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:26 INFO 140391921509760] Epoch[61] Batch[0] avg_epoch_loss=10.553664\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:26 INFO 140391921509760] #quality_metric: host=algo-1, epoch=61, batch=0 train loss <loss>=10.553664207458496\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:26 INFO 140391921509760] Epoch[61] Batch[5] avg_epoch_loss=9.975845\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:26 INFO 140391921509760] #quality_metric: host=algo-1, epoch=61, batch=5 train loss <loss>=9.975844860076904\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:26 INFO 140391921509760] Epoch[61] Batch [5]#011Speed: 2007.17 samples/sec#011loss=9.975845\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:26 INFO 140391921509760] processed a total of 639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461105.8874278, \"EndTime\": 1631461106.4357166, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 547.7855205535889, \"count\": 1, \"min\": 547.7855205535889, \"max\": 547.7855205535889}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:26 INFO 140391921509760] #throughput_metric: host=algo-1, train throughput=1166.245924801652 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:26 INFO 140391921509760] #progress_metric: host=algo-1, completed 31.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:26 INFO 140391921509760] #quality_metric: host=algo-1, epoch=61, train loss <loss>=10.157677745819091\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:26 INFO 140391921509760] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:26 INFO 140391921509760] Epoch[62] Batch[0] avg_epoch_loss=10.219802\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:26 INFO 140391921509760] #quality_metric: host=algo-1, epoch=62, batch=0 train loss <loss>=10.219801902770996\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:26 INFO 140391921509760] Epoch[62] Batch[5] avg_epoch_loss=9.980067\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:26 INFO 140391921509760] #quality_metric: host=algo-1, epoch=62, batch=5 train loss <loss>=9.980067094167074\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:26 INFO 140391921509760] Epoch[62] Batch [5]#011Speed: 2066.19 samples/sec#011loss=9.980067\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:27 INFO 140391921509760] Epoch[62] Batch[10] avg_epoch_loss=10.204203\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:27 INFO 140391921509760] #quality_metric: host=algo-1, epoch=62, batch=10 train loss <loss>=10.473165702819824\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:27 INFO 140391921509760] Epoch[62] Batch [10]#011Speed: 2005.99 samples/sec#011loss=10.473166\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:27 INFO 140391921509760] processed a total of 651 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461106.4358056, \"EndTime\": 1631461107.0452352, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 608.9155673980713, \"count\": 1, \"min\": 608.9155673980713, \"max\": 608.9155673980713}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:27 INFO 140391921509760] #throughput_metric: host=algo-1, train throughput=1068.9094992493535 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:27 INFO 140391921509760] #progress_metric: host=algo-1, completed 31.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:27 INFO 140391921509760] #quality_metric: host=algo-1, epoch=62, train loss <loss>=10.20420282537287\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:27 INFO 140391921509760] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:27 INFO 140391921509760] Epoch[63] Batch[0] avg_epoch_loss=9.368517\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:27 INFO 140391921509760] #quality_metric: host=algo-1, epoch=63, batch=0 train loss <loss>=9.36851692199707\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:27 INFO 140391921509760] Epoch[63] Batch[5] avg_epoch_loss=9.893823\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:27 INFO 140391921509760] #quality_metric: host=algo-1, epoch=63, batch=5 train loss <loss>=9.89382298787435\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:27 INFO 140391921509760] Epoch[63] Batch [5]#011Speed: 1943.07 samples/sec#011loss=9.893823\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:27 INFO 140391921509760] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461107.0453146, \"EndTime\": 1631461107.5981216, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 552.3386001586914, \"count\": 1, \"min\": 552.3386001586914, \"max\": 552.3386001586914}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:27 INFO 140391921509760] #throughput_metric: host=algo-1, train throughput=1153.016287747033 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:27 INFO 140391921509760] #progress_metric: host=algo-1, completed 32.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:27 INFO 140391921509760] #quality_metric: host=algo-1, epoch=63, train loss <loss>=9.894373607635497\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:27 INFO 140391921509760] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:27 INFO 140391921509760] Epoch[64] Batch[0] avg_epoch_loss=9.277812\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:27 INFO 140391921509760] #quality_metric: host=algo-1, epoch=64, batch=0 train loss <loss>=9.277812004089355\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:28 INFO 140391921509760] Epoch[64] Batch[5] avg_epoch_loss=10.012980\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:28 INFO 140391921509760] #quality_metric: host=algo-1, epoch=64, batch=5 train loss <loss>=10.012979825337728\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:28 INFO 140391921509760] Epoch[64] Batch [5]#011Speed: 2089.36 samples/sec#011loss=10.012980\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:28 INFO 140391921509760] Epoch[64] Batch[10] avg_epoch_loss=9.776597\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:28 INFO 140391921509760] #quality_metric: host=algo-1, epoch=64, batch=10 train loss <loss>=9.492938232421874\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:28 INFO 140391921509760] Epoch[64] Batch [10]#011Speed: 1796.43 samples/sec#011loss=9.492938\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:28 INFO 140391921509760] processed a total of 688 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461107.5982082, \"EndTime\": 1631461108.1981301, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 599.4045734405518, \"count\": 1, \"min\": 599.4045734405518, \"max\": 599.4045734405518}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:28 INFO 140391921509760] #throughput_metric: host=algo-1, train throughput=1147.5752118635642 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:28 INFO 140391921509760] #progress_metric: host=algo-1, completed 32.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:28 INFO 140391921509760] #quality_metric: host=algo-1, epoch=64, train loss <loss>=9.77659728310325\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:28 INFO 140391921509760] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:28 INFO 140391921509760] Epoch[65] Batch[0] avg_epoch_loss=10.474006\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:28 INFO 140391921509760] #quality_metric: host=algo-1, epoch=65, batch=0 train loss <loss>=10.474005699157715\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:28 INFO 140391921509760] Epoch[65] Batch[5] avg_epoch_loss=10.036734\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:28 INFO 140391921509760] #quality_metric: host=algo-1, epoch=65, batch=5 train loss <loss>=10.036733945210775\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:28 INFO 140391921509760] Epoch[65] Batch [5]#011Speed: 1912.38 samples/sec#011loss=10.036734\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:28 INFO 140391921509760] Epoch[65] Batch[10] avg_epoch_loss=10.282353\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:28 INFO 140391921509760] #quality_metric: host=algo-1, epoch=65, batch=10 train loss <loss>=10.577095413208008\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:28 INFO 140391921509760] Epoch[65] Batch [10]#011Speed: 1538.75 samples/sec#011loss=10.577095\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:28 INFO 140391921509760] processed a total of 642 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461108.1982133, \"EndTime\": 1631461108.8360565, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 637.3574733734131, \"count\": 1, \"min\": 637.3574733734131, \"max\": 637.3574733734131}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:28 INFO 140391921509760] #throughput_metric: host=algo-1, train throughput=1007.0964125921125 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:28 INFO 140391921509760] #progress_metric: host=algo-1, completed 33.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:28 INFO 140391921509760] #quality_metric: host=algo-1, epoch=65, train loss <loss>=10.282352794300426\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:28 INFO 140391921509760] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:29 INFO 140391921509760] Epoch[66] Batch[0] avg_epoch_loss=10.132139\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:29 INFO 140391921509760] #quality_metric: host=algo-1, epoch=66, batch=0 train loss <loss>=10.132139205932617\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:29 INFO 140391921509760] Epoch[66] Batch[5] avg_epoch_loss=10.057499\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:29 INFO 140391921509760] #quality_metric: host=algo-1, epoch=66, batch=5 train loss <loss>=10.057499249776205\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:29 INFO 140391921509760] Epoch[66] Batch [5]#011Speed: 2149.01 samples/sec#011loss=10.057499\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:29 INFO 140391921509760] processed a total of 632 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461108.8361382, \"EndTime\": 1631461109.3728447, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 536.2205505371094, \"count\": 1, \"min\": 536.2205505371094, \"max\": 536.2205505371094}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:29 INFO 140391921509760] #throughput_metric: host=algo-1, train throughput=1178.2810681160927 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:29 INFO 140391921509760] #progress_metric: host=algo-1, completed 33.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:29 INFO 140391921509760] #quality_metric: host=algo-1, epoch=66, train loss <loss>=9.975451374053955\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:29 INFO 140391921509760] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:29 INFO 140391921509760] Epoch[67] Batch[0] avg_epoch_loss=10.160340\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:29 INFO 140391921509760] #quality_metric: host=algo-1, epoch=67, batch=0 train loss <loss>=10.160340309143066\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:29 INFO 140391921509760] Epoch[67] Batch[5] avg_epoch_loss=10.156876\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:29 INFO 140391921509760] #quality_metric: host=algo-1, epoch=67, batch=5 train loss <loss>=10.15687608718872\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:29 INFO 140391921509760] Epoch[67] Batch [5]#011Speed: 2061.40 samples/sec#011loss=10.156876\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:29 INFO 140391921509760] processed a total of 609 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461109.3729584, \"EndTime\": 1631461109.9648328, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 591.3412570953369, \"count\": 1, \"min\": 591.3412570953369, \"max\": 591.3412570953369}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:29 INFO 140391921509760] #throughput_metric: host=algo-1, train throughput=1029.6392602069814 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:29 INFO 140391921509760] #progress_metric: host=algo-1, completed 34.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:29 INFO 140391921509760] #quality_metric: host=algo-1, epoch=67, train loss <loss>=9.762600183486938\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:29 INFO 140391921509760] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:30 INFO 140391921509760] Epoch[68] Batch[0] avg_epoch_loss=10.441343\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:30 INFO 140391921509760] #quality_metric: host=algo-1, epoch=68, batch=0 train loss <loss>=10.441343307495117\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:30 INFO 140391921509760] Epoch[68] Batch[5] avg_epoch_loss=9.679549\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:30 INFO 140391921509760] #quality_metric: host=algo-1, epoch=68, batch=5 train loss <loss>=9.679548581441244\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:30 INFO 140391921509760] Epoch[68] Batch [5]#011Speed: 2105.13 samples/sec#011loss=9.679549\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:30 INFO 140391921509760] processed a total of 639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461109.964921, \"EndTime\": 1631461110.5195801, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 554.1002750396729, \"count\": 1, \"min\": 554.1002750396729, \"max\": 554.1002750396729}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:30 INFO 140391921509760] #throughput_metric: host=algo-1, train throughput=1152.9539693444428 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:30 INFO 140391921509760] #progress_metric: host=algo-1, completed 34.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:30 INFO 140391921509760] #quality_metric: host=algo-1, epoch=68, train loss <loss>=9.800920581817627\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:30 INFO 140391921509760] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:30 INFO 140391921509760] Epoch[69] Batch[0] avg_epoch_loss=10.216166\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:30 INFO 140391921509760] #quality_metric: host=algo-1, epoch=69, batch=0 train loss <loss>=10.216165542602539\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:30 INFO 140391921509760] Epoch[69] Batch[5] avg_epoch_loss=10.080379\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:30 INFO 140391921509760] #quality_metric: host=algo-1, epoch=69, batch=5 train loss <loss>=10.080379327138266\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:30 INFO 140391921509760] Epoch[69] Batch [5]#011Speed: 1871.68 samples/sec#011loss=10.080379\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:31 INFO 140391921509760] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461110.51967, \"EndTime\": 1631461111.077752, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 557.5485229492188, \"count\": 1, \"min\": 557.5485229492188, \"max\": 557.5485229492188}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:31 INFO 140391921509760] #throughput_metric: host=algo-1, train throughput=1136.836371735782 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:31 INFO 140391921509760] #progress_metric: host=algo-1, completed 35.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:31 INFO 140391921509760] #quality_metric: host=algo-1, epoch=69, train loss <loss>=10.025253105163575\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:31 INFO 140391921509760] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:31 INFO 140391921509760] Epoch[70] Batch[0] avg_epoch_loss=9.563259\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:31 INFO 140391921509760] #quality_metric: host=algo-1, epoch=70, batch=0 train loss <loss>=9.56325912475586\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:31 INFO 140391921509760] Epoch[70] Batch[5] avg_epoch_loss=9.887204\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:31 INFO 140391921509760] #quality_metric: host=algo-1, epoch=70, batch=5 train loss <loss>=9.88720448811849\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:31 INFO 140391921509760] Epoch[70] Batch [5]#011Speed: 2122.17 samples/sec#011loss=9.887204\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:31 INFO 140391921509760] Epoch[70] Batch[10] avg_epoch_loss=9.852835\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:31 INFO 140391921509760] #quality_metric: host=algo-1, epoch=70, batch=10 train loss <loss>=9.811591529846192\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:31 INFO 140391921509760] Epoch[70] Batch [10]#011Speed: 2011.63 samples/sec#011loss=9.811592\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:31 INFO 140391921509760] processed a total of 647 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461111.0778403, \"EndTime\": 1631461111.659726, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 581.364631652832, \"count\": 1, \"min\": 581.364631652832, \"max\": 581.364631652832}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:31 INFO 140391921509760] #throughput_metric: host=algo-1, train throughput=1112.657870524505 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:31 INFO 140391921509760] #progress_metric: host=algo-1, completed 35.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:31 INFO 140391921509760] #quality_metric: host=algo-1, epoch=70, train loss <loss>=9.852834961631082\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:31 INFO 140391921509760] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:31 INFO 140391921509760] Epoch[71] Batch[0] avg_epoch_loss=10.213848\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:31 INFO 140391921509760] #quality_metric: host=algo-1, epoch=71, batch=0 train loss <loss>=10.213848114013672\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:32 INFO 140391921509760] Epoch[71] Batch[5] avg_epoch_loss=9.892037\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:32 INFO 140391921509760] #quality_metric: host=algo-1, epoch=71, batch=5 train loss <loss>=9.892036596934\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:32 INFO 140391921509760] Epoch[71] Batch [5]#011Speed: 2079.59 samples/sec#011loss=9.892037\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:32 INFO 140391921509760] Epoch[71] Batch[10] avg_epoch_loss=9.762560\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:32 INFO 140391921509760] #quality_metric: host=algo-1, epoch=71, batch=10 train loss <loss>=9.607187080383301\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:32 INFO 140391921509760] Epoch[71] Batch [10]#011Speed: 1781.53 samples/sec#011loss=9.607187\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:32 INFO 140391921509760] processed a total of 718 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461111.6598134, \"EndTime\": 1631461112.2655158, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 605.156421661377, \"count\": 1, \"min\": 605.156421661377, \"max\": 605.156421661377}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:32 INFO 140391921509760] #throughput_metric: host=algo-1, train throughput=1186.2415638876448 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:32 INFO 140391921509760] #progress_metric: host=algo-1, completed 36.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:32 INFO 140391921509760] #quality_metric: host=algo-1, epoch=71, train loss <loss>=10.218342940012613\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:32 INFO 140391921509760] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:32 INFO 140391921509760] Epoch[72] Batch[0] avg_epoch_loss=10.671380\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:32 INFO 140391921509760] #quality_metric: host=algo-1, epoch=72, batch=0 train loss <loss>=10.671380043029785\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:32 INFO 140391921509760] Epoch[72] Batch[5] avg_epoch_loss=10.450207\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:32 INFO 140391921509760] #quality_metric: host=algo-1, epoch=72, batch=5 train loss <loss>=10.450207233428955\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:32 INFO 140391921509760] Epoch[72] Batch [5]#011Speed: 2093.91 samples/sec#011loss=10.450207\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:32 INFO 140391921509760] processed a total of 590 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461112.2655997, \"EndTime\": 1631461112.8134828, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 547.316312789917, \"count\": 1, \"min\": 547.316312789917, \"max\": 547.316312789917}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:32 INFO 140391921509760] #throughput_metric: host=algo-1, train throughput=1077.7323123797446 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:32 INFO 140391921509760] #progress_metric: host=algo-1, completed 36.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:32 INFO 140391921509760] #quality_metric: host=algo-1, epoch=72, train loss <loss>=10.592658805847169\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:32 INFO 140391921509760] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:33 INFO 140391921509760] Epoch[73] Batch[0] avg_epoch_loss=9.694938\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:33 INFO 140391921509760] #quality_metric: host=algo-1, epoch=73, batch=0 train loss <loss>=9.694937705993652\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:33 INFO 140391921509760] Epoch[73] Batch[5] avg_epoch_loss=10.371412\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:33 INFO 140391921509760] #quality_metric: host=algo-1, epoch=73, batch=5 train loss <loss>=10.371411641438803\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:33 INFO 140391921509760] Epoch[73] Batch [5]#011Speed: 1984.17 samples/sec#011loss=10.371412\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:33 INFO 140391921509760] Epoch[73] Batch[10] avg_epoch_loss=9.887344\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:33 INFO 140391921509760] #quality_metric: host=algo-1, epoch=73, batch=10 train loss <loss>=9.30646333694458\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:33 INFO 140391921509760] Epoch[73] Batch [10]#011Speed: 1887.33 samples/sec#011loss=9.306463\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:33 INFO 140391921509760] processed a total of 649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461112.81357, \"EndTime\": 1631461113.4126344, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 598.5486507415771, \"count\": 1, \"min\": 598.5486507415771, \"max\": 598.5486507415771}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:33 INFO 140391921509760] #throughput_metric: host=algo-1, train throughput=1084.0821975527435 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:33 INFO 140391921509760] #progress_metric: host=algo-1, completed 37.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:33 INFO 140391921509760] #quality_metric: host=algo-1, epoch=73, train loss <loss>=9.887344230305064\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:33 INFO 140391921509760] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:33 INFO 140391921509760] Epoch[74] Batch[0] avg_epoch_loss=9.626037\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:33 INFO 140391921509760] #quality_metric: host=algo-1, epoch=74, batch=0 train loss <loss>=9.626036643981934\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:33 INFO 140391921509760] Epoch[74] Batch[5] avg_epoch_loss=10.037052\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:33 INFO 140391921509760] #quality_metric: host=algo-1, epoch=74, batch=5 train loss <loss>=10.037051518758139\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:33 INFO 140391921509760] Epoch[74] Batch [5]#011Speed: 1683.06 samples/sec#011loss=10.037052\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:34 INFO 140391921509760] Epoch[74] Batch[10] avg_epoch_loss=10.144862\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:34 INFO 140391921509760] #quality_metric: host=algo-1, epoch=74, batch=10 train loss <loss>=10.274234580993653\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:34 INFO 140391921509760] Epoch[74] Batch [10]#011Speed: 1807.22 samples/sec#011loss=10.274235\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:34 INFO 140391921509760] processed a total of 691 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461113.4127123, \"EndTime\": 1631461114.0345619, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 621.3581562042236, \"count\": 1, \"min\": 621.3581562042236, \"max\": 621.3581562042236}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:34 INFO 140391921509760] #throughput_metric: host=algo-1, train throughput=1111.8641608396304 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:34 INFO 140391921509760] #progress_metric: host=algo-1, completed 37.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:34 INFO 140391921509760] #quality_metric: host=algo-1, epoch=74, train loss <loss>=10.144862001592463\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:34 INFO 140391921509760] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:34 INFO 140391921509760] Loading parameters from best epoch (34)\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461114.0346448, \"EndTime\": 1631461114.0443358, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.deserialize.time\": {\"sum\": 9.112834930419922, \"count\": 1, \"min\": 9.112834930419922, \"max\": 9.112834930419922}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:34 INFO 140391921509760] stopping training now\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:34 INFO 140391921509760] #progress_metric: host=algo-1, completed 100 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:34 INFO 140391921509760] Final loss: 9.177153392271562 (occurred at epoch 34)\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:34 INFO 140391921509760] #quality_metric: host=algo-1, train final_loss <loss>=9.177153392271562\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:34 INFO 140391921509760] Worker algo-1 finished training.\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:34 WARNING 140391921509760] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:34 INFO 140391921509760] All workers finished. Serializing model for prediction.\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461114.0444095, \"EndTime\": 1631461114.1525507, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"get_graph.time\": {\"sum\": 107.34438896179199, \"count\": 1, \"min\": 107.34438896179199, \"max\": 107.34438896179199}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:34 INFO 140391921509760] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461114.1526093, \"EndTime\": 1631461114.1866138, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"finalize.time\": {\"sum\": 141.4484977722168, \"count\": 1, \"min\": 141.4484977722168, \"max\": 141.4484977722168}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:34 INFO 140391921509760] Serializing to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:34 INFO 140391921509760] Saved checkpoint to \"/opt/ml/model/model_algo-1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461114.1866763, \"EndTime\": 1631461114.1920643, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"model.serialize.time\": {\"sum\": 5.347967147827148, \"count\": 1, \"min\": 5.347967147827148, \"max\": 5.347967147827148}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:34 INFO 140391921509760] Successfully serialized the model for prediction.\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:38:34 INFO 140391921509760] No test data passed, skipping evaluation.\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461114.1921117, \"EndTime\": 1631461114.197243, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"setuptime\": {\"sum\": 6.795406341552734, \"count\": 1, \"min\": 6.795406341552734, \"max\": 6.795406341552734}, \"totaltime\": {\"sum\": 45235.88681221008, \"count\": 1, \"min\": 45235.88681221008, \"max\": 45235.88681221008}}}\n",
      "\u001b[0m\n",
      "\n",
      "2021-09-12 15:38:47 Uploading - Uploading generated training model\n",
      "2021-09-12 15:38:47 Completed - Training job completed\n",
      "Training seconds: 98\n",
      "Billable seconds: 98\n",
      "-------------!Overwriting existing file\n",
      "Uploading file to s3://sagemaker-us-east-2-222195595958/deepar-covid-demo-notebook/data/train/train.json\n",
      "2021-09-12 15:45:49 Starting - Starting the training job...\n",
      "2021-09-12 15:46:13 Starting - Launching requested ML instancesProfilerReport-1631461549: InProgress\n",
      "...\n",
      "2021-09-12 15:46:40 Starting - Preparing the instances for training.........\n",
      "2021-09-12 15:48:13 Downloading - Downloading input data\n",
      "2021-09-12 15:48:13 Training - Downloading the training image...\n",
      "2021-09-12 15:48:41 Training - Training image download completed. Training in progress..\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:44 INFO 140202221626752] Reading default configuration from /opt/amazon/lib/python3.6/site-packages/algorithm/resources/default-input.json: {'_kvstore': 'auto', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_tuning_objective_metric': '', 'cardinality': 'auto', 'dropout_rate': '0.10', 'early_stopping_patience': '', 'embedding_dimension': '10', 'learning_rate': '0.001', 'likelihood': 'student-t', 'mini_batch_size': '128', 'num_cells': '40', 'num_dynamic_feat': 'auto', 'num_eval_samples': '100', 'num_layers': '2', 'test_quantiles': '[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]'}\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:44 INFO 140202221626752] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {'prediction_length': '14', 'time_freq': 'D', 'context_length': '14', 'epochs': '200', 'learning_rate': '5E-4', 'early_stopping_patience': '40', 'mini_batch_size': '64'}\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:44 INFO 140202221626752] Final configuration: {'_kvstore': 'auto', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_tuning_objective_metric': '', 'cardinality': 'auto', 'dropout_rate': '0.10', 'early_stopping_patience': '40', 'embedding_dimension': '10', 'learning_rate': '5E-4', 'likelihood': 'student-t', 'mini_batch_size': '64', 'num_cells': '40', 'num_dynamic_feat': 'auto', 'num_eval_samples': '100', 'num_layers': '2', 'test_quantiles': '[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', 'prediction_length': '14', 'time_freq': 'D', 'context_length': '14', 'epochs': '200'}\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:44 INFO 140202221626752] Detected entry point for worker worker\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:44 INFO 140202221626752] Using early stopping with patience 40\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:44 INFO 140202221626752] random_seed is None\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:44 INFO 140202221626752] [cardinality=auto] `cat` field was NOT found in the file `/opt/ml/input/data/train/train.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:44 INFO 140202221626752] [num_dynamic_feat=auto] `dynamic_feat` field was NOT found in the file `/opt/ml/input/data/train/train.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:44 INFO 140202221626752] Training set statistics:\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:44 INFO 140202221626752] Integer time series\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:44 INFO 140202221626752] number of time series: 41\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:44 INFO 140202221626752] number of observations: 13495\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:44 INFO 140202221626752] mean target length: 329.1463414634146\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:44 INFO 140202221626752] min/mean/max target: 0.0/15420503.35850315/352550944.0\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:44 INFO 140202221626752] mean abs(target): 15420503.35850315\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:44 INFO 140202221626752] contains missing values: no\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:44 INFO 140202221626752] Small number of time series. Doing 16 passes over dataset with prob 0.975609756097561 per epoch.\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:44 INFO 140202221626752] No test channel found not running evaluations\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:44 INFO 140202221626752] #memory_usage::<batchbuffer> = 2.2568130493164062 mb\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:44 INFO 140202221626752] nvidia-smi took: 0.025309085845947266 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:44 INFO 140202221626752] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:44 INFO 140202221626752] Create Store: local\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461724.7080266, \"EndTime\": 1631461724.7976758, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"get_graph.time\": {\"sum\": 87.56113052368164, \"count\": 1, \"min\": 87.56113052368164, \"max\": 87.56113052368164}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:44 INFO 140202221626752] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:44 INFO 140202221626752] #memory_usage::<model> = 12 mb\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461724.797771, \"EndTime\": 1631461724.918364, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"initialize.time\": {\"sum\": 210.19983291625977, \"count\": 1, \"min\": 210.19983291625977, \"max\": 210.19983291625977}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:45 INFO 140202221626752] Epoch[0] Batch[0] avg_epoch_loss=14.543978\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:45 INFO 140202221626752] #quality_metric: host=algo-1, epoch=0, batch=0 train loss <loss>=14.543977737426758\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:45 INFO 140202221626752] Epoch[0] Batch[5] avg_epoch_loss=14.340891\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:45 INFO 140202221626752] #quality_metric: host=algo-1, epoch=0, batch=5 train loss <loss>=14.340891202290853\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:45 INFO 140202221626752] Epoch[0] Batch [5]#011Speed: 1968.44 samples/sec#011loss=14.340891\u001b[0m\n",
      "\u001b[34m/opt/amazon/python3.6/lib/python3.6/contextlib.py:99: DeprecationWarning: generator 'local_timer' raised StopIteration\n",
      "  self.gen.throw(type, value, traceback)\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:45 INFO 140202221626752] processed a total of 627 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461724.91847, \"EndTime\": 1631461725.5340848, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"epochs\": {\"sum\": 200.0, \"count\": 1, \"min\": 200, \"max\": 200}, \"update.time\": {\"sum\": 615.5068874359131, \"count\": 1, \"min\": 615.5068874359131, \"max\": 615.5068874359131}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:45 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=1018.4228860345372 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:45 INFO 140202221626752] #progress_metric: host=algo-1, completed 0.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:45 INFO 140202221626752] #quality_metric: host=algo-1, epoch=0, train loss <loss>=13.857499694824218\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:45 INFO 140202221626752] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:45 INFO 140202221626752] Saved checkpoint to \"/opt/ml/model/state_adf0886e-c748-4c15-9854-240dd54fdd0e-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461725.5341907, \"EndTime\": 1631461725.5554945, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 20.681381225585938, \"count\": 1, \"min\": 20.681381225585938, \"max\": 20.681381225585938}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:45 INFO 140202221626752] Epoch[1] Batch[0] avg_epoch_loss=12.725010\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:45 INFO 140202221626752] #quality_metric: host=algo-1, epoch=1, batch=0 train loss <loss>=12.72500991821289\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:45 INFO 140202221626752] Epoch[1] Batch[5] avg_epoch_loss=12.836722\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:45 INFO 140202221626752] #quality_metric: host=algo-1, epoch=1, batch=5 train loss <loss>=12.836721579233805\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:45 INFO 140202221626752] Epoch[1] Batch [5]#011Speed: 1850.16 samples/sec#011loss=12.836722\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:46 INFO 140202221626752] Epoch[1] Batch[10] avg_epoch_loss=12.589791\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:46 INFO 140202221626752] #quality_metric: host=algo-1, epoch=1, batch=10 train loss <loss>=12.293473243713379\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:46 INFO 140202221626752] Epoch[1] Batch [10]#011Speed: 1548.15 samples/sec#011loss=12.293473\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:46 INFO 140202221626752] processed a total of 665 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461725.5555844, \"EndTime\": 1631461726.198084, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 642.4281597137451, \"count\": 1, \"min\": 642.4281597137451, \"max\": 642.4281597137451}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:46 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=1034.9285604826875 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:46 INFO 140202221626752] #progress_metric: host=algo-1, completed 1.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:46 INFO 140202221626752] #quality_metric: host=algo-1, epoch=1, train loss <loss>=12.589790517633611\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:46 INFO 140202221626752] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:46 INFO 140202221626752] Saved checkpoint to \"/opt/ml/model/state_290a4290-7e2b-4da4-af36-f87d1e501301-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461726.198172, \"EndTime\": 1631461726.2183964, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 19.649744033813477, \"count\": 1, \"min\": 19.649744033813477, \"max\": 19.649744033813477}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:46 INFO 140202221626752] Epoch[2] Batch[0] avg_epoch_loss=12.439980\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:46 INFO 140202221626752] #quality_metric: host=algo-1, epoch=2, batch=0 train loss <loss>=12.439979553222656\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:46 INFO 140202221626752] Epoch[2] Batch[5] avg_epoch_loss=11.909962\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:46 INFO 140202221626752] #quality_metric: host=algo-1, epoch=2, batch=5 train loss <loss>=11.90996233622233\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:46 INFO 140202221626752] Epoch[2] Batch [5]#011Speed: 1836.98 samples/sec#011loss=11.909962\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:46 INFO 140202221626752] processed a total of 629 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461726.2185092, \"EndTime\": 1631461726.8540564, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 635.4708671569824, \"count\": 1, \"min\": 635.4708671569824, \"max\": 635.4708671569824}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:46 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=989.6055767447583 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:46 INFO 140202221626752] #progress_metric: host=algo-1, completed 1.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:46 INFO 140202221626752] #quality_metric: host=algo-1, epoch=2, train loss <loss>=12.153730869293213\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:46 INFO 140202221626752] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:46 INFO 140202221626752] Saved checkpoint to \"/opt/ml/model/state_5b8de492-5b52-426b-bf11-160668350925-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461726.8541498, \"EndTime\": 1631461726.8740587, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 19.323110580444336, \"count\": 1, \"min\": 19.323110580444336, \"max\": 19.323110580444336}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:47 INFO 140202221626752] Epoch[3] Batch[0] avg_epoch_loss=11.717546\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:47 INFO 140202221626752] #quality_metric: host=algo-1, epoch=3, batch=0 train loss <loss>=11.717546463012695\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:47 INFO 140202221626752] Epoch[3] Batch[5] avg_epoch_loss=12.009584\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:47 INFO 140202221626752] #quality_metric: host=algo-1, epoch=3, batch=5 train loss <loss>=12.009584108988443\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:47 INFO 140202221626752] Epoch[3] Batch [5]#011Speed: 1391.03 samples/sec#011loss=12.009584\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:47 INFO 140202221626752] Epoch[3] Batch[10] avg_epoch_loss=12.309113\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:47 INFO 140202221626752] #quality_metric: host=algo-1, epoch=3, batch=10 train loss <loss>=12.668547058105469\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:47 INFO 140202221626752] Epoch[3] Batch [10]#011Speed: 1425.38 samples/sec#011loss=12.668547\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:47 INFO 140202221626752] processed a total of 656 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461726.8741465, \"EndTime\": 1631461727.6155617, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 741.3384914398193, \"count\": 1, \"min\": 741.3384914398193, \"max\": 741.3384914398193}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:47 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=884.7163497047422 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:47 INFO 140202221626752] #progress_metric: host=algo-1, completed 2.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:47 INFO 140202221626752] #quality_metric: host=algo-1, epoch=3, train loss <loss>=12.309112722223455\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:47 INFO 140202221626752] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:47 INFO 140202221626752] Epoch[4] Batch[0] avg_epoch_loss=12.106223\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:47 INFO 140202221626752] #quality_metric: host=algo-1, epoch=4, batch=0 train loss <loss>=12.106223106384277\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:48 INFO 140202221626752] Epoch[4] Batch[5] avg_epoch_loss=12.025667\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:48 INFO 140202221626752] #quality_metric: host=algo-1, epoch=4, batch=5 train loss <loss>=12.025666872660318\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:48 INFO 140202221626752] Epoch[4] Batch [5]#011Speed: 1763.63 samples/sec#011loss=12.025667\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:48 INFO 140202221626752] Epoch[4] Batch[10] avg_epoch_loss=11.701162\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:48 INFO 140202221626752] #quality_metric: host=algo-1, epoch=4, batch=10 train loss <loss>=11.31175651550293\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:48 INFO 140202221626752] Epoch[4] Batch [10]#011Speed: 1800.44 samples/sec#011loss=11.311757\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:48 INFO 140202221626752] processed a total of 679 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461727.6156597, \"EndTime\": 1631461728.287623, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 670.9530353546143, \"count\": 1, \"min\": 670.9530353546143, \"max\": 670.9530353546143}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:48 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=1011.7898397262702 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:48 INFO 140202221626752] #progress_metric: host=algo-1, completed 2.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:48 INFO 140202221626752] #quality_metric: host=algo-1, epoch=4, train loss <loss>=11.701162164861506\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:48 INFO 140202221626752] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:48 INFO 140202221626752] Saved checkpoint to \"/opt/ml/model/state_95c29c7e-6b74-4930-ba3c-2331c5d1bbd2-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461728.2877166, \"EndTime\": 1631461728.3030298, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 14.725208282470703, \"count\": 1, \"min\": 14.725208282470703, \"max\": 14.725208282470703}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:48 INFO 140202221626752] Epoch[5] Batch[0] avg_epoch_loss=11.163613\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:48 INFO 140202221626752] #quality_metric: host=algo-1, epoch=5, batch=0 train loss <loss>=11.163613319396973\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:48 INFO 140202221626752] Epoch[5] Batch[5] avg_epoch_loss=11.325300\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:48 INFO 140202221626752] #quality_metric: host=algo-1, epoch=5, batch=5 train loss <loss>=11.325300057729086\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:48 INFO 140202221626752] Epoch[5] Batch [5]#011Speed: 1977.55 samples/sec#011loss=11.325300\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:48 INFO 140202221626752] processed a total of 635 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461728.3031068, \"EndTime\": 1631461728.84569, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 542.5205230712891, \"count\": 1, \"min\": 542.5205230712891, \"max\": 542.5205230712891}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:48 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=1170.1746529962356 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:48 INFO 140202221626752] #progress_metric: host=algo-1, completed 3.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:48 INFO 140202221626752] #quality_metric: host=algo-1, epoch=5, train loss <loss>=11.228671455383301\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:48 INFO 140202221626752] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:48 INFO 140202221626752] Saved checkpoint to \"/opt/ml/model/state_ca4fd380-5997-4de0-be2c-20df8bdfb87e-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461728.8457856, \"EndTime\": 1631461728.866166, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 19.84262466430664, \"count\": 1, \"min\": 19.84262466430664, \"max\": 19.84262466430664}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:49 INFO 140202221626752] Epoch[6] Batch[0] avg_epoch_loss=10.624952\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:49 INFO 140202221626752] #quality_metric: host=algo-1, epoch=6, batch=0 train loss <loss>=10.62495231628418\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:49 INFO 140202221626752] Epoch[6] Batch[5] avg_epoch_loss=10.891114\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:49 INFO 140202221626752] #quality_metric: host=algo-1, epoch=6, batch=5 train loss <loss>=10.891113758087158\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:49 INFO 140202221626752] Epoch[6] Batch [5]#011Speed: 1976.82 samples/sec#011loss=10.891114\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:49 INFO 140202221626752] Epoch[6] Batch[10] avg_epoch_loss=10.325186\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:49 INFO 140202221626752] #quality_metric: host=algo-1, epoch=6, batch=10 train loss <loss>=9.64607343673706\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:49 INFO 140202221626752] Epoch[6] Batch [10]#011Speed: 1923.45 samples/sec#011loss=9.646073\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:49 INFO 140202221626752] processed a total of 650 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461728.8662534, \"EndTime\": 1631461729.443243, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 576.9162178039551, \"count\": 1, \"min\": 576.9162178039551, \"max\": 576.9162178039551}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:49 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=1126.4379935891768 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:49 INFO 140202221626752] #progress_metric: host=algo-1, completed 3.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:49 INFO 140202221626752] #quality_metric: host=algo-1, epoch=6, train loss <loss>=10.32518633929166\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:49 INFO 140202221626752] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:49 INFO 140202221626752] Saved checkpoint to \"/opt/ml/model/state_97339b43-640e-4a4f-9b71-ef8d90fabc17-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461729.4433274, \"EndTime\": 1631461729.4639497, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 20.108461380004883, \"count\": 1, \"min\": 20.108461380004883, \"max\": 20.108461380004883}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:49 INFO 140202221626752] Epoch[7] Batch[0] avg_epoch_loss=12.276792\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:49 INFO 140202221626752] #quality_metric: host=algo-1, epoch=7, batch=0 train loss <loss>=12.2767915725708\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:49 INFO 140202221626752] Epoch[7] Batch[5] avg_epoch_loss=11.036349\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:49 INFO 140202221626752] #quality_metric: host=algo-1, epoch=7, batch=5 train loss <loss>=11.036348660786947\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:49 INFO 140202221626752] Epoch[7] Batch [5]#011Speed: 1618.95 samples/sec#011loss=11.036349\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:50 INFO 140202221626752] processed a total of 612 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461729.4640384, \"EndTime\": 1631461730.0772347, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 613.1200790405273, \"count\": 1, \"min\": 613.1200790405273, \"max\": 613.1200790405273}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:50 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=997.9426406749693 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:50 INFO 140202221626752] #progress_metric: host=algo-1, completed 4.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:50 INFO 140202221626752] #quality_metric: host=algo-1, epoch=7, train loss <loss>=11.1300217628479\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:50 INFO 140202221626752] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:50 INFO 140202221626752] Epoch[8] Batch[0] avg_epoch_loss=10.467491\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:50 INFO 140202221626752] #quality_metric: host=algo-1, epoch=8, batch=0 train loss <loss>=10.467491149902344\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:50 INFO 140202221626752] Epoch[8] Batch[5] avg_epoch_loss=10.440192\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:50 INFO 140202221626752] #quality_metric: host=algo-1, epoch=8, batch=5 train loss <loss>=10.440192063649496\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:50 INFO 140202221626752] Epoch[8] Batch [5]#011Speed: 1941.80 samples/sec#011loss=10.440192\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:50 INFO 140202221626752] Epoch[8] Batch[10] avg_epoch_loss=10.244722\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:50 INFO 140202221626752] #quality_metric: host=algo-1, epoch=8, batch=10 train loss <loss>=10.010157585144043\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:50 INFO 140202221626752] Epoch[8] Batch [10]#011Speed: 1832.23 samples/sec#011loss=10.010158\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:50 INFO 140202221626752] processed a total of 653 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461730.0773335, \"EndTime\": 1631461730.706536, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 628.6807060241699, \"count\": 1, \"min\": 628.6807060241699, \"max\": 628.6807060241699}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:50 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=1038.4660637310046 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:50 INFO 140202221626752] #progress_metric: host=algo-1, completed 4.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:50 INFO 140202221626752] #quality_metric: host=algo-1, epoch=8, train loss <loss>=10.244721846147018\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:50 INFO 140202221626752] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:50 INFO 140202221626752] Saved checkpoint to \"/opt/ml/model/state_eec7f5a9-980f-4e83-b453-cf3f75959d1b-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461730.7066271, \"EndTime\": 1631461730.7272732, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 20.10941505432129, \"count\": 1, \"min\": 20.10941505432129, \"max\": 20.10941505432129}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:50 INFO 140202221626752] Epoch[9] Batch[0] avg_epoch_loss=11.215280\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:50 INFO 140202221626752] #quality_metric: host=algo-1, epoch=9, batch=0 train loss <loss>=11.215279579162598\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:51 INFO 140202221626752] Epoch[9] Batch[5] avg_epoch_loss=10.821734\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:51 INFO 140202221626752] #quality_metric: host=algo-1, epoch=9, batch=5 train loss <loss>=10.821734269460043\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:51 INFO 140202221626752] Epoch[9] Batch [5]#011Speed: 1516.90 samples/sec#011loss=10.821734\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:51 INFO 140202221626752] processed a total of 617 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461730.7273495, \"EndTime\": 1631461731.36109, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 633.674144744873, \"count\": 1, \"min\": 633.674144744873, \"max\": 633.674144744873}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:51 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=973.4836486636955 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:51 INFO 140202221626752] #progress_metric: host=algo-1, completed 5.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:51 INFO 140202221626752] #quality_metric: host=algo-1, epoch=9, train loss <loss>=10.488184881210326\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:51 INFO 140202221626752] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:51 INFO 140202221626752] Epoch[10] Batch[0] avg_epoch_loss=10.678218\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:51 INFO 140202221626752] #quality_metric: host=algo-1, epoch=10, batch=0 train loss <loss>=10.678217887878418\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:51 INFO 140202221626752] Epoch[10] Batch[5] avg_epoch_loss=10.654565\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:51 INFO 140202221626752] #quality_metric: host=algo-1, epoch=10, batch=5 train loss <loss>=10.654564698537191\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:51 INFO 140202221626752] Epoch[10] Batch [5]#011Speed: 1534.00 samples/sec#011loss=10.654565\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:51 INFO 140202221626752] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461731.3611803, \"EndTime\": 1631461731.9795327, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 617.8302764892578, \"count\": 1, \"min\": 617.8302764892578, \"max\": 617.8302764892578}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:51 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=1025.9500788602152 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:51 INFO 140202221626752] #progress_metric: host=algo-1, completed 5.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:51 INFO 140202221626752] #quality_metric: host=algo-1, epoch=10, train loss <loss>=10.768620777130128\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:51 INFO 140202221626752] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:52 INFO 140202221626752] Epoch[11] Batch[0] avg_epoch_loss=10.031788\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:52 INFO 140202221626752] #quality_metric: host=algo-1, epoch=11, batch=0 train loss <loss>=10.031787872314453\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:52 INFO 140202221626752] Epoch[11] Batch[5] avg_epoch_loss=10.731290\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:52 INFO 140202221626752] #quality_metric: host=algo-1, epoch=11, batch=5 train loss <loss>=10.731290340423584\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:52 INFO 140202221626752] Epoch[11] Batch [5]#011Speed: 1717.60 samples/sec#011loss=10.731290\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:52 INFO 140202221626752] processed a total of 622 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461731.9796255, \"EndTime\": 1631461732.5685363, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 588.4275436401367, \"count\": 1, \"min\": 588.4275436401367, \"max\": 588.4275436401367}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:52 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=1056.8074597113125 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:52 INFO 140202221626752] #progress_metric: host=algo-1, completed 6.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:52 INFO 140202221626752] #quality_metric: host=algo-1, epoch=11, train loss <loss>=10.662439918518066\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:52 INFO 140202221626752] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:52 INFO 140202221626752] Epoch[12] Batch[0] avg_epoch_loss=10.262134\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:52 INFO 140202221626752] #quality_metric: host=algo-1, epoch=12, batch=0 train loss <loss>=10.262133598327637\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:52 INFO 140202221626752] Epoch[12] Batch[5] avg_epoch_loss=10.429925\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:52 INFO 140202221626752] #quality_metric: host=algo-1, epoch=12, batch=5 train loss <loss>=10.429924805959066\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:52 INFO 140202221626752] Epoch[12] Batch [5]#011Speed: 1827.54 samples/sec#011loss=10.429925\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:53 INFO 140202221626752] processed a total of 611 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461732.5686324, \"EndTime\": 1631461733.1223655, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 553.2112121582031, \"count\": 1, \"min\": 553.2112121582031, \"max\": 553.2112121582031}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:53 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=1104.119918037166 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:53 INFO 140202221626752] #progress_metric: host=algo-1, completed 6.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:53 INFO 140202221626752] #quality_metric: host=algo-1, epoch=12, train loss <loss>=10.30662498474121\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:53 INFO 140202221626752] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:53 INFO 140202221626752] Epoch[13] Batch[0] avg_epoch_loss=10.847816\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:53 INFO 140202221626752] #quality_metric: host=algo-1, epoch=13, batch=0 train loss <loss>=10.84781551361084\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:53 INFO 140202221626752] Epoch[13] Batch[5] avg_epoch_loss=10.802298\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:53 INFO 140202221626752] #quality_metric: host=algo-1, epoch=13, batch=5 train loss <loss>=10.802297910054525\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:53 INFO 140202221626752] Epoch[13] Batch [5]#011Speed: 1936.53 samples/sec#011loss=10.802298\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:53 INFO 140202221626752] processed a total of 619 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461733.1224937, \"EndTime\": 1631461733.6799455, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 556.9374561309814, \"count\": 1, \"min\": 556.9374561309814, \"max\": 556.9374561309814}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:53 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=1111.1580742188905 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:53 INFO 140202221626752] #progress_metric: host=algo-1, completed 7.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:53 INFO 140202221626752] #quality_metric: host=algo-1, epoch=13, train loss <loss>=10.553753662109376\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:53 INFO 140202221626752] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:53 INFO 140202221626752] Epoch[14] Batch[0] avg_epoch_loss=10.606725\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:53 INFO 140202221626752] #quality_metric: host=algo-1, epoch=14, batch=0 train loss <loss>=10.606724739074707\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:54 INFO 140202221626752] Epoch[14] Batch[5] avg_epoch_loss=10.511556\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:54 INFO 140202221626752] #quality_metric: host=algo-1, epoch=14, batch=5 train loss <loss>=10.511555671691895\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:54 INFO 140202221626752] Epoch[14] Batch [5]#011Speed: 1984.56 samples/sec#011loss=10.511556\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:54 INFO 140202221626752] processed a total of 624 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461733.6800435, \"EndTime\": 1631461734.221951, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 541.3501262664795, \"count\": 1, \"min\": 541.3501262664795, \"max\": 541.3501262664795}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:54 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=1152.3802524255173 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:54 INFO 140202221626752] #progress_metric: host=algo-1, completed 7.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:54 INFO 140202221626752] #quality_metric: host=algo-1, epoch=14, train loss <loss>=10.384928703308105\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:54 INFO 140202221626752] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:54 INFO 140202221626752] Epoch[15] Batch[0] avg_epoch_loss=10.392501\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:54 INFO 140202221626752] #quality_metric: host=algo-1, epoch=15, batch=0 train loss <loss>=10.392500877380371\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:54 INFO 140202221626752] Epoch[15] Batch[5] avg_epoch_loss=10.466198\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:54 INFO 140202221626752] #quality_metric: host=algo-1, epoch=15, batch=5 train loss <loss>=10.466198444366455\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:54 INFO 140202221626752] Epoch[15] Batch [5]#011Speed: 2020.00 samples/sec#011loss=10.466198\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:54 INFO 140202221626752] processed a total of 625 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461734.2220488, \"EndTime\": 1631461734.7574334, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 534.7962379455566, \"count\": 1, \"min\": 534.7962379455566, \"max\": 534.7962379455566}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:54 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=1168.3688591262103 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:54 INFO 140202221626752] #progress_metric: host=algo-1, completed 8.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:54 INFO 140202221626752] #quality_metric: host=algo-1, epoch=15, train loss <loss>=10.434659481048584\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:54 INFO 140202221626752] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:55 INFO 140202221626752] Epoch[16] Batch[0] avg_epoch_loss=10.542738\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:55 INFO 140202221626752] #quality_metric: host=algo-1, epoch=16, batch=0 train loss <loss>=10.54273796081543\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:55 INFO 140202221626752] Epoch[16] Batch[5] avg_epoch_loss=10.481727\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:55 INFO 140202221626752] #quality_metric: host=algo-1, epoch=16, batch=5 train loss <loss>=10.481726805369059\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:55 INFO 140202221626752] Epoch[16] Batch [5]#011Speed: 1981.68 samples/sec#011loss=10.481727\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:55 INFO 140202221626752] processed a total of 624 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461734.7575288, \"EndTime\": 1631461735.3614414, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 603.398323059082, \"count\": 1, \"min\": 603.398323059082, \"max\": 603.398323059082}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:55 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=1033.9098888127783 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:55 INFO 140202221626752] #progress_metric: host=algo-1, completed 8.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:55 INFO 140202221626752] #quality_metric: host=algo-1, epoch=16, train loss <loss>=10.509271907806397\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:55 INFO 140202221626752] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:55 INFO 140202221626752] Epoch[17] Batch[0] avg_epoch_loss=10.552974\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:55 INFO 140202221626752] #quality_metric: host=algo-1, epoch=17, batch=0 train loss <loss>=10.552973747253418\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:55 INFO 140202221626752] Epoch[17] Batch[5] avg_epoch_loss=10.688395\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:55 INFO 140202221626752] #quality_metric: host=algo-1, epoch=17, batch=5 train loss <loss>=10.688394705454508\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:55 INFO 140202221626752] Epoch[17] Batch [5]#011Speed: 1744.91 samples/sec#011loss=10.688395\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:56 INFO 140202221626752] Epoch[17] Batch[10] avg_epoch_loss=10.549945\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:56 INFO 140202221626752] #quality_metric: host=algo-1, epoch=17, batch=10 train loss <loss>=10.383806228637695\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:56 INFO 140202221626752] Epoch[17] Batch [10]#011Speed: 1705.51 samples/sec#011loss=10.383806\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:56 INFO 140202221626752] processed a total of 649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461735.3615355, \"EndTime\": 1631461736.031147, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 669.0945625305176, \"count\": 1, \"min\": 669.0945625305176, \"max\": 669.0945625305176}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:56 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=969.7926731773078 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:56 INFO 140202221626752] #progress_metric: host=algo-1, completed 9.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:56 INFO 140202221626752] #quality_metric: host=algo-1, epoch=17, train loss <loss>=10.549945397810502\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:56 INFO 140202221626752] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:56 INFO 140202221626752] Epoch[18] Batch[0] avg_epoch_loss=10.504050\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:56 INFO 140202221626752] #quality_metric: host=algo-1, epoch=18, batch=0 train loss <loss>=10.504050254821777\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:56 INFO 140202221626752] Epoch[18] Batch[5] avg_epoch_loss=10.388709\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:56 INFO 140202221626752] #quality_metric: host=algo-1, epoch=18, batch=5 train loss <loss>=10.38870938618978\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:56 INFO 140202221626752] Epoch[18] Batch [5]#011Speed: 1785.76 samples/sec#011loss=10.388709\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:56 INFO 140202221626752] processed a total of 613 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461736.0312269, \"EndTime\": 1631461736.5883574, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 556.6205978393555, \"count\": 1, \"min\": 556.6205978393555, \"max\": 556.6205978393555}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:56 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=1101.0147459529458 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:56 INFO 140202221626752] #progress_metric: host=algo-1, completed 9.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:56 INFO 140202221626752] #quality_metric: host=algo-1, epoch=18, train loss <loss>=10.398952198028564\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:56 INFO 140202221626752] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:56 INFO 140202221626752] Epoch[19] Batch[0] avg_epoch_loss=10.458633\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:56 INFO 140202221626752] #quality_metric: host=algo-1, epoch=19, batch=0 train loss <loss>=10.458633422851562\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:57 INFO 140202221626752] Epoch[19] Batch[5] avg_epoch_loss=9.938379\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:57 INFO 140202221626752] #quality_metric: host=algo-1, epoch=19, batch=5 train loss <loss>=9.938379287719727\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:57 INFO 140202221626752] Epoch[19] Batch [5]#011Speed: 1929.05 samples/sec#011loss=9.938379\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:57 INFO 140202221626752] processed a total of 638 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461736.5884547, \"EndTime\": 1631461737.1423216, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 553.3502101898193, \"count\": 1, \"min\": 553.3502101898193, \"max\": 553.3502101898193}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:57 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=1152.6893204301705 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:57 INFO 140202221626752] #progress_metric: host=algo-1, completed 10.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:57 INFO 140202221626752] #quality_metric: host=algo-1, epoch=19, train loss <loss>=10.180258464813232\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:57 INFO 140202221626752] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:57 INFO 140202221626752] Saved checkpoint to \"/opt/ml/model/state_2eac6dae-98e9-428f-90c0-d6486b8b9cab-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461737.1424181, \"EndTime\": 1631461737.1630735, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 20.04384994506836, \"count\": 1, \"min\": 20.04384994506836, \"max\": 20.04384994506836}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:57 INFO 140202221626752] Epoch[20] Batch[0] avg_epoch_loss=10.277932\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:57 INFO 140202221626752] #quality_metric: host=algo-1, epoch=20, batch=0 train loss <loss>=10.277932167053223\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:57 INFO 140202221626752] Epoch[20] Batch[5] avg_epoch_loss=10.179046\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:57 INFO 140202221626752] #quality_metric: host=algo-1, epoch=20, batch=5 train loss <loss>=10.17904551823934\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:57 INFO 140202221626752] Epoch[20] Batch [5]#011Speed: 1814.18 samples/sec#011loss=10.179046\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:57 INFO 140202221626752] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461737.1631665, \"EndTime\": 1631461737.7243373, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 561.0957145690918, \"count\": 1, \"min\": 561.0957145690918, \"max\": 561.0957145690918}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:57 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=1135.0288488498354 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:57 INFO 140202221626752] #progress_metric: host=algo-1, completed 10.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:57 INFO 140202221626752] #quality_metric: host=algo-1, epoch=20, train loss <loss>=10.227530193328857\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:57 INFO 140202221626752] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:57 INFO 140202221626752] Epoch[21] Batch[0] avg_epoch_loss=10.169245\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:57 INFO 140202221626752] #quality_metric: host=algo-1, epoch=21, batch=0 train loss <loss>=10.169244766235352\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:58 INFO 140202221626752] Epoch[21] Batch[5] avg_epoch_loss=10.072457\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:58 INFO 140202221626752] #quality_metric: host=algo-1, epoch=21, batch=5 train loss <loss>=10.072457472483316\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:58 INFO 140202221626752] Epoch[21] Batch [5]#011Speed: 1931.16 samples/sec#011loss=10.072457\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:58 INFO 140202221626752] Epoch[21] Batch[10] avg_epoch_loss=10.494882\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:58 INFO 140202221626752] #quality_metric: host=algo-1, epoch=21, batch=10 train loss <loss>=11.001791191101074\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:58 INFO 140202221626752] Epoch[21] Batch [10]#011Speed: 1753.45 samples/sec#011loss=11.001791\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:58 INFO 140202221626752] processed a total of 669 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461737.724419, \"EndTime\": 1631461738.3241947, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 599.2186069488525, \"count\": 1, \"min\": 599.2186069488525, \"max\": 599.2186069488525}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:58 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=1116.207940063464 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:58 INFO 140202221626752] #progress_metric: host=algo-1, completed 11.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:58 INFO 140202221626752] #quality_metric: host=algo-1, epoch=21, train loss <loss>=10.494881890036844\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:58 INFO 140202221626752] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:58 INFO 140202221626752] Epoch[22] Batch[0] avg_epoch_loss=9.911341\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:58 INFO 140202221626752] #quality_metric: host=algo-1, epoch=22, batch=0 train loss <loss>=9.911340713500977\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:58 INFO 140202221626752] Epoch[22] Batch[5] avg_epoch_loss=10.633690\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:58 INFO 140202221626752] #quality_metric: host=algo-1, epoch=22, batch=5 train loss <loss>=10.633690039316813\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:58 INFO 140202221626752] Epoch[22] Batch [5]#011Speed: 1973.25 samples/sec#011loss=10.633690\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:58 INFO 140202221626752] processed a total of 629 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461738.3242853, \"EndTime\": 1631461738.9149594, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 590.179443359375, \"count\": 1, \"min\": 590.179443359375, \"max\": 590.179443359375}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:58 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=1065.5722156543823 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:58 INFO 140202221626752] #progress_metric: host=algo-1, completed 11.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:58 INFO 140202221626752] #quality_metric: host=algo-1, epoch=22, train loss <loss>=10.565046119689942\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:58 INFO 140202221626752] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:59 INFO 140202221626752] Epoch[23] Batch[0] avg_epoch_loss=9.907011\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:59 INFO 140202221626752] #quality_metric: host=algo-1, epoch=23, batch=0 train loss <loss>=9.907011032104492\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:59 INFO 140202221626752] Epoch[23] Batch[5] avg_epoch_loss=10.261030\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:59 INFO 140202221626752] #quality_metric: host=algo-1, epoch=23, batch=5 train loss <loss>=10.261029720306396\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:59 INFO 140202221626752] Epoch[23] Batch [5]#011Speed: 1714.43 samples/sec#011loss=10.261030\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:59 INFO 140202221626752] processed a total of 639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461738.9150372, \"EndTime\": 1631461739.522103, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 606.5516471862793, \"count\": 1, \"min\": 606.5516471862793, \"max\": 606.5516471862793}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:59 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=1053.2269281447457 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:59 INFO 140202221626752] #progress_metric: host=algo-1, completed 12.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:59 INFO 140202221626752] #quality_metric: host=algo-1, epoch=23, train loss <loss>=10.288703727722169\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:59 INFO 140202221626752] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:59 INFO 140202221626752] Epoch[24] Batch[0] avg_epoch_loss=10.911802\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:59 INFO 140202221626752] #quality_metric: host=algo-1, epoch=24, batch=0 train loss <loss>=10.911802291870117\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:59 INFO 140202221626752] Epoch[24] Batch[5] avg_epoch_loss=10.196558\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:59 INFO 140202221626752] #quality_metric: host=algo-1, epoch=24, batch=5 train loss <loss>=10.196558316548666\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:48:59 INFO 140202221626752] Epoch[24] Batch [5]#011Speed: 1915.80 samples/sec#011loss=10.196558\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:00 INFO 140202221626752] processed a total of 610 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461739.5222151, \"EndTime\": 1631461740.121409, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 598.6552238464355, \"count\": 1, \"min\": 598.6552238464355, \"max\": 598.6552238464355}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:00 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=1018.7155273636971 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:00 INFO 140202221626752] #progress_metric: host=algo-1, completed 12.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:00 INFO 140202221626752] #quality_metric: host=algo-1, epoch=24, train loss <loss>=10.020250225067139\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:00 INFO 140202221626752] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:00 INFO 140202221626752] Saved checkpoint to \"/opt/ml/model/state_7f0a065d-8e52-424e-9c8c-6196b68cd28b-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461740.121506, \"EndTime\": 1631461740.1418138, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 19.754886627197266, \"count\": 1, \"min\": 19.754886627197266, \"max\": 19.754886627197266}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:00 INFO 140202221626752] Epoch[25] Batch[0] avg_epoch_loss=9.462094\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:00 INFO 140202221626752] #quality_metric: host=algo-1, epoch=25, batch=0 train loss <loss>=9.4620943069458\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:00 INFO 140202221626752] Epoch[25] Batch[5] avg_epoch_loss=9.689702\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:00 INFO 140202221626752] #quality_metric: host=algo-1, epoch=25, batch=5 train loss <loss>=9.6897021929423\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:00 INFO 140202221626752] Epoch[25] Batch [5]#011Speed: 1759.48 samples/sec#011loss=9.689702\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:00 INFO 140202221626752] Epoch[25] Batch[10] avg_epoch_loss=10.187928\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:00 INFO 140202221626752] #quality_metric: host=algo-1, epoch=25, batch=10 train loss <loss>=10.785798072814941\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:00 INFO 140202221626752] Epoch[25] Batch [10]#011Speed: 1939.20 samples/sec#011loss=10.785798\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:00 INFO 140202221626752] processed a total of 641 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461740.1419, \"EndTime\": 1631461740.7506447, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 608.6685657501221, \"count\": 1, \"min\": 608.6685657501221, \"max\": 608.6685657501221}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:00 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=1052.8947574743986 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:00 INFO 140202221626752] #progress_metric: host=algo-1, completed 13.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:00 INFO 140202221626752] #quality_metric: host=algo-1, epoch=25, train loss <loss>=10.18792759288441\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:00 INFO 140202221626752] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:01 INFO 140202221626752] Epoch[26] Batch[0] avg_epoch_loss=9.603303\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:01 INFO 140202221626752] #quality_metric: host=algo-1, epoch=26, batch=0 train loss <loss>=9.603302955627441\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:01 INFO 140202221626752] Epoch[26] Batch[5] avg_epoch_loss=10.342302\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:01 INFO 140202221626752] #quality_metric: host=algo-1, epoch=26, batch=5 train loss <loss>=10.342302322387695\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:01 INFO 140202221626752] Epoch[26] Batch [5]#011Speed: 1526.31 samples/sec#011loss=10.342302\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:01 INFO 140202221626752] processed a total of 624 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461740.7507336, \"EndTime\": 1631461741.3929882, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 641.7603492736816, \"count\": 1, \"min\": 641.7603492736816, \"max\": 641.7603492736816}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:01 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=972.0893896579696 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:01 INFO 140202221626752] #progress_metric: host=algo-1, completed 13.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:01 INFO 140202221626752] #quality_metric: host=algo-1, epoch=26, train loss <loss>=10.41370964050293\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:01 INFO 140202221626752] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:01 INFO 140202221626752] Epoch[27] Batch[0] avg_epoch_loss=10.751230\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:01 INFO 140202221626752] #quality_metric: host=algo-1, epoch=27, batch=0 train loss <loss>=10.751230239868164\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:01 INFO 140202221626752] Epoch[27] Batch[5] avg_epoch_loss=10.339742\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:01 INFO 140202221626752] #quality_metric: host=algo-1, epoch=27, batch=5 train loss <loss>=10.339741547902426\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:01 INFO 140202221626752] Epoch[27] Batch [5]#011Speed: 1879.45 samples/sec#011loss=10.339742\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:01 INFO 140202221626752] processed a total of 609 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461741.3931036, \"EndTime\": 1631461741.99039, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 596.7884063720703, \"count\": 1, \"min\": 596.7884063720703, \"max\": 596.7884063720703}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:01 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=1020.1707769746073 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:01 INFO 140202221626752] #progress_metric: host=algo-1, completed 14.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:01 INFO 140202221626752] #quality_metric: host=algo-1, epoch=27, train loss <loss>=10.026342582702636\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:01 INFO 140202221626752] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:02 INFO 140202221626752] Epoch[28] Batch[0] avg_epoch_loss=10.747637\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:02 INFO 140202221626752] #quality_metric: host=algo-1, epoch=28, batch=0 train loss <loss>=10.747636795043945\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:02 INFO 140202221626752] Epoch[28] Batch[5] avg_epoch_loss=10.268417\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:02 INFO 140202221626752] #quality_metric: host=algo-1, epoch=28, batch=5 train loss <loss>=10.268417358398438\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:02 INFO 140202221626752] Epoch[28] Batch [5]#011Speed: 2003.85 samples/sec#011loss=10.268417\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:02 INFO 140202221626752] processed a total of 610 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461741.990518, \"EndTime\": 1631461742.5831695, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 592.1399593353271, \"count\": 1, \"min\": 592.1399593353271, \"max\": 592.1399593353271}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:02 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=1029.9258992087543 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:02 INFO 140202221626752] #progress_metric: host=algo-1, completed 14.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:02 INFO 140202221626752] #quality_metric: host=algo-1, epoch=28, train loss <loss>=10.547098922729493\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:02 INFO 140202221626752] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:02 INFO 140202221626752] Epoch[29] Batch[0] avg_epoch_loss=9.286609\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:02 INFO 140202221626752] #quality_metric: host=algo-1, epoch=29, batch=0 train loss <loss>=9.286608695983887\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:03 INFO 140202221626752] Epoch[29] Batch[5] avg_epoch_loss=10.008594\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:03 INFO 140202221626752] #quality_metric: host=algo-1, epoch=29, batch=5 train loss <loss>=10.008593877156576\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:03 INFO 140202221626752] Epoch[29] Batch [5]#011Speed: 1521.66 samples/sec#011loss=10.008594\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:03 INFO 140202221626752] Epoch[29] Batch[10] avg_epoch_loss=10.885109\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:03 INFO 140202221626752] #quality_metric: host=algo-1, epoch=29, batch=10 train loss <loss>=11.936926651000977\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:03 INFO 140202221626752] Epoch[29] Batch [10]#011Speed: 1511.06 samples/sec#011loss=11.936927\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:03 INFO 140202221626752] processed a total of 643 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461742.5832644, \"EndTime\": 1631461743.2704952, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 686.7086887359619, \"count\": 1, \"min\": 686.7086887359619, \"max\": 686.7086887359619}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:03 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=936.1729985063234 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:03 INFO 140202221626752] #progress_metric: host=algo-1, completed 15.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:03 INFO 140202221626752] #quality_metric: host=algo-1, epoch=29, train loss <loss>=10.885108774358576\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:03 INFO 140202221626752] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:03 INFO 140202221626752] Epoch[30] Batch[0] avg_epoch_loss=10.344794\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:03 INFO 140202221626752] #quality_metric: host=algo-1, epoch=30, batch=0 train loss <loss>=10.344794273376465\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:03 INFO 140202221626752] Epoch[30] Batch[5] avg_epoch_loss=10.144200\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:03 INFO 140202221626752] #quality_metric: host=algo-1, epoch=30, batch=5 train loss <loss>=10.144200483957926\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:03 INFO 140202221626752] Epoch[30] Batch [5]#011Speed: 1883.49 samples/sec#011loss=10.144200\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:03 INFO 140202221626752] Epoch[30] Batch[10] avg_epoch_loss=10.070168\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:03 INFO 140202221626752] #quality_metric: host=algo-1, epoch=30, batch=10 train loss <loss>=9.981328582763672\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:03 INFO 140202221626752] Epoch[30] Batch [10]#011Speed: 1860.62 samples/sec#011loss=9.981329\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:03 INFO 140202221626752] processed a total of 657 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461743.270586, \"EndTime\": 1631461743.9269466, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 655.8718681335449, \"count\": 1, \"min\": 655.8718681335449, \"max\": 655.8718681335449}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:03 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=1001.5281845628473 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:03 INFO 140202221626752] #progress_metric: host=algo-1, completed 15.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:03 INFO 140202221626752] #quality_metric: host=algo-1, epoch=30, train loss <loss>=10.070167801596902\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:03 INFO 140202221626752] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:04 INFO 140202221626752] Epoch[31] Batch[0] avg_epoch_loss=10.016655\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:04 INFO 140202221626752] #quality_metric: host=algo-1, epoch=31, batch=0 train loss <loss>=10.016654968261719\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:04 INFO 140202221626752] Epoch[31] Batch[5] avg_epoch_loss=10.247066\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:04 INFO 140202221626752] #quality_metric: host=algo-1, epoch=31, batch=5 train loss <loss>=10.247065544128418\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:04 INFO 140202221626752] Epoch[31] Batch [5]#011Speed: 1493.23 samples/sec#011loss=10.247066\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:04 INFO 140202221626752] Epoch[31] Batch[10] avg_epoch_loss=10.202548\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:04 INFO 140202221626752] #quality_metric: host=algo-1, epoch=31, batch=10 train loss <loss>=10.149127769470216\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:04 INFO 140202221626752] Epoch[31] Batch [10]#011Speed: 1476.58 samples/sec#011loss=10.149128\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:04 INFO 140202221626752] processed a total of 696 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461743.9270334, \"EndTime\": 1631461744.6407464, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 713.2236957550049, \"count\": 1, \"min\": 713.2236957550049, \"max\": 713.2236957550049}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:04 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=975.6858859157957 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:04 INFO 140202221626752] #progress_metric: host=algo-1, completed 16.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:04 INFO 140202221626752] #quality_metric: host=algo-1, epoch=31, train loss <loss>=10.202548373829234\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:04 INFO 140202221626752] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:04 INFO 140202221626752] Epoch[32] Batch[0] avg_epoch_loss=9.089632\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:04 INFO 140202221626752] #quality_metric: host=algo-1, epoch=32, batch=0 train loss <loss>=9.089632034301758\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:05 INFO 140202221626752] Epoch[32] Batch[5] avg_epoch_loss=9.771497\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:05 INFO 140202221626752] #quality_metric: host=algo-1, epoch=32, batch=5 train loss <loss>=9.77149740854899\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:05 INFO 140202221626752] Epoch[32] Batch [5]#011Speed: 1533.90 samples/sec#011loss=9.771497\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:05 INFO 140202221626752] Epoch[32] Batch[10] avg_epoch_loss=9.300367\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:05 INFO 140202221626752] #quality_metric: host=algo-1, epoch=32, batch=10 train loss <loss>=8.735010766983033\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:05 INFO 140202221626752] Epoch[32] Batch [10]#011Speed: 1437.03 samples/sec#011loss=8.735011\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:05 INFO 140202221626752] processed a total of 648 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461744.6408267, \"EndTime\": 1631461745.342371, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 701.0147571563721, \"count\": 1, \"min\": 701.0147571563721, \"max\": 701.0147571563721}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:05 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=924.1646169790843 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:05 INFO 140202221626752] #progress_metric: host=algo-1, completed 16.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:05 INFO 140202221626752] #quality_metric: host=algo-1, epoch=32, train loss <loss>=9.3003671169281\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:05 INFO 140202221626752] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:05 INFO 140202221626752] Saved checkpoint to \"/opt/ml/model/state_f4c0ea6e-552f-4b75-a3cd-d03eed167a71-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461745.3424885, \"EndTime\": 1631461745.355788, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 12.774467468261719, \"count\": 1, \"min\": 12.774467468261719, \"max\": 12.774467468261719}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:05 INFO 140202221626752] Epoch[33] Batch[0] avg_epoch_loss=9.979781\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:05 INFO 140202221626752] #quality_metric: host=algo-1, epoch=33, batch=0 train loss <loss>=9.979781150817871\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:05 INFO 140202221626752] Epoch[33] Batch[5] avg_epoch_loss=9.863408\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:05 INFO 140202221626752] #quality_metric: host=algo-1, epoch=33, batch=5 train loss <loss>=9.863407611846924\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:05 INFO 140202221626752] Epoch[33] Batch [5]#011Speed: 1610.49 samples/sec#011loss=9.863408\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:06 INFO 140202221626752] Epoch[33] Batch[10] avg_epoch_loss=9.775707\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:06 INFO 140202221626752] #quality_metric: host=algo-1, epoch=33, batch=10 train loss <loss>=9.670466804504395\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:06 INFO 140202221626752] Epoch[33] Batch [10]#011Speed: 1631.61 samples/sec#011loss=9.670467\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:06 INFO 140202221626752] processed a total of 684 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461745.3558776, \"EndTime\": 1631461746.034071, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 678.1151294708252, \"count\": 1, \"min\": 678.1151294708252, \"max\": 678.1151294708252}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:06 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=1008.5027595021788 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:06 INFO 140202221626752] #progress_metric: host=algo-1, completed 17.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:06 INFO 140202221626752] #quality_metric: host=algo-1, epoch=33, train loss <loss>=9.775707244873047\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:06 INFO 140202221626752] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:06 INFO 140202221626752] Epoch[34] Batch[0] avg_epoch_loss=9.505257\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:06 INFO 140202221626752] #quality_metric: host=algo-1, epoch=34, batch=0 train loss <loss>=9.505256652832031\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:06 INFO 140202221626752] Epoch[34] Batch[5] avg_epoch_loss=9.895768\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:06 INFO 140202221626752] #quality_metric: host=algo-1, epoch=34, batch=5 train loss <loss>=9.895767529805502\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:06 INFO 140202221626752] Epoch[34] Batch [5]#011Speed: 1690.01 samples/sec#011loss=9.895768\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:06 INFO 140202221626752] processed a total of 625 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461746.0341475, \"EndTime\": 1631461746.6245635, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 589.8537635803223, \"count\": 1, \"min\": 589.8537635803223, \"max\": 589.8537635803223}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:06 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=1059.3525819456163 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:06 INFO 140202221626752] #progress_metric: host=algo-1, completed 17.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:06 INFO 140202221626752] #quality_metric: host=algo-1, epoch=34, train loss <loss>=9.835481929779053\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:06 INFO 140202221626752] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:06 INFO 140202221626752] Epoch[35] Batch[0] avg_epoch_loss=11.161825\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:06 INFO 140202221626752] #quality_metric: host=algo-1, epoch=35, batch=0 train loss <loss>=11.161825180053711\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:07 INFO 140202221626752] Epoch[35] Batch[5] avg_epoch_loss=10.316259\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:07 INFO 140202221626752] #quality_metric: host=algo-1, epoch=35, batch=5 train loss <loss>=10.316258589426676\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:07 INFO 140202221626752] Epoch[35] Batch [5]#011Speed: 1969.49 samples/sec#011loss=10.316259\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:07 INFO 140202221626752] processed a total of 595 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461746.6246533, \"EndTime\": 1631461747.1865275, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 561.384916305542, \"count\": 1, \"min\": 561.384916305542, \"max\": 561.384916305542}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:07 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=1059.6124423671595 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:07 INFO 140202221626752] #progress_metric: host=algo-1, completed 18.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:07 INFO 140202221626752] #quality_metric: host=algo-1, epoch=35, train loss <loss>=10.084021472930909\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:07 INFO 140202221626752] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:07 INFO 140202221626752] Epoch[36] Batch[0] avg_epoch_loss=10.819252\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:07 INFO 140202221626752] #quality_metric: host=algo-1, epoch=36, batch=0 train loss <loss>=10.819252014160156\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:07 INFO 140202221626752] Epoch[36] Batch[5] avg_epoch_loss=9.972454\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:07 INFO 140202221626752] #quality_metric: host=algo-1, epoch=36, batch=5 train loss <loss>=9.97245422999064\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:07 INFO 140202221626752] Epoch[36] Batch [5]#011Speed: 1893.50 samples/sec#011loss=9.972454\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:07 INFO 140202221626752] Epoch[36] Batch[10] avg_epoch_loss=9.510804\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:07 INFO 140202221626752] #quality_metric: host=algo-1, epoch=36, batch=10 train loss <loss>=8.956823587417603\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:07 INFO 140202221626752] Epoch[36] Batch [10]#011Speed: 1641.85 samples/sec#011loss=8.956824\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:07 INFO 140202221626752] processed a total of 651 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461747.1866257, \"EndTime\": 1631461747.797253, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 610.0924015045166, \"count\": 1, \"min\": 610.0924015045166, \"max\": 610.0924015045166}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:07 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=1066.7892552651997 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:07 INFO 140202221626752] #progress_metric: host=algo-1, completed 18.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:07 INFO 140202221626752] #quality_metric: host=algo-1, epoch=36, train loss <loss>=9.510803937911987\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:07 INFO 140202221626752] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:08 INFO 140202221626752] Epoch[37] Batch[0] avg_epoch_loss=10.066299\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:08 INFO 140202221626752] #quality_metric: host=algo-1, epoch=37, batch=0 train loss <loss>=10.066299438476562\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:08 INFO 140202221626752] Epoch[37] Batch[5] avg_epoch_loss=10.144200\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:08 INFO 140202221626752] #quality_metric: host=algo-1, epoch=37, batch=5 train loss <loss>=10.14419968922933\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:08 INFO 140202221626752] Epoch[37] Batch [5]#011Speed: 1711.48 samples/sec#011loss=10.144200\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:08 INFO 140202221626752] Epoch[37] Batch[10] avg_epoch_loss=10.391316\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:08 INFO 140202221626752] #quality_metric: host=algo-1, epoch=37, batch=10 train loss <loss>=10.687854957580566\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:08 INFO 140202221626752] Epoch[37] Batch [10]#011Speed: 1953.14 samples/sec#011loss=10.687855\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:08 INFO 140202221626752] processed a total of 644 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461747.7973619, \"EndTime\": 1631461748.4399219, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 642.0576572418213, \"count\": 1, \"min\": 642.0576572418213, \"max\": 642.0576572418213}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:08 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=1002.8259585969289 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:08 INFO 140202221626752] #progress_metric: host=algo-1, completed 19.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:08 INFO 140202221626752] #quality_metric: host=algo-1, epoch=37, train loss <loss>=10.391315720298074\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:08 INFO 140202221626752] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:08 INFO 140202221626752] Epoch[38] Batch[0] avg_epoch_loss=9.578884\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:08 INFO 140202221626752] #quality_metric: host=algo-1, epoch=38, batch=0 train loss <loss>=9.57888412475586\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:08 INFO 140202221626752] Epoch[38] Batch[5] avg_epoch_loss=10.095666\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:08 INFO 140202221626752] #quality_metric: host=algo-1, epoch=38, batch=5 train loss <loss>=10.09566593170166\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:08 INFO 140202221626752] Epoch[38] Batch [5]#011Speed: 1992.89 samples/sec#011loss=10.095666\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:09 INFO 140202221626752] Epoch[38] Batch[10] avg_epoch_loss=10.099410\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:09 INFO 140202221626752] #quality_metric: host=algo-1, epoch=38, batch=10 train loss <loss>=10.103902244567871\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:09 INFO 140202221626752] Epoch[38] Batch [10]#011Speed: 1796.76 samples/sec#011loss=10.103902\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:09 INFO 140202221626752] processed a total of 652 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461748.440009, \"EndTime\": 1631461749.0316603, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 591.1731719970703, \"count\": 1, \"min\": 591.1731719970703, \"max\": 591.1731719970703}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:09 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=1102.6902713816419 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:09 INFO 140202221626752] #progress_metric: host=algo-1, completed 19.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:09 INFO 140202221626752] #quality_metric: host=algo-1, epoch=38, train loss <loss>=10.099409710277211\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:09 INFO 140202221626752] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:09 INFO 140202221626752] Epoch[39] Batch[0] avg_epoch_loss=9.662577\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:09 INFO 140202221626752] #quality_metric: host=algo-1, epoch=39, batch=0 train loss <loss>=9.662576675415039\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:09 INFO 140202221626752] Epoch[39] Batch[5] avg_epoch_loss=10.328505\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:09 INFO 140202221626752] #quality_metric: host=algo-1, epoch=39, batch=5 train loss <loss>=10.32850456237793\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:09 INFO 140202221626752] Epoch[39] Batch [5]#011Speed: 2008.32 samples/sec#011loss=10.328505\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:09 INFO 140202221626752] Epoch[39] Batch[10] avg_epoch_loss=10.509967\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:09 INFO 140202221626752] #quality_metric: host=algo-1, epoch=39, batch=10 train loss <loss>=10.72772216796875\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:09 INFO 140202221626752] Epoch[39] Batch [10]#011Speed: 1874.84 samples/sec#011loss=10.727722\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:09 INFO 140202221626752] processed a total of 642 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461749.031735, \"EndTime\": 1631461749.6124003, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 580.1246166229248, \"count\": 1, \"min\": 580.1246166229248, \"max\": 580.1246166229248}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:09 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=1106.3991420788154 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:09 INFO 140202221626752] #progress_metric: host=algo-1, completed 20.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:09 INFO 140202221626752] #quality_metric: host=algo-1, epoch=39, train loss <loss>=10.509967110373758\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:09 INFO 140202221626752] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:09 INFO 140202221626752] Epoch[40] Batch[0] avg_epoch_loss=10.651989\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:09 INFO 140202221626752] #quality_metric: host=algo-1, epoch=40, batch=0 train loss <loss>=10.651988983154297\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:10 INFO 140202221626752] Epoch[40] Batch[5] avg_epoch_loss=10.102285\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:10 INFO 140202221626752] #quality_metric: host=algo-1, epoch=40, batch=5 train loss <loss>=10.102285385131836\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:10 INFO 140202221626752] Epoch[40] Batch [5]#011Speed: 2021.23 samples/sec#011loss=10.102285\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:10 INFO 140202221626752] Epoch[40] Batch[10] avg_epoch_loss=10.139586\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:10 INFO 140202221626752] #quality_metric: host=algo-1, epoch=40, batch=10 train loss <loss>=10.184347152709961\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:10 INFO 140202221626752] Epoch[40] Batch [10]#011Speed: 1758.86 samples/sec#011loss=10.184347\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:10 INFO 140202221626752] processed a total of 660 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461749.612486, \"EndTime\": 1631461750.2092063, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 596.2450504302979, \"count\": 1, \"min\": 596.2450504302979, \"max\": 596.2450504302979}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:10 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=1106.6698808474403 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:10 INFO 140202221626752] #progress_metric: host=algo-1, completed 20.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:10 INFO 140202221626752] #quality_metric: host=algo-1, epoch=40, train loss <loss>=10.139586188576438\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:10 INFO 140202221626752] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:10 INFO 140202221626752] Epoch[41] Batch[0] avg_epoch_loss=10.028830\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:10 INFO 140202221626752] #quality_metric: host=algo-1, epoch=41, batch=0 train loss <loss>=10.028829574584961\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:10 INFO 140202221626752] Epoch[41] Batch[5] avg_epoch_loss=10.213912\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:10 INFO 140202221626752] #quality_metric: host=algo-1, epoch=41, batch=5 train loss <loss>=10.213912010192871\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:10 INFO 140202221626752] Epoch[41] Batch [5]#011Speed: 1975.73 samples/sec#011loss=10.213912\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:10 INFO 140202221626752] processed a total of 640 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461750.209302, \"EndTime\": 1631461750.7440848, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 534.2190265655518, \"count\": 1, \"min\": 534.2190265655518, \"max\": 534.2190265655518}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:10 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=1197.7255857327068 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:10 INFO 140202221626752] #progress_metric: host=algo-1, completed 21.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:10 INFO 140202221626752] #quality_metric: host=algo-1, epoch=41, train loss <loss>=10.1243182182312\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:10 INFO 140202221626752] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:10 INFO 140202221626752] Epoch[42] Batch[0] avg_epoch_loss=9.458947\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:10 INFO 140202221626752] #quality_metric: host=algo-1, epoch=42, batch=0 train loss <loss>=9.45894718170166\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:11 INFO 140202221626752] Epoch[42] Batch[5] avg_epoch_loss=10.177960\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:11 INFO 140202221626752] #quality_metric: host=algo-1, epoch=42, batch=5 train loss <loss>=10.17795960108439\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:11 INFO 140202221626752] Epoch[42] Batch [5]#011Speed: 1840.16 samples/sec#011loss=10.177960\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:11 INFO 140202221626752] Epoch[42] Batch[10] avg_epoch_loss=10.092755\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:11 INFO 140202221626752] #quality_metric: host=algo-1, epoch=42, batch=10 train loss <loss>=9.990508842468262\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:11 INFO 140202221626752] Epoch[42] Batch [10]#011Speed: 1635.26 samples/sec#011loss=9.990509\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:11 INFO 140202221626752] processed a total of 702 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461750.7441683, \"EndTime\": 1631461751.3480904, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 603.3904552459717, \"count\": 1, \"min\": 603.3904552459717, \"max\": 603.3904552459717}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:11 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=1163.18033039457 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:11 INFO 140202221626752] #progress_metric: host=algo-1, completed 21.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:11 INFO 140202221626752] #quality_metric: host=algo-1, epoch=42, train loss <loss>=10.092754710804332\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:11 INFO 140202221626752] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:11 INFO 140202221626752] Epoch[43] Batch[0] avg_epoch_loss=10.034153\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:11 INFO 140202221626752] #quality_metric: host=algo-1, epoch=43, batch=0 train loss <loss>=10.03415298461914\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:11 INFO 140202221626752] Epoch[43] Batch[5] avg_epoch_loss=9.984829\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:11 INFO 140202221626752] #quality_metric: host=algo-1, epoch=43, batch=5 train loss <loss>=9.98482894897461\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:11 INFO 140202221626752] Epoch[43] Batch [5]#011Speed: 1897.24 samples/sec#011loss=9.984829\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:11 INFO 140202221626752] Epoch[43] Batch[10] avg_epoch_loss=9.810737\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:11 INFO 140202221626752] #quality_metric: host=algo-1, epoch=43, batch=10 train loss <loss>=9.601827335357665\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:11 INFO 140202221626752] Epoch[43] Batch [10]#011Speed: 1905.90 samples/sec#011loss=9.601827\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:11 INFO 140202221626752] processed a total of 656 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461751.3481767, \"EndTime\": 1631461751.9396226, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 590.958833694458, \"count\": 1, \"min\": 590.958833694458, \"max\": 590.958833694458}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:11 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=1109.8293603078764 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:11 INFO 140202221626752] #progress_metric: host=algo-1, completed 22.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:11 INFO 140202221626752] #quality_metric: host=algo-1, epoch=43, train loss <loss>=9.810737306421453\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:11 INFO 140202221626752] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:12 INFO 140202221626752] Epoch[44] Batch[0] avg_epoch_loss=10.201842\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:12 INFO 140202221626752] #quality_metric: host=algo-1, epoch=44, batch=0 train loss <loss>=10.201842308044434\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:12 INFO 140202221626752] Epoch[44] Batch[5] avg_epoch_loss=10.077592\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:12 INFO 140202221626752] #quality_metric: host=algo-1, epoch=44, batch=5 train loss <loss>=10.077592055002848\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:12 INFO 140202221626752] Epoch[44] Batch [5]#011Speed: 2003.11 samples/sec#011loss=10.077592\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:12 INFO 140202221626752] Epoch[44] Batch[10] avg_epoch_loss=9.846396\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:12 INFO 140202221626752] #quality_metric: host=algo-1, epoch=44, batch=10 train loss <loss>=9.568960762023925\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:12 INFO 140202221626752] Epoch[44] Batch [10]#011Speed: 1871.34 samples/sec#011loss=9.568961\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:12 INFO 140202221626752] processed a total of 660 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461751.9397044, \"EndTime\": 1631461752.5134482, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 573.2090473175049, \"count\": 1, \"min\": 573.2090473175049, \"max\": 573.2090473175049}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:12 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=1151.157186663373 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:12 INFO 140202221626752] #progress_metric: host=algo-1, completed 22.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:12 INFO 140202221626752] #quality_metric: host=algo-1, epoch=44, train loss <loss>=9.846396012739701\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:12 INFO 140202221626752] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:12 INFO 140202221626752] Epoch[45] Batch[0] avg_epoch_loss=9.789759\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:12 INFO 140202221626752] #quality_metric: host=algo-1, epoch=45, batch=0 train loss <loss>=9.789758682250977\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:12 INFO 140202221626752] Epoch[45] Batch[5] avg_epoch_loss=9.563175\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:12 INFO 140202221626752] #quality_metric: host=algo-1, epoch=45, batch=5 train loss <loss>=9.563174724578857\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:12 INFO 140202221626752] Epoch[45] Batch [5]#011Speed: 1795.58 samples/sec#011loss=9.563175\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:13 INFO 140202221626752] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461752.5135353, \"EndTime\": 1631461753.0894616, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 575.4311084747314, \"count\": 1, \"min\": 575.4311084747314, \"max\": 575.4311084747314}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:13 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=1106.7310691181972 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:13 INFO 140202221626752] #progress_metric: host=algo-1, completed 23.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:13 INFO 140202221626752] #quality_metric: host=algo-1, epoch=45, train loss <loss>=9.648063468933106\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:13 INFO 140202221626752] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:13 INFO 140202221626752] Epoch[46] Batch[0] avg_epoch_loss=10.427053\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:13 INFO 140202221626752] #quality_metric: host=algo-1, epoch=46, batch=0 train loss <loss>=10.427053451538086\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:13 INFO 140202221626752] Epoch[46] Batch[5] avg_epoch_loss=9.887195\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:13 INFO 140202221626752] #quality_metric: host=algo-1, epoch=46, batch=5 train loss <loss>=9.887195428212484\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:13 INFO 140202221626752] Epoch[46] Batch [5]#011Speed: 1907.28 samples/sec#011loss=9.887195\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:13 INFO 140202221626752] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461753.0895576, \"EndTime\": 1631461753.6611946, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 571.1104869842529, \"count\": 1, \"min\": 571.1104869842529, \"max\": 571.1104869842529}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:13 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=1109.8538494253503 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:13 INFO 140202221626752] #progress_metric: host=algo-1, completed 23.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:13 INFO 140202221626752] #quality_metric: host=algo-1, epoch=46, train loss <loss>=10.069741439819335\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:13 INFO 140202221626752] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:13 INFO 140202221626752] Epoch[47] Batch[0] avg_epoch_loss=10.079068\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:13 INFO 140202221626752] #quality_metric: host=algo-1, epoch=47, batch=0 train loss <loss>=10.079068183898926\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:14 INFO 140202221626752] Epoch[47] Batch[5] avg_epoch_loss=10.077706\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:14 INFO 140202221626752] #quality_metric: host=algo-1, epoch=47, batch=5 train loss <loss>=10.07770586013794\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:14 INFO 140202221626752] Epoch[47] Batch [5]#011Speed: 1849.68 samples/sec#011loss=10.077706\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:14 INFO 140202221626752] Epoch[47] Batch[10] avg_epoch_loss=9.667135\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:14 INFO 140202221626752] #quality_metric: host=algo-1, epoch=47, batch=10 train loss <loss>=9.174449062347412\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:14 INFO 140202221626752] Epoch[47] Batch [10]#011Speed: 1758.88 samples/sec#011loss=9.174449\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:14 INFO 140202221626752] processed a total of 679 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461753.6612892, \"EndTime\": 1631461754.318241, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 656.4226150512695, \"count\": 1, \"min\": 656.4226150512695, \"max\": 656.4226150512695}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:14 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=1034.1955039026554 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:14 INFO 140202221626752] #progress_metric: host=algo-1, completed 24.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:14 INFO 140202221626752] #quality_metric: host=algo-1, epoch=47, train loss <loss>=9.667134588414973\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:14 INFO 140202221626752] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:14 INFO 140202221626752] Epoch[48] Batch[0] avg_epoch_loss=9.699265\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:14 INFO 140202221626752] #quality_metric: host=algo-1, epoch=48, batch=0 train loss <loss>=9.699264526367188\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:14 INFO 140202221626752] Epoch[48] Batch[5] avg_epoch_loss=9.909899\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:14 INFO 140202221626752] #quality_metric: host=algo-1, epoch=48, batch=5 train loss <loss>=9.909899393717447\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:14 INFO 140202221626752] Epoch[48] Batch [5]#011Speed: 1998.57 samples/sec#011loss=9.909899\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:14 INFO 140202221626752] processed a total of 621 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461754.3183284, \"EndTime\": 1631461754.8903632, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 571.5057849884033, \"count\": 1, \"min\": 571.5057849884033, \"max\": 571.5057849884033}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:14 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=1086.2764395547567 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:14 INFO 140202221626752] #progress_metric: host=algo-1, completed 24.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:14 INFO 140202221626752] #quality_metric: host=algo-1, epoch=48, train loss <loss>=9.972887706756591\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:14 INFO 140202221626752] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:15 INFO 140202221626752] Epoch[49] Batch[0] avg_epoch_loss=10.355206\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:15 INFO 140202221626752] #quality_metric: host=algo-1, epoch=49, batch=0 train loss <loss>=10.355206489562988\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:15 INFO 140202221626752] Epoch[49] Batch[5] avg_epoch_loss=9.912496\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:15 INFO 140202221626752] #quality_metric: host=algo-1, epoch=49, batch=5 train loss <loss>=9.912496248881022\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:15 INFO 140202221626752] Epoch[49] Batch [5]#011Speed: 1640.01 samples/sec#011loss=9.912496\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:15 INFO 140202221626752] Epoch[49] Batch[10] avg_epoch_loss=9.242279\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:15 INFO 140202221626752] #quality_metric: host=algo-1, epoch=49, batch=10 train loss <loss>=8.438017511367798\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:15 INFO 140202221626752] Epoch[49] Batch [10]#011Speed: 1929.41 samples/sec#011loss=8.438018\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:15 INFO 140202221626752] processed a total of 645 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461754.8904924, \"EndTime\": 1631461755.5271888, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 636.1732482910156, \"count\": 1, \"min\": 636.1732482910156, \"max\": 636.1732482910156}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:15 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=1013.6726572932919 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:15 INFO 140202221626752] #progress_metric: host=algo-1, completed 25.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:15 INFO 140202221626752] #quality_metric: host=algo-1, epoch=49, train loss <loss>=9.242278640920466\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:15 INFO 140202221626752] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:15 INFO 140202221626752] Saved checkpoint to \"/opt/ml/model/state_ecac5d20-7bcc-4caf-a29b-45f5f4743c52-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461755.5272765, \"EndTime\": 1631461755.5479562, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 20.149707794189453, \"count\": 1, \"min\": 20.149707794189453, \"max\": 20.149707794189453}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:15 INFO 140202221626752] Epoch[50] Batch[0] avg_epoch_loss=10.115324\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:15 INFO 140202221626752] #quality_metric: host=algo-1, epoch=50, batch=0 train loss <loss>=10.115324020385742\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:15 INFO 140202221626752] Epoch[50] Batch[5] avg_epoch_loss=10.094272\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:15 INFO 140202221626752] #quality_metric: host=algo-1, epoch=50, batch=5 train loss <loss>=10.094272136688232\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:15 INFO 140202221626752] Epoch[50] Batch [5]#011Speed: 1968.20 samples/sec#011loss=10.094272\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:16 INFO 140202221626752] processed a total of 584 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461755.5480456, \"EndTime\": 1631461756.1060717, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 557.9500198364258, \"count\": 1, \"min\": 557.9500198364258, \"max\": 557.9500198364258}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:16 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=1046.4302803754288 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:16 INFO 140202221626752] #progress_metric: host=algo-1, completed 25.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:16 INFO 140202221626752] #quality_metric: host=algo-1, epoch=50, train loss <loss>=10.000071620941162\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:16 INFO 140202221626752] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:16 INFO 140202221626752] Epoch[51] Batch[0] avg_epoch_loss=9.159825\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:16 INFO 140202221626752] #quality_metric: host=algo-1, epoch=51, batch=0 train loss <loss>=9.159825325012207\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:16 INFO 140202221626752] Epoch[51] Batch[5] avg_epoch_loss=9.765816\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:16 INFO 140202221626752] #quality_metric: host=algo-1, epoch=51, batch=5 train loss <loss>=9.765815734863281\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:16 INFO 140202221626752] Epoch[51] Batch [5]#011Speed: 1783.40 samples/sec#011loss=9.765816\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:16 INFO 140202221626752] Epoch[51] Batch[10] avg_epoch_loss=9.517061\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:16 INFO 140202221626752] #quality_metric: host=algo-1, epoch=51, batch=10 train loss <loss>=9.21855525970459\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:16 INFO 140202221626752] Epoch[51] Batch [10]#011Speed: 1665.29 samples/sec#011loss=9.218555\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:16 INFO 140202221626752] processed a total of 656 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461756.1061678, \"EndTime\": 1631461756.750222, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 643.4895992279053, \"count\": 1, \"min\": 643.4895992279053, \"max\": 643.4895992279053}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:16 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=1019.2345522698505 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:16 INFO 140202221626752] #progress_metric: host=algo-1, completed 26.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:16 INFO 140202221626752] #quality_metric: host=algo-1, epoch=51, train loss <loss>=9.517060973427512\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:16 INFO 140202221626752] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:17 INFO 140202221626752] Epoch[52] Batch[0] avg_epoch_loss=10.490646\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:17 INFO 140202221626752] #quality_metric: host=algo-1, epoch=52, batch=0 train loss <loss>=10.490646362304688\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:17 INFO 140202221626752] Epoch[52] Batch[5] avg_epoch_loss=10.061736\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:17 INFO 140202221626752] #quality_metric: host=algo-1, epoch=52, batch=5 train loss <loss>=10.061736424763998\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:17 INFO 140202221626752] Epoch[52] Batch [5]#011Speed: 2008.23 samples/sec#011loss=10.061736\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:17 INFO 140202221626752] Epoch[52] Batch[10] avg_epoch_loss=10.129547\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:17 INFO 140202221626752] #quality_metric: host=algo-1, epoch=52, batch=10 train loss <loss>=10.210919761657715\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:17 INFO 140202221626752] Epoch[52] Batch [10]#011Speed: 1680.19 samples/sec#011loss=10.210920\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:17 INFO 140202221626752] processed a total of 668 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461756.7503126, \"EndTime\": 1631461757.393094, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 642.2469615936279, \"count\": 1, \"min\": 642.2469615936279, \"max\": 642.2469615936279}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:17 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=1039.8902698196723 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:17 INFO 140202221626752] #progress_metric: host=algo-1, completed 26.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:17 INFO 140202221626752] #quality_metric: host=algo-1, epoch=52, train loss <loss>=10.129547032442959\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:17 INFO 140202221626752] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:17 INFO 140202221626752] Epoch[53] Batch[0] avg_epoch_loss=10.498072\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:17 INFO 140202221626752] #quality_metric: host=algo-1, epoch=53, batch=0 train loss <loss>=10.498071670532227\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:17 INFO 140202221626752] Epoch[53] Batch[5] avg_epoch_loss=10.072588\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:17 INFO 140202221626752] #quality_metric: host=algo-1, epoch=53, batch=5 train loss <loss>=10.072588443756104\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:17 INFO 140202221626752] Epoch[53] Batch [5]#011Speed: 1801.69 samples/sec#011loss=10.072588\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:17 INFO 140202221626752] processed a total of 629 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461757.3931828, \"EndTime\": 1631461757.9937031, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 600.0106334686279, \"count\": 1, \"min\": 600.0106334686279, \"max\": 600.0106334686279}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:17 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=1048.0740408508643 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:17 INFO 140202221626752] #progress_metric: host=algo-1, completed 27.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:17 INFO 140202221626752] #quality_metric: host=algo-1, epoch=53, train loss <loss>=10.139374542236329\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:17 INFO 140202221626752] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:18 INFO 140202221626752] Epoch[54] Batch[0] avg_epoch_loss=9.595863\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:18 INFO 140202221626752] #quality_metric: host=algo-1, epoch=54, batch=0 train loss <loss>=9.595863342285156\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:18 INFO 140202221626752] Epoch[54] Batch[5] avg_epoch_loss=9.881902\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:18 INFO 140202221626752] #quality_metric: host=algo-1, epoch=54, batch=5 train loss <loss>=9.881901582082113\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:18 INFO 140202221626752] Epoch[54] Batch [5]#011Speed: 1842.28 samples/sec#011loss=9.881902\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:18 INFO 140202221626752] processed a total of 640 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461757.9937997, \"EndTime\": 1631461758.5791955, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 584.8710536956787, \"count\": 1, \"min\": 584.8710536956787, \"max\": 584.8710536956787}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:18 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=1094.0254942740385 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:18 INFO 140202221626752] #progress_metric: host=algo-1, completed 27.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:18 INFO 140202221626752] #quality_metric: host=algo-1, epoch=54, train loss <loss>=9.961786556243897\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:18 INFO 140202221626752] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:18 INFO 140202221626752] Epoch[55] Batch[0] avg_epoch_loss=10.173645\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:18 INFO 140202221626752] #quality_metric: host=algo-1, epoch=55, batch=0 train loss <loss>=10.17364501953125\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:19 INFO 140202221626752] Epoch[55] Batch[5] avg_epoch_loss=9.867223\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:19 INFO 140202221626752] #quality_metric: host=algo-1, epoch=55, batch=5 train loss <loss>=9.867223103841146\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:19 INFO 140202221626752] Epoch[55] Batch [5]#011Speed: 1575.64 samples/sec#011loss=9.867223\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:19 INFO 140202221626752] processed a total of 620 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461758.5792773, \"EndTime\": 1631461759.1712484, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 591.4304256439209, \"count\": 1, \"min\": 591.4304256439209, \"max\": 591.4304256439209}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:19 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=1048.0612666843463 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:19 INFO 140202221626752] #progress_metric: host=algo-1, completed 28.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:19 INFO 140202221626752] #quality_metric: host=algo-1, epoch=55, train loss <loss>=9.956803607940675\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:19 INFO 140202221626752] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:19 INFO 140202221626752] Epoch[56] Batch[0] avg_epoch_loss=9.902785\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:19 INFO 140202221626752] #quality_metric: host=algo-1, epoch=56, batch=0 train loss <loss>=9.902785301208496\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:19 INFO 140202221626752] Epoch[56] Batch[5] avg_epoch_loss=10.118939\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:19 INFO 140202221626752] #quality_metric: host=algo-1, epoch=56, batch=5 train loss <loss>=10.118939081827799\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:19 INFO 140202221626752] Epoch[56] Batch [5]#011Speed: 2028.19 samples/sec#011loss=10.118939\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:19 INFO 140202221626752] Epoch[56] Batch[10] avg_epoch_loss=9.859488\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:19 INFO 140202221626752] #quality_metric: host=algo-1, epoch=56, batch=10 train loss <loss>=9.54814624786377\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:19 INFO 140202221626752] Epoch[56] Batch [10]#011Speed: 1927.46 samples/sec#011loss=9.548146\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:19 INFO 140202221626752] processed a total of 649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461759.1713436, \"EndTime\": 1631461759.7923813, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 620.5201148986816, \"count\": 1, \"min\": 620.5201148986816, \"max\": 620.5201148986816}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:19 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=1045.6846577529466 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:19 INFO 140202221626752] #progress_metric: host=algo-1, completed 28.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:19 INFO 140202221626752] #quality_metric: host=algo-1, epoch=56, train loss <loss>=9.859487793662332\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:19 INFO 140202221626752] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:20 INFO 140202221626752] Epoch[57] Batch[0] avg_epoch_loss=10.642154\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:20 INFO 140202221626752] #quality_metric: host=algo-1, epoch=57, batch=0 train loss <loss>=10.6421537399292\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:20 INFO 140202221626752] Epoch[57] Batch[5] avg_epoch_loss=10.027572\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:20 INFO 140202221626752] #quality_metric: host=algo-1, epoch=57, batch=5 train loss <loss>=10.02757199605306\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:20 INFO 140202221626752] Epoch[57] Batch [5]#011Speed: 1959.66 samples/sec#011loss=10.027572\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:20 INFO 140202221626752] processed a total of 600 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461759.7924676, \"EndTime\": 1631461760.3521748, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 559.2272281646729, \"count\": 1, \"min\": 559.2272281646729, \"max\": 559.2272281646729}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:20 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=1072.6507522588224 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:20 INFO 140202221626752] #progress_metric: host=algo-1, completed 29.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:20 INFO 140202221626752] #quality_metric: host=algo-1, epoch=57, train loss <loss>=10.21949872970581\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:20 INFO 140202221626752] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:20 INFO 140202221626752] Epoch[58] Batch[0] avg_epoch_loss=10.612895\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:20 INFO 140202221626752] #quality_metric: host=algo-1, epoch=58, batch=0 train loss <loss>=10.612895011901855\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:20 INFO 140202221626752] Epoch[58] Batch[5] avg_epoch_loss=9.804601\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:20 INFO 140202221626752] #quality_metric: host=algo-1, epoch=58, batch=5 train loss <loss>=9.804601192474365\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:20 INFO 140202221626752] Epoch[58] Batch [5]#011Speed: 1862.06 samples/sec#011loss=9.804601\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:20 INFO 140202221626752] processed a total of 626 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461760.3522692, \"EndTime\": 1631461760.9223113, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 569.530725479126, \"count\": 1, \"min\": 569.530725479126, \"max\": 569.530725479126}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:20 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=1098.8800425885786 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:20 INFO 140202221626752] #progress_metric: host=algo-1, completed 29.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:20 INFO 140202221626752] #quality_metric: host=algo-1, epoch=58, train loss <loss>=9.696066188812257\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:20 INFO 140202221626752] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:21 INFO 140202221626752] Epoch[59] Batch[0] avg_epoch_loss=9.983481\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:21 INFO 140202221626752] #quality_metric: host=algo-1, epoch=59, batch=0 train loss <loss>=9.983481407165527\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:21 INFO 140202221626752] Epoch[59] Batch[5] avg_epoch_loss=9.911041\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:21 INFO 140202221626752] #quality_metric: host=algo-1, epoch=59, batch=5 train loss <loss>=9.911040782928467\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:21 INFO 140202221626752] Epoch[59] Batch [5]#011Speed: 1776.96 samples/sec#011loss=9.911041\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:21 INFO 140202221626752] Epoch[59] Batch[10] avg_epoch_loss=9.212922\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:21 INFO 140202221626752] #quality_metric: host=algo-1, epoch=59, batch=10 train loss <loss>=8.375178909301757\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:21 INFO 140202221626752] Epoch[59] Batch [10]#011Speed: 1774.97 samples/sec#011loss=8.375179\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:21 INFO 140202221626752] processed a total of 645 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461760.922408, \"EndTime\": 1631461761.5562165, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 633.2042217254639, \"count\": 1, \"min\": 633.2042217254639, \"max\": 633.2042217254639}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:21 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=1018.4419603228204 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:21 INFO 140202221626752] #progress_metric: host=algo-1, completed 30.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:21 INFO 140202221626752] #quality_metric: host=algo-1, epoch=59, train loss <loss>=9.212921749461781\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:21 INFO 140202221626752] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:21 INFO 140202221626752] Saved checkpoint to \"/opt/ml/model/state_ab0323f3-5776-426b-8e41-44bc129e4a79-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461761.5562904, \"EndTime\": 1631461761.5691235, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 12.298822402954102, \"count\": 1, \"min\": 12.298822402954102, \"max\": 12.298822402954102}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:21 INFO 140202221626752] Epoch[60] Batch[0] avg_epoch_loss=9.653354\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:21 INFO 140202221626752] #quality_metric: host=algo-1, epoch=60, batch=0 train loss <loss>=9.653353691101074\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:22 INFO 140202221626752] Epoch[60] Batch[5] avg_epoch_loss=10.337281\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:22 INFO 140202221626752] #quality_metric: host=algo-1, epoch=60, batch=5 train loss <loss>=10.337281068166098\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:22 INFO 140202221626752] Epoch[60] Batch [5]#011Speed: 1537.96 samples/sec#011loss=10.337281\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:22 INFO 140202221626752] Epoch[60] Batch[10] avg_epoch_loss=10.621597\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:22 INFO 140202221626752] #quality_metric: host=algo-1, epoch=60, batch=10 train loss <loss>=10.962775230407715\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:22 INFO 140202221626752] Epoch[60] Batch [10]#011Speed: 1816.89 samples/sec#011loss=10.962775\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:22 INFO 140202221626752] processed a total of 660 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461761.5691938, \"EndTime\": 1631461762.208698, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 639.4312381744385, \"count\": 1, \"min\": 639.4312381744385, \"max\": 639.4312381744385}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:22 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=1031.968345780742 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:22 INFO 140202221626752] #progress_metric: host=algo-1, completed 30.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:22 INFO 140202221626752] #quality_metric: host=algo-1, epoch=60, train loss <loss>=10.621596596457742\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:22 INFO 140202221626752] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:22 INFO 140202221626752] Epoch[61] Batch[0] avg_epoch_loss=10.580983\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:22 INFO 140202221626752] #quality_metric: host=algo-1, epoch=61, batch=0 train loss <loss>=10.58098316192627\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:22 INFO 140202221626752] Epoch[61] Batch[5] avg_epoch_loss=10.690613\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:22 INFO 140202221626752] #quality_metric: host=algo-1, epoch=61, batch=5 train loss <loss>=10.69061279296875\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:22 INFO 140202221626752] Epoch[61] Batch [5]#011Speed: 1981.90 samples/sec#011loss=10.690613\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:22 INFO 140202221626752] Epoch[61] Batch[10] avg_epoch_loss=9.946707\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:22 INFO 140202221626752] #quality_metric: host=algo-1, epoch=61, batch=10 train loss <loss>=9.054020118713378\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:22 INFO 140202221626752] Epoch[61] Batch [10]#011Speed: 1852.18 samples/sec#011loss=9.054020\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:22 INFO 140202221626752] processed a total of 664 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461762.208784, \"EndTime\": 1631461762.837474, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 628.2064914703369, \"count\": 1, \"min\": 628.2064914703369, \"max\": 628.2064914703369}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:22 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=1056.759520260815 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:22 INFO 140202221626752] #progress_metric: host=algo-1, completed 31.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:22 INFO 140202221626752] #quality_metric: host=algo-1, epoch=61, train loss <loss>=9.946707031943582\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:22 INFO 140202221626752] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:23 INFO 140202221626752] Epoch[62] Batch[0] avg_epoch_loss=10.092764\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:23 INFO 140202221626752] #quality_metric: host=algo-1, epoch=62, batch=0 train loss <loss>=10.092763900756836\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:23 INFO 140202221626752] Epoch[62] Batch[5] avg_epoch_loss=10.142454\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:23 INFO 140202221626752] #quality_metric: host=algo-1, epoch=62, batch=5 train loss <loss>=10.142453829447428\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:23 INFO 140202221626752] Epoch[62] Batch [5]#011Speed: 1963.71 samples/sec#011loss=10.142454\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:23 INFO 140202221626752] Epoch[62] Batch[10] avg_epoch_loss=9.444149\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:23 INFO 140202221626752] #quality_metric: host=algo-1, epoch=62, batch=10 train loss <loss>=8.606183338165284\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:23 INFO 140202221626752] Epoch[62] Batch [10]#011Speed: 1746.16 samples/sec#011loss=8.606183\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:23 INFO 140202221626752] processed a total of 650 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461762.8375633, \"EndTime\": 1631461763.452823, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 614.7315502166748, \"count\": 1, \"min\": 614.7315502166748, \"max\": 614.7315502166748}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:23 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=1057.1310362049182 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:23 INFO 140202221626752] #progress_metric: host=algo-1, completed 31.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:23 INFO 140202221626752] #quality_metric: host=algo-1, epoch=62, train loss <loss>=9.444149060682816\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:23 INFO 140202221626752] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:23 INFO 140202221626752] Epoch[63] Batch[0] avg_epoch_loss=9.519843\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:23 INFO 140202221626752] #quality_metric: host=algo-1, epoch=63, batch=0 train loss <loss>=9.519843101501465\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:23 INFO 140202221626752] Epoch[63] Batch[5] avg_epoch_loss=9.787313\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:23 INFO 140202221626752] #quality_metric: host=algo-1, epoch=63, batch=5 train loss <loss>=9.787313461303711\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:23 INFO 140202221626752] Epoch[63] Batch [5]#011Speed: 1531.56 samples/sec#011loss=9.787313\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:24 INFO 140202221626752] processed a total of 591 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461763.452922, \"EndTime\": 1631461764.070302, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 616.8959140777588, \"count\": 1, \"min\": 616.8959140777588, \"max\": 616.8959140777588}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:24 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=957.8304762000442 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:24 INFO 140202221626752] #progress_metric: host=algo-1, completed 32.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:24 INFO 140202221626752] #quality_metric: host=algo-1, epoch=63, train loss <loss>=10.046136379241943\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:24 INFO 140202221626752] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:24 INFO 140202221626752] Epoch[64] Batch[0] avg_epoch_loss=10.057546\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:24 INFO 140202221626752] #quality_metric: host=algo-1, epoch=64, batch=0 train loss <loss>=10.05754566192627\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:24 INFO 140202221626752] Epoch[64] Batch[5] avg_epoch_loss=10.231621\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:24 INFO 140202221626752] #quality_metric: host=algo-1, epoch=64, batch=5 train loss <loss>=10.231621265411377\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:24 INFO 140202221626752] Epoch[64] Batch [5]#011Speed: 1699.67 samples/sec#011loss=10.231621\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:24 INFO 140202221626752] processed a total of 594 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461764.0703804, \"EndTime\": 1631461764.6444485, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 573.4438896179199, \"count\": 1, \"min\": 573.4438896179199, \"max\": 573.4438896179199}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:24 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=1035.6116760553077 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:24 INFO 140202221626752] #progress_metric: host=algo-1, completed 32.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:24 INFO 140202221626752] #quality_metric: host=algo-1, epoch=64, train loss <loss>=10.395158290863037\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:24 INFO 140202221626752] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:24 INFO 140202221626752] Epoch[65] Batch[0] avg_epoch_loss=10.581943\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:24 INFO 140202221626752] #quality_metric: host=algo-1, epoch=65, batch=0 train loss <loss>=10.581942558288574\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:25 INFO 140202221626752] Epoch[65] Batch[5] avg_epoch_loss=9.936306\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:25 INFO 140202221626752] #quality_metric: host=algo-1, epoch=65, batch=5 train loss <loss>=9.93630584081014\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:25 INFO 140202221626752] Epoch[65] Batch [5]#011Speed: 1603.44 samples/sec#011loss=9.936306\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:25 INFO 140202221626752] processed a total of 635 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461764.6445358, \"EndTime\": 1631461765.2370794, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 591.9816493988037, \"count\": 1, \"min\": 591.9816493988037, \"max\": 591.9816493988037}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:25 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=1072.4411943416428 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:25 INFO 140202221626752] #progress_metric: host=algo-1, completed 33.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:25 INFO 140202221626752] #quality_metric: host=algo-1, epoch=65, train loss <loss>=10.024916172027588\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:25 INFO 140202221626752] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:25 INFO 140202221626752] Epoch[66] Batch[0] avg_epoch_loss=9.994007\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:25 INFO 140202221626752] #quality_metric: host=algo-1, epoch=66, batch=0 train loss <loss>=9.994007110595703\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:25 INFO 140202221626752] Epoch[66] Batch[5] avg_epoch_loss=9.922962\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:25 INFO 140202221626752] #quality_metric: host=algo-1, epoch=66, batch=5 train loss <loss>=9.922961711883545\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:25 INFO 140202221626752] Epoch[66] Batch [5]#011Speed: 1967.94 samples/sec#011loss=9.922962\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:25 INFO 140202221626752] processed a total of 629 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461765.23716, \"EndTime\": 1631461765.79583, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 558.0589771270752, \"count\": 1, \"min\": 558.0589771270752, \"max\": 558.0589771270752}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:25 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=1126.867300302922 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:25 INFO 140202221626752] #progress_metric: host=algo-1, completed 33.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:25 INFO 140202221626752] #quality_metric: host=algo-1, epoch=66, train loss <loss>=9.90751600265503\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:25 INFO 140202221626752] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:26 INFO 140202221626752] Epoch[67] Batch[0] avg_epoch_loss=10.089583\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:26 INFO 140202221626752] #quality_metric: host=algo-1, epoch=67, batch=0 train loss <loss>=10.089583396911621\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:26 INFO 140202221626752] Epoch[67] Batch[5] avg_epoch_loss=10.081349\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:26 INFO 140202221626752] #quality_metric: host=algo-1, epoch=67, batch=5 train loss <loss>=10.08134905497233\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:26 INFO 140202221626752] Epoch[67] Batch [5]#011Speed: 1543.56 samples/sec#011loss=10.081349\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:26 INFO 140202221626752] processed a total of 632 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461765.7959104, \"EndTime\": 1631461766.4299371, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 633.4860324859619, \"count\": 1, \"min\": 633.4860324859619, \"max\": 633.4860324859619}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:26 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=997.4222078068188 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:26 INFO 140202221626752] #progress_metric: host=algo-1, completed 34.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:26 INFO 140202221626752] #quality_metric: host=algo-1, epoch=67, train loss <loss>=9.870938873291015\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:26 INFO 140202221626752] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:26 INFO 140202221626752] Epoch[68] Batch[0] avg_epoch_loss=10.138490\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:26 INFO 140202221626752] #quality_metric: host=algo-1, epoch=68, batch=0 train loss <loss>=10.138489723205566\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:26 INFO 140202221626752] Epoch[68] Batch[5] avg_epoch_loss=9.960646\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:26 INFO 140202221626752] #quality_metric: host=algo-1, epoch=68, batch=5 train loss <loss>=9.960645993550619\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:26 INFO 140202221626752] Epoch[68] Batch [5]#011Speed: 1543.56 samples/sec#011loss=9.960646\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:27 INFO 140202221626752] Epoch[68] Batch[10] avg_epoch_loss=9.358281\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:27 INFO 140202221626752] #quality_metric: host=algo-1, epoch=68, batch=10 train loss <loss>=8.635442686080932\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:27 INFO 140202221626752] Epoch[68] Batch [10]#011Speed: 1522.46 samples/sec#011loss=8.635443\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:27 INFO 140202221626752] processed a total of 646 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461766.4300394, \"EndTime\": 1631461767.1191115, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 688.4291172027588, \"count\": 1, \"min\": 688.4291172027588, \"max\": 688.4291172027588}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:27 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=938.2041270283166 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:27 INFO 140202221626752] #progress_metric: host=algo-1, completed 34.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:27 INFO 140202221626752] #quality_metric: host=algo-1, epoch=68, train loss <loss>=9.35828085379167\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:27 INFO 140202221626752] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:27 INFO 140202221626752] Epoch[69] Batch[0] avg_epoch_loss=9.371856\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:27 INFO 140202221626752] #quality_metric: host=algo-1, epoch=69, batch=0 train loss <loss>=9.371855735778809\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:27 INFO 140202221626752] Epoch[69] Batch[5] avg_epoch_loss=9.642988\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:27 INFO 140202221626752] #quality_metric: host=algo-1, epoch=69, batch=5 train loss <loss>=9.642988046010336\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:27 INFO 140202221626752] Epoch[69] Batch [5]#011Speed: 1821.70 samples/sec#011loss=9.642988\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:27 INFO 140202221626752] Epoch[69] Batch[10] avg_epoch_loss=9.735404\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:27 INFO 140202221626752] #quality_metric: host=algo-1, epoch=69, batch=10 train loss <loss>=9.846303558349609\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:27 INFO 140202221626752] Epoch[69] Batch [10]#011Speed: 1695.01 samples/sec#011loss=9.846304\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:27 INFO 140202221626752] processed a total of 700 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461767.1191988, \"EndTime\": 1631461767.7368379, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 617.0928478240967, \"count\": 1, \"min\": 617.0928478240967, \"max\": 617.0928478240967}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:27 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=1134.1281466510918 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:27 INFO 140202221626752] #progress_metric: host=algo-1, completed 35.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:27 INFO 140202221626752] #quality_metric: host=algo-1, epoch=69, train loss <loss>=9.735404187982732\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:27 INFO 140202221626752] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:28 INFO 140202221626752] Epoch[70] Batch[0] avg_epoch_loss=9.378588\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:28 INFO 140202221626752] #quality_metric: host=algo-1, epoch=70, batch=0 train loss <loss>=9.37858772277832\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:28 INFO 140202221626752] Epoch[70] Batch[5] avg_epoch_loss=9.707813\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:28 INFO 140202221626752] #quality_metric: host=algo-1, epoch=70, batch=5 train loss <loss>=9.707813421885172\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:28 INFO 140202221626752] Epoch[70] Batch [5]#011Speed: 1856.70 samples/sec#011loss=9.707813\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:28 INFO 140202221626752] Epoch[70] Batch[10] avg_epoch_loss=9.918766\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:28 INFO 140202221626752] #quality_metric: host=algo-1, epoch=70, batch=10 train loss <loss>=10.17190990447998\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:28 INFO 140202221626752] Epoch[70] Batch [10]#011Speed: 1881.07 samples/sec#011loss=10.171910\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:28 INFO 140202221626752] processed a total of 656 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461767.73692, \"EndTime\": 1631461768.3699834, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 632.6003074645996, \"count\": 1, \"min\": 632.6003074645996, \"max\": 632.6003074645996}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:28 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=1036.7837699639579 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:28 INFO 140202221626752] #progress_metric: host=algo-1, completed 35.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:28 INFO 140202221626752] #quality_metric: host=algo-1, epoch=70, train loss <loss>=9.918766368519176\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:28 INFO 140202221626752] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:28 INFO 140202221626752] Epoch[71] Batch[0] avg_epoch_loss=9.777949\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:28 INFO 140202221626752] #quality_metric: host=algo-1, epoch=71, batch=0 train loss <loss>=9.777949333190918\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:28 INFO 140202221626752] Epoch[71] Batch[5] avg_epoch_loss=10.394480\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:28 INFO 140202221626752] #quality_metric: host=algo-1, epoch=71, batch=5 train loss <loss>=10.394479910532633\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:28 INFO 140202221626752] Epoch[71] Batch [5]#011Speed: 1878.21 samples/sec#011loss=10.394480\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:28 INFO 140202221626752] Epoch[71] Batch[10] avg_epoch_loss=10.175134\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:28 INFO 140202221626752] #quality_metric: host=algo-1, epoch=71, batch=10 train loss <loss>=9.911918640136719\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:28 INFO 140202221626752] Epoch[71] Batch [10]#011Speed: 1718.38 samples/sec#011loss=9.911919\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:28 INFO 140202221626752] processed a total of 653 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461768.3700705, \"EndTime\": 1631461768.9944649, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 623.8534450531006, \"count\": 1, \"min\": 623.8534450531006, \"max\": 623.8534450531006}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:28 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=1046.470638894567 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:28 INFO 140202221626752] #progress_metric: host=algo-1, completed 36.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:28 INFO 140202221626752] #quality_metric: host=algo-1, epoch=71, train loss <loss>=10.17513387853449\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:28 INFO 140202221626752] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:29 INFO 140202221626752] Epoch[72] Batch[0] avg_epoch_loss=8.923639\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:29 INFO 140202221626752] #quality_metric: host=algo-1, epoch=72, batch=0 train loss <loss>=8.923639297485352\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:29 INFO 140202221626752] Epoch[72] Batch[5] avg_epoch_loss=9.703077\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:29 INFO 140202221626752] #quality_metric: host=algo-1, epoch=72, batch=5 train loss <loss>=9.703077475229898\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:29 INFO 140202221626752] Epoch[72] Batch [5]#011Speed: 1971.89 samples/sec#011loss=9.703077\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:29 INFO 140202221626752] processed a total of 589 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461768.9945521, \"EndTime\": 1631461769.5435708, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 548.4960079193115, \"count\": 1, \"min\": 548.4960079193115, \"max\": 548.4960079193115}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:29 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=1073.5856236712968 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:29 INFO 140202221626752] #progress_metric: host=algo-1, completed 36.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:29 INFO 140202221626752] #quality_metric: host=algo-1, epoch=72, train loss <loss>=9.582987117767335\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:29 INFO 140202221626752] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:29 INFO 140202221626752] Epoch[73] Batch[0] avg_epoch_loss=9.401601\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:29 INFO 140202221626752] #quality_metric: host=algo-1, epoch=73, batch=0 train loss <loss>=9.40160083770752\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:30 INFO 140202221626752] Epoch[73] Batch[5] avg_epoch_loss=9.692903\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:30 INFO 140202221626752] #quality_metric: host=algo-1, epoch=73, batch=5 train loss <loss>=9.692903200785318\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:30 INFO 140202221626752] Epoch[73] Batch [5]#011Speed: 1525.76 samples/sec#011loss=9.692903\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:30 INFO 140202221626752] processed a total of 609 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461769.5436633, \"EndTime\": 1631461770.1888144, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 644.6361541748047, \"count\": 1, \"min\": 644.6361541748047, \"max\": 644.6361541748047}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:30 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=944.5184490015065 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:30 INFO 140202221626752] #progress_metric: host=algo-1, completed 37.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:30 INFO 140202221626752] #quality_metric: host=algo-1, epoch=73, train loss <loss>=9.581549167633057\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:30 INFO 140202221626752] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:30 INFO 140202221626752] Epoch[74] Batch[0] avg_epoch_loss=9.657549\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:30 INFO 140202221626752] #quality_metric: host=algo-1, epoch=74, batch=0 train loss <loss>=9.657548904418945\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:30 INFO 140202221626752] Epoch[74] Batch[5] avg_epoch_loss=9.795487\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:30 INFO 140202221626752] #quality_metric: host=algo-1, epoch=74, batch=5 train loss <loss>=9.79548708597819\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:30 INFO 140202221626752] Epoch[74] Batch [5]#011Speed: 1989.17 samples/sec#011loss=9.795487\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:30 INFO 140202221626752] processed a total of 587 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461770.1889093, \"EndTime\": 1631461770.752919, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 563.483476638794, \"count\": 1, \"min\": 563.483476638794, \"max\": 563.483476638794}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:30 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=1041.4825633927694 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:30 INFO 140202221626752] #progress_metric: host=algo-1, completed 37.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:30 INFO 140202221626752] #quality_metric: host=algo-1, epoch=74, train loss <loss>=10.332263565063476\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:30 INFO 140202221626752] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:31 INFO 140202221626752] Epoch[75] Batch[0] avg_epoch_loss=10.093317\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:31 INFO 140202221626752] #quality_metric: host=algo-1, epoch=75, batch=0 train loss <loss>=10.093317031860352\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:31 INFO 140202221626752] Epoch[75] Batch[5] avg_epoch_loss=9.545942\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:31 INFO 140202221626752] #quality_metric: host=algo-1, epoch=75, batch=5 train loss <loss>=9.545942306518555\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:31 INFO 140202221626752] Epoch[75] Batch [5]#011Speed: 1915.45 samples/sec#011loss=9.545942\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:31 INFO 140202221626752] processed a total of 617 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461770.7530131, \"EndTime\": 1631461771.3011699, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 547.6446151733398, \"count\": 1, \"min\": 547.6446151733398, \"max\": 547.6446151733398}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:31 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=1126.371386464317 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:31 INFO 140202221626752] #progress_metric: host=algo-1, completed 38.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:31 INFO 140202221626752] #quality_metric: host=algo-1, epoch=75, train loss <loss>=9.724230098724366\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:31 INFO 140202221626752] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:31 INFO 140202221626752] Epoch[76] Batch[0] avg_epoch_loss=9.551901\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:31 INFO 140202221626752] #quality_metric: host=algo-1, epoch=76, batch=0 train loss <loss>=9.551900863647461\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:31 INFO 140202221626752] Epoch[76] Batch[5] avg_epoch_loss=9.637703\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:31 INFO 140202221626752] #quality_metric: host=algo-1, epoch=76, batch=5 train loss <loss>=9.637702941894531\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:31 INFO 140202221626752] Epoch[76] Batch [5]#011Speed: 1912.50 samples/sec#011loss=9.637703\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:31 INFO 140202221626752] processed a total of 638 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461771.3012624, \"EndTime\": 1631461771.9009233, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 599.1487503051758, \"count\": 1, \"min\": 599.1487503051758, \"max\": 599.1487503051758}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:31 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=1064.61827896457 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:31 INFO 140202221626752] #progress_metric: host=algo-1, completed 38.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:31 INFO 140202221626752] #quality_metric: host=algo-1, epoch=76, train loss <loss>=9.55834674835205\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:31 INFO 140202221626752] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:32 INFO 140202221626752] Epoch[77] Batch[0] avg_epoch_loss=10.216319\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:32 INFO 140202221626752] #quality_metric: host=algo-1, epoch=77, batch=0 train loss <loss>=10.21631908416748\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:32 INFO 140202221626752] Epoch[77] Batch[5] avg_epoch_loss=10.094112\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:32 INFO 140202221626752] #quality_metric: host=algo-1, epoch=77, batch=5 train loss <loss>=10.094112396240234\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:32 INFO 140202221626752] Epoch[77] Batch [5]#011Speed: 1888.30 samples/sec#011loss=10.094112\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:32 INFO 140202221626752] Epoch[77] Batch[10] avg_epoch_loss=10.142585\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:32 INFO 140202221626752] #quality_metric: host=algo-1, epoch=77, batch=10 train loss <loss>=10.200751304626465\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:32 INFO 140202221626752] Epoch[77] Batch [10]#011Speed: 1871.29 samples/sec#011loss=10.200751\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:32 INFO 140202221626752] processed a total of 655 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461771.9010122, \"EndTime\": 1631461772.4915414, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 590.0492668151855, \"count\": 1, \"min\": 590.0492668151855, \"max\": 590.0492668151855}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:32 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=1109.8449479934861 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:32 INFO 140202221626752] #progress_metric: host=algo-1, completed 39.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:32 INFO 140202221626752] #quality_metric: host=algo-1, epoch=77, train loss <loss>=10.142584627324885\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:32 INFO 140202221626752] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:32 INFO 140202221626752] Epoch[78] Batch[0] avg_epoch_loss=9.764064\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:32 INFO 140202221626752] #quality_metric: host=algo-1, epoch=78, batch=0 train loss <loss>=9.764063835144043\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:32 INFO 140202221626752] Epoch[78] Batch[5] avg_epoch_loss=9.876740\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:32 INFO 140202221626752] #quality_metric: host=algo-1, epoch=78, batch=5 train loss <loss>=9.876740455627441\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:32 INFO 140202221626752] Epoch[78] Batch [5]#011Speed: 2009.21 samples/sec#011loss=9.876740\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:33 INFO 140202221626752] Epoch[78] Batch[10] avg_epoch_loss=9.843510\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:33 INFO 140202221626752] #quality_metric: host=algo-1, epoch=78, batch=10 train loss <loss>=9.803633117675782\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:33 INFO 140202221626752] Epoch[78] Batch [10]#011Speed: 1480.09 samples/sec#011loss=9.803633\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:33 INFO 140202221626752] processed a total of 685 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461772.4916272, \"EndTime\": 1631461773.1215355, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 629.4307708740234, \"count\": 1, \"min\": 629.4307708740234, \"max\": 629.4307708740234}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:33 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=1088.0718155862758 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:33 INFO 140202221626752] #progress_metric: host=algo-1, completed 39.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:33 INFO 140202221626752] #quality_metric: host=algo-1, epoch=78, train loss <loss>=9.843509847467596\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:33 INFO 140202221626752] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:33 INFO 140202221626752] Epoch[79] Batch[0] avg_epoch_loss=10.536724\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:33 INFO 140202221626752] #quality_metric: host=algo-1, epoch=79, batch=0 train loss <loss>=10.536724090576172\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:33 INFO 140202221626752] Epoch[79] Batch[5] avg_epoch_loss=10.252876\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:33 INFO 140202221626752] #quality_metric: host=algo-1, epoch=79, batch=5 train loss <loss>=10.252876122792562\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:33 INFO 140202221626752] Epoch[79] Batch [5]#011Speed: 1948.84 samples/sec#011loss=10.252876\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:33 INFO 140202221626752] Epoch[79] Batch[10] avg_epoch_loss=9.336044\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:33 INFO 140202221626752] #quality_metric: host=algo-1, epoch=79, batch=10 train loss <loss>=8.235844564437866\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:33 INFO 140202221626752] Epoch[79] Batch [10]#011Speed: 1928.96 samples/sec#011loss=8.235845\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:33 INFO 140202221626752] processed a total of 650 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461773.1216192, \"EndTime\": 1631461773.7066312, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 584.5205783843994, \"count\": 1, \"min\": 584.5205783843994, \"max\": 584.5205783843994}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:33 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=1111.7830016903292 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:33 INFO 140202221626752] #progress_metric: host=algo-1, completed 40.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:33 INFO 140202221626752] #quality_metric: host=algo-1, epoch=79, train loss <loss>=9.3360435962677\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:33 INFO 140202221626752] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:33 INFO 140202221626752] Epoch[80] Batch[0] avg_epoch_loss=10.093057\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:33 INFO 140202221626752] #quality_metric: host=algo-1, epoch=80, batch=0 train loss <loss>=10.093056678771973\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:34 INFO 140202221626752] Epoch[80] Batch[5] avg_epoch_loss=9.554056\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:34 INFO 140202221626752] #quality_metric: host=algo-1, epoch=80, batch=5 train loss <loss>=9.5540558497111\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:34 INFO 140202221626752] Epoch[80] Batch [5]#011Speed: 1848.99 samples/sec#011loss=9.554056\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:34 INFO 140202221626752] processed a total of 595 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461773.7067177, \"EndTime\": 1631461774.2748525, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 567.6553249359131, \"count\": 1, \"min\": 567.6553249359131, \"max\": 567.6553249359131}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:34 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=1047.936230528169 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:34 INFO 140202221626752] #progress_metric: host=algo-1, completed 40.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:34 INFO 140202221626752] #quality_metric: host=algo-1, epoch=80, train loss <loss>=9.757116031646728\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:34 INFO 140202221626752] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:34 INFO 140202221626752] Epoch[81] Batch[0] avg_epoch_loss=10.163910\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:34 INFO 140202221626752] #quality_metric: host=algo-1, epoch=81, batch=0 train loss <loss>=10.163909912109375\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:34 INFO 140202221626752] Epoch[81] Batch[5] avg_epoch_loss=9.696173\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:34 INFO 140202221626752] #quality_metric: host=algo-1, epoch=81, batch=5 train loss <loss>=9.696173032124838\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:34 INFO 140202221626752] Epoch[81] Batch [5]#011Speed: 1766.16 samples/sec#011loss=9.696173\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:34 INFO 140202221626752] processed a total of 628 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461774.2749395, \"EndTime\": 1631461774.8783545, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 602.921724319458, \"count\": 1, \"min\": 602.921724319458, \"max\": 602.921724319458}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:34 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=1041.322393613278 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:34 INFO 140202221626752] #progress_metric: host=algo-1, completed 41.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:34 INFO 140202221626752] #quality_metric: host=algo-1, epoch=81, train loss <loss>=9.905452156066895\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:34 INFO 140202221626752] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:35 INFO 140202221626752] Epoch[82] Batch[0] avg_epoch_loss=8.983294\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:35 INFO 140202221626752] #quality_metric: host=algo-1, epoch=82, batch=0 train loss <loss>=8.983293533325195\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:35 INFO 140202221626752] Epoch[82] Batch[5] avg_epoch_loss=9.742503\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:35 INFO 140202221626752] #quality_metric: host=algo-1, epoch=82, batch=5 train loss <loss>=9.742502530415853\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:35 INFO 140202221626752] Epoch[82] Batch [5]#011Speed: 1971.03 samples/sec#011loss=9.742503\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:35 INFO 140202221626752] Epoch[82] Batch[10] avg_epoch_loss=10.039556\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:35 INFO 140202221626752] #quality_metric: host=algo-1, epoch=82, batch=10 train loss <loss>=10.396020126342773\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:35 INFO 140202221626752] Epoch[82] Batch [10]#011Speed: 1783.86 samples/sec#011loss=10.396020\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:35 INFO 140202221626752] processed a total of 683 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461774.8784692, \"EndTime\": 1631461775.4882073, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 609.2233657836914, \"count\": 1, \"min\": 609.2233657836914, \"max\": 609.2233657836914}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:35 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=1120.8691908930027 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:35 INFO 140202221626752] #progress_metric: host=algo-1, completed 41.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:35 INFO 140202221626752] #quality_metric: host=algo-1, epoch=82, train loss <loss>=10.039555983109908\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:35 INFO 140202221626752] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:35 INFO 140202221626752] Epoch[83] Batch[0] avg_epoch_loss=10.036480\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:35 INFO 140202221626752] #quality_metric: host=algo-1, epoch=83, batch=0 train loss <loss>=10.036479949951172\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:35 INFO 140202221626752] Epoch[83] Batch[5] avg_epoch_loss=9.949013\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:35 INFO 140202221626752] #quality_metric: host=algo-1, epoch=83, batch=5 train loss <loss>=9.949012756347656\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:35 INFO 140202221626752] Epoch[83] Batch [5]#011Speed: 1946.68 samples/sec#011loss=9.949013\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:36 INFO 140202221626752] Epoch[83] Batch[10] avg_epoch_loss=10.205027\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:36 INFO 140202221626752] #quality_metric: host=algo-1, epoch=83, batch=10 train loss <loss>=10.512243270874023\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:36 INFO 140202221626752] Epoch[83] Batch [10]#011Speed: 1858.42 samples/sec#011loss=10.512243\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:36 INFO 140202221626752] processed a total of 662 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461775.4882917, \"EndTime\": 1631461776.0743706, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 585.5391025543213, \"count\": 1, \"min\": 585.5391025543213, \"max\": 585.5391025543213}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:36 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=1130.252078647066 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:36 INFO 140202221626752] #progress_metric: host=algo-1, completed 42.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:36 INFO 140202221626752] #quality_metric: host=algo-1, epoch=83, train loss <loss>=10.205026626586914\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:36 INFO 140202221626752] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:36 INFO 140202221626752] Epoch[84] Batch[0] avg_epoch_loss=9.168192\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:36 INFO 140202221626752] #quality_metric: host=algo-1, epoch=84, batch=0 train loss <loss>=9.168191909790039\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:36 INFO 140202221626752] Epoch[84] Batch[5] avg_epoch_loss=9.672343\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:36 INFO 140202221626752] #quality_metric: host=algo-1, epoch=84, batch=5 train loss <loss>=9.672343413035074\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:36 INFO 140202221626752] Epoch[84] Batch [5]#011Speed: 1973.49 samples/sec#011loss=9.672343\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:36 INFO 140202221626752] Epoch[84] Batch[10] avg_epoch_loss=9.949884\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:36 INFO 140202221626752] #quality_metric: host=algo-1, epoch=84, batch=10 train loss <loss>=10.282932662963868\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:36 INFO 140202221626752] Epoch[84] Batch [10]#011Speed: 1768.93 samples/sec#011loss=10.282933\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:36 INFO 140202221626752] processed a total of 674 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461776.0744975, \"EndTime\": 1631461776.6598544, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 584.815502166748, \"count\": 1, \"min\": 584.815502166748, \"max\": 584.815502166748}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:36 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=1152.24233699336 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:36 INFO 140202221626752] #progress_metric: host=algo-1, completed 42.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:36 INFO 140202221626752] #quality_metric: host=algo-1, epoch=84, train loss <loss>=9.949883981184525\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:36 INFO 140202221626752] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:36 INFO 140202221626752] Epoch[85] Batch[0] avg_epoch_loss=9.835921\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:36 INFO 140202221626752] #quality_metric: host=algo-1, epoch=85, batch=0 train loss <loss>=9.835921287536621\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:37 INFO 140202221626752] Epoch[85] Batch[5] avg_epoch_loss=10.228800\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:37 INFO 140202221626752] #quality_metric: host=algo-1, epoch=85, batch=5 train loss <loss>=10.228800455729166\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:37 INFO 140202221626752] Epoch[85] Batch [5]#011Speed: 1932.90 samples/sec#011loss=10.228800\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:37 INFO 140202221626752] processed a total of 605 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461776.6599433, \"EndTime\": 1631461777.211819, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 551.3668060302734, \"count\": 1, \"min\": 551.3668060302734, \"max\": 551.3668060302734}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:37 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=1096.9999342894791 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:37 INFO 140202221626752] #progress_metric: host=algo-1, completed 43.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:37 INFO 140202221626752] #quality_metric: host=algo-1, epoch=85, train loss <loss>=10.20012674331665\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:37 INFO 140202221626752] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:37 INFO 140202221626752] Epoch[86] Batch[0] avg_epoch_loss=9.225493\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:37 INFO 140202221626752] #quality_metric: host=algo-1, epoch=86, batch=0 train loss <loss>=9.225493431091309\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:37 INFO 140202221626752] Epoch[86] Batch[5] avg_epoch_loss=9.568655\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:37 INFO 140202221626752] #quality_metric: host=algo-1, epoch=86, batch=5 train loss <loss>=9.568655172983805\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:37 INFO 140202221626752] Epoch[86] Batch [5]#011Speed: 1921.75 samples/sec#011loss=9.568655\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:37 INFO 140202221626752] Epoch[86] Batch[10] avg_epoch_loss=9.742493\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:37 INFO 140202221626752] #quality_metric: host=algo-1, epoch=86, batch=10 train loss <loss>=9.951097679138183\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:37 INFO 140202221626752] Epoch[86] Batch [10]#011Speed: 1773.70 samples/sec#011loss=9.951098\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:37 INFO 140202221626752] processed a total of 666 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461777.211914, \"EndTime\": 1631461777.8155568, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 603.1174659729004, \"count\": 1, \"min\": 603.1174659729004, \"max\": 603.1174659729004}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:37 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=1104.059551255018 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:37 INFO 140202221626752] #progress_metric: host=algo-1, completed 43.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:37 INFO 140202221626752] #quality_metric: host=algo-1, epoch=86, train loss <loss>=9.74249267578125\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:37 INFO 140202221626752] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:38 INFO 140202221626752] Epoch[87] Batch[0] avg_epoch_loss=9.996668\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:38 INFO 140202221626752] #quality_metric: host=algo-1, epoch=87, batch=0 train loss <loss>=9.996667861938477\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:38 INFO 140202221626752] Epoch[87] Batch[5] avg_epoch_loss=10.223903\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:38 INFO 140202221626752] #quality_metric: host=algo-1, epoch=87, batch=5 train loss <loss>=10.223902543385824\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:38 INFO 140202221626752] Epoch[87] Batch [5]#011Speed: 1587.33 samples/sec#011loss=10.223903\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:38 INFO 140202221626752] Epoch[87] Batch[10] avg_epoch_loss=9.408673\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:38 INFO 140202221626752] #quality_metric: host=algo-1, epoch=87, batch=10 train loss <loss>=8.430398559570312\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:38 INFO 140202221626752] Epoch[87] Batch [10]#011Speed: 1748.53 samples/sec#011loss=8.430399\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:38 INFO 140202221626752] processed a total of 648 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461777.815628, \"EndTime\": 1631461778.451341, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 635.265588760376, \"count\": 1, \"min\": 635.265588760376, \"max\": 635.265588760376}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:38 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=1019.8471573904246 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:38 INFO 140202221626752] #progress_metric: host=algo-1, completed 44.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:38 INFO 140202221626752] #quality_metric: host=algo-1, epoch=87, train loss <loss>=9.408673459833318\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:38 INFO 140202221626752] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:38 INFO 140202221626752] Epoch[88] Batch[0] avg_epoch_loss=9.664405\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:38 INFO 140202221626752] #quality_metric: host=algo-1, epoch=88, batch=0 train loss <loss>=9.66440486907959\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:38 INFO 140202221626752] Epoch[88] Batch[5] avg_epoch_loss=10.348740\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:38 INFO 140202221626752] #quality_metric: host=algo-1, epoch=88, batch=5 train loss <loss>=10.348739782969156\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:38 INFO 140202221626752] Epoch[88] Batch [5]#011Speed: 1939.72 samples/sec#011loss=10.348740\u001b[0m\n",
      "\n",
      "2021-09-12 15:49:48 Uploading - Uploading generated training model\u001b[34m[09/12/2021 15:49:39 INFO 140202221626752] Epoch[88] Batch[10] avg_epoch_loss=9.880565\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:39 INFO 140202221626752] #quality_metric: host=algo-1, epoch=88, batch=10 train loss <loss>=9.318755340576171\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:39 INFO 140202221626752] Epoch[88] Batch [10]#011Speed: 1717.33 samples/sec#011loss=9.318755\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:39 INFO 140202221626752] processed a total of 668 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461778.4514263, \"EndTime\": 1631461779.0430624, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 591.1169052124023, \"count\": 1, \"min\": 591.1169052124023, \"max\": 591.1169052124023}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:39 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=1129.8221355684102 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:39 INFO 140202221626752] #progress_metric: host=algo-1, completed 44.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:39 INFO 140202221626752] #quality_metric: host=algo-1, epoch=88, train loss <loss>=9.88056503642689\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:39 INFO 140202221626752] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:39 INFO 140202221626752] Epoch[89] Batch[0] avg_epoch_loss=9.398695\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:39 INFO 140202221626752] #quality_metric: host=algo-1, epoch=89, batch=0 train loss <loss>=9.39869499206543\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:39 INFO 140202221626752] Epoch[89] Batch[5] avg_epoch_loss=9.760515\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:39 INFO 140202221626752] #quality_metric: host=algo-1, epoch=89, batch=5 train loss <loss>=9.760515054066977\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:39 INFO 140202221626752] Epoch[89] Batch [5]#011Speed: 1987.28 samples/sec#011loss=9.760515\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:39 INFO 140202221626752] Epoch[89] Batch[10] avg_epoch_loss=9.436183\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:39 INFO 140202221626752] #quality_metric: host=algo-1, epoch=89, batch=10 train loss <loss>=9.046985340118407\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:39 INFO 140202221626752] Epoch[89] Batch [10]#011Speed: 1758.57 samples/sec#011loss=9.046985\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:39 INFO 140202221626752] processed a total of 650 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461779.0431504, \"EndTime\": 1631461779.6346128, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 590.9526348114014, \"count\": 1, \"min\": 590.9526348114014, \"max\": 590.9526348114014}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:39 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=1099.7268328258845 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:39 INFO 140202221626752] #progress_metric: host=algo-1, completed 45.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:39 INFO 140202221626752] #quality_metric: host=algo-1, epoch=89, train loss <loss>=9.436183365908535\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:39 INFO 140202221626752] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:39 INFO 140202221626752] Epoch[90] Batch[0] avg_epoch_loss=9.776410\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:39 INFO 140202221626752] #quality_metric: host=algo-1, epoch=90, batch=0 train loss <loss>=9.776410102844238\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:40 INFO 140202221626752] Epoch[90] Batch[5] avg_epoch_loss=9.525408\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:40 INFO 140202221626752] #quality_metric: host=algo-1, epoch=90, batch=5 train loss <loss>=9.525407791137695\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:40 INFO 140202221626752] Epoch[90] Batch [5]#011Speed: 1858.50 samples/sec#011loss=9.525408\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:40 INFO 140202221626752] processed a total of 638 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461779.6346765, \"EndTime\": 1631461780.1882284, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 553.0133247375488, \"count\": 1, \"min\": 553.0133247375488, \"max\": 553.0133247375488}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:40 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=1153.402279162247 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:40 INFO 140202221626752] #progress_metric: host=algo-1, completed 45.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:40 INFO 140202221626752] #quality_metric: host=algo-1, epoch=90, train loss <loss>=9.603749084472657\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:40 INFO 140202221626752] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:40 INFO 140202221626752] Epoch[91] Batch[0] avg_epoch_loss=9.794277\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:40 INFO 140202221626752] #quality_metric: host=algo-1, epoch=91, batch=0 train loss <loss>=9.79427719116211\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:40 INFO 140202221626752] Epoch[91] Batch[5] avg_epoch_loss=9.825682\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:40 INFO 140202221626752] #quality_metric: host=algo-1, epoch=91, batch=5 train loss <loss>=9.825681845347086\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:40 INFO 140202221626752] Epoch[91] Batch [5]#011Speed: 1919.68 samples/sec#011loss=9.825682\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:40 INFO 140202221626752] Epoch[91] Batch[10] avg_epoch_loss=10.081460\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:40 INFO 140202221626752] #quality_metric: host=algo-1, epoch=91, batch=10 train loss <loss>=10.388394355773926\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:40 INFO 140202221626752] Epoch[91] Batch [10]#011Speed: 1497.89 samples/sec#011loss=10.388394\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:40 INFO 140202221626752] processed a total of 655 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461780.1883204, \"EndTime\": 1631461780.8164637, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 627.6049613952637, \"count\": 1, \"min\": 627.6049613952637, \"max\": 627.6049613952637}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:40 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=1043.4432152070274 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:40 INFO 140202221626752] #progress_metric: host=algo-1, completed 46.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:40 INFO 140202221626752] #quality_metric: host=algo-1, epoch=91, train loss <loss>=10.081460259177469\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:40 INFO 140202221626752] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:41 INFO 140202221626752] Epoch[92] Batch[0] avg_epoch_loss=10.862690\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:41 INFO 140202221626752] #quality_metric: host=algo-1, epoch=92, batch=0 train loss <loss>=10.862689971923828\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:41 INFO 140202221626752] Epoch[92] Batch[5] avg_epoch_loss=10.332086\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:41 INFO 140202221626752] #quality_metric: host=algo-1, epoch=92, batch=5 train loss <loss>=10.332086245218912\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:41 INFO 140202221626752] Epoch[92] Batch [5]#011Speed: 1997.92 samples/sec#011loss=10.332086\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:41 INFO 140202221626752] processed a total of 623 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461780.8165488, \"EndTime\": 1631461781.3694077, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 552.3269176483154, \"count\": 1, \"min\": 552.3269176483154, \"max\": 552.3269176483154}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:41 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=1127.6762437424477 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:41 INFO 140202221626752] #progress_metric: host=algo-1, completed 46.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:41 INFO 140202221626752] #quality_metric: host=algo-1, epoch=92, train loss <loss>=10.339681720733642\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:41 INFO 140202221626752] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:41 INFO 140202221626752] Epoch[93] Batch[0] avg_epoch_loss=9.255089\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:41 INFO 140202221626752] #quality_metric: host=algo-1, epoch=93, batch=0 train loss <loss>=9.255088806152344\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:41 INFO 140202221626752] Epoch[93] Batch[5] avg_epoch_loss=10.142771\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:41 INFO 140202221626752] #quality_metric: host=algo-1, epoch=93, batch=5 train loss <loss>=10.142770608266195\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:41 INFO 140202221626752] Epoch[93] Batch [5]#011Speed: 2013.58 samples/sec#011loss=10.142771\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:41 INFO 140202221626752] Epoch[93] Batch[10] avg_epoch_loss=10.385430\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:41 INFO 140202221626752] #quality_metric: host=algo-1, epoch=93, batch=10 train loss <loss>=10.676621246337891\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:41 INFO 140202221626752] Epoch[93] Batch [10]#011Speed: 1932.06 samples/sec#011loss=10.676621\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:41 INFO 140202221626752] processed a total of 654 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461781.3695023, \"EndTime\": 1631461781.9806094, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 610.584020614624, \"count\": 1, \"min\": 610.584020614624, \"max\": 610.584020614624}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:41 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=1070.8898903021716 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:41 INFO 140202221626752] #progress_metric: host=algo-1, completed 47.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:41 INFO 140202221626752] #quality_metric: host=algo-1, epoch=93, train loss <loss>=10.385429989207875\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:41 INFO 140202221626752] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:42 INFO 140202221626752] Epoch[94] Batch[0] avg_epoch_loss=9.890472\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:42 INFO 140202221626752] #quality_metric: host=algo-1, epoch=94, batch=0 train loss <loss>=9.890472412109375\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:42 INFO 140202221626752] Epoch[94] Batch[5] avg_epoch_loss=9.958175\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:42 INFO 140202221626752] #quality_metric: host=algo-1, epoch=94, batch=5 train loss <loss>=9.958174705505371\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:42 INFO 140202221626752] Epoch[94] Batch [5]#011Speed: 1845.85 samples/sec#011loss=9.958175\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:42 INFO 140202221626752] Epoch[94] Batch[10] avg_epoch_loss=10.278903\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:42 INFO 140202221626752] #quality_metric: host=algo-1, epoch=94, batch=10 train loss <loss>=10.663777732849121\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:42 INFO 140202221626752] Epoch[94] Batch [10]#011Speed: 1859.07 samples/sec#011loss=10.663778\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:42 INFO 140202221626752] processed a total of 670 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461781.980694, \"EndTime\": 1631461782.5754187, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 594.249963760376, \"count\": 1, \"min\": 594.249963760376, \"max\": 594.249963760376}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:42 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=1127.2387584336818 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:42 INFO 140202221626752] #progress_metric: host=algo-1, completed 47.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:42 INFO 140202221626752] #quality_metric: host=algo-1, epoch=94, train loss <loss>=10.278903354297984\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:42 INFO 140202221626752] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:42 INFO 140202221626752] Epoch[95] Batch[0] avg_epoch_loss=10.710062\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:42 INFO 140202221626752] #quality_metric: host=algo-1, epoch=95, batch=0 train loss <loss>=10.710062026977539\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:42 INFO 140202221626752] Epoch[95] Batch[5] avg_epoch_loss=10.174856\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:42 INFO 140202221626752] #quality_metric: host=algo-1, epoch=95, batch=5 train loss <loss>=10.174855550130209\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:42 INFO 140202221626752] Epoch[95] Batch [5]#011Speed: 2012.02 samples/sec#011loss=10.174856\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:43 INFO 140202221626752] Epoch[95] Batch[10] avg_epoch_loss=10.420383\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:43 INFO 140202221626752] #quality_metric: host=algo-1, epoch=95, batch=10 train loss <loss>=10.71501636505127\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:43 INFO 140202221626752] Epoch[95] Batch [10]#011Speed: 1872.83 samples/sec#011loss=10.715016\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:43 INFO 140202221626752] processed a total of 641 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461782.5755029, \"EndTime\": 1631461783.169898, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 593.9121246337891, \"count\": 1, \"min\": 593.9121246337891, \"max\": 593.9121246337891}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:43 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=1079.0542318746466 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:43 INFO 140202221626752] #progress_metric: host=algo-1, completed 48.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:43 INFO 140202221626752] #quality_metric: host=algo-1, epoch=95, train loss <loss>=10.420383193276145\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:43 INFO 140202221626752] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:43 INFO 140202221626752] Epoch[96] Batch[0] avg_epoch_loss=9.970479\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:43 INFO 140202221626752] #quality_metric: host=algo-1, epoch=96, batch=0 train loss <loss>=9.970479011535645\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:43 INFO 140202221626752] Epoch[96] Batch[5] avg_epoch_loss=9.839706\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:43 INFO 140202221626752] #quality_metric: host=algo-1, epoch=96, batch=5 train loss <loss>=9.839706261952719\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:43 INFO 140202221626752] Epoch[96] Batch [5]#011Speed: 1797.31 samples/sec#011loss=9.839706\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:43 INFO 140202221626752] Epoch[96] Batch[10] avg_epoch_loss=9.870435\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:43 INFO 140202221626752] #quality_metric: host=algo-1, epoch=96, batch=10 train loss <loss>=9.907309913635254\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:43 INFO 140202221626752] Epoch[96] Batch [10]#011Speed: 1826.38 samples/sec#011loss=9.907310\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:43 INFO 140202221626752] processed a total of 662 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461783.169985, \"EndTime\": 1631461783.7882724, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 617.7551746368408, \"count\": 1, \"min\": 617.7551746368408, \"max\": 617.7551746368408}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:43 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=1071.396585427215 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:43 INFO 140202221626752] #progress_metric: host=algo-1, completed 48.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:43 INFO 140202221626752] #quality_metric: host=algo-1, epoch=96, train loss <loss>=9.87043519453569\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:43 INFO 140202221626752] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:44 INFO 140202221626752] Epoch[97] Batch[0] avg_epoch_loss=10.190667\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:44 INFO 140202221626752] #quality_metric: host=algo-1, epoch=97, batch=0 train loss <loss>=10.190667152404785\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:44 INFO 140202221626752] Epoch[97] Batch[5] avg_epoch_loss=10.013865\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:44 INFO 140202221626752] #quality_metric: host=algo-1, epoch=97, batch=5 train loss <loss>=10.01386547088623\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:44 INFO 140202221626752] Epoch[97] Batch [5]#011Speed: 1895.00 samples/sec#011loss=10.013865\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:44 INFO 140202221626752] Epoch[97] Batch[10] avg_epoch_loss=9.987705\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:44 INFO 140202221626752] #quality_metric: host=algo-1, epoch=97, batch=10 train loss <loss>=9.956313133239746\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:44 INFO 140202221626752] Epoch[97] Batch [10]#011Speed: 1668.14 samples/sec#011loss=9.956313\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:44 INFO 140202221626752] processed a total of 691 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461783.7883613, \"EndTime\": 1631461784.3844771, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 595.5600738525391, \"count\": 1, \"min\": 595.5600738525391, \"max\": 595.5600738525391}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:44 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=1160.0201980419938 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:44 INFO 140202221626752] #progress_metric: host=algo-1, completed 49.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:44 INFO 140202221626752] #quality_metric: host=algo-1, epoch=97, train loss <loss>=9.987705317410557\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:44 INFO 140202221626752] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:44 INFO 140202221626752] Epoch[98] Batch[0] avg_epoch_loss=10.945921\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:44 INFO 140202221626752] #quality_metric: host=algo-1, epoch=98, batch=0 train loss <loss>=10.945920944213867\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:44 INFO 140202221626752] Epoch[98] Batch[5] avg_epoch_loss=10.197956\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:44 INFO 140202221626752] #quality_metric: host=algo-1, epoch=98, batch=5 train loss <loss>=10.197955767313639\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:44 INFO 140202221626752] Epoch[98] Batch [5]#011Speed: 1897.41 samples/sec#011loss=10.197956\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:44 INFO 140202221626752] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461784.384555, \"EndTime\": 1631461784.9281633, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 543.081521987915, \"count\": 1, \"min\": 543.081521987915, \"max\": 543.081521987915}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:44 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=1172.7169231106868 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:44 INFO 140202221626752] #progress_metric: host=algo-1, completed 49.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:44 INFO 140202221626752] #quality_metric: host=algo-1, epoch=98, train loss <loss>=10.13212423324585\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:44 INFO 140202221626752] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:45 INFO 140202221626752] Epoch[99] Batch[0] avg_epoch_loss=9.666066\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:45 INFO 140202221626752] #quality_metric: host=algo-1, epoch=99, batch=0 train loss <loss>=9.66606616973877\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:45 INFO 140202221626752] Epoch[99] Batch[5] avg_epoch_loss=9.434211\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:45 INFO 140202221626752] #quality_metric: host=algo-1, epoch=99, batch=5 train loss <loss>=9.434211095174154\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:45 INFO 140202221626752] Epoch[99] Batch [5]#011Speed: 1941.81 samples/sec#011loss=9.434211\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:45 INFO 140202221626752] Epoch[99] Batch[10] avg_epoch_loss=9.840513\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:45 INFO 140202221626752] #quality_metric: host=algo-1, epoch=99, batch=10 train loss <loss>=10.32807502746582\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:45 INFO 140202221626752] Epoch[99] Batch [10]#011Speed: 1592.78 samples/sec#011loss=10.328075\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:45 INFO 140202221626752] processed a total of 657 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461784.9282317, \"EndTime\": 1631461785.5327368, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 604.0225028991699, \"count\": 1, \"min\": 604.0225028991699, \"max\": 604.0225028991699}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:45 INFO 140202221626752] #throughput_metric: host=algo-1, train throughput=1087.4833228557322 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:45 INFO 140202221626752] #progress_metric: host=algo-1, completed 50.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:45 INFO 140202221626752] #quality_metric: host=algo-1, epoch=99, train loss <loss>=9.840512882579457\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:45 INFO 140202221626752] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:45 INFO 140202221626752] Loading parameters from best epoch (59)\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461785.5328228, \"EndTime\": 1631461785.5430899, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.deserialize.time\": {\"sum\": 9.696483612060547, \"count\": 1, \"min\": 9.696483612060547, \"max\": 9.696483612060547}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:45 INFO 140202221626752] stopping training now\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:45 INFO 140202221626752] #progress_metric: host=algo-1, completed 100 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:45 INFO 140202221626752] Final loss: 9.212921749461781 (occurred at epoch 59)\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:45 INFO 140202221626752] #quality_metric: host=algo-1, train final_loss <loss>=9.212921749461781\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:45 INFO 140202221626752] Worker algo-1 finished training.\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:45 WARNING 140202221626752] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:45 INFO 140202221626752] All workers finished. Serializing model for prediction.\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461785.5431962, \"EndTime\": 1631461785.65555, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"get_graph.time\": {\"sum\": 111.51242256164551, \"count\": 1, \"min\": 111.51242256164551, \"max\": 111.51242256164551}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:45 INFO 140202221626752] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461785.6556296, \"EndTime\": 1631461785.7036006, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"finalize.time\": {\"sum\": 159.60383415222168, \"count\": 1, \"min\": 159.60383415222168, \"max\": 159.60383415222168}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:45 INFO 140202221626752] Serializing to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:45 INFO 140202221626752] Saved checkpoint to \"/opt/ml/model/model_algo-1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461785.7036912, \"EndTime\": 1631461785.710382, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"model.serialize.time\": {\"sum\": 6.639957427978516, \"count\": 1, \"min\": 6.639957427978516, \"max\": 6.639957427978516}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:45 INFO 140202221626752] Successfully serialized the model for prediction.\u001b[0m\n",
      "\u001b[34m[09/12/2021 15:49:45 INFO 140202221626752] No test data passed, skipping evaluation.\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631461785.710468, \"EndTime\": 1631461785.718128, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"setuptime\": {\"sum\": 7.128000259399414, \"count\": 1, \"min\": 7.128000259399414, \"max\": 7.128000259399414}, \"totaltime\": {\"sum\": 61284.66439247131, \"count\": 1, \"min\": 61284.66439247131, \"max\": 61284.66439247131}}}\n",
      "\u001b[0m\n",
      "\n",
      "2021-09-12 15:50:14 Completed - Training job completed\n",
      "ProfilerReport-1631461549: NoIssuesFound\n",
      "Training seconds: 120\n",
      "Billable seconds: 120\n",
      "-------------!Overwriting existing file\n",
      "Uploading file to s3://sagemaker-us-east-2-222195595958/deepar-covid-demo-notebook/data/train/train.json\n",
      "2021-09-12 15:57:05 Starting - Starting the training job...\n",
      "2021-09-12 15:57:29 Starting - Launching requested ML instancesProfilerReport-1631462225: InProgress\n",
      "......\n",
      "2021-09-12 15:58:29 Starting - Preparing the instances for training......\n",
      "2021-09-12 15:59:36 Downloading - Downloading input data\n",
      "2021-09-12 15:59:36 Training - Downloading the training image....\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:10 INFO 140006395942272] Reading default configuration from /opt/amazon/lib/python3.6/site-packages/algorithm/resources/default-input.json: {'_kvstore': 'auto', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_tuning_objective_metric': '', 'cardinality': 'auto', 'dropout_rate': '0.10', 'early_stopping_patience': '', 'embedding_dimension': '10', 'learning_rate': '0.001', 'likelihood': 'student-t', 'mini_batch_size': '128', 'num_cells': '40', 'num_dynamic_feat': 'auto', 'num_eval_samples': '100', 'num_layers': '2', 'test_quantiles': '[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]'}\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:10 INFO 140006395942272] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {'prediction_length': '14', 'time_freq': 'D', 'context_length': '14', 'epochs': '200', 'learning_rate': '5E-4', 'early_stopping_patience': '40', 'mini_batch_size': '64'}\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:10 INFO 140006395942272] Final configuration: {'_kvstore': 'auto', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_tuning_objective_metric': '', 'cardinality': 'auto', 'dropout_rate': '0.10', 'early_stopping_patience': '40', 'embedding_dimension': '10', 'learning_rate': '5E-4', 'likelihood': 'student-t', 'mini_batch_size': '64', 'num_cells': '40', 'num_dynamic_feat': 'auto', 'num_eval_samples': '100', 'num_layers': '2', 'test_quantiles': '[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', 'prediction_length': '14', 'time_freq': 'D', 'context_length': '14', 'epochs': '200'}\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:10 INFO 140006395942272] Detected entry point for worker worker\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:10 INFO 140006395942272] Using early stopping with patience 40\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:10 INFO 140006395942272] random_seed is None\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:10 INFO 140006395942272] [cardinality=auto] `cat` field was NOT found in the file `/opt/ml/input/data/train/train.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:10 INFO 140006395942272] [num_dynamic_feat=auto] `dynamic_feat` field was NOT found in the file `/opt/ml/input/data/train/train.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:10 INFO 140006395942272] Training set statistics:\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:10 INFO 140006395942272] Integer time series\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:10 INFO 140006395942272] number of time series: 41\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:10 INFO 140006395942272] number of observations: 13498\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:10 INFO 140006395942272] mean target length: 329.219512195122\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:10 INFO 140006395942272] min/mean/max target: 0.0/15471344.522966366/353205536.0\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:10 INFO 140006395942272] mean abs(target): 15471344.522966366\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:10 INFO 140006395942272] contains missing values: no\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:10 INFO 140006395942272] Small number of time series. Doing 16 passes over dataset with prob 0.975609756097561 per epoch.\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:10 INFO 140006395942272] No test channel found not running evaluations\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:10 INFO 140006395942272] #memory_usage::<batchbuffer> = 2.2568130493164062 mb\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:10 INFO 140006395942272] nvidia-smi took: 0.025220155715942383 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:10 INFO 140006395942272] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:10 INFO 140006395942272] Create Store: local\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462410.5738256, \"EndTime\": 1631462410.65535, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"get_graph.time\": {\"sum\": 79.48446273803711, \"count\": 1, \"min\": 79.48446273803711, \"max\": 79.48446273803711}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:10 INFO 140006395942272] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:10 INFO 140006395942272] #memory_usage::<model> = 12 mb\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462410.655419, \"EndTime\": 1631462410.7654915, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"initialize.time\": {\"sum\": 191.54834747314453, \"count\": 1, \"min\": 191.54834747314453, \"max\": 191.54834747314453}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:11 INFO 140006395942272] Epoch[0] Batch[0] avg_epoch_loss=14.994324\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:11 INFO 140006395942272] #quality_metric: host=algo-1, epoch=0, batch=0 train loss <loss>=14.99432373046875\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:11 INFO 140006395942272] Epoch[0] Batch[5] avg_epoch_loss=14.069825\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:11 INFO 140006395942272] #quality_metric: host=algo-1, epoch=0, batch=5 train loss <loss>=14.069825172424316\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:11 INFO 140006395942272] Epoch[0] Batch [5]#011Speed: 1851.71 samples/sec#011loss=14.069825\u001b[0m\n",
      "\u001b[34m/opt/amazon/python3.6/lib/python3.6/contextlib.py:99: DeprecationWarning: generator 'local_timer' raised StopIteration\n",
      "  self.gen.throw(type, value, traceback)\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:11 INFO 140006395942272] processed a total of 616 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462410.7655573, \"EndTime\": 1631462411.4110093, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"epochs\": {\"sum\": 200.0, \"count\": 1, \"min\": 200, \"max\": 200}, \"update.time\": {\"sum\": 645.374059677124, \"count\": 1, \"min\": 645.374059677124, \"max\": 645.374059677124}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:11 INFO 140006395942272] #throughput_metric: host=algo-1, train throughput=954.2832136889522 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:11 INFO 140006395942272] #progress_metric: host=algo-1, completed 0.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:11 INFO 140006395942272] #quality_metric: host=algo-1, epoch=0, train loss <loss>=13.99860496520996\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:11 INFO 140006395942272] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:11 INFO 140006395942272] Saved checkpoint to \"/opt/ml/model/state_655fb939-c533-47ef-82ec-a56b80b2ca51-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462411.4111056, \"EndTime\": 1631462411.4320242, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 20.358800888061523, \"count\": 1, \"min\": 20.358800888061523, \"max\": 20.358800888061523}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:11 INFO 140006395942272] Epoch[1] Batch[0] avg_epoch_loss=10.934529\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:11 INFO 140006395942272] #quality_metric: host=algo-1, epoch=1, batch=0 train loss <loss>=10.934529304504395\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:11 INFO 140006395942272] Epoch[1] Batch[5] avg_epoch_loss=12.552156\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:11 INFO 140006395942272] #quality_metric: host=algo-1, epoch=1, batch=5 train loss <loss>=12.55215581258138\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:11 INFO 140006395942272] Epoch[1] Batch [5]#011Speed: 1515.41 samples/sec#011loss=12.552156\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:12 INFO 140006395942272] processed a total of 631 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462411.4321015, \"EndTime\": 1631462412.1169465, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 684.7858428955078, \"count\": 1, \"min\": 684.7858428955078, \"max\": 684.7858428955078}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:12 INFO 140006395942272] #throughput_metric: host=algo-1, train throughput=921.294621824811 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:12 INFO 140006395942272] #progress_metric: host=algo-1, completed 1.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:12 INFO 140006395942272] #quality_metric: host=algo-1, epoch=1, train loss <loss>=12.586376667022705\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:12 INFO 140006395942272] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:12 INFO 140006395942272] Saved checkpoint to \"/opt/ml/model/state_5dc498d4-6ab1-4dd7-b9ba-75d62b0e5be3-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462412.1170301, \"EndTime\": 1631462412.1361318, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 18.623828887939453, \"count\": 1, \"min\": 18.623828887939453, \"max\": 18.623828887939453}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:12 INFO 140006395942272] Epoch[2] Batch[0] avg_epoch_loss=12.460551\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:12 INFO 140006395942272] #quality_metric: host=algo-1, epoch=2, batch=0 train loss <loss>=12.460551261901855\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:12 INFO 140006395942272] Epoch[2] Batch[5] avg_epoch_loss=12.139872\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:12 INFO 140006395942272] #quality_metric: host=algo-1, epoch=2, batch=5 train loss <loss>=12.139871597290039\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:12 INFO 140006395942272] Epoch[2] Batch [5]#011Speed: 1464.66 samples/sec#011loss=12.139872\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:12 INFO 140006395942272] Epoch[2] Batch[10] avg_epoch_loss=12.779381\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:12 INFO 140006395942272] #quality_metric: host=algo-1, epoch=2, batch=10 train loss <loss>=13.546793365478516\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:12 INFO 140006395942272] Epoch[2] Batch [10]#011Speed: 1551.19 samples/sec#011loss=13.546793\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:12 INFO 140006395942272] processed a total of 646 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462412.1362026, \"EndTime\": 1631462412.8854945, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 749.2332458496094, \"count\": 1, \"min\": 749.2332458496094, \"max\": 749.2332458496094}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:12 INFO 140006395942272] #throughput_metric: host=algo-1, train throughput=862.0718853413248 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:12 INFO 140006395942272] #progress_metric: host=algo-1, completed 1.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:12 INFO 140006395942272] #quality_metric: host=algo-1, epoch=2, train loss <loss>=12.779381491921164\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:12 INFO 140006395942272] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:13 INFO 140006395942272] Epoch[3] Batch[0] avg_epoch_loss=11.755954\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:13 INFO 140006395942272] #quality_metric: host=algo-1, epoch=3, batch=0 train loss <loss>=11.755953788757324\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:13 INFO 140006395942272] Epoch[3] Batch[5] avg_epoch_loss=12.000761\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:13 INFO 140006395942272] #quality_metric: host=algo-1, epoch=3, batch=5 train loss <loss>=12.000760555267334\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:13 INFO 140006395942272] Epoch[3] Batch [5]#011Speed: 2146.82 samples/sec#011loss=12.000761\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:13 INFO 140006395942272] processed a total of 639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462412.885582, \"EndTime\": 1631462413.4196367, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 533.5609912872314, \"count\": 1, \"min\": 533.5609912872314, \"max\": 533.5609912872314}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:13 INFO 140006395942272] #throughput_metric: host=algo-1, train throughput=1197.3355765652552 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:13 INFO 140006395942272] #progress_metric: host=algo-1, completed 2.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:13 INFO 140006395942272] #quality_metric: host=algo-1, epoch=3, train loss <loss>=11.725217533111572\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:13 INFO 140006395942272] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:13 INFO 140006395942272] Saved checkpoint to \"/opt/ml/model/state_5e0221a9-f39b-4b6d-9f9f-fd48cbd82f62-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462413.4197223, \"EndTime\": 1631462413.4387257, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 18.485307693481445, \"count\": 1, \"min\": 18.485307693481445, \"max\": 18.485307693481445}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:13 INFO 140006395942272] Epoch[4] Batch[0] avg_epoch_loss=12.055577\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:13 INFO 140006395942272] #quality_metric: host=algo-1, epoch=4, batch=0 train loss <loss>=12.055577278137207\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:13 INFO 140006395942272] Epoch[4] Batch[5] avg_epoch_loss=12.048296\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:13 INFO 140006395942272] #quality_metric: host=algo-1, epoch=4, batch=5 train loss <loss>=12.048295815785727\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:13 INFO 140006395942272] Epoch[4] Batch [5]#011Speed: 2051.73 samples/sec#011loss=12.048296\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:13 INFO 140006395942272] processed a total of 631 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462413.4388044, \"EndTime\": 1631462413.9814065, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 542.5450801849365, \"count\": 1, \"min\": 542.5450801849365, \"max\": 542.5450801849365}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:13 INFO 140006395942272] #throughput_metric: host=algo-1, train throughput=1162.8009196590074 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:13 INFO 140006395942272] #progress_metric: host=algo-1, completed 2.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:13 INFO 140006395942272] #quality_metric: host=algo-1, epoch=4, train loss <loss>=11.81630563735962\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:13 INFO 140006395942272] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:14 INFO 140006395942272] Epoch[5] Batch[0] avg_epoch_loss=12.984858\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:14 INFO 140006395942272] #quality_metric: host=algo-1, epoch=5, batch=0 train loss <loss>=12.984857559204102\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:14 INFO 140006395942272] Epoch[5] Batch[5] avg_epoch_loss=11.658638\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:14 INFO 140006395942272] #quality_metric: host=algo-1, epoch=5, batch=5 train loss <loss>=11.65863831837972\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:14 INFO 140006395942272] Epoch[5] Batch [5]#011Speed: 1656.78 samples/sec#011loss=11.658638\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:14 INFO 140006395942272] Epoch[5] Batch[10] avg_epoch_loss=11.001141\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:14 INFO 140006395942272] #quality_metric: host=algo-1, epoch=5, batch=10 train loss <loss>=10.212143516540527\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:14 INFO 140006395942272] Epoch[5] Batch [10]#011Speed: 2022.41 samples/sec#011loss=10.212144\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:14 INFO 140006395942272] processed a total of 645 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462413.9814856, \"EndTime\": 1631462414.593406, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 611.3712787628174, \"count\": 1, \"min\": 611.3712787628174, \"max\": 611.3712787628174}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:14 INFO 140006395942272] #throughput_metric: host=algo-1, train throughput=1054.8030173477612 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:14 INFO 140006395942272] #progress_metric: host=algo-1, completed 3.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:14 INFO 140006395942272] #quality_metric: host=algo-1, epoch=5, train loss <loss>=11.001140681180088\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:14 INFO 140006395942272] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:14 INFO 140006395942272] Saved checkpoint to \"/opt/ml/model/state_5ebdca8f-67c5-46a6-8e22-e8f343473f97-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462414.5934868, \"EndTime\": 1631462414.6068408, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 12.821674346923828, \"count\": 1, \"min\": 12.821674346923828, \"max\": 12.821674346923828}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:14 INFO 140006395942272] Epoch[6] Batch[0] avg_epoch_loss=10.367051\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:14 INFO 140006395942272] #quality_metric: host=algo-1, epoch=6, batch=0 train loss <loss>=10.367051124572754\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:15 INFO 140006395942272] Epoch[6] Batch[5] avg_epoch_loss=11.228414\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:15 INFO 140006395942272] #quality_metric: host=algo-1, epoch=6, batch=5 train loss <loss>=11.228414058685303\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:15 INFO 140006395942272] Epoch[6] Batch [5]#011Speed: 1685.41 samples/sec#011loss=11.228414\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:15 INFO 140006395942272] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462414.6069112, \"EndTime\": 1631462415.1804802, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 573.5089778900146, \"count\": 1, \"min\": 573.5089778900146, \"max\": 573.5089778900146}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:15 INFO 140006395942272] #throughput_metric: host=algo-1, train throughput=1103.4862057700605 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:15 INFO 140006395942272] #progress_metric: host=algo-1, completed 3.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:15 INFO 140006395942272] #quality_metric: host=algo-1, epoch=6, train loss <loss>=11.137936782836913\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:15 INFO 140006395942272] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:15 INFO 140006395942272] Epoch[7] Batch[0] avg_epoch_loss=12.221642\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:15 INFO 140006395942272] #quality_metric: host=algo-1, epoch=7, batch=0 train loss <loss>=12.221641540527344\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:15 INFO 140006395942272] Epoch[7] Batch[5] avg_epoch_loss=10.987233\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:15 INFO 140006395942272] #quality_metric: host=algo-1, epoch=7, batch=5 train loss <loss>=10.98723316192627\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:15 INFO 140006395942272] Epoch[7] Batch [5]#011Speed: 2071.27 samples/sec#011loss=10.987233\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:15 INFO 140006395942272] processed a total of 640 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462415.1805708, \"EndTime\": 1631462415.7082818, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 527.1661281585693, \"count\": 1, \"min\": 527.1661281585693, \"max\": 527.1661281585693}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:15 INFO 140006395942272] #throughput_metric: host=algo-1, train throughput=1213.7772470338957 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:15 INFO 140006395942272] #progress_metric: host=algo-1, completed 4.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:15 INFO 140006395942272] #quality_metric: host=algo-1, epoch=7, train loss <loss>=10.942751502990722\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:15 INFO 140006395942272] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:15 INFO 140006395942272] Saved checkpoint to \"/opt/ml/model/state_46f55db1-21e5-476f-8429-cd528c98d669-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462415.708362, \"EndTime\": 1631462415.7216513, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 12.732505798339844, \"count\": 1, \"min\": 12.732505798339844, \"max\": 12.732505798339844}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:15 INFO 140006395942272] Epoch[8] Batch[0] avg_epoch_loss=10.831705\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:15 INFO 140006395942272] #quality_metric: host=algo-1, epoch=8, batch=0 train loss <loss>=10.831705093383789\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:16 INFO 140006395942272] Epoch[8] Batch[5] avg_epoch_loss=10.898956\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:16 INFO 140006395942272] #quality_metric: host=algo-1, epoch=8, batch=5 train loss <loss>=10.898955663045248\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:16 INFO 140006395942272] Epoch[8] Batch [5]#011Speed: 2053.45 samples/sec#011loss=10.898956\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:16 INFO 140006395942272] processed a total of 613 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462415.7217224, \"EndTime\": 1631462416.2567306, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 534.9488258361816, \"count\": 1, \"min\": 534.9488258361816, \"max\": 534.9488258361816}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:16 INFO 140006395942272] #throughput_metric: host=algo-1, train throughput=1145.6236889488732 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:16 INFO 140006395942272] #progress_metric: host=algo-1, completed 4.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:16 INFO 140006395942272] #quality_metric: host=algo-1, epoch=8, train loss <loss>=10.909886360168457\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:16 INFO 140006395942272] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:16 INFO 140006395942272] Saved checkpoint to \"/opt/ml/model/state_1b1ba2b9-8e44-4592-abf9-8265dadc177d-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462416.256824, \"EndTime\": 1631462416.2702003, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 12.767314910888672, \"count\": 1, \"min\": 12.767314910888672, \"max\": 12.767314910888672}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:16 INFO 140006395942272] Epoch[9] Batch[0] avg_epoch_loss=10.671526\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:16 INFO 140006395942272] #quality_metric: host=algo-1, epoch=9, batch=0 train loss <loss>=10.671525955200195\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:16 INFO 140006395942272] Epoch[9] Batch[5] avg_epoch_loss=11.258801\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:16 INFO 140006395942272] #quality_metric: host=algo-1, epoch=9, batch=5 train loss <loss>=11.258801142374674\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:16 INFO 140006395942272] Epoch[9] Batch [5]#011Speed: 2053.44 samples/sec#011loss=11.258801\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:16 INFO 140006395942272] Epoch[9] Batch[10] avg_epoch_loss=10.833720\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:16 INFO 140006395942272] #quality_metric: host=algo-1, epoch=9, batch=10 train loss <loss>=10.32362232208252\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:16 INFO 140006395942272] Epoch[9] Batch [10]#011Speed: 2000.81 samples/sec#011loss=10.323622\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:16 INFO 140006395942272] processed a total of 645 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462416.2702951, \"EndTime\": 1631462416.8385704, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 568.2117938995361, \"count\": 1, \"min\": 568.2117938995361, \"max\": 568.2117938995361}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:16 INFO 140006395942272] #throughput_metric: host=algo-1, train throughput=1134.9034420555847 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:16 INFO 140006395942272] #progress_metric: host=algo-1, completed 5.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:16 INFO 140006395942272] #quality_metric: host=algo-1, epoch=9, train loss <loss>=10.833719860423695\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:16 INFO 140006395942272] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:16 INFO 140006395942272] Saved checkpoint to \"/opt/ml/model/state_eace0c41-d946-4a67-86aa-9b3ef4187ae6-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462416.838654, \"EndTime\": 1631462416.8518033, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 12.64500617980957, \"count\": 1, \"min\": 12.64500617980957, \"max\": 12.64500617980957}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:17 INFO 140006395942272] Epoch[10] Batch[0] avg_epoch_loss=11.495455\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:17 INFO 140006395942272] #quality_metric: host=algo-1, epoch=10, batch=0 train loss <loss>=11.495454788208008\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:17 INFO 140006395942272] Epoch[10] Batch[5] avg_epoch_loss=10.734778\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:17 INFO 140006395942272] #quality_metric: host=algo-1, epoch=10, batch=5 train loss <loss>=10.734777609507242\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:17 INFO 140006395942272] Epoch[10] Batch [5]#011Speed: 1782.13 samples/sec#011loss=10.734778\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:17 INFO 140006395942272] processed a total of 639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462416.8518693, \"EndTime\": 1631462417.4336107, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 581.6824436187744, \"count\": 1, \"min\": 581.6824436187744, \"max\": 581.6824436187744}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:17 INFO 140006395942272] #throughput_metric: host=algo-1, train throughput=1098.2921987396621 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:17 INFO 140006395942272] #progress_metric: host=algo-1, completed 5.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:17 INFO 140006395942272] #quality_metric: host=algo-1, epoch=10, train loss <loss>=10.539905738830566\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:17 INFO 140006395942272] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:17 INFO 140006395942272] Saved checkpoint to \"/opt/ml/model/state_7af0b142-be0b-4bb4-972e-cfbdbd6704dd-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462417.4337041, \"EndTime\": 1631462417.4530828, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 18.818140029907227, \"count\": 1, \"min\": 18.818140029907227, \"max\": 18.818140029907227}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:17 INFO 140006395942272] Epoch[11] Batch[0] avg_epoch_loss=10.645989\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:17 INFO 140006395942272] #quality_metric: host=algo-1, epoch=11, batch=0 train loss <loss>=10.645989418029785\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:17 INFO 140006395942272] Epoch[11] Batch[5] avg_epoch_loss=11.014434\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:17 INFO 140006395942272] #quality_metric: host=algo-1, epoch=11, batch=5 train loss <loss>=11.014434178670248\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:17 INFO 140006395942272] Epoch[11] Batch [5]#011Speed: 1918.37 samples/sec#011loss=11.014434\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:18 INFO 140006395942272] processed a total of 610 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462417.4531624, \"EndTime\": 1631462418.035094, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 581.8672180175781, \"count\": 1, \"min\": 581.8672180175781, \"max\": 581.8672180175781}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:18 INFO 140006395942272] #throughput_metric: host=algo-1, train throughput=1048.1296857608584 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:18 INFO 140006395942272] #progress_metric: host=algo-1, completed 6.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:18 INFO 140006395942272] #quality_metric: host=algo-1, epoch=11, train loss <loss>=10.848756217956543\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:18 INFO 140006395942272] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:18 INFO 140006395942272] Epoch[12] Batch[0] avg_epoch_loss=10.656822\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:18 INFO 140006395942272] #quality_metric: host=algo-1, epoch=12, batch=0 train loss <loss>=10.656822204589844\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:18 INFO 140006395942272] Epoch[12] Batch[5] avg_epoch_loss=10.600955\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:18 INFO 140006395942272] #quality_metric: host=algo-1, epoch=12, batch=5 train loss <loss>=10.600955327351889\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:18 INFO 140006395942272] Epoch[12] Batch [5]#011Speed: 1720.53 samples/sec#011loss=10.600955\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:18 INFO 140006395942272] processed a total of 625 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462418.0351796, \"EndTime\": 1631462418.6681705, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 632.5082778930664, \"count\": 1, \"min\": 632.5082778930664, \"max\": 632.5082778930664}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:18 INFO 140006395942272] #throughput_metric: host=algo-1, train throughput=987.9666190416787 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:18 INFO 140006395942272] #progress_metric: host=algo-1, completed 6.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:18 INFO 140006395942272] #quality_metric: host=algo-1, epoch=12, train loss <loss>=10.579570007324218\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:18 INFO 140006395942272] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:18 INFO 140006395942272] Epoch[13] Batch[0] avg_epoch_loss=10.458413\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:18 INFO 140006395942272] #quality_metric: host=algo-1, epoch=13, batch=0 train loss <loss>=10.458413124084473\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:19 INFO 140006395942272] Epoch[13] Batch[5] avg_epoch_loss=10.568335\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:19 INFO 140006395942272] #quality_metric: host=algo-1, epoch=13, batch=5 train loss <loss>=10.56833521525065\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:19 INFO 140006395942272] Epoch[13] Batch [5]#011Speed: 1768.33 samples/sec#011loss=10.568335\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:19 INFO 140006395942272] Epoch[13] Batch[10] avg_epoch_loss=10.732676\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:19 INFO 140006395942272] #quality_metric: host=algo-1, epoch=13, batch=10 train loss <loss>=10.929885864257812\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:19 INFO 140006395942272] Epoch[13] Batch [10]#011Speed: 1859.17 samples/sec#011loss=10.929886\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:19 INFO 140006395942272] processed a total of 672 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462418.6682396, \"EndTime\": 1631462419.2804332, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 611.6385459899902, \"count\": 1, \"min\": 611.6385459899902, \"max\": 611.6385459899902}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:19 INFO 140006395942272] #throughput_metric: host=algo-1, train throughput=1098.4650287090124 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:19 INFO 140006395942272] #progress_metric: host=algo-1, completed 7.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:19 INFO 140006395942272] #quality_metric: host=algo-1, epoch=13, train loss <loss>=10.732676419344815\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:19 INFO 140006395942272] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:19 INFO 140006395942272] Epoch[14] Batch[0] avg_epoch_loss=10.462970\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:19 INFO 140006395942272] #quality_metric: host=algo-1, epoch=14, batch=0 train loss <loss>=10.462969779968262\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:19 INFO 140006395942272] Epoch[14] Batch[5] avg_epoch_loss=10.462022\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:19 INFO 140006395942272] #quality_metric: host=algo-1, epoch=14, batch=5 train loss <loss>=10.462021986643473\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:19 INFO 140006395942272] Epoch[14] Batch [5]#011Speed: 2006.46 samples/sec#011loss=10.462022\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:19 INFO 140006395942272] Epoch[14] Batch[10] avg_epoch_loss=10.423553\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:19 INFO 140006395942272] #quality_metric: host=algo-1, epoch=14, batch=10 train loss <loss>=10.377389717102051\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:19 INFO 140006395942272] Epoch[14] Batch [10]#011Speed: 1844.47 samples/sec#011loss=10.377390\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:19 INFO 140006395942272] processed a total of 662 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462419.2805197, \"EndTime\": 1631462419.8625085, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 581.4564228057861, \"count\": 1, \"min\": 581.4564228057861, \"max\": 581.4564228057861}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:19 INFO 140006395942272] #throughput_metric: host=algo-1, train throughput=1138.291698216083 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:19 INFO 140006395942272] #progress_metric: host=algo-1, completed 7.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:19 INFO 140006395942272] #quality_metric: host=algo-1, epoch=14, train loss <loss>=10.423552773215555\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:19 INFO 140006395942272] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:19 INFO 140006395942272] Saved checkpoint to \"/opt/ml/model/state_9ce3c1ec-cef0-4030-bf10-2f420e640dd6-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462419.862589, \"EndTime\": 1631462419.8759706, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 12.903451919555664, \"count\": 1, \"min\": 12.903451919555664, \"max\": 12.903451919555664}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:20 INFO 140006395942272] Epoch[15] Batch[0] avg_epoch_loss=10.798578\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:20 INFO 140006395942272] #quality_metric: host=algo-1, epoch=15, batch=0 train loss <loss>=10.798578262329102\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:20 INFO 140006395942272] Epoch[15] Batch[5] avg_epoch_loss=10.704438\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:20 INFO 140006395942272] #quality_metric: host=algo-1, epoch=15, batch=5 train loss <loss>=10.704437891642252\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:20 INFO 140006395942272] Epoch[15] Batch [5]#011Speed: 1976.05 samples/sec#011loss=10.704438\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:20 INFO 140006395942272] processed a total of 620 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462419.8760448, \"EndTime\": 1631462420.4186995, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 542.5887107849121, \"count\": 1, \"min\": 542.5887107849121, \"max\": 542.5887107849121}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:20 INFO 140006395942272] #throughput_metric: host=algo-1, train throughput=1142.4062473092392 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:20 INFO 140006395942272] #progress_metric: host=algo-1, completed 8.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:20 INFO 140006395942272] #quality_metric: host=algo-1, epoch=15, train loss <loss>=10.721946048736573\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:20 INFO 140006395942272] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:20 INFO 140006395942272] Epoch[16] Batch[0] avg_epoch_loss=10.650146\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:20 INFO 140006395942272] #quality_metric: host=algo-1, epoch=16, batch=0 train loss <loss>=10.650146484375\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:20 INFO 140006395942272] Epoch[16] Batch[5] avg_epoch_loss=10.548619\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:20 INFO 140006395942272] #quality_metric: host=algo-1, epoch=16, batch=5 train loss <loss>=10.548619270324707\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:20 INFO 140006395942272] Epoch[16] Batch [5]#011Speed: 2076.50 samples/sec#011loss=10.548619\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:21 INFO 140006395942272] Epoch[16] Batch[10] avg_epoch_loss=10.425457\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:21 INFO 140006395942272] #quality_metric: host=algo-1, epoch=16, batch=10 train loss <loss>=10.27766284942627\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:21 INFO 140006395942272] Epoch[16] Batch [10]#011Speed: 1823.68 samples/sec#011loss=10.277663\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:21 INFO 140006395942272] processed a total of 690 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462420.4187884, \"EndTime\": 1631462421.0042121, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 584.9239826202393, \"count\": 1, \"min\": 584.9239826202393, \"max\": 584.9239826202393}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:21 INFO 140006395942272] #throughput_metric: host=algo-1, train throughput=1179.3707837695758 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:21 INFO 140006395942272] #progress_metric: host=algo-1, completed 8.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:21 INFO 140006395942272] #quality_metric: host=algo-1, epoch=16, train loss <loss>=10.425457260825418\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:21 INFO 140006395942272] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:21 INFO 140006395942272] Epoch[17] Batch[0] avg_epoch_loss=10.845606\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:21 INFO 140006395942272] #quality_metric: host=algo-1, epoch=17, batch=0 train loss <loss>=10.845605850219727\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:21 INFO 140006395942272] Epoch[17] Batch[5] avg_epoch_loss=10.227026\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:21 INFO 140006395942272] #quality_metric: host=algo-1, epoch=17, batch=5 train loss <loss>=10.227025508880615\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:21 INFO 140006395942272] Epoch[17] Batch [5]#011Speed: 2069.18 samples/sec#011loss=10.227026\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:21 INFO 140006395942272] processed a total of 612 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462421.0043054, \"EndTime\": 1631462421.5297582, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 524.970531463623, \"count\": 1, \"min\": 524.970531463623, \"max\": 524.970531463623}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:21 INFO 140006395942272] #throughput_metric: host=algo-1, train throughput=1165.5171754945327 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:21 INFO 140006395942272] #progress_metric: host=algo-1, completed 9.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:21 INFO 140006395942272] #quality_metric: host=algo-1, epoch=17, train loss <loss>=10.447612190246582\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:21 INFO 140006395942272] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:21 INFO 140006395942272] Epoch[18] Batch[0] avg_epoch_loss=10.051050\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:21 INFO 140006395942272] #quality_metric: host=algo-1, epoch=18, batch=0 train loss <loss>=10.051050186157227\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:21 INFO 140006395942272] Epoch[18] Batch[5] avg_epoch_loss=10.022276\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:21 INFO 140006395942272] #quality_metric: host=algo-1, epoch=18, batch=5 train loss <loss>=10.022276242574057\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:21 INFO 140006395942272] Epoch[18] Batch [5]#011Speed: 2115.80 samples/sec#011loss=10.022276\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:22 INFO 140006395942272] processed a total of 614 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462421.529829, \"EndTime\": 1631462422.0685139, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 538.1186008453369, \"count\": 1, \"min\": 538.1186008453369, \"max\": 538.1186008453369}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:22 INFO 140006395942272] #throughput_metric: host=algo-1, train throughput=1140.7455461890065 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:22 INFO 140006395942272] #progress_metric: host=algo-1, completed 9.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:22 INFO 140006395942272] #quality_metric: host=algo-1, epoch=18, train loss <loss>=9.680403757095338\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:22 INFO 140006395942272] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:22 INFO 140006395942272] Saved checkpoint to \"/opt/ml/model/state_81e653da-4178-4eac-88dc-01d1f5b15c2b-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462422.0686007, \"EndTime\": 1631462422.0877, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 18.548965454101562, \"count\": 1, \"min\": 18.548965454101562, \"max\": 18.548965454101562}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:22 INFO 140006395942272] Epoch[19] Batch[0] avg_epoch_loss=9.928272\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:22 INFO 140006395942272] #quality_metric: host=algo-1, epoch=19, batch=0 train loss <loss>=9.928272247314453\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:22 INFO 140006395942272] Epoch[19] Batch[5] avg_epoch_loss=10.128736\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:22 INFO 140006395942272] #quality_metric: host=algo-1, epoch=19, batch=5 train loss <loss>=10.128736019134521\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:22 INFO 140006395942272] Epoch[19] Batch [5]#011Speed: 2054.54 samples/sec#011loss=10.128736\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:22 INFO 140006395942272] Epoch[19] Batch[10] avg_epoch_loss=10.550288\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:22 INFO 140006395942272] #quality_metric: host=algo-1, epoch=19, batch=10 train loss <loss>=11.056151390075684\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:22 INFO 140006395942272] Epoch[19] Batch [10]#011Speed: 1654.87 samples/sec#011loss=11.056151\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:22 INFO 140006395942272] processed a total of 647 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462422.0877714, \"EndTime\": 1631462422.6965106, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 608.6766719818115, \"count\": 1, \"min\": 608.6766719818115, \"max\": 608.6766719818115}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:22 INFO 140006395942272] #throughput_metric: host=algo-1, train throughput=1062.752743807757 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:22 INFO 140006395942272] #progress_metric: host=algo-1, completed 10.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:22 INFO 140006395942272] #quality_metric: host=algo-1, epoch=19, train loss <loss>=10.550288460471414\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:22 INFO 140006395942272] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:22 INFO 140006395942272] Epoch[20] Batch[0] avg_epoch_loss=10.115387\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:22 INFO 140006395942272] #quality_metric: host=algo-1, epoch=20, batch=0 train loss <loss>=10.115386962890625\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:23 INFO 140006395942272] Epoch[20] Batch[5] avg_epoch_loss=10.164612\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:23 INFO 140006395942272] #quality_metric: host=algo-1, epoch=20, batch=5 train loss <loss>=10.164611975351969\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:23 INFO 140006395942272] Epoch[20] Batch [5]#011Speed: 2071.10 samples/sec#011loss=10.164612\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:23 INFO 140006395942272] Epoch[20] Batch[10] avg_epoch_loss=10.236346\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:23 INFO 140006395942272] #quality_metric: host=algo-1, epoch=20, batch=10 train loss <loss>=10.3224271774292\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:23 INFO 140006395942272] Epoch[20] Batch [10]#011Speed: 1857.47 samples/sec#011loss=10.322427\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:23 INFO 140006395942272] processed a total of 674 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462422.696594, \"EndTime\": 1631462423.2736285, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 576.4944553375244, \"count\": 1, \"min\": 576.4944553375244, \"max\": 576.4944553375244}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:23 INFO 140006395942272] #throughput_metric: host=algo-1, train throughput=1168.8983945721347 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:23 INFO 140006395942272] #progress_metric: host=algo-1, completed 10.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:23 INFO 140006395942272] #quality_metric: host=algo-1, epoch=20, train loss <loss>=10.236346158114346\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:23 INFO 140006395942272] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:23 INFO 140006395942272] Epoch[21] Batch[0] avg_epoch_loss=9.851757\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:23 INFO 140006395942272] #quality_metric: host=algo-1, epoch=21, batch=0 train loss <loss>=9.851757049560547\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:23 INFO 140006395942272] Epoch[21] Batch[5] avg_epoch_loss=10.218736\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:23 INFO 140006395942272] #quality_metric: host=algo-1, epoch=21, batch=5 train loss <loss>=10.218735535939535\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:23 INFO 140006395942272] Epoch[21] Batch [5]#011Speed: 1974.22 samples/sec#011loss=10.218736\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:23 INFO 140006395942272] processed a total of 639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462423.2737103, \"EndTime\": 1631462423.835716, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 561.5530014038086, \"count\": 1, \"min\": 561.5530014038086, \"max\": 561.5530014038086}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:23 INFO 140006395942272] #throughput_metric: host=algo-1, train throughput=1137.6592600138804 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:23 INFO 140006395942272] #progress_metric: host=algo-1, completed 11.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:23 INFO 140006395942272] #quality_metric: host=algo-1, epoch=21, train loss <loss>=10.379564380645752\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:23 INFO 140006395942272] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:24 INFO 140006395942272] Epoch[22] Batch[0] avg_epoch_loss=9.250273\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:24 INFO 140006395942272] #quality_metric: host=algo-1, epoch=22, batch=0 train loss <loss>=9.250272750854492\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:24 INFO 140006395942272] Epoch[22] Batch[5] avg_epoch_loss=10.044428\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:24 INFO 140006395942272] #quality_metric: host=algo-1, epoch=22, batch=5 train loss <loss>=10.044427871704102\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:24 INFO 140006395942272] Epoch[22] Batch [5]#011Speed: 2080.38 samples/sec#011loss=10.044428\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:24 INFO 140006395942272] Epoch[22] Batch[10] avg_epoch_loss=9.421792\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:24 INFO 140006395942272] #quality_metric: host=algo-1, epoch=22, batch=10 train loss <loss>=8.674628162384034\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:24 INFO 140006395942272] Epoch[22] Batch [10]#011Speed: 1936.34 samples/sec#011loss=8.674628\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:24 INFO 140006395942272] processed a total of 643 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462423.8358042, \"EndTime\": 1631462424.4195387, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 583.2328796386719, \"count\": 1, \"min\": 583.2328796386719, \"max\": 583.2328796386719}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:24 INFO 140006395942272] #throughput_metric: host=algo-1, train throughput=1102.2529772146725 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:24 INFO 140006395942272] #progress_metric: host=algo-1, completed 11.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:24 INFO 140006395942272] #quality_metric: host=algo-1, epoch=22, train loss <loss>=9.42179164019498\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:24 INFO 140006395942272] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:24 INFO 140006395942272] Saved checkpoint to \"/opt/ml/model/state_6afd8c36-5d27-4134-ad95-86f50482a780-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462424.4196205, \"EndTime\": 1631462424.4362504, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 16.15118980407715, \"count\": 1, \"min\": 16.15118980407715, \"max\": 16.15118980407715}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:24 INFO 140006395942272] Epoch[23] Batch[0] avg_epoch_loss=9.061571\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:24 INFO 140006395942272] #quality_metric: host=algo-1, epoch=23, batch=0 train loss <loss>=9.06157112121582\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:24 INFO 140006395942272] Epoch[23] Batch[5] avg_epoch_loss=9.804306\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:24 INFO 140006395942272] #quality_metric: host=algo-1, epoch=23, batch=5 train loss <loss>=9.80430555343628\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:24 INFO 140006395942272] Epoch[23] Batch [5]#011Speed: 2118.20 samples/sec#011loss=9.804306\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:24 INFO 140006395942272] processed a total of 614 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462424.4363306, \"EndTime\": 1631462424.9520392, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 515.64621925354, \"count\": 1, \"min\": 515.64621925354, \"max\": 515.64621925354}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:24 INFO 140006395942272] #throughput_metric: host=algo-1, train throughput=1190.482371143695 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:24 INFO 140006395942272] #progress_metric: host=algo-1, completed 12.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:24 INFO 140006395942272] #quality_metric: host=algo-1, epoch=23, train loss <loss>=9.790508365631103\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:24 INFO 140006395942272] loss did not improve\u001b[0m\n",
      "\n",
      "2021-09-12 16:00:29 Training - Training image download completed. Training in progress.\u001b[34m[09/12/2021 16:00:25 INFO 140006395942272] Epoch[24] Batch[0] avg_epoch_loss=9.501193\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:25 INFO 140006395942272] #quality_metric: host=algo-1, epoch=24, batch=0 train loss <loss>=9.501193046569824\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:25 INFO 140006395942272] Epoch[24] Batch[5] avg_epoch_loss=10.216077\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:25 INFO 140006395942272] #quality_metric: host=algo-1, epoch=24, batch=5 train loss <loss>=10.216077327728271\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:25 INFO 140006395942272] Epoch[24] Batch [5]#011Speed: 2117.98 samples/sec#011loss=10.216077\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:25 INFO 140006395942272] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462424.952115, \"EndTime\": 1631462425.4760804, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 523.4372615814209, \"count\": 1, \"min\": 523.4372615814209, \"max\": 523.4372615814209}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:25 INFO 140006395942272] #throughput_metric: host=algo-1, train throughput=1209.0259702055303 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:25 INFO 140006395942272] #progress_metric: host=algo-1, completed 12.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:25 INFO 140006395942272] #quality_metric: host=algo-1, epoch=24, train loss <loss>=10.21421127319336\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:25 INFO 140006395942272] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:25 INFO 140006395942272] Epoch[25] Batch[0] avg_epoch_loss=9.706301\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:25 INFO 140006395942272] #quality_metric: host=algo-1, epoch=25, batch=0 train loss <loss>=9.706300735473633\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:25 INFO 140006395942272] Epoch[25] Batch[5] avg_epoch_loss=9.903668\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:25 INFO 140006395942272] #quality_metric: host=algo-1, epoch=25, batch=5 train loss <loss>=9.90366824467977\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:25 INFO 140006395942272] Epoch[25] Batch [5]#011Speed: 2119.79 samples/sec#011loss=9.903668\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:26 INFO 140006395942272] Epoch[25] Batch[10] avg_epoch_loss=10.762584\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:26 INFO 140006395942272] #quality_metric: host=algo-1, epoch=25, batch=10 train loss <loss>=11.793282508850098\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:26 INFO 140006395942272] Epoch[25] Batch [10]#011Speed: 1868.28 samples/sec#011loss=11.793283\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:26 INFO 140006395942272] processed a total of 644 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462425.476169, \"EndTime\": 1631462426.0432205, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 566.523551940918, \"count\": 1, \"min\": 566.523551940918, \"max\": 566.523551940918}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:26 INFO 140006395942272] #throughput_metric: host=algo-1, train throughput=1136.5151861063273 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:26 INFO 140006395942272] #progress_metric: host=algo-1, completed 13.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:26 INFO 140006395942272] #quality_metric: host=algo-1, epoch=25, train loss <loss>=10.762583819302646\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:26 INFO 140006395942272] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:26 INFO 140006395942272] Epoch[26] Batch[0] avg_epoch_loss=10.808821\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:26 INFO 140006395942272] #quality_metric: host=algo-1, epoch=26, batch=0 train loss <loss>=10.808820724487305\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:26 INFO 140006395942272] Epoch[26] Batch[5] avg_epoch_loss=11.128530\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:26 INFO 140006395942272] #quality_metric: host=algo-1, epoch=26, batch=5 train loss <loss>=11.128530184427897\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:26 INFO 140006395942272] Epoch[26] Batch [5]#011Speed: 1927.64 samples/sec#011loss=11.128530\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:26 INFO 140006395942272] Epoch[26] Batch[10] avg_epoch_loss=11.206614\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:26 INFO 140006395942272] #quality_metric: host=algo-1, epoch=26, batch=10 train loss <loss>=11.300315666198731\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:26 INFO 140006395942272] Epoch[26] Batch [10]#011Speed: 1867.99 samples/sec#011loss=11.300316\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:26 INFO 140006395942272] processed a total of 648 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462426.0433037, \"EndTime\": 1631462426.650098, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 606.3246726989746, \"count\": 1, \"min\": 606.3246726989746, \"max\": 606.3246726989746}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:26 INFO 140006395942272] #throughput_metric: host=algo-1, train throughput=1068.526360028385 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:26 INFO 140006395942272] #progress_metric: host=algo-1, completed 13.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:26 INFO 140006395942272] #quality_metric: host=algo-1, epoch=26, train loss <loss>=11.20661449432373\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:26 INFO 140006395942272] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:26 INFO 140006395942272] Epoch[27] Batch[0] avg_epoch_loss=10.865097\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:26 INFO 140006395942272] #quality_metric: host=algo-1, epoch=27, batch=0 train loss <loss>=10.865097045898438\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:27 INFO 140006395942272] Epoch[27] Batch[5] avg_epoch_loss=10.610985\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:27 INFO 140006395942272] #quality_metric: host=algo-1, epoch=27, batch=5 train loss <loss>=10.610985120137533\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:27 INFO 140006395942272] Epoch[27] Batch [5]#011Speed: 1731.59 samples/sec#011loss=10.610985\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:27 INFO 140006395942272] processed a total of 628 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462426.6501796, \"EndTime\": 1631462427.243442, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 592.7369594573975, \"count\": 1, \"min\": 592.7369594573975, \"max\": 592.7369594573975}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:27 INFO 140006395942272] #throughput_metric: host=algo-1, train throughput=1059.2771449471834 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:27 INFO 140006395942272] #progress_metric: host=algo-1, completed 14.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:27 INFO 140006395942272] #quality_metric: host=algo-1, epoch=27, train loss <loss>=10.228029346466064\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:27 INFO 140006395942272] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:27 INFO 140006395942272] Epoch[28] Batch[0] avg_epoch_loss=9.910769\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:27 INFO 140006395942272] #quality_metric: host=algo-1, epoch=28, batch=0 train loss <loss>=9.910768508911133\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:27 INFO 140006395942272] Epoch[28] Batch[5] avg_epoch_loss=10.427664\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:27 INFO 140006395942272] #quality_metric: host=algo-1, epoch=28, batch=5 train loss <loss>=10.427664279937744\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:27 INFO 140006395942272] Epoch[28] Batch [5]#011Speed: 1996.54 samples/sec#011loss=10.427664\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:27 INFO 140006395942272] Epoch[28] Batch[10] avg_epoch_loss=10.487944\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:27 INFO 140006395942272] #quality_metric: host=algo-1, epoch=28, batch=10 train loss <loss>=10.56027946472168\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:27 INFO 140006395942272] Epoch[28] Batch [10]#011Speed: 1377.84 samples/sec#011loss=10.560279\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:27 INFO 140006395942272] processed a total of 645 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462427.243519, \"EndTime\": 1631462427.8887458, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 644.7272300720215, \"count\": 1, \"min\": 644.7272300720215, \"max\": 644.7272300720215}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:27 INFO 140006395942272] #throughput_metric: host=algo-1, train throughput=1000.2355464898183 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:27 INFO 140006395942272] #progress_metric: host=algo-1, completed 14.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:27 INFO 140006395942272] #quality_metric: host=algo-1, epoch=28, train loss <loss>=10.487943909384988\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:27 INFO 140006395942272] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:28 INFO 140006395942272] Epoch[29] Batch[0] avg_epoch_loss=10.003172\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:28 INFO 140006395942272] #quality_metric: host=algo-1, epoch=29, batch=0 train loss <loss>=10.003171920776367\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:28 INFO 140006395942272] Epoch[29] Batch[5] avg_epoch_loss=9.822045\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:28 INFO 140006395942272] #quality_metric: host=algo-1, epoch=29, batch=5 train loss <loss>=9.822044849395752\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:28 INFO 140006395942272] Epoch[29] Batch [5]#011Speed: 1863.62 samples/sec#011loss=9.822045\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:28 INFO 140006395942272] Epoch[29] Batch[10] avg_epoch_loss=10.001782\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:28 INFO 140006395942272] #quality_metric: host=algo-1, epoch=29, batch=10 train loss <loss>=10.217467498779296\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:28 INFO 140006395942272] Epoch[29] Batch [10]#011Speed: 1755.35 samples/sec#011loss=10.217467\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:28 INFO 140006395942272] processed a total of 668 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462427.8888297, \"EndTime\": 1631462428.5489922, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 659.7185134887695, \"count\": 1, \"min\": 659.7185134887695, \"max\": 659.7185134887695}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:28 INFO 140006395942272] #throughput_metric: host=algo-1, train throughput=1012.3639096581595 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:28 INFO 140006395942272] #progress_metric: host=algo-1, completed 15.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:28 INFO 140006395942272] #quality_metric: host=algo-1, epoch=29, train loss <loss>=10.001782417297363\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:28 INFO 140006395942272] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:28 INFO 140006395942272] Epoch[30] Batch[0] avg_epoch_loss=10.020070\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:28 INFO 140006395942272] #quality_metric: host=algo-1, epoch=30, batch=0 train loss <loss>=10.02007007598877\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:28 INFO 140006395942272] Epoch[30] Batch[5] avg_epoch_loss=10.137257\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:28 INFO 140006395942272] #quality_metric: host=algo-1, epoch=30, batch=5 train loss <loss>=10.137256622314453\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:28 INFO 140006395942272] Epoch[30] Batch [5]#011Speed: 2036.80 samples/sec#011loss=10.137257\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:29 INFO 140006395942272] processed a total of 620 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462428.5490797, \"EndTime\": 1631462429.1041815, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 554.5899868011475, \"count\": 1, \"min\": 554.5899868011475, \"max\": 554.5899868011475}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:29 INFO 140006395942272] #throughput_metric: host=algo-1, train throughput=1117.6888295951944 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:29 INFO 140006395942272] #progress_metric: host=algo-1, completed 15.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:29 INFO 140006395942272] #quality_metric: host=algo-1, epoch=30, train loss <loss>=10.05446548461914\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:29 INFO 140006395942272] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:29 INFO 140006395942272] Epoch[31] Batch[0] avg_epoch_loss=10.742880\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:29 INFO 140006395942272] #quality_metric: host=algo-1, epoch=31, batch=0 train loss <loss>=10.742879867553711\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:29 INFO 140006395942272] Epoch[31] Batch[5] avg_epoch_loss=10.445596\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:29 INFO 140006395942272] #quality_metric: host=algo-1, epoch=31, batch=5 train loss <loss>=10.445596059163412\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:29 INFO 140006395942272] Epoch[31] Batch [5]#011Speed: 2105.61 samples/sec#011loss=10.445596\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:29 INFO 140006395942272] Epoch[31] Batch[10] avg_epoch_loss=10.163716\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:29 INFO 140006395942272] #quality_metric: host=algo-1, epoch=31, batch=10 train loss <loss>=9.825459480285645\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:29 INFO 140006395942272] Epoch[31] Batch [10]#011Speed: 1981.53 samples/sec#011loss=9.825459\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:29 INFO 140006395942272] processed a total of 661 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462429.1042702, \"EndTime\": 1631462429.7167957, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 612.0259761810303, \"count\": 1, \"min\": 612.0259761810303, \"max\": 612.0259761810303}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:29 INFO 140006395942272] #throughput_metric: host=algo-1, train throughput=1079.816344732982 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:29 INFO 140006395942272] #progress_metric: host=algo-1, completed 16.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:29 INFO 140006395942272] #quality_metric: host=algo-1, epoch=31, train loss <loss>=10.163715796037154\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:29 INFO 140006395942272] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:29 INFO 140006395942272] Epoch[32] Batch[0] avg_epoch_loss=10.842167\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:29 INFO 140006395942272] #quality_metric: host=algo-1, epoch=32, batch=0 train loss <loss>=10.842166900634766\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:30 INFO 140006395942272] Epoch[32] Batch[5] avg_epoch_loss=10.781250\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:30 INFO 140006395942272] #quality_metric: host=algo-1, epoch=32, batch=5 train loss <loss>=10.78124968210856\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:30 INFO 140006395942272] Epoch[32] Batch [5]#011Speed: 1952.94 samples/sec#011loss=10.781250\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:30 INFO 140006395942272] Epoch[32] Batch[10] avg_epoch_loss=10.999499\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:30 INFO 140006395942272] #quality_metric: host=algo-1, epoch=32, batch=10 train loss <loss>=11.261398124694825\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:30 INFO 140006395942272] Epoch[32] Batch [10]#011Speed: 2032.66 samples/sec#011loss=11.261398\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:30 INFO 140006395942272] processed a total of 654 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462429.7168748, \"EndTime\": 1631462430.3164833, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 599.146842956543, \"count\": 1, \"min\": 599.146842956543, \"max\": 599.146842956543}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:30 INFO 140006395942272] #throughput_metric: host=algo-1, train throughput=1091.3371423660801 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:30 INFO 140006395942272] #progress_metric: host=algo-1, completed 16.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:30 INFO 140006395942272] #quality_metric: host=algo-1, epoch=32, train loss <loss>=10.999498974193227\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:30 INFO 140006395942272] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:30 INFO 140006395942272] Epoch[33] Batch[0] avg_epoch_loss=9.562727\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:30 INFO 140006395942272] #quality_metric: host=algo-1, epoch=33, batch=0 train loss <loss>=9.562726974487305\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:30 INFO 140006395942272] Epoch[33] Batch[5] avg_epoch_loss=10.292005\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:30 INFO 140006395942272] #quality_metric: host=algo-1, epoch=33, batch=5 train loss <loss>=10.29200537999471\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:30 INFO 140006395942272] Epoch[33] Batch [5]#011Speed: 2105.47 samples/sec#011loss=10.292005\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:30 INFO 140006395942272] processed a total of 604 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462430.3165634, \"EndTime\": 1631462430.8368878, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 519.8402404785156, \"count\": 1, \"min\": 519.8402404785156, \"max\": 519.8402404785156}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:30 INFO 140006395942272] #throughput_metric: host=algo-1, train throughput=1161.6492876565344 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:30 INFO 140006395942272] #progress_metric: host=algo-1, completed 17.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:30 INFO 140006395942272] #quality_metric: host=algo-1, epoch=33, train loss <loss>=10.200199794769286\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:30 INFO 140006395942272] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:31 INFO 140006395942272] Epoch[34] Batch[0] avg_epoch_loss=10.464560\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:31 INFO 140006395942272] #quality_metric: host=algo-1, epoch=34, batch=0 train loss <loss>=10.464559555053711\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:31 INFO 140006395942272] Epoch[34] Batch[5] avg_epoch_loss=10.130219\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:31 INFO 140006395942272] #quality_metric: host=algo-1, epoch=34, batch=5 train loss <loss>=10.130219459533691\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:31 INFO 140006395942272] Epoch[34] Batch [5]#011Speed: 2064.86 samples/sec#011loss=10.130219\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:31 INFO 140006395942272] processed a total of 632 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462430.8369653, \"EndTime\": 1631462431.3686647, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 531.1424732208252, \"count\": 1, \"min\": 531.1424732208252, \"max\": 531.1424732208252}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:31 INFO 140006395942272] #throughput_metric: host=algo-1, train throughput=1189.5952728534173 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:31 INFO 140006395942272] #progress_metric: host=algo-1, completed 17.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:31 INFO 140006395942272] #quality_metric: host=algo-1, epoch=34, train loss <loss>=10.400178241729737\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:31 INFO 140006395942272] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:31 INFO 140006395942272] Epoch[35] Batch[0] avg_epoch_loss=9.988894\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:31 INFO 140006395942272] #quality_metric: host=algo-1, epoch=35, batch=0 train loss <loss>=9.988893508911133\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:31 INFO 140006395942272] Epoch[35] Batch[5] avg_epoch_loss=10.366412\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:31 INFO 140006395942272] #quality_metric: host=algo-1, epoch=35, batch=5 train loss <loss>=10.366412480672201\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:31 INFO 140006395942272] Epoch[35] Batch [5]#011Speed: 2103.25 samples/sec#011loss=10.366412\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:31 INFO 140006395942272] Epoch[35] Batch[10] avg_epoch_loss=10.112660\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:31 INFO 140006395942272] #quality_metric: host=algo-1, epoch=35, batch=10 train loss <loss>=9.808156776428223\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:31 INFO 140006395942272] Epoch[35] Batch [10]#011Speed: 1751.39 samples/sec#011loss=9.808157\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:31 INFO 140006395942272] processed a total of 707 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462431.3687592, \"EndTime\": 1631462431.9805133, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 611.271858215332, \"count\": 1, \"min\": 611.271858215332, \"max\": 611.271858215332}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:31 INFO 140006395942272] #throughput_metric: host=algo-1, train throughput=1156.3707735868995 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:31 INFO 140006395942272] #progress_metric: host=algo-1, completed 18.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:31 INFO 140006395942272] #quality_metric: host=algo-1, epoch=35, train loss <loss>=10.018690665562948\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:31 INFO 140006395942272] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:32 INFO 140006395942272] Epoch[36] Batch[0] avg_epoch_loss=10.414835\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:32 INFO 140006395942272] #quality_metric: host=algo-1, epoch=36, batch=0 train loss <loss>=10.414834976196289\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:32 INFO 140006395942272] Epoch[36] Batch[5] avg_epoch_loss=10.121523\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:32 INFO 140006395942272] #quality_metric: host=algo-1, epoch=36, batch=5 train loss <loss>=10.121523221333822\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:32 INFO 140006395942272] Epoch[36] Batch [5]#011Speed: 2115.81 samples/sec#011loss=10.121523\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:32 INFO 140006395942272] processed a total of 640 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462431.9805994, \"EndTime\": 1631462432.5265617, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 545.4633235931396, \"count\": 1, \"min\": 545.4633235931396, \"max\": 545.4633235931396}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:32 INFO 140006395942272] #throughput_metric: host=algo-1, train throughput=1173.0580827403633 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:32 INFO 140006395942272] #progress_metric: host=algo-1, completed 18.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:32 INFO 140006395942272] #quality_metric: host=algo-1, epoch=36, train loss <loss>=10.230789947509766\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:32 INFO 140006395942272] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:32 INFO 140006395942272] Epoch[37] Batch[0] avg_epoch_loss=9.937617\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:32 INFO 140006395942272] #quality_metric: host=algo-1, epoch=37, batch=0 train loss <loss>=9.937617301940918\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:32 INFO 140006395942272] Epoch[37] Batch[5] avg_epoch_loss=10.284408\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:32 INFO 140006395942272] #quality_metric: host=algo-1, epoch=37, batch=5 train loss <loss>=10.28440793355306\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:32 INFO 140006395942272] Epoch[37] Batch [5]#011Speed: 1651.09 samples/sec#011loss=10.284408\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:33 INFO 140006395942272] Epoch[37] Batch[10] avg_epoch_loss=9.964997\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:33 INFO 140006395942272] #quality_metric: host=algo-1, epoch=37, batch=10 train loss <loss>=9.581703758239746\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:33 INFO 140006395942272] Epoch[37] Batch [10]#011Speed: 1775.31 samples/sec#011loss=9.581704\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:33 INFO 140006395942272] processed a total of 671 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462432.5266442, \"EndTime\": 1631462433.146925, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 619.8115348815918, \"count\": 1, \"min\": 619.8115348815918, \"max\": 619.8115348815918}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:33 INFO 140006395942272] #throughput_metric: host=algo-1, train throughput=1082.37772062791 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:33 INFO 140006395942272] #progress_metric: host=algo-1, completed 19.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:33 INFO 140006395942272] #quality_metric: host=algo-1, epoch=37, train loss <loss>=9.964996944774281\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:33 INFO 140006395942272] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:33 INFO 140006395942272] Epoch[38] Batch[0] avg_epoch_loss=10.157870\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:33 INFO 140006395942272] #quality_metric: host=algo-1, epoch=38, batch=0 train loss <loss>=10.157870292663574\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:33 INFO 140006395942272] Epoch[38] Batch[5] avg_epoch_loss=9.843372\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:33 INFO 140006395942272] #quality_metric: host=algo-1, epoch=38, batch=5 train loss <loss>=9.843372186024984\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:33 INFO 140006395942272] Epoch[38] Batch [5]#011Speed: 1888.43 samples/sec#011loss=9.843372\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:33 INFO 140006395942272] processed a total of 639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462433.1470068, \"EndTime\": 1631462433.7008824, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 553.3766746520996, \"count\": 1, \"min\": 553.3766746520996, \"max\": 553.3766746520996}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:33 INFO 140006395942272] #throughput_metric: host=algo-1, train throughput=1154.437364318647 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:33 INFO 140006395942272] #progress_metric: host=algo-1, completed 19.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:33 INFO 140006395942272] #quality_metric: host=algo-1, epoch=38, train loss <loss>=9.955124378204346\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:33 INFO 140006395942272] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:33 INFO 140006395942272] Epoch[39] Batch[0] avg_epoch_loss=9.512488\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:33 INFO 140006395942272] #quality_metric: host=algo-1, epoch=39, batch=0 train loss <loss>=9.51248836517334\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:34 INFO 140006395942272] Epoch[39] Batch[5] avg_epoch_loss=10.057478\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:34 INFO 140006395942272] #quality_metric: host=algo-1, epoch=39, batch=5 train loss <loss>=10.057477951049805\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:34 INFO 140006395942272] Epoch[39] Batch [5]#011Speed: 2112.50 samples/sec#011loss=10.057478\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:34 INFO 140006395942272] Epoch[39] Batch[10] avg_epoch_loss=10.153810\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:34 INFO 140006395942272] #quality_metric: host=algo-1, epoch=39, batch=10 train loss <loss>=10.269407653808594\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:34 INFO 140006395942272] Epoch[39] Batch [10]#011Speed: 1825.37 samples/sec#011loss=10.269408\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:34 INFO 140006395942272] processed a total of 705 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462433.7009823, \"EndTime\": 1631462434.2955773, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 594.0392017364502, \"count\": 1, \"min\": 594.0392017364502, \"max\": 594.0392017364502}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:34 INFO 140006395942272] #throughput_metric: host=algo-1, train throughput=1186.5612974316048 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:34 INFO 140006395942272] #progress_metric: host=algo-1, completed 20.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:34 INFO 140006395942272] #quality_metric: host=algo-1, epoch=39, train loss <loss>=10.450658639272055\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:34 INFO 140006395942272] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:34 INFO 140006395942272] Epoch[40] Batch[0] avg_epoch_loss=9.620851\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:34 INFO 140006395942272] #quality_metric: host=algo-1, epoch=40, batch=0 train loss <loss>=9.620850563049316\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:34 INFO 140006395942272] Epoch[40] Batch[5] avg_epoch_loss=10.174674\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:34 INFO 140006395942272] #quality_metric: host=algo-1, epoch=40, batch=5 train loss <loss>=10.174673875172934\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:34 INFO 140006395942272] Epoch[40] Batch [5]#011Speed: 2119.57 samples/sec#011loss=10.174674\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:34 INFO 140006395942272] processed a total of 614 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462434.295659, \"EndTime\": 1631462434.8099172, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 513.7677192687988, \"count\": 1, \"min\": 513.7677192687988, \"max\": 513.7677192687988}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:34 INFO 140006395942272] #throughput_metric: host=algo-1, train throughput=1194.8469666748788 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:34 INFO 140006395942272] #progress_metric: host=algo-1, completed 20.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:34 INFO 140006395942272] #quality_metric: host=algo-1, epoch=40, train loss <loss>=10.11527910232544\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:34 INFO 140006395942272] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:35 INFO 140006395942272] Epoch[41] Batch[0] avg_epoch_loss=10.141668\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:35 INFO 140006395942272] #quality_metric: host=algo-1, epoch=41, batch=0 train loss <loss>=10.141668319702148\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:35 INFO 140006395942272] Epoch[41] Batch[5] avg_epoch_loss=10.064596\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:35 INFO 140006395942272] #quality_metric: host=algo-1, epoch=41, batch=5 train loss <loss>=10.064595699310303\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:35 INFO 140006395942272] Epoch[41] Batch [5]#011Speed: 1901.99 samples/sec#011loss=10.064596\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:35 INFO 140006395942272] Epoch[41] Batch[10] avg_epoch_loss=10.048156\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:35 INFO 140006395942272] #quality_metric: host=algo-1, epoch=41, batch=10 train loss <loss>=10.028427886962891\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:35 INFO 140006395942272] Epoch[41] Batch [10]#011Speed: 1437.49 samples/sec#011loss=10.028428\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:35 INFO 140006395942272] processed a total of 697 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462434.8099856, \"EndTime\": 1631462435.4409502, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 630.4011344909668, \"count\": 1, \"min\": 630.4011344909668, \"max\": 630.4011344909668}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:35 INFO 140006395942272] #throughput_metric: host=algo-1, train throughput=1105.4374336144983 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:35 INFO 140006395942272] #progress_metric: host=algo-1, completed 21.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:35 INFO 140006395942272] #quality_metric: host=algo-1, epoch=41, train loss <loss>=10.048155784606934\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:35 INFO 140006395942272] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:35 INFO 140006395942272] Epoch[42] Batch[0] avg_epoch_loss=9.238476\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:35 INFO 140006395942272] #quality_metric: host=algo-1, epoch=42, batch=0 train loss <loss>=9.238475799560547\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:35 INFO 140006395942272] Epoch[42] Batch[5] avg_epoch_loss=9.974087\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:35 INFO 140006395942272] #quality_metric: host=algo-1, epoch=42, batch=5 train loss <loss>=9.974087397257486\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:35 INFO 140006395942272] Epoch[42] Batch [5]#011Speed: 2155.51 samples/sec#011loss=9.974087\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:36 INFO 140006395942272] Epoch[42] Batch[10] avg_epoch_loss=10.218209\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:36 INFO 140006395942272] #quality_metric: host=algo-1, epoch=42, batch=10 train loss <loss>=10.511154174804688\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:36 INFO 140006395942272] Epoch[42] Batch [10]#011Speed: 1703.33 samples/sec#011loss=10.511154\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:36 INFO 140006395942272] processed a total of 647 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462435.4410331, \"EndTime\": 1631462436.0408094, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 599.2891788482666, \"count\": 1, \"min\": 599.2891788482666, \"max\": 599.2891788482666}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:36 INFO 140006395942272] #throughput_metric: host=algo-1, train throughput=1079.4010754574313 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:36 INFO 140006395942272] #progress_metric: host=algo-1, completed 21.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:36 INFO 140006395942272] #quality_metric: host=algo-1, epoch=42, train loss <loss>=10.218208659778941\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:36 INFO 140006395942272] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:36 INFO 140006395942272] Epoch[43] Batch[0] avg_epoch_loss=9.348581\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:36 INFO 140006395942272] #quality_metric: host=algo-1, epoch=43, batch=0 train loss <loss>=9.348581314086914\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:36 INFO 140006395942272] Epoch[43] Batch[5] avg_epoch_loss=9.775427\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:36 INFO 140006395942272] #quality_metric: host=algo-1, epoch=43, batch=5 train loss <loss>=9.775427023569742\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:36 INFO 140006395942272] Epoch[43] Batch [5]#011Speed: 2112.91 samples/sec#011loss=9.775427\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:36 INFO 140006395942272] Epoch[43] Batch[10] avg_epoch_loss=10.194135\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:36 INFO 140006395942272] #quality_metric: host=algo-1, epoch=43, batch=10 train loss <loss>=10.696584129333496\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:36 INFO 140006395942272] Epoch[43] Batch [10]#011Speed: 1855.39 samples/sec#011loss=10.696584\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:36 INFO 140006395942272] processed a total of 660 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462436.0408914, \"EndTime\": 1631462436.6743486, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 632.9925060272217, \"count\": 1, \"min\": 632.9925060272217, \"max\": 632.9925060272217}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:36 INFO 140006395942272] #throughput_metric: host=algo-1, train throughput=1042.4672995740866 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:36 INFO 140006395942272] #progress_metric: host=algo-1, completed 22.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:36 INFO 140006395942272] #quality_metric: host=algo-1, epoch=43, train loss <loss>=10.194134798916904\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:36 INFO 140006395942272] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:36 INFO 140006395942272] Epoch[44] Batch[0] avg_epoch_loss=10.295765\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:36 INFO 140006395942272] #quality_metric: host=algo-1, epoch=44, batch=0 train loss <loss>=10.295764923095703\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:37 INFO 140006395942272] Epoch[44] Batch[5] avg_epoch_loss=10.095134\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:37 INFO 140006395942272] #quality_metric: host=algo-1, epoch=44, batch=5 train loss <loss>=10.095133622487387\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:37 INFO 140006395942272] Epoch[44] Batch [5]#011Speed: 2018.05 samples/sec#011loss=10.095134\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:37 INFO 140006395942272] processed a total of 620 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462436.6744328, \"EndTime\": 1631462437.2319343, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 557.0230484008789, \"count\": 1, \"min\": 557.0230484008789, \"max\": 557.0230484008789}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:37 INFO 140006395942272] #throughput_metric: host=algo-1, train throughput=1112.8169480587837 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:37 INFO 140006395942272] #progress_metric: host=algo-1, completed 22.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:37 INFO 140006395942272] #quality_metric: host=algo-1, epoch=44, train loss <loss>=10.035093975067138\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:37 INFO 140006395942272] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:37 INFO 140006395942272] Epoch[45] Batch[0] avg_epoch_loss=10.335551\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:37 INFO 140006395942272] #quality_metric: host=algo-1, epoch=45, batch=0 train loss <loss>=10.335551261901855\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:37 INFO 140006395942272] Epoch[45] Batch[5] avg_epoch_loss=9.900580\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:37 INFO 140006395942272] #quality_metric: host=algo-1, epoch=45, batch=5 train loss <loss>=9.900579770406088\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:37 INFO 140006395942272] Epoch[45] Batch [5]#011Speed: 1926.66 samples/sec#011loss=9.900580\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:37 INFO 140006395942272] Epoch[45] Batch[10] avg_epoch_loss=9.781999\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:37 INFO 140006395942272] #quality_metric: host=algo-1, epoch=45, batch=10 train loss <loss>=9.639701080322265\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:37 INFO 140006395942272] Epoch[45] Batch [10]#011Speed: 1892.72 samples/sec#011loss=9.639701\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:37 INFO 140006395942272] processed a total of 663 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462437.2320187, \"EndTime\": 1631462437.8321342, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 599.5824337005615, \"count\": 1, \"min\": 599.5824337005615, \"max\": 599.5824337005615}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:37 INFO 140006395942272] #throughput_metric: host=algo-1, train throughput=1105.5576590269281 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:37 INFO 140006395942272] #progress_metric: host=algo-1, completed 23.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:37 INFO 140006395942272] #quality_metric: host=algo-1, epoch=45, train loss <loss>=9.781998547640713\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:37 INFO 140006395942272] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:38 INFO 140006395942272] Epoch[46] Batch[0] avg_epoch_loss=10.074100\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:38 INFO 140006395942272] #quality_metric: host=algo-1, epoch=46, batch=0 train loss <loss>=10.07409954071045\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:38 INFO 140006395942272] Epoch[46] Batch[5] avg_epoch_loss=10.310820\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:38 INFO 140006395942272] #quality_metric: host=algo-1, epoch=46, batch=5 train loss <loss>=10.31082026163737\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:38 INFO 140006395942272] Epoch[46] Batch [5]#011Speed: 1738.77 samples/sec#011loss=10.310820\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:38 INFO 140006395942272] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462437.8322139, \"EndTime\": 1631462438.4456182, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 612.9474639892578, \"count\": 1, \"min\": 612.9474639892578, \"max\": 612.9474639892578}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:38 INFO 140006395942272] #throughput_metric: host=algo-1, train throughput=1032.5093069923 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:38 INFO 140006395942272] #progress_metric: host=algo-1, completed 23.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:38 INFO 140006395942272] #quality_metric: host=algo-1, epoch=46, train loss <loss>=10.35226936340332\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:38 INFO 140006395942272] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:38 INFO 140006395942272] Epoch[47] Batch[0] avg_epoch_loss=10.079659\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:38 INFO 140006395942272] #quality_metric: host=algo-1, epoch=47, batch=0 train loss <loss>=10.079659461975098\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:38 INFO 140006395942272] Epoch[47] Batch[5] avg_epoch_loss=10.199169\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:38 INFO 140006395942272] #quality_metric: host=algo-1, epoch=47, batch=5 train loss <loss>=10.199169158935547\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:38 INFO 140006395942272] Epoch[47] Batch [5]#011Speed: 1946.32 samples/sec#011loss=10.199169\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:39 INFO 140006395942272] Epoch[47] Batch[10] avg_epoch_loss=9.993695\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:39 INFO 140006395942272] #quality_metric: host=algo-1, epoch=47, batch=10 train loss <loss>=9.747126388549805\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:39 INFO 140006395942272] Epoch[47] Batch [10]#011Speed: 1915.80 samples/sec#011loss=9.747126\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:39 INFO 140006395942272] processed a total of 664 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462438.4457026, \"EndTime\": 1631462439.0607252, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 614.5243644714355, \"count\": 1, \"min\": 614.5243644714355, \"max\": 614.5243644714355}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:39 INFO 140006395942272] #throughput_metric: host=algo-1, train throughput=1080.3054999028313 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:39 INFO 140006395942272] #progress_metric: host=algo-1, completed 24.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:39 INFO 140006395942272] #quality_metric: host=algo-1, epoch=47, train loss <loss>=9.993695172396572\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:39 INFO 140006395942272] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:39 INFO 140006395942272] Epoch[48] Batch[0] avg_epoch_loss=10.361125\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:39 INFO 140006395942272] #quality_metric: host=algo-1, epoch=48, batch=0 train loss <loss>=10.361124992370605\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:39 INFO 140006395942272] Epoch[48] Batch[5] avg_epoch_loss=10.253882\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:39 INFO 140006395942272] #quality_metric: host=algo-1, epoch=48, batch=5 train loss <loss>=10.25388240814209\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:39 INFO 140006395942272] Epoch[48] Batch [5]#011Speed: 2067.42 samples/sec#011loss=10.253882\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:39 INFO 140006395942272] Epoch[48] Batch[10] avg_epoch_loss=10.016800\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:39 INFO 140006395942272] #quality_metric: host=algo-1, epoch=48, batch=10 train loss <loss>=9.732301902770995\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:39 INFO 140006395942272] Epoch[48] Batch [10]#011Speed: 1920.74 samples/sec#011loss=9.732302\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:39 INFO 140006395942272] processed a total of 679 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462439.0608072, \"EndTime\": 1631462439.637834, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 576.5624046325684, \"count\": 1, \"min\": 576.5624046325684, \"max\": 576.5624046325684}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:39 INFO 140006395942272] #throughput_metric: host=algo-1, train throughput=1177.4397585529716 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:39 INFO 140006395942272] #progress_metric: host=algo-1, completed 24.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:39 INFO 140006395942272] #quality_metric: host=algo-1, epoch=48, train loss <loss>=10.016800360246139\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:39 INFO 140006395942272] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:39 INFO 140006395942272] Epoch[49] Batch[0] avg_epoch_loss=9.852318\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:39 INFO 140006395942272] #quality_metric: host=algo-1, epoch=49, batch=0 train loss <loss>=9.852317810058594\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:40 INFO 140006395942272] Epoch[49] Batch[5] avg_epoch_loss=9.879217\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:40 INFO 140006395942272] #quality_metric: host=algo-1, epoch=49, batch=5 train loss <loss>=9.87921667098999\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:40 INFO 140006395942272] Epoch[49] Batch [5]#011Speed: 2113.97 samples/sec#011loss=9.879217\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:40 INFO 140006395942272] processed a total of 594 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462439.6379087, \"EndTime\": 1631462440.1480708, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 509.6855163574219, \"count\": 1, \"min\": 509.6855163574219, \"max\": 509.6855163574219}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:40 INFO 140006395942272] #throughput_metric: host=algo-1, train throughput=1165.1105973198053 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:40 INFO 140006395942272] #progress_metric: host=algo-1, completed 25.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:40 INFO 140006395942272] #quality_metric: host=algo-1, epoch=49, train loss <loss>=9.908193588256836\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:40 INFO 140006395942272] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:40 INFO 140006395942272] Epoch[50] Batch[0] avg_epoch_loss=9.676616\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:40 INFO 140006395942272] #quality_metric: host=algo-1, epoch=50, batch=0 train loss <loss>=9.676615715026855\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:40 INFO 140006395942272] Epoch[50] Batch[5] avg_epoch_loss=9.781147\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:40 INFO 140006395942272] #quality_metric: host=algo-1, epoch=50, batch=5 train loss <loss>=9.781147480010986\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:40 INFO 140006395942272] Epoch[50] Batch [5]#011Speed: 2116.12 samples/sec#011loss=9.781147\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:40 INFO 140006395942272] Epoch[50] Batch[10] avg_epoch_loss=9.197479\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:40 INFO 140006395942272] #quality_metric: host=algo-1, epoch=50, batch=10 train loss <loss>=8.497075939178467\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:40 INFO 140006395942272] Epoch[50] Batch [10]#011Speed: 2064.20 samples/sec#011loss=8.497076\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:40 INFO 140006395942272] processed a total of 647 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462440.1481702, \"EndTime\": 1631462440.7000003, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 551.3200759887695, \"count\": 1, \"min\": 551.3200759887695, \"max\": 551.3200759887695}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:40 INFO 140006395942272] #throughput_metric: host=algo-1, train throughput=1173.2932659548342 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:40 INFO 140006395942272] #progress_metric: host=algo-1, completed 25.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:40 INFO 140006395942272] #quality_metric: host=algo-1, epoch=50, train loss <loss>=9.197478597814387\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:40 INFO 140006395942272] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:40 INFO 140006395942272] Saved checkpoint to \"/opt/ml/model/state_c0b9cee7-a694-49f0-bd74-3596d6f50a32-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462440.7000809, \"EndTime\": 1631462440.7131357, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 12.569904327392578, \"count\": 1, \"min\": 12.569904327392578, \"max\": 12.569904327392578}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:40 INFO 140006395942272] Epoch[51] Batch[0] avg_epoch_loss=9.392975\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:40 INFO 140006395942272] #quality_metric: host=algo-1, epoch=51, batch=0 train loss <loss>=9.392974853515625\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:41 INFO 140006395942272] Epoch[51] Batch[5] avg_epoch_loss=9.893995\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:41 INFO 140006395942272] #quality_metric: host=algo-1, epoch=51, batch=5 train loss <loss>=9.893995443979898\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:41 INFO 140006395942272] Epoch[51] Batch [5]#011Speed: 2114.10 samples/sec#011loss=9.893995\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:41 INFO 140006395942272] Epoch[51] Batch[10] avg_epoch_loss=9.328373\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:41 INFO 140006395942272] #quality_metric: host=algo-1, epoch=51, batch=10 train loss <loss>=8.649626684188842\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:41 INFO 140006395942272] Epoch[51] Batch [10]#011Speed: 1685.38 samples/sec#011loss=8.649627\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:41 INFO 140006395942272] processed a total of 647 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462440.7132041, \"EndTime\": 1631462441.2961988, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 582.935094833374, \"count\": 1, \"min\": 582.935094833374, \"max\": 582.935094833374}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:41 INFO 140006395942272] #throughput_metric: host=algo-1, train throughput=1109.7099557664162 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:41 INFO 140006395942272] #progress_metric: host=algo-1, completed 26.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:41 INFO 140006395942272] #quality_metric: host=algo-1, epoch=51, train loss <loss>=9.32837328043851\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:41 INFO 140006395942272] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:41 INFO 140006395942272] Epoch[52] Batch[0] avg_epoch_loss=10.077503\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:41 INFO 140006395942272] #quality_metric: host=algo-1, epoch=52, batch=0 train loss <loss>=10.077503204345703\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:41 INFO 140006395942272] Epoch[52] Batch[5] avg_epoch_loss=9.756572\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:41 INFO 140006395942272] #quality_metric: host=algo-1, epoch=52, batch=5 train loss <loss>=9.756571610768637\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:41 INFO 140006395942272] Epoch[52] Batch [5]#011Speed: 2095.96 samples/sec#011loss=9.756572\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:41 INFO 140006395942272] Epoch[52] Batch[10] avg_epoch_loss=10.210849\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:41 INFO 140006395942272] #quality_metric: host=algo-1, epoch=52, batch=10 train loss <loss>=10.7559814453125\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:41 INFO 140006395942272] Epoch[52] Batch [10]#011Speed: 1988.15 samples/sec#011loss=10.755981\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:41 INFO 140006395942272] processed a total of 655 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462441.2962632, \"EndTime\": 1631462441.8514872, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 554.7566413879395, \"count\": 1, \"min\": 554.7566413879395, \"max\": 554.7566413879395}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:41 INFO 140006395942272] #throughput_metric: host=algo-1, train throughput=1180.4619893670001 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:41 INFO 140006395942272] #progress_metric: host=algo-1, completed 26.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:41 INFO 140006395942272] #quality_metric: host=algo-1, epoch=52, train loss <loss>=10.210848808288574\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:41 INFO 140006395942272] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:42 INFO 140006395942272] Epoch[53] Batch[0] avg_epoch_loss=8.172144\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:42 INFO 140006395942272] #quality_metric: host=algo-1, epoch=53, batch=0 train loss <loss>=8.172143936157227\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:42 INFO 140006395942272] Epoch[53] Batch[5] avg_epoch_loss=9.452901\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:42 INFO 140006395942272] #quality_metric: host=algo-1, epoch=53, batch=5 train loss <loss>=9.452901045481363\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:42 INFO 140006395942272] Epoch[53] Batch [5]#011Speed: 2032.85 samples/sec#011loss=9.452901\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:42 INFO 140006395942272] processed a total of 628 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462441.8515627, \"EndTime\": 1631462442.3931527, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 541.132926940918, \"count\": 1, \"min\": 541.132926940918, \"max\": 541.132926940918}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:42 INFO 140006395942272] #throughput_metric: host=algo-1, train throughput=1160.3068011271705 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:42 INFO 140006395942272] #progress_metric: host=algo-1, completed 27.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:42 INFO 140006395942272] #quality_metric: host=algo-1, epoch=53, train loss <loss>=9.645211982727051\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:42 INFO 140006395942272] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:42 INFO 140006395942272] Epoch[54] Batch[0] avg_epoch_loss=11.388788\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:42 INFO 140006395942272] #quality_metric: host=algo-1, epoch=54, batch=0 train loss <loss>=11.388788223266602\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:42 INFO 140006395942272] Epoch[54] Batch[5] avg_epoch_loss=10.543855\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:42 INFO 140006395942272] #quality_metric: host=algo-1, epoch=54, batch=5 train loss <loss>=10.543854713439941\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:42 INFO 140006395942272] Epoch[54] Batch [5]#011Speed: 2082.21 samples/sec#011loss=10.543855\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:42 INFO 140006395942272] Epoch[54] Batch[10] avg_epoch_loss=10.507672\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:42 INFO 140006395942272] #quality_metric: host=algo-1, epoch=54, batch=10 train loss <loss>=10.46425266265869\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:42 INFO 140006395942272] Epoch[54] Batch [10]#011Speed: 1747.51 samples/sec#011loss=10.464253\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:42 INFO 140006395942272] processed a total of 652 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462442.3932254, \"EndTime\": 1631462442.971735, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 577.9709815979004, \"count\": 1, \"min\": 577.9709815979004, \"max\": 577.9709815979004}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:42 INFO 140006395942272] #throughput_metric: host=algo-1, train throughput=1127.8512269831117 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:42 INFO 140006395942272] #progress_metric: host=algo-1, completed 27.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:42 INFO 140006395942272] #quality_metric: host=algo-1, epoch=54, train loss <loss>=10.507671963084828\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:42 INFO 140006395942272] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:43 INFO 140006395942272] Epoch[55] Batch[0] avg_epoch_loss=9.187878\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:43 INFO 140006395942272] #quality_metric: host=algo-1, epoch=55, batch=0 train loss <loss>=9.187877655029297\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:43 INFO 140006395942272] Epoch[55] Batch[5] avg_epoch_loss=10.126574\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:43 INFO 140006395942272] #quality_metric: host=algo-1, epoch=55, batch=5 train loss <loss>=10.126574198404947\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:43 INFO 140006395942272] Epoch[55] Batch [5]#011Speed: 2097.22 samples/sec#011loss=10.126574\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:43 INFO 140006395942272] Epoch[55] Batch[10] avg_epoch_loss=10.537390\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:43 INFO 140006395942272] #quality_metric: host=algo-1, epoch=55, batch=10 train loss <loss>=11.030368614196778\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:43 INFO 140006395942272] Epoch[55] Batch [10]#011Speed: 2028.76 samples/sec#011loss=11.030369\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:43 INFO 140006395942272] processed a total of 649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462442.971819, \"EndTime\": 1631462443.5859954, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 613.710880279541, \"count\": 1, \"min\": 613.710880279541, \"max\": 613.710880279541}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:43 INFO 140006395942272] #throughput_metric: host=algo-1, train throughput=1057.3024104181895 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:43 INFO 140006395942272] #progress_metric: host=algo-1, completed 28.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:43 INFO 140006395942272] #quality_metric: host=algo-1, epoch=55, train loss <loss>=10.53738984194669\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:43 INFO 140006395942272] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:43 INFO 140006395942272] Epoch[56] Batch[0] avg_epoch_loss=9.590012\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:43 INFO 140006395942272] #quality_metric: host=algo-1, epoch=56, batch=0 train loss <loss>=9.590011596679688\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:44 INFO 140006395942272] Epoch[56] Batch[5] avg_epoch_loss=9.850674\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:44 INFO 140006395942272] #quality_metric: host=algo-1, epoch=56, batch=5 train loss <loss>=9.850674311319986\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:44 INFO 140006395942272] Epoch[56] Batch [5]#011Speed: 1950.67 samples/sec#011loss=9.850674\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:44 INFO 140006395942272] processed a total of 632 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462443.5860765, \"EndTime\": 1631462444.1306448, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 544.0804958343506, \"count\": 1, \"min\": 544.0804958343506, \"max\": 544.0804958343506}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:44 INFO 140006395942272] #throughput_metric: host=algo-1, train throughput=1161.3409167179984 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:44 INFO 140006395942272] #progress_metric: host=algo-1, completed 28.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:44 INFO 140006395942272] #quality_metric: host=algo-1, epoch=56, train loss <loss>=9.833469772338868\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:44 INFO 140006395942272] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:44 INFO 140006395942272] Epoch[57] Batch[0] avg_epoch_loss=11.001892\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:44 INFO 140006395942272] #quality_metric: host=algo-1, epoch=57, batch=0 train loss <loss>=11.00189208984375\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:44 INFO 140006395942272] Epoch[57] Batch[5] avg_epoch_loss=9.930137\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:44 INFO 140006395942272] #quality_metric: host=algo-1, epoch=57, batch=5 train loss <loss>=9.930136998494467\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:44 INFO 140006395942272] Epoch[57] Batch [5]#011Speed: 1984.45 samples/sec#011loss=9.930137\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:44 INFO 140006395942272] processed a total of 618 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462444.1307263, \"EndTime\": 1631462444.6870527, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 555.8507442474365, \"count\": 1, \"min\": 555.8507442474365, \"max\": 555.8507442474365}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:44 INFO 140006395942272] #throughput_metric: host=algo-1, train throughput=1111.5827782023275 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:44 INFO 140006395942272] #progress_metric: host=algo-1, completed 29.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:44 INFO 140006395942272] #quality_metric: host=algo-1, epoch=57, train loss <loss>=9.864357566833496\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:44 INFO 140006395942272] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:44 INFO 140006395942272] Epoch[58] Batch[0] avg_epoch_loss=10.038219\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:44 INFO 140006395942272] #quality_metric: host=algo-1, epoch=58, batch=0 train loss <loss>=10.038219451904297\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:45 INFO 140006395942272] Epoch[58] Batch[5] avg_epoch_loss=9.805387\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:45 INFO 140006395942272] #quality_metric: host=algo-1, epoch=58, batch=5 train loss <loss>=9.805387496948242\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:45 INFO 140006395942272] Epoch[58] Batch [5]#011Speed: 2075.21 samples/sec#011loss=9.805387\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:45 INFO 140006395942272] Epoch[58] Batch[10] avg_epoch_loss=9.857709\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:45 INFO 140006395942272] #quality_metric: host=algo-1, epoch=58, batch=10 train loss <loss>=9.920494842529298\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:45 INFO 140006395942272] Epoch[58] Batch [10]#011Speed: 1723.06 samples/sec#011loss=9.920495\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:45 INFO 140006395942272] processed a total of 649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462444.6871252, \"EndTime\": 1631462445.2776306, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 589.9410247802734, \"count\": 1, \"min\": 589.9410247802734, \"max\": 589.9410247802734}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:45 INFO 140006395942272] #throughput_metric: host=algo-1, train throughput=1099.8970435226724 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:45 INFO 140006395942272] #progress_metric: host=algo-1, completed 29.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:45 INFO 140006395942272] #quality_metric: host=algo-1, epoch=58, train loss <loss>=9.857709017666904\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:45 INFO 140006395942272] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:45 INFO 140006395942272] Epoch[59] Batch[0] avg_epoch_loss=10.456561\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:45 INFO 140006395942272] #quality_metric: host=algo-1, epoch=59, batch=0 train loss <loss>=10.456561088562012\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:45 INFO 140006395942272] Epoch[59] Batch[5] avg_epoch_loss=9.761896\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:45 INFO 140006395942272] #quality_metric: host=algo-1, epoch=59, batch=5 train loss <loss>=9.761896451314291\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:45 INFO 140006395942272] Epoch[59] Batch [5]#011Speed: 2098.48 samples/sec#011loss=9.761896\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:45 INFO 140006395942272] Epoch[59] Batch[10] avg_epoch_loss=9.979808\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:45 INFO 140006395942272] #quality_metric: host=algo-1, epoch=59, batch=10 train loss <loss>=10.241302108764648\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:45 INFO 140006395942272] Epoch[59] Batch [10]#011Speed: 1773.05 samples/sec#011loss=10.241302\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:45 INFO 140006395942272] processed a total of 662 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462445.2777061, \"EndTime\": 1631462445.8688588, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 590.6376838684082, \"count\": 1, \"min\": 590.6376838684082, \"max\": 590.6376838684082}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:45 INFO 140006395942272] #throughput_metric: host=algo-1, train throughput=1120.617123152773 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:45 INFO 140006395942272] #progress_metric: host=algo-1, completed 30.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:45 INFO 140006395942272] #quality_metric: host=algo-1, epoch=59, train loss <loss>=9.979808113791726\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:45 INFO 140006395942272] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:46 INFO 140006395942272] Epoch[60] Batch[0] avg_epoch_loss=9.352268\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:46 INFO 140006395942272] #quality_metric: host=algo-1, epoch=60, batch=0 train loss <loss>=9.35226821899414\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:46 INFO 140006395942272] Epoch[60] Batch[5] avg_epoch_loss=10.128512\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:46 INFO 140006395942272] #quality_metric: host=algo-1, epoch=60, batch=5 train loss <loss>=10.128511905670166\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:46 INFO 140006395942272] Epoch[60] Batch [5]#011Speed: 2060.75 samples/sec#011loss=10.128512\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:46 INFO 140006395942272] Epoch[60] Batch[10] avg_epoch_loss=10.554887\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:46 INFO 140006395942272] #quality_metric: host=algo-1, epoch=60, batch=10 train loss <loss>=11.066537475585937\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:46 INFO 140006395942272] Epoch[60] Batch [10]#011Speed: 1915.54 samples/sec#011loss=11.066537\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:46 INFO 140006395942272] processed a total of 659 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462445.8689287, \"EndTime\": 1631462446.4402025, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 570.8000659942627, \"count\": 1, \"min\": 570.8000659942627, \"max\": 570.8000659942627}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:46 INFO 140006395942272] #throughput_metric: host=algo-1, train throughput=1154.281664225761 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:46 INFO 140006395942272] #progress_metric: host=algo-1, completed 30.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:46 INFO 140006395942272] #quality_metric: host=algo-1, epoch=60, train loss <loss>=10.554887164722789\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:46 INFO 140006395942272] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:46 INFO 140006395942272] Epoch[61] Batch[0] avg_epoch_loss=9.958853\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:46 INFO 140006395942272] #quality_metric: host=algo-1, epoch=61, batch=0 train loss <loss>=9.958852767944336\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:46 INFO 140006395942272] Epoch[61] Batch[5] avg_epoch_loss=10.521740\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:46 INFO 140006395942272] #quality_metric: host=algo-1, epoch=61, batch=5 train loss <loss>=10.521739800771078\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:46 INFO 140006395942272] Epoch[61] Batch [5]#011Speed: 2097.01 samples/sec#011loss=10.521740\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:47 INFO 140006395942272] Epoch[61] Batch[10] avg_epoch_loss=10.051615\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:47 INFO 140006395942272] #quality_metric: host=algo-1, epoch=61, batch=10 train loss <loss>=9.48746633529663\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:47 INFO 140006395942272] Epoch[61] Batch [10]#011Speed: 2040.06 samples/sec#011loss=9.487466\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:47 INFO 140006395942272] processed a total of 652 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462446.4402833, \"EndTime\": 1631462447.0071876, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 566.4410591125488, \"count\": 1, \"min\": 566.4410591125488, \"max\": 566.4410591125488}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:47 INFO 140006395942272] #throughput_metric: host=algo-1, train throughput=1150.8101235779448 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:47 INFO 140006395942272] #progress_metric: host=algo-1, completed 31.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:47 INFO 140006395942272] #quality_metric: host=algo-1, epoch=61, train loss <loss>=10.051615498282693\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:47 INFO 140006395942272] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:47 INFO 140006395942272] Epoch[62] Batch[0] avg_epoch_loss=9.895182\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:47 INFO 140006395942272] #quality_metric: host=algo-1, epoch=62, batch=0 train loss <loss>=9.895181655883789\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:47 INFO 140006395942272] Epoch[62] Batch[5] avg_epoch_loss=10.129971\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:47 INFO 140006395942272] #quality_metric: host=algo-1, epoch=62, batch=5 train loss <loss>=10.12997055053711\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:47 INFO 140006395942272] Epoch[62] Batch [5]#011Speed: 2131.17 samples/sec#011loss=10.129971\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:47 INFO 140006395942272] Epoch[62] Batch[10] avg_epoch_loss=9.732185\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:47 INFO 140006395942272] #quality_metric: host=algo-1, epoch=62, batch=10 train loss <loss>=9.25484209060669\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:47 INFO 140006395942272] Epoch[62] Batch [10]#011Speed: 2014.11 samples/sec#011loss=9.254842\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:47 INFO 140006395942272] processed a total of 656 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462447.0072694, \"EndTime\": 1631462447.6263413, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 618.6063289642334, \"count\": 1, \"min\": 618.6063289642334, \"max\": 618.6063289642334}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:47 INFO 140006395942272] #throughput_metric: host=algo-1, train throughput=1060.2414847200273 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:47 INFO 140006395942272] #progress_metric: host=algo-1, completed 31.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:47 INFO 140006395942272] #quality_metric: host=algo-1, epoch=62, train loss <loss>=9.732184886932373\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:47 INFO 140006395942272] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:47 INFO 140006395942272] Epoch[63] Batch[0] avg_epoch_loss=10.550885\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:47 INFO 140006395942272] #quality_metric: host=algo-1, epoch=63, batch=0 train loss <loss>=10.550885200500488\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:48 INFO 140006395942272] Epoch[63] Batch[5] avg_epoch_loss=10.314583\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:48 INFO 140006395942272] #quality_metric: host=algo-1, epoch=63, batch=5 train loss <loss>=10.31458298365275\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:48 INFO 140006395942272] Epoch[63] Batch [5]#011Speed: 2084.54 samples/sec#011loss=10.314583\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:48 INFO 140006395942272] Epoch[63] Batch[10] avg_epoch_loss=9.988677\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:48 INFO 140006395942272] #quality_metric: host=algo-1, epoch=63, batch=10 train loss <loss>=9.597588729858398\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:48 INFO 140006395942272] Epoch[63] Batch [10]#011Speed: 1720.05 samples/sec#011loss=9.597589\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:48 INFO 140006395942272] processed a total of 659 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462447.6264243, \"EndTime\": 1631462448.2315373, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 604.6478748321533, \"count\": 1, \"min\": 604.6478748321533, \"max\": 604.6478748321533}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:48 INFO 140006395942272] #throughput_metric: host=algo-1, train throughput=1089.6864467986463 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:48 INFO 140006395942272] #progress_metric: host=algo-1, completed 32.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:48 INFO 140006395942272] #quality_metric: host=algo-1, epoch=63, train loss <loss>=9.988676504655318\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:48 INFO 140006395942272] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:48 INFO 140006395942272] Epoch[64] Batch[0] avg_epoch_loss=9.434701\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:48 INFO 140006395942272] #quality_metric: host=algo-1, epoch=64, batch=0 train loss <loss>=9.434700965881348\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:48 INFO 140006395942272] Epoch[64] Batch[5] avg_epoch_loss=10.134146\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:48 INFO 140006395942272] #quality_metric: host=algo-1, epoch=64, batch=5 train loss <loss>=10.134146372477213\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:48 INFO 140006395942272] Epoch[64] Batch [5]#011Speed: 2074.48 samples/sec#011loss=10.134146\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:48 INFO 140006395942272] processed a total of 626 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462448.23162, \"EndTime\": 1631462448.7681339, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 536.0229015350342, \"count\": 1, \"min\": 536.0229015350342, \"max\": 536.0229015350342}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:48 INFO 140006395942272] #throughput_metric: host=algo-1, train throughput=1167.5686882588752 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:48 INFO 140006395942272] #progress_metric: host=algo-1, completed 32.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:48 INFO 140006395942272] #quality_metric: host=algo-1, epoch=64, train loss <loss>=10.20332841873169\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:48 INFO 140006395942272] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:49 INFO 140006395942272] Epoch[65] Batch[0] avg_epoch_loss=11.077612\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:49 INFO 140006395942272] #quality_metric: host=algo-1, epoch=65, batch=0 train loss <loss>=11.077611923217773\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:49 INFO 140006395942272] Epoch[65] Batch[5] avg_epoch_loss=10.090272\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:49 INFO 140006395942272] #quality_metric: host=algo-1, epoch=65, batch=5 train loss <loss>=10.090271949768066\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:49 INFO 140006395942272] Epoch[65] Batch [5]#011Speed: 2071.26 samples/sec#011loss=10.090272\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:49 INFO 140006395942272] processed a total of 622 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462448.7682276, \"EndTime\": 1631462449.2967372, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 527.9407501220703, \"count\": 1, \"min\": 527.9407501220703, \"max\": 527.9407501220703}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:49 INFO 140006395942272] #throughput_metric: host=algo-1, train throughput=1177.8981371574755 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:49 INFO 140006395942272] #progress_metric: host=algo-1, completed 33.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:49 INFO 140006395942272] #quality_metric: host=algo-1, epoch=65, train loss <loss>=9.834410095214844\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:49 INFO 140006395942272] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:49 INFO 140006395942272] Epoch[66] Batch[0] avg_epoch_loss=9.568267\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:49 INFO 140006395942272] #quality_metric: host=algo-1, epoch=66, batch=0 train loss <loss>=9.568266868591309\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:49 INFO 140006395942272] Epoch[66] Batch[5] avg_epoch_loss=10.141032\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:49 INFO 140006395942272] #quality_metric: host=algo-1, epoch=66, batch=5 train loss <loss>=10.141031742095947\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:49 INFO 140006395942272] Epoch[66] Batch [5]#011Speed: 2108.22 samples/sec#011loss=10.141032\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:49 INFO 140006395942272] Epoch[66] Batch[10] avg_epoch_loss=10.127017\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:49 INFO 140006395942272] #quality_metric: host=algo-1, epoch=66, batch=10 train loss <loss>=10.110199356079102\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:49 INFO 140006395942272] Epoch[66] Batch [10]#011Speed: 1789.63 samples/sec#011loss=10.110199\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:49 INFO 140006395942272] processed a total of 682 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462449.2968147, \"EndTime\": 1631462449.8752408, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 577.9225826263428, \"count\": 1, \"min\": 577.9225826263428, \"max\": 577.9225826263428}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:49 INFO 140006395942272] #throughput_metric: host=algo-1, train throughput=1179.8756602091798 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:49 INFO 140006395942272] #progress_metric: host=algo-1, completed 33.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:49 INFO 140006395942272] #quality_metric: host=algo-1, epoch=66, train loss <loss>=10.1270170211792\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:49 INFO 140006395942272] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:50 INFO 140006395942272] Epoch[67] Batch[0] avg_epoch_loss=9.812562\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:50 INFO 140006395942272] #quality_metric: host=algo-1, epoch=67, batch=0 train loss <loss>=9.812561988830566\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:50 INFO 140006395942272] Epoch[67] Batch[5] avg_epoch_loss=10.028929\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:50 INFO 140006395942272] #quality_metric: host=algo-1, epoch=67, batch=5 train loss <loss>=10.028929233551025\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:50 INFO 140006395942272] Epoch[67] Batch [5]#011Speed: 2105.83 samples/sec#011loss=10.028929\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:50 INFO 140006395942272] Epoch[67] Batch[10] avg_epoch_loss=9.971030\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:50 INFO 140006395942272] #quality_metric: host=algo-1, epoch=67, batch=10 train loss <loss>=9.901551818847656\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:50 INFO 140006395942272] Epoch[67] Batch [10]#011Speed: 1679.30 samples/sec#011loss=9.901552\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:50 INFO 140006395942272] processed a total of 657 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462449.8753085, \"EndTime\": 1631462450.4671936, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 591.3829803466797, \"count\": 1, \"min\": 591.3829803466797, \"max\": 591.3829803466797}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:50 INFO 140006395942272] #throughput_metric: host=algo-1, train throughput=1110.7563787746626 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:50 INFO 140006395942272] #progress_metric: host=algo-1, completed 34.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:50 INFO 140006395942272] #quality_metric: host=algo-1, epoch=67, train loss <loss>=9.971030408685857\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:50 INFO 140006395942272] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:50 INFO 140006395942272] Epoch[68] Batch[0] avg_epoch_loss=10.308972\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:50 INFO 140006395942272] #quality_metric: host=algo-1, epoch=68, batch=0 train loss <loss>=10.308972358703613\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:50 INFO 140006395942272] Epoch[68] Batch[5] avg_epoch_loss=9.805112\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:50 INFO 140006395942272] #quality_metric: host=algo-1, epoch=68, batch=5 train loss <loss>=9.805112361907959\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:50 INFO 140006395942272] Epoch[68] Batch [5]#011Speed: 1908.14 samples/sec#011loss=9.805112\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:51 INFO 140006395942272] Epoch[68] Batch[10] avg_epoch_loss=9.953464\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:51 INFO 140006395942272] #quality_metric: host=algo-1, epoch=68, batch=10 train loss <loss>=10.131486701965333\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:51 INFO 140006395942272] Epoch[68] Batch [10]#011Speed: 1844.83 samples/sec#011loss=10.131487\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:51 INFO 140006395942272] processed a total of 658 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462450.4672632, \"EndTime\": 1631462451.0636134, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 595.8750247955322, \"count\": 1, \"min\": 595.8750247955322, \"max\": 595.8750247955322}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:51 INFO 140006395942272] #throughput_metric: host=algo-1, train throughput=1104.033993269032 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:51 INFO 140006395942272] #progress_metric: host=algo-1, completed 34.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:51 INFO 140006395942272] #quality_metric: host=algo-1, epoch=68, train loss <loss>=9.95346433466131\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:51 INFO 140006395942272] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:51 INFO 140006395942272] Epoch[69] Batch[0] avg_epoch_loss=9.410394\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:51 INFO 140006395942272] #quality_metric: host=algo-1, epoch=69, batch=0 train loss <loss>=9.410393714904785\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:51 INFO 140006395942272] Epoch[69] Batch[5] avg_epoch_loss=9.955305\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:51 INFO 140006395942272] #quality_metric: host=algo-1, epoch=69, batch=5 train loss <loss>=9.955304940541586\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:51 INFO 140006395942272] Epoch[69] Batch [5]#011Speed: 1888.60 samples/sec#011loss=9.955305\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:51 INFO 140006395942272] processed a total of 593 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462451.063696, \"EndTime\": 1631462451.619704, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 555.5500984191895, \"count\": 1, \"min\": 555.5500984191895, \"max\": 555.5500984191895}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:51 INFO 140006395942272] #throughput_metric: host=algo-1, train throughput=1067.2007222162151 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:51 INFO 140006395942272] #progress_metric: host=algo-1, completed 35.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:51 INFO 140006395942272] #quality_metric: host=algo-1, epoch=69, train loss <loss>=9.469941425323487\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:51 INFO 140006395942272] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:51 INFO 140006395942272] Epoch[70] Batch[0] avg_epoch_loss=9.730577\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:51 INFO 140006395942272] #quality_metric: host=algo-1, epoch=70, batch=0 train loss <loss>=9.730576515197754\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:52 INFO 140006395942272] Epoch[70] Batch[5] avg_epoch_loss=10.098734\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:52 INFO 140006395942272] #quality_metric: host=algo-1, epoch=70, batch=5 train loss <loss>=10.098734060923258\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:52 INFO 140006395942272] Epoch[70] Batch [5]#011Speed: 2078.25 samples/sec#011loss=10.098734\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:52 INFO 140006395942272] processed a total of 615 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462451.6197836, \"EndTime\": 1631462452.1467986, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 526.4596939086914, \"count\": 1, \"min\": 526.4596939086914, \"max\": 526.4596939086914}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:52 INFO 140006395942272] #throughput_metric: host=algo-1, train throughput=1167.9008050579062 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:52 INFO 140006395942272] #progress_metric: host=algo-1, completed 35.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:52 INFO 140006395942272] #quality_metric: host=algo-1, epoch=70, train loss <loss>=10.138121318817138\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:52 INFO 140006395942272] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:52 INFO 140006395942272] Epoch[71] Batch[0] avg_epoch_loss=9.839622\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:52 INFO 140006395942272] #quality_metric: host=algo-1, epoch=71, batch=0 train loss <loss>=9.839621543884277\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:52 INFO 140006395942272] Epoch[71] Batch[5] avg_epoch_loss=9.894539\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:52 INFO 140006395942272] #quality_metric: host=algo-1, epoch=71, batch=5 train loss <loss>=9.89453919728597\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:52 INFO 140006395942272] Epoch[71] Batch [5]#011Speed: 2103.03 samples/sec#011loss=9.894539\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:52 INFO 140006395942272] Epoch[71] Batch[10] avg_epoch_loss=10.349475\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:52 INFO 140006395942272] #quality_metric: host=algo-1, epoch=71, batch=10 train loss <loss>=10.895397758483886\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:52 INFO 140006395942272] Epoch[71] Batch [10]#011Speed: 2063.53 samples/sec#011loss=10.895398\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:52 INFO 140006395942272] processed a total of 644 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462452.1468728, \"EndTime\": 1631462452.706386, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 558.9799880981445, \"count\": 1, \"min\": 558.9799880981445, \"max\": 558.9799880981445}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:52 INFO 140006395942272] #throughput_metric: host=algo-1, train throughput=1151.836633952577 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:52 INFO 140006395942272] #progress_metric: host=algo-1, completed 36.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:52 INFO 140006395942272] #quality_metric: host=algo-1, epoch=71, train loss <loss>=10.349474906921387\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:52 INFO 140006395942272] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:52 INFO 140006395942272] Epoch[72] Batch[0] avg_epoch_loss=9.710241\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:52 INFO 140006395942272] #quality_metric: host=algo-1, epoch=72, batch=0 train loss <loss>=9.710241317749023\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:53 INFO 140006395942272] Epoch[72] Batch[5] avg_epoch_loss=10.141673\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:53 INFO 140006395942272] #quality_metric: host=algo-1, epoch=72, batch=5 train loss <loss>=10.141672770182291\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:53 INFO 140006395942272] Epoch[72] Batch [5]#011Speed: 1883.39 samples/sec#011loss=10.141673\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:53 INFO 140006395942272] Epoch[72] Batch[10] avg_epoch_loss=10.012801\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:53 INFO 140006395942272] #quality_metric: host=algo-1, epoch=72, batch=10 train loss <loss>=9.858153915405273\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:53 INFO 140006395942272] Epoch[72] Batch [10]#011Speed: 1562.80 samples/sec#011loss=9.858154\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:53 INFO 140006395942272] processed a total of 708 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462452.7064726, \"EndTime\": 1631462453.3759782, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 668.982982635498, \"count\": 1, \"min\": 668.982982635498, \"max\": 668.982982635498}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:53 INFO 140006395942272] #throughput_metric: host=algo-1, train throughput=1058.1358388320468 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:53 INFO 140006395942272] #progress_metric: host=algo-1, completed 36.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:53 INFO 140006395942272] #quality_metric: host=algo-1, epoch=72, train loss <loss>=10.52912982304891\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:53 INFO 140006395942272] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:53 INFO 140006395942272] Epoch[73] Batch[0] avg_epoch_loss=10.097156\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:53 INFO 140006395942272] #quality_metric: host=algo-1, epoch=73, batch=0 train loss <loss>=10.097155570983887\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:53 INFO 140006395942272] Epoch[73] Batch[5] avg_epoch_loss=10.260500\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:53 INFO 140006395942272] #quality_metric: host=algo-1, epoch=73, batch=5 train loss <loss>=10.260500113169352\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:53 INFO 140006395942272] Epoch[73] Batch [5]#011Speed: 2090.92 samples/sec#011loss=10.260500\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:53 INFO 140006395942272] processed a total of 621 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462453.3760548, \"EndTime\": 1631462453.9128745, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 536.3059043884277, \"count\": 1, \"min\": 536.3059043884277, \"max\": 536.3059043884277}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:53 INFO 140006395942272] #throughput_metric: host=algo-1, train throughput=1157.6536296362142 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:53 INFO 140006395942272] #progress_metric: host=algo-1, completed 37.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:53 INFO 140006395942272] #quality_metric: host=algo-1, epoch=73, train loss <loss>=10.033531856536865\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:53 INFO 140006395942272] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:54 INFO 140006395942272] Epoch[74] Batch[0] avg_epoch_loss=9.341982\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:54 INFO 140006395942272] #quality_metric: host=algo-1, epoch=74, batch=0 train loss <loss>=9.341981887817383\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:54 INFO 140006395942272] Epoch[74] Batch[5] avg_epoch_loss=9.935980\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:54 INFO 140006395942272] #quality_metric: host=algo-1, epoch=74, batch=5 train loss <loss>=9.935980319976807\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:54 INFO 140006395942272] Epoch[74] Batch [5]#011Speed: 2108.41 samples/sec#011loss=9.935980\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:54 INFO 140006395942272] Epoch[74] Batch[10] avg_epoch_loss=9.758516\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:54 INFO 140006395942272] #quality_metric: host=algo-1, epoch=74, batch=10 train loss <loss>=9.545558166503906\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:54 INFO 140006395942272] Epoch[74] Batch [10]#011Speed: 1982.65 samples/sec#011loss=9.545558\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:54 INFO 140006395942272] processed a total of 647 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462453.9129596, \"EndTime\": 1631462454.4978561, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 584.4080448150635, \"count\": 1, \"min\": 584.4080448150635, \"max\": 584.4080448150635}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:54 INFO 140006395942272] #throughput_metric: host=algo-1, train throughput=1106.8800964563468 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:54 INFO 140006395942272] #progress_metric: host=algo-1, completed 37.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:54 INFO 140006395942272] #quality_metric: host=algo-1, epoch=74, train loss <loss>=9.758515704761852\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:54 INFO 140006395942272] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:54 INFO 140006395942272] Epoch[75] Batch[0] avg_epoch_loss=9.954127\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:54 INFO 140006395942272] #quality_metric: host=algo-1, epoch=75, batch=0 train loss <loss>=9.954127311706543\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:54 INFO 140006395942272] Epoch[75] Batch[5] avg_epoch_loss=10.231513\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:54 INFO 140006395942272] #quality_metric: host=algo-1, epoch=75, batch=5 train loss <loss>=10.231513341267904\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:54 INFO 140006395942272] Epoch[75] Batch [5]#011Speed: 2055.03 samples/sec#011loss=10.231513\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:55 INFO 140006395942272] Epoch[75] Batch[10] avg_epoch_loss=10.151495\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:55 INFO 140006395942272] #quality_metric: host=algo-1, epoch=75, batch=10 train loss <loss>=10.055471992492675\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:55 INFO 140006395942272] Epoch[75] Batch [10]#011Speed: 1858.23 samples/sec#011loss=10.055472\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:55 INFO 140006395942272] processed a total of 662 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462454.497936, \"EndTime\": 1631462455.0729158, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 574.439525604248, \"count\": 1, \"min\": 574.439525604248, \"max\": 574.439525604248}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:55 INFO 140006395942272] #throughput_metric: host=algo-1, train throughput=1152.1847326964557 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:55 INFO 140006395942272] #progress_metric: host=algo-1, completed 38.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:55 INFO 140006395942272] #quality_metric: host=algo-1, epoch=75, train loss <loss>=10.151494546370072\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:55 INFO 140006395942272] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:55 INFO 140006395942272] Epoch[76] Batch[0] avg_epoch_loss=9.540526\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:55 INFO 140006395942272] #quality_metric: host=algo-1, epoch=76, batch=0 train loss <loss>=9.540526390075684\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:55 INFO 140006395942272] Epoch[76] Batch[5] avg_epoch_loss=9.550112\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:55 INFO 140006395942272] #quality_metric: host=algo-1, epoch=76, batch=5 train loss <loss>=9.55011240641276\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:55 INFO 140006395942272] Epoch[76] Batch [5]#011Speed: 1649.40 samples/sec#011loss=9.550112\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:55 INFO 140006395942272] Epoch[76] Batch[10] avg_epoch_loss=9.658007\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:55 INFO 140006395942272] #quality_metric: host=algo-1, epoch=76, batch=10 train loss <loss>=9.787479591369628\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:55 INFO 140006395942272] Epoch[76] Batch [10]#011Speed: 1895.86 samples/sec#011loss=9.787480\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:55 INFO 140006395942272] processed a total of 678 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462455.0730004, \"EndTime\": 1631462455.6985154, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 625.035285949707, \"count\": 1, \"min\": 625.035285949707, \"max\": 625.035285949707}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:55 INFO 140006395942272] #throughput_metric: host=algo-1, train throughput=1084.5290166832247 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:55 INFO 140006395942272] #progress_metric: host=algo-1, completed 38.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:55 INFO 140006395942272] #quality_metric: host=algo-1, epoch=76, train loss <loss>=9.658006581393154\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:55 INFO 140006395942272] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:55 INFO 140006395942272] Epoch[77] Batch[0] avg_epoch_loss=10.216003\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:55 INFO 140006395942272] #quality_metric: host=algo-1, epoch=77, batch=0 train loss <loss>=10.21600341796875\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:56 INFO 140006395942272] Epoch[77] Batch[5] avg_epoch_loss=9.930863\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:56 INFO 140006395942272] #quality_metric: host=algo-1, epoch=77, batch=5 train loss <loss>=9.93086290359497\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:56 INFO 140006395942272] Epoch[77] Batch [5]#011Speed: 1970.50 samples/sec#011loss=9.930863\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:56 INFO 140006395942272] processed a total of 616 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462455.6986005, \"EndTime\": 1631462456.2394195, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 540.3585433959961, \"count\": 1, \"min\": 540.3585433959961, \"max\": 540.3585433959961}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:56 INFO 140006395942272] #throughput_metric: host=algo-1, train throughput=1139.723841557691 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:56 INFO 140006395942272] #progress_metric: host=algo-1, completed 39.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:56 INFO 140006395942272] #quality_metric: host=algo-1, epoch=77, train loss <loss>=10.041709423065186\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:56 INFO 140006395942272] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:56 INFO 140006395942272] Epoch[78] Batch[0] avg_epoch_loss=9.525713\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:56 INFO 140006395942272] #quality_metric: host=algo-1, epoch=78, batch=0 train loss <loss>=9.525712966918945\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:56 INFO 140006395942272] Epoch[78] Batch[5] avg_epoch_loss=9.668545\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:56 INFO 140006395942272] #quality_metric: host=algo-1, epoch=78, batch=5 train loss <loss>=9.66854476928711\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:56 INFO 140006395942272] Epoch[78] Batch [5]#011Speed: 2112.81 samples/sec#011loss=9.668545\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:56 INFO 140006395942272] Epoch[78] Batch[10] avg_epoch_loss=9.203489\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:56 INFO 140006395942272] #quality_metric: host=algo-1, epoch=78, batch=10 train loss <loss>=8.645422077178955\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:56 INFO 140006395942272] Epoch[78] Batch [10]#011Speed: 2010.36 samples/sec#011loss=8.645422\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:56 INFO 140006395942272] processed a total of 650 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462456.2395058, \"EndTime\": 1631462456.8146007, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 574.6045112609863, \"count\": 1, \"min\": 574.6045112609863, \"max\": 574.6045112609863}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:56 INFO 140006395942272] #throughput_metric: host=algo-1, train throughput=1130.9814860370561 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:56 INFO 140006395942272] #progress_metric: host=algo-1, completed 39.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:56 INFO 140006395942272] #quality_metric: host=algo-1, epoch=78, train loss <loss>=9.203489000147039\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:56 INFO 140006395942272] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:57 INFO 140006395942272] Epoch[79] Batch[0] avg_epoch_loss=10.367587\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:57 INFO 140006395942272] #quality_metric: host=algo-1, epoch=79, batch=0 train loss <loss>=10.367587089538574\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:57 INFO 140006395942272] Epoch[79] Batch[5] avg_epoch_loss=10.025588\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:57 INFO 140006395942272] #quality_metric: host=algo-1, epoch=79, batch=5 train loss <loss>=10.025587717692057\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:57 INFO 140006395942272] Epoch[79] Batch [5]#011Speed: 1771.86 samples/sec#011loss=10.025588\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:57 INFO 140006395942272] Epoch[79] Batch[10] avg_epoch_loss=9.920811\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:57 INFO 140006395942272] #quality_metric: host=algo-1, epoch=79, batch=10 train loss <loss>=9.795079612731934\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:57 INFO 140006395942272] Epoch[79] Batch [10]#011Speed: 1839.44 samples/sec#011loss=9.795080\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:57 INFO 140006395942272] processed a total of 659 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462456.8146832, \"EndTime\": 1631462457.4664018, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 651.2534618377686, \"count\": 1, \"min\": 651.2534618377686, \"max\": 651.2534618377686}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:57 INFO 140006395942272] #throughput_metric: host=algo-1, train throughput=1011.7040690758289 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:57 INFO 140006395942272] #progress_metric: host=algo-1, completed 40.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:57 INFO 140006395942272] #quality_metric: host=algo-1, epoch=79, train loss <loss>=9.920811306346547\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:57 INFO 140006395942272] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:57 INFO 140006395942272] Epoch[80] Batch[0] avg_epoch_loss=9.493565\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:57 INFO 140006395942272] #quality_metric: host=algo-1, epoch=80, batch=0 train loss <loss>=9.49356460571289\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:57 INFO 140006395942272] Epoch[80] Batch[5] avg_epoch_loss=9.683394\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:57 INFO 140006395942272] #quality_metric: host=algo-1, epoch=80, batch=5 train loss <loss>=9.683393637339273\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:57 INFO 140006395942272] Epoch[80] Batch [5]#011Speed: 2038.61 samples/sec#011loss=9.683394\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:58 INFO 140006395942272] Epoch[80] Batch[10] avg_epoch_loss=9.651765\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:58 INFO 140006395942272] #quality_metric: host=algo-1, epoch=80, batch=10 train loss <loss>=9.613809967041016\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:58 INFO 140006395942272] Epoch[80] Batch [10]#011Speed: 1858.49 samples/sec#011loss=9.613810\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:58 INFO 140006395942272] processed a total of 675 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462457.4664876, \"EndTime\": 1631462458.0600133, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 593.0664539337158, \"count\": 1, \"min\": 593.0664539337158, \"max\": 593.0664539337158}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:58 INFO 140006395942272] #throughput_metric: host=algo-1, train throughput=1137.9575043801526 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:58 INFO 140006395942272] #progress_metric: host=algo-1, completed 40.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:58 INFO 140006395942272] #quality_metric: host=algo-1, epoch=80, train loss <loss>=9.651764696294611\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:58 INFO 140006395942272] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:58 INFO 140006395942272] Epoch[81] Batch[0] avg_epoch_loss=9.604801\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:58 INFO 140006395942272] #quality_metric: host=algo-1, epoch=81, batch=0 train loss <loss>=9.604801177978516\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:58 INFO 140006395942272] Epoch[81] Batch[5] avg_epoch_loss=9.787723\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:58 INFO 140006395942272] #quality_metric: host=algo-1, epoch=81, batch=5 train loss <loss>=9.78772258758545\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:58 INFO 140006395942272] Epoch[81] Batch [5]#011Speed: 1673.02 samples/sec#011loss=9.787723\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:58 INFO 140006395942272] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462458.0600855, \"EndTime\": 1631462458.6725566, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 611.9725704193115, \"count\": 1, \"min\": 611.9725704193115, \"max\": 611.9725704193115}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:58 INFO 140006395942272] #throughput_metric: host=algo-1, train throughput=1035.79157099226 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:58 INFO 140006395942272] #progress_metric: host=algo-1, completed 41.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:58 INFO 140006395942272] #quality_metric: host=algo-1, epoch=81, train loss <loss>=9.867171001434325\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:58 INFO 140006395942272] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:58 INFO 140006395942272] Epoch[82] Batch[0] avg_epoch_loss=9.159739\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:58 INFO 140006395942272] #quality_metric: host=algo-1, epoch=82, batch=0 train loss <loss>=9.159738540649414\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:59 INFO 140006395942272] Epoch[82] Batch[5] avg_epoch_loss=9.279759\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:59 INFO 140006395942272] #quality_metric: host=algo-1, epoch=82, batch=5 train loss <loss>=9.279758930206299\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:59 INFO 140006395942272] Epoch[82] Batch [5]#011Speed: 2090.22 samples/sec#011loss=9.279759\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:59 INFO 140006395942272] Epoch[82] Batch[10] avg_epoch_loss=9.451207\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:59 INFO 140006395942272] #quality_metric: host=algo-1, epoch=82, batch=10 train loss <loss>=9.65694465637207\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:59 INFO 140006395942272] Epoch[82] Batch [10]#011Speed: 1936.98 samples/sec#011loss=9.656945\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:59 INFO 140006395942272] processed a total of 665 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462458.6726358, \"EndTime\": 1631462459.2888188, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 615.673303604126, \"count\": 1, \"min\": 615.673303604126, \"max\": 615.673303604126}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:59 INFO 140006395942272] #throughput_metric: host=algo-1, train throughput=1079.9112906223286 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:59 INFO 140006395942272] #progress_metric: host=algo-1, completed 41.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:59 INFO 140006395942272] #quality_metric: host=algo-1, epoch=82, train loss <loss>=9.451206987554377\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:59 INFO 140006395942272] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:59 INFO 140006395942272] Epoch[83] Batch[0] avg_epoch_loss=9.478906\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:59 INFO 140006395942272] #quality_metric: host=algo-1, epoch=83, batch=0 train loss <loss>=9.47890567779541\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:59 INFO 140006395942272] Epoch[83] Batch[5] avg_epoch_loss=9.662042\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:59 INFO 140006395942272] #quality_metric: host=algo-1, epoch=83, batch=5 train loss <loss>=9.662042299906412\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:59 INFO 140006395942272] Epoch[83] Batch [5]#011Speed: 2107.72 samples/sec#011loss=9.662042\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:59 INFO 140006395942272] processed a total of 617 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462459.2889013, \"EndTime\": 1631462459.8338451, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 544.4808006286621, \"count\": 1, \"min\": 544.4808006286621, \"max\": 544.4808006286621}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:59 INFO 140006395942272] #throughput_metric: host=algo-1, train throughput=1132.932660197983 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:59 INFO 140006395942272] #progress_metric: host=algo-1, completed 42.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:59 INFO 140006395942272] #quality_metric: host=algo-1, epoch=83, train loss <loss>=9.796823215484618\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:00:59 INFO 140006395942272] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:00 INFO 140006395942272] Epoch[84] Batch[0] avg_epoch_loss=10.566353\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:00 INFO 140006395942272] #quality_metric: host=algo-1, epoch=84, batch=0 train loss <loss>=10.566352844238281\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:00 INFO 140006395942272] Epoch[84] Batch[5] avg_epoch_loss=9.806932\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:00 INFO 140006395942272] #quality_metric: host=algo-1, epoch=84, batch=5 train loss <loss>=9.80693244934082\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:00 INFO 140006395942272] Epoch[84] Batch [5]#011Speed: 1981.85 samples/sec#011loss=9.806932\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:00 INFO 140006395942272] Epoch[84] Batch[10] avg_epoch_loss=9.821054\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:00 INFO 140006395942272] #quality_metric: host=algo-1, epoch=84, batch=10 train loss <loss>=9.83799991607666\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:00 INFO 140006395942272] Epoch[84] Batch [10]#011Speed: 1922.83 samples/sec#011loss=9.838000\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:00 INFO 140006395942272] processed a total of 658 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462459.833937, \"EndTime\": 1631462460.4351432, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 600.6584167480469, \"count\": 1, \"min\": 600.6584167480469, \"max\": 600.6584167480469}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:00 INFO 140006395942272] #throughput_metric: host=algo-1, train throughput=1095.2458748487506 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:00 INFO 140006395942272] #progress_metric: host=algo-1, completed 42.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:00 INFO 140006395942272] #quality_metric: host=algo-1, epoch=84, train loss <loss>=9.821054025129838\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:00 INFO 140006395942272] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:00 INFO 140006395942272] Epoch[85] Batch[0] avg_epoch_loss=9.883053\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:00 INFO 140006395942272] #quality_metric: host=algo-1, epoch=85, batch=0 train loss <loss>=9.883052825927734\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:00 INFO 140006395942272] Epoch[85] Batch[5] avg_epoch_loss=10.034390\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:00 INFO 140006395942272] #quality_metric: host=algo-1, epoch=85, batch=5 train loss <loss>=10.034389972686768\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:00 INFO 140006395942272] Epoch[85] Batch [5]#011Speed: 2090.89 samples/sec#011loss=10.034390\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:00 INFO 140006395942272] processed a total of 638 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462460.4352267, \"EndTime\": 1631462460.9857252, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 550.0316619873047, \"count\": 1, \"min\": 550.0316619873047, \"max\": 550.0316619873047}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:00 INFO 140006395942272] #throughput_metric: host=algo-1, train throughput=1159.643691908739 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:00 INFO 140006395942272] #progress_metric: host=algo-1, completed 43.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:00 INFO 140006395942272] #quality_metric: host=algo-1, epoch=85, train loss <loss>=10.133511638641357\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:00 INFO 140006395942272] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:01 INFO 140006395942272] Epoch[86] Batch[0] avg_epoch_loss=9.336171\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:01 INFO 140006395942272] #quality_metric: host=algo-1, epoch=86, batch=0 train loss <loss>=9.33617115020752\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:01 INFO 140006395942272] Epoch[86] Batch[5] avg_epoch_loss=9.604639\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:01 INFO 140006395942272] #quality_metric: host=algo-1, epoch=86, batch=5 train loss <loss>=9.604638576507568\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:01 INFO 140006395942272] Epoch[86] Batch [5]#011Speed: 2075.67 samples/sec#011loss=9.604639\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:01 INFO 140006395942272] Epoch[86] Batch[10] avg_epoch_loss=9.801949\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:01 INFO 140006395942272] #quality_metric: host=algo-1, epoch=86, batch=10 train loss <loss>=10.038722229003906\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:01 INFO 140006395942272] Epoch[86] Batch [10]#011Speed: 1706.06 samples/sec#011loss=10.038722\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:01 INFO 140006395942272] processed a total of 657 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462460.9858172, \"EndTime\": 1631462461.5838838, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 597.5480079650879, \"count\": 1, \"min\": 597.5480079650879, \"max\": 597.5480079650879}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:01 INFO 140006395942272] #throughput_metric: host=algo-1, train throughput=1099.3090279800122 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:01 INFO 140006395942272] #progress_metric: host=algo-1, completed 43.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:01 INFO 140006395942272] #quality_metric: host=algo-1, epoch=86, train loss <loss>=9.801949327642268\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:01 INFO 140006395942272] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:01 INFO 140006395942272] Epoch[87] Batch[0] avg_epoch_loss=10.337623\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:01 INFO 140006395942272] #quality_metric: host=algo-1, epoch=87, batch=0 train loss <loss>=10.33762264251709\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:02 INFO 140006395942272] Epoch[87] Batch[5] avg_epoch_loss=10.075822\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:02 INFO 140006395942272] #quality_metric: host=algo-1, epoch=87, batch=5 train loss <loss>=10.075822035471598\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:02 INFO 140006395942272] Epoch[87] Batch [5]#011Speed: 1474.72 samples/sec#011loss=10.075822\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:02 INFO 140006395942272] processed a total of 640 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462461.5839481, \"EndTime\": 1631462462.2127845, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 628.3009052276611, \"count\": 1, \"min\": 628.3009052276611, \"max\": 628.3009052276611}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:02 INFO 140006395942272] #throughput_metric: host=algo-1, train throughput=1018.4153895474454 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:02 INFO 140006395942272] #progress_metric: host=algo-1, completed 44.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:02 INFO 140006395942272] #quality_metric: host=algo-1, epoch=87, train loss <loss>=9.874745750427246\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:02 INFO 140006395942272] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:02 INFO 140006395942272] Epoch[88] Batch[0] avg_epoch_loss=9.083016\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:02 INFO 140006395942272] #quality_metric: host=algo-1, epoch=88, batch=0 train loss <loss>=9.083016395568848\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:02 INFO 140006395942272] Epoch[88] Batch[5] avg_epoch_loss=9.384429\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:02 INFO 140006395942272] #quality_metric: host=algo-1, epoch=88, batch=5 train loss <loss>=9.384428977966309\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:02 INFO 140006395942272] Epoch[88] Batch [5]#011Speed: 2090.52 samples/sec#011loss=9.384429\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:02 INFO 140006395942272] Epoch[88] Batch[10] avg_epoch_loss=9.387098\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:02 INFO 140006395942272] #quality_metric: host=algo-1, epoch=88, batch=10 train loss <loss>=9.390299987792968\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:02 INFO 140006395942272] Epoch[88] Batch [10]#011Speed: 1845.85 samples/sec#011loss=9.390300\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:02 INFO 140006395942272] processed a total of 679 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462462.212873, \"EndTime\": 1631462462.8453026, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 631.8783760070801, \"count\": 1, \"min\": 631.8783760070801, \"max\": 631.8783760070801}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:02 INFO 140006395942272] #throughput_metric: host=algo-1, train throughput=1074.2512374174207 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:02 INFO 140006395942272] #progress_metric: host=algo-1, completed 44.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:02 INFO 140006395942272] #quality_metric: host=algo-1, epoch=88, train loss <loss>=9.38709761879661\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:02 INFO 140006395942272] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:03 INFO 140006395942272] Epoch[89] Batch[0] avg_epoch_loss=9.905917\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:03 INFO 140006395942272] #quality_metric: host=algo-1, epoch=89, batch=0 train loss <loss>=9.905917167663574\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:03 INFO 140006395942272] Epoch[89] Batch[5] avg_epoch_loss=9.607354\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:03 INFO 140006395942272] #quality_metric: host=algo-1, epoch=89, batch=5 train loss <loss>=9.607353846232096\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:03 INFO 140006395942272] Epoch[89] Batch [5]#011Speed: 2157.37 samples/sec#011loss=9.607354\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:03 INFO 140006395942272] Epoch[89] Batch[10] avg_epoch_loss=10.045219\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:03 INFO 140006395942272] #quality_metric: host=algo-1, epoch=89, batch=10 train loss <loss>=10.570656394958496\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:03 INFO 140006395942272] Epoch[89] Batch [10]#011Speed: 1596.52 samples/sec#011loss=10.570656\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:03 INFO 140006395942272] processed a total of 656 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462462.8453896, \"EndTime\": 1631462463.4623883, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 616.5256500244141, \"count\": 1, \"min\": 616.5256500244141, \"max\": 616.5256500244141}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:03 INFO 140006395942272] #throughput_metric: host=algo-1, train throughput=1063.8246945160588 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:03 INFO 140006395942272] #progress_metric: host=algo-1, completed 45.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:03 INFO 140006395942272] #quality_metric: host=algo-1, epoch=89, train loss <loss>=10.045218641107732\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:03 INFO 140006395942272] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:03 INFO 140006395942272] Epoch[90] Batch[0] avg_epoch_loss=9.714103\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:03 INFO 140006395942272] #quality_metric: host=algo-1, epoch=90, batch=0 train loss <loss>=9.714102745056152\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:03 INFO 140006395942272] Epoch[90] Batch[5] avg_epoch_loss=9.876275\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:03 INFO 140006395942272] #quality_metric: host=algo-1, epoch=90, batch=5 train loss <loss>=9.876275062561035\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:03 INFO 140006395942272] Epoch[90] Batch [5]#011Speed: 2112.25 samples/sec#011loss=9.876275\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:04 INFO 140006395942272] processed a total of 617 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462463.4624698, \"EndTime\": 1631462464.0687966, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 605.8611869812012, \"count\": 1, \"min\": 605.8611869812012, \"max\": 605.8611869812012}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:04 INFO 140006395942272] #throughput_metric: host=algo-1, train throughput=1018.1539077711633 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:04 INFO 140006395942272] #progress_metric: host=algo-1, completed 45.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:04 INFO 140006395942272] #quality_metric: host=algo-1, epoch=90, train loss <loss>=9.808804988861084\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:04 INFO 140006395942272] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:04 INFO 140006395942272] Loading parameters from best epoch (50)\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462464.0688941, \"EndTime\": 1631462464.0772896, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.deserialize.time\": {\"sum\": 7.744789123535156, \"count\": 1, \"min\": 7.744789123535156, \"max\": 7.744789123535156}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:04 INFO 140006395942272] stopping training now\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:04 INFO 140006395942272] #progress_metric: host=algo-1, completed 100 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:04 INFO 140006395942272] Final loss: 9.197478597814387 (occurred at epoch 50)\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:04 INFO 140006395942272] #quality_metric: host=algo-1, train final_loss <loss>=9.197478597814387\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:04 INFO 140006395942272] Worker algo-1 finished training.\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:04 WARNING 140006395942272] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:04 INFO 140006395942272] All workers finished. Serializing model for prediction.\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462464.0773444, \"EndTime\": 1631462464.2209806, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"get_graph.time\": {\"sum\": 142.94815063476562, \"count\": 1, \"min\": 142.94815063476562, \"max\": 142.94815063476562}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:04 INFO 140006395942272] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462464.221046, \"EndTime\": 1631462464.2590623, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"finalize.time\": {\"sum\": 181.05411529541016, \"count\": 1, \"min\": 181.05411529541016, \"max\": 181.05411529541016}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:04 INFO 140006395942272] Serializing to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:04 INFO 140006395942272] Saved checkpoint to \"/opt/ml/model/model_algo-1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462464.2591503, \"EndTime\": 1631462464.2653809, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"model.serialize.time\": {\"sum\": 6.183624267578125, \"count\": 1, \"min\": 6.183624267578125, \"max\": 6.183624267578125}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:04 INFO 140006395942272] Successfully serialized the model for prediction.\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:01:04 INFO 140006395942272] No test data passed, skipping evaluation.\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631462464.2654576, \"EndTime\": 1631462464.2735012, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"setuptime\": {\"sum\": 7.876157760620117, \"count\": 1, \"min\": 7.876157760620117, \"max\": 7.876157760620117}, \"totaltime\": {\"sum\": 53995.95260620117, \"count\": 1, \"min\": 53995.95260620117, \"max\": 53995.95260620117}}}\n",
      "\u001b[0m\n",
      "\n",
      "2021-09-12 16:01:29 Uploading - Uploading generated training model\n",
      "2021-09-12 16:01:29 Completed - Training job completed\n",
      "ProfilerReport-1631462225: NoIssuesFound\n",
      "Training seconds: 110\n",
      "Billable seconds: 110\n",
      "-------------!Overwriting existing file\n",
      "Uploading file to s3://sagemaker-us-east-2-222195595958/deepar-covid-demo-notebook/data/train/train.json\n",
      "2021-09-12 16:08:21 Starting - Starting the training job...\n",
      "2021-09-12 16:08:44 Starting - Launching requested ML instancesProfilerReport-1631462901: InProgress\n",
      "...\n",
      "2021-09-12 16:09:14 Starting - Preparing the instances for training.........\n",
      "2021-09-12 16:10:44 Downloading - Downloading input data\n",
      "2021-09-12 16:10:44 Training - Downloading the training image...\n",
      "2021-09-12 16:11:13 Training - Training image download completed. Training in progress..\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:16 INFO 140084067562880] Reading default configuration from /opt/amazon/lib/python3.6/site-packages/algorithm/resources/default-input.json: {'_kvstore': 'auto', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_tuning_objective_metric': '', 'cardinality': 'auto', 'dropout_rate': '0.10', 'early_stopping_patience': '', 'embedding_dimension': '10', 'learning_rate': '0.001', 'likelihood': 'student-t', 'mini_batch_size': '128', 'num_cells': '40', 'num_dynamic_feat': 'auto', 'num_eval_samples': '100', 'num_layers': '2', 'test_quantiles': '[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]'}\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:16 INFO 140084067562880] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {'prediction_length': '14', 'time_freq': 'D', 'context_length': '14', 'epochs': '200', 'learning_rate': '5E-4', 'early_stopping_patience': '40', 'mini_batch_size': '64'}\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:16 INFO 140084067562880] Final configuration: {'_kvstore': 'auto', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_tuning_objective_metric': '', 'cardinality': 'auto', 'dropout_rate': '0.10', 'early_stopping_patience': '40', 'embedding_dimension': '10', 'learning_rate': '5E-4', 'likelihood': 'student-t', 'mini_batch_size': '64', 'num_cells': '40', 'num_dynamic_feat': 'auto', 'num_eval_samples': '100', 'num_layers': '2', 'test_quantiles': '[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', 'prediction_length': '14', 'time_freq': 'D', 'context_length': '14', 'epochs': '200'}\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:16 INFO 140084067562880] Detected entry point for worker worker\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:17 INFO 140084067562880] Using early stopping with patience 40\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:17 INFO 140084067562880] random_seed is None\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:17 INFO 140084067562880] [cardinality=auto] `cat` field was NOT found in the file `/opt/ml/input/data/train/train.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:17 INFO 140084067562880] [num_dynamic_feat=auto] `dynamic_feat` field was NOT found in the file `/opt/ml/input/data/train/train.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:17 INFO 140084067562880] Training set statistics:\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:17 INFO 140084067562880] Integer time series\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:17 INFO 140084067562880] number of time series: 41\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:17 INFO 140084067562880] number of observations: 13497\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:17 INFO 140084067562880] mean target length: 329.1951219512195\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:17 INFO 140084067562880] min/mean/max target: 0.0/15526209.589760687/353859904.0\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:17 INFO 140084067562880] mean abs(target): 15526209.589760687\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:17 INFO 140084067562880] contains missing values: no\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:17 INFO 140084067562880] Small number of time series. Doing 16 passes over dataset with prob 0.975609756097561 per epoch.\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:17 INFO 140084067562880] No test channel found not running evaluations\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:17 INFO 140084067562880] #memory_usage::<batchbuffer> = 2.2568130493164062 mb\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:17 INFO 140084067562880] nvidia-smi took: 0.025193452835083008 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:17 INFO 140084067562880] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:17 INFO 140084067562880] Create Store: local\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463077.0900166, \"EndTime\": 1631463077.1710865, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"get_graph.time\": {\"sum\": 79.1473388671875, \"count\": 1, \"min\": 79.1473388671875, \"max\": 79.1473388671875}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:17 INFO 140084067562880] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:17 INFO 140084067562880] #memory_usage::<model> = 12 mb\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463077.1711621, \"EndTime\": 1631463077.2783828, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"initialize.time\": {\"sum\": 188.25864791870117, \"count\": 1, \"min\": 188.25864791870117, \"max\": 188.25864791870117}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:17 INFO 140084067562880] Epoch[0] Batch[0] avg_epoch_loss=13.153594\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:17 INFO 140084067562880] #quality_metric: host=algo-1, epoch=0, batch=0 train loss <loss>=13.153594017028809\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:17 INFO 140084067562880] Epoch[0] Batch[5] avg_epoch_loss=13.964646\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:17 INFO 140084067562880] #quality_metric: host=algo-1, epoch=0, batch=5 train loss <loss>=13.964646180470785\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:17 INFO 140084067562880] Epoch[0] Batch [5]#011Speed: 1979.92 samples/sec#011loss=13.964646\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:17 INFO 140084067562880] Epoch[0] Batch[10] avg_epoch_loss=13.816078\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:17 INFO 140084067562880] #quality_metric: host=algo-1, epoch=0, batch=10 train loss <loss>=13.637796401977539\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:17 INFO 140084067562880] Epoch[0] Batch [10]#011Speed: 1789.09 samples/sec#011loss=13.637796\u001b[0m\n",
      "\u001b[34m/opt/amazon/python3.6/lib/python3.6/contextlib.py:99: DeprecationWarning: generator 'local_timer' raised StopIteration\n",
      "  self.gen.throw(type, value, traceback)\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:17 INFO 140084067562880] processed a total of 677 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463077.2784514, \"EndTime\": 1631463077.9463499, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"epochs\": {\"sum\": 200.0, \"count\": 1, \"min\": 200, \"max\": 200}, \"update.time\": {\"sum\": 667.8006649017334, \"count\": 1, \"min\": 667.8006649017334, \"max\": 667.8006649017334}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:17 INFO 140084067562880] #throughput_metric: host=algo-1, train throughput=1013.5696691234212 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:17 INFO 140084067562880] #progress_metric: host=algo-1, completed 0.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:17 INFO 140084067562880] #quality_metric: host=algo-1, epoch=0, train loss <loss>=13.81607809933749\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:17 INFO 140084067562880] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:17 INFO 140084067562880] Saved checkpoint to \"/opt/ml/model/state_5ade79f8-a526-404a-9451-05745c47f314-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463077.9464426, \"EndTime\": 1631463077.9609315, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 13.87333869934082, \"count\": 1, \"min\": 13.87333869934082, \"max\": 13.87333869934082}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:18 INFO 140084067562880] Epoch[1] Batch[0] avg_epoch_loss=12.279064\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:18 INFO 140084067562880] #quality_metric: host=algo-1, epoch=1, batch=0 train loss <loss>=12.279064178466797\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:18 INFO 140084067562880] Epoch[1] Batch[5] avg_epoch_loss=12.766219\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:18 INFO 140084067562880] #quality_metric: host=algo-1, epoch=1, batch=5 train loss <loss>=12.76621929804484\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:18 INFO 140084067562880] Epoch[1] Batch [5]#011Speed: 2030.41 samples/sec#011loss=12.766219\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:18 INFO 140084067562880] processed a total of 573 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463077.9610043, \"EndTime\": 1631463078.4517074, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 490.63873291015625, \"count\": 1, \"min\": 490.63873291015625, \"max\": 490.63873291015625}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:18 INFO 140084067562880] #throughput_metric: host=algo-1, train throughput=1167.5533545338747 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:18 INFO 140084067562880] #progress_metric: host=algo-1, completed 1.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:18 INFO 140084067562880] #quality_metric: host=algo-1, epoch=1, train loss <loss>=12.893027729458279\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:18 INFO 140084067562880] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:18 INFO 140084067562880] Saved checkpoint to \"/opt/ml/model/state_513d2c07-cd21-4105-a66e-ca8d3f81fd9b-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463078.451801, \"EndTime\": 1631463078.471348, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 19.011497497558594, \"count\": 1, \"min\": 19.011497497558594, \"max\": 19.011497497558594}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:18 INFO 140084067562880] Epoch[2] Batch[0] avg_epoch_loss=12.612071\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:18 INFO 140084067562880] #quality_metric: host=algo-1, epoch=2, batch=0 train loss <loss>=12.61207103729248\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:18 INFO 140084067562880] Epoch[2] Batch[5] avg_epoch_loss=12.235383\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:18 INFO 140084067562880] #quality_metric: host=algo-1, epoch=2, batch=5 train loss <loss>=12.23538319269816\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:18 INFO 140084067562880] Epoch[2] Batch [5]#011Speed: 1659.59 samples/sec#011loss=12.235383\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:19 INFO 140084067562880] processed a total of 635 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463078.4714231, \"EndTime\": 1631463079.1362638, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 664.7794246673584, \"count\": 1, \"min\": 664.7794246673584, \"max\": 664.7794246673584}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:19 INFO 140084067562880] #throughput_metric: host=algo-1, train throughput=954.9804011559947 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:19 INFO 140084067562880] #progress_metric: host=algo-1, completed 1.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:19 INFO 140084067562880] #quality_metric: host=algo-1, epoch=2, train loss <loss>=12.208929920196534\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:19 INFO 140084067562880] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:19 INFO 140084067562880] Saved checkpoint to \"/opt/ml/model/state_6cd5d0de-63bb-4f4e-995b-051ac97cce93-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463079.136382, \"EndTime\": 1631463079.1496844, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 12.644529342651367, \"count\": 1, \"min\": 12.644529342651367, \"max\": 12.644529342651367}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:19 INFO 140084067562880] Epoch[3] Batch[0] avg_epoch_loss=11.860546\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:19 INFO 140084067562880] #quality_metric: host=algo-1, epoch=3, batch=0 train loss <loss>=11.860546112060547\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:19 INFO 140084067562880] Epoch[3] Batch[5] avg_epoch_loss=12.186070\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:19 INFO 140084067562880] #quality_metric: host=algo-1, epoch=3, batch=5 train loss <loss>=12.186070283253988\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:19 INFO 140084067562880] Epoch[3] Batch [5]#011Speed: 1703.69 samples/sec#011loss=12.186070\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:19 INFO 140084067562880] Epoch[3] Batch[10] avg_epoch_loss=11.832931\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:19 INFO 140084067562880] #quality_metric: host=algo-1, epoch=3, batch=10 train loss <loss>=11.409164428710938\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:19 INFO 140084067562880] Epoch[3] Batch [10]#011Speed: 1598.19 samples/sec#011loss=11.409164\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:19 INFO 140084067562880] processed a total of 655 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463079.1497567, \"EndTime\": 1631463079.783106, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 633.2900524139404, \"count\": 1, \"min\": 633.2900524139404, \"max\": 633.2900524139404}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:19 INFO 140084067562880] #throughput_metric: host=algo-1, train throughput=1034.0865528843747 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:19 INFO 140084067562880] #progress_metric: host=algo-1, completed 2.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:19 INFO 140084067562880] #quality_metric: host=algo-1, epoch=3, train loss <loss>=11.832931258461691\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:19 INFO 140084067562880] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:19 INFO 140084067562880] Saved checkpoint to \"/opt/ml/model/state_f80fa889-9db9-49e9-9acf-488f25a49da5-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463079.7831907, \"EndTime\": 1631463079.80264, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 18.95618438720703, \"count\": 1, \"min\": 18.95618438720703, \"max\": 18.95618438720703}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:20 INFO 140084067562880] Epoch[4] Batch[0] avg_epoch_loss=12.411824\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:20 INFO 140084067562880] #quality_metric: host=algo-1, epoch=4, batch=0 train loss <loss>=12.411824226379395\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:20 INFO 140084067562880] Epoch[4] Batch[5] avg_epoch_loss=11.855111\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:20 INFO 140084067562880] #quality_metric: host=algo-1, epoch=4, batch=5 train loss <loss>=11.85511064529419\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:20 INFO 140084067562880] Epoch[4] Batch [5]#011Speed: 1640.79 samples/sec#011loss=11.855111\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:20 INFO 140084067562880] Epoch[4] Batch[10] avg_epoch_loss=11.880547\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:20 INFO 140084067562880] #quality_metric: host=algo-1, epoch=4, batch=10 train loss <loss>=11.911071014404296\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:20 INFO 140084067562880] Epoch[4] Batch [10]#011Speed: 1679.72 samples/sec#011loss=11.911071\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:20 INFO 140084067562880] processed a total of 655 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463079.8027205, \"EndTime\": 1631463080.5001576, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 697.3731517791748, \"count\": 1, \"min\": 697.3731517791748, \"max\": 697.3731517791748}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:20 INFO 140084067562880] #throughput_metric: host=algo-1, train throughput=939.0783850254402 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:20 INFO 140084067562880] #progress_metric: host=algo-1, completed 2.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:20 INFO 140084067562880] #quality_metric: host=algo-1, epoch=4, train loss <loss>=11.880547176707875\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:20 INFO 140084067562880] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:20 INFO 140084067562880] Epoch[5] Batch[0] avg_epoch_loss=11.676244\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:20 INFO 140084067562880] #quality_metric: host=algo-1, epoch=5, batch=0 train loss <loss>=11.676243782043457\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:20 INFO 140084067562880] Epoch[5] Batch[5] avg_epoch_loss=11.226632\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:20 INFO 140084067562880] #quality_metric: host=algo-1, epoch=5, batch=5 train loss <loss>=11.22663164138794\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:20 INFO 140084067562880] Epoch[5] Batch [5]#011Speed: 2005.40 samples/sec#011loss=11.226632\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:21 INFO 140084067562880] Epoch[5] Batch[10] avg_epoch_loss=11.642669\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:21 INFO 140084067562880] #quality_metric: host=algo-1, epoch=5, batch=10 train loss <loss>=12.141912841796875\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:21 INFO 140084067562880] Epoch[5] Batch [10]#011Speed: 1965.05 samples/sec#011loss=12.141913\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:21 INFO 140084067562880] processed a total of 644 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463080.5002415, \"EndTime\": 1631463081.0796018, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 578.8545608520508, \"count\": 1, \"min\": 578.8545608520508, \"max\": 578.8545608520508}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:21 INFO 140084067562880] #throughput_metric: host=algo-1, train throughput=1112.3156598662652 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:21 INFO 140084067562880] #progress_metric: host=algo-1, completed 3.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:21 INFO 140084067562880] #quality_metric: host=algo-1, epoch=5, train loss <loss>=11.642668550664728\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:21 INFO 140084067562880] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:21 INFO 140084067562880] Saved checkpoint to \"/opt/ml/model/state_f47c900e-ec4f-44e8-898d-0d26a7b5f6d0-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463081.0796835, \"EndTime\": 1631463081.098992, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 18.81694793701172, \"count\": 1, \"min\": 18.81694793701172, \"max\": 18.81694793701172}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:21 INFO 140084067562880] Epoch[6] Batch[0] avg_epoch_loss=11.400175\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:21 INFO 140084067562880] #quality_metric: host=algo-1, epoch=6, batch=0 train loss <loss>=11.400175094604492\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:21 INFO 140084067562880] Epoch[6] Batch[5] avg_epoch_loss=11.051128\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:21 INFO 140084067562880] #quality_metric: host=algo-1, epoch=6, batch=5 train loss <loss>=11.051127592722574\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:21 INFO 140084067562880] Epoch[6] Batch [5]#011Speed: 2096.45 samples/sec#011loss=11.051128\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:21 INFO 140084067562880] Epoch[6] Batch[10] avg_epoch_loss=11.077455\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:21 INFO 140084067562880] #quality_metric: host=algo-1, epoch=6, batch=10 train loss <loss>=11.109047889709473\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:21 INFO 140084067562880] Epoch[6] Batch [10]#011Speed: 1977.16 samples/sec#011loss=11.109048\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:21 INFO 140084067562880] processed a total of 655 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463081.0990634, \"EndTime\": 1631463081.714369, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 615.2493953704834, \"count\": 1, \"min\": 615.2493953704834, \"max\": 615.2493953704834}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:21 INFO 140084067562880] #throughput_metric: host=algo-1, train throughput=1064.3998062803898 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:21 INFO 140084067562880] #progress_metric: host=algo-1, completed 3.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:21 INFO 140084067562880] #quality_metric: host=algo-1, epoch=6, train loss <loss>=11.077455000443893\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:21 INFO 140084067562880] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:21 INFO 140084067562880] Saved checkpoint to \"/opt/ml/model/state_9cbf1b8b-f97f-44b6-833c-33d5aeb0c227-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463081.7144558, \"EndTime\": 1631463081.7266805, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 11.709213256835938, \"count\": 1, \"min\": 11.709213256835938, \"max\": 11.709213256835938}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:21 INFO 140084067562880] Epoch[7] Batch[0] avg_epoch_loss=10.541051\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:21 INFO 140084067562880] #quality_metric: host=algo-1, epoch=7, batch=0 train loss <loss>=10.541050910949707\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:22 INFO 140084067562880] Epoch[7] Batch[5] avg_epoch_loss=10.908966\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:22 INFO 140084067562880] #quality_metric: host=algo-1, epoch=7, batch=5 train loss <loss>=10.908966382344564\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:22 INFO 140084067562880] Epoch[7] Batch [5]#011Speed: 2124.24 samples/sec#011loss=10.908966\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:22 INFO 140084067562880] Epoch[7] Batch[10] avg_epoch_loss=10.503394\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:22 INFO 140084067562880] #quality_metric: host=algo-1, epoch=7, batch=10 train loss <loss>=10.016707229614259\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:22 INFO 140084067562880] Epoch[7] Batch [10]#011Speed: 2017.59 samples/sec#011loss=10.016707\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:22 INFO 140084067562880] processed a total of 648 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463081.7267528, \"EndTime\": 1631463082.3003874, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 573.5697746276855, \"count\": 1, \"min\": 573.5697746276855, \"max\": 573.5697746276855}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:22 INFO 140084067562880] #throughput_metric: host=algo-1, train throughput=1129.5159259597883 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:22 INFO 140084067562880] #progress_metric: host=algo-1, completed 4.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:22 INFO 140084067562880] #quality_metric: host=algo-1, epoch=7, train loss <loss>=10.503394040194424\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:22 INFO 140084067562880] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:22 INFO 140084067562880] Saved checkpoint to \"/opt/ml/model/state_7fa662eb-ac0b-4909-b282-8090e570fa02-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463082.3004763, \"EndTime\": 1631463082.313547, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 12.531042098999023, \"count\": 1, \"min\": 12.531042098999023, \"max\": 12.531042098999023}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:22 INFO 140084067562880] Epoch[8] Batch[0] avg_epoch_loss=10.484356\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:22 INFO 140084067562880] #quality_metric: host=algo-1, epoch=8, batch=0 train loss <loss>=10.484355926513672\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:22 INFO 140084067562880] Epoch[8] Batch[5] avg_epoch_loss=10.929914\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:22 INFO 140084067562880] #quality_metric: host=algo-1, epoch=8, batch=5 train loss <loss>=10.929914474487305\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:22 INFO 140084067562880] Epoch[8] Batch [5]#011Speed: 1921.16 samples/sec#011loss=10.929914\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:22 INFO 140084067562880] processed a total of 624 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463082.3136492, \"EndTime\": 1631463082.8747606, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 561.0494613647461, \"count\": 1, \"min\": 561.0494613647461, \"max\": 561.0494613647461}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:22 INFO 140084067562880] #throughput_metric: host=algo-1, train throughput=1111.9556924917536 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:22 INFO 140084067562880] #progress_metric: host=algo-1, completed 4.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:22 INFO 140084067562880] #quality_metric: host=algo-1, epoch=8, train loss <loss>=11.016218090057373\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:22 INFO 140084067562880] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:23 INFO 140084067562880] Epoch[9] Batch[0] avg_epoch_loss=11.103786\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:23 INFO 140084067562880] #quality_metric: host=algo-1, epoch=9, batch=0 train loss <loss>=11.103785514831543\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:23 INFO 140084067562880] Epoch[9] Batch[5] avg_epoch_loss=10.784797\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:23 INFO 140084067562880] #quality_metric: host=algo-1, epoch=9, batch=5 train loss <loss>=10.784796873728434\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:23 INFO 140084067562880] Epoch[9] Batch [5]#011Speed: 2102.24 samples/sec#011loss=10.784797\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:23 INFO 140084067562880] Epoch[9] Batch[10] avg_epoch_loss=10.898239\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:23 INFO 140084067562880] #quality_metric: host=algo-1, epoch=9, batch=10 train loss <loss>=11.034369468688965\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:23 INFO 140084067562880] Epoch[9] Batch [10]#011Speed: 1853.13 samples/sec#011loss=11.034369\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:23 INFO 140084067562880] processed a total of 649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463082.8748484, \"EndTime\": 1631463083.4857867, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 610.4283332824707, \"count\": 1, \"min\": 610.4283332824707, \"max\": 610.4283332824707}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:23 INFO 140084067562880] #throughput_metric: host=algo-1, train throughput=1062.9806987680904 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:23 INFO 140084067562880] #progress_metric: host=algo-1, completed 5.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:23 INFO 140084067562880] #quality_metric: host=algo-1, epoch=9, train loss <loss>=10.898238962346857\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:23 INFO 140084067562880] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:23 INFO 140084067562880] Epoch[10] Batch[0] avg_epoch_loss=10.628722\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:23 INFO 140084067562880] #quality_metric: host=algo-1, epoch=10, batch=0 train loss <loss>=10.628722190856934\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:23 INFO 140084067562880] Epoch[10] Batch[5] avg_epoch_loss=10.901284\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:23 INFO 140084067562880] #quality_metric: host=algo-1, epoch=10, batch=5 train loss <loss>=10.901283582051596\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:23 INFO 140084067562880] Epoch[10] Batch [5]#011Speed: 2135.99 samples/sec#011loss=10.901284\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:24 INFO 140084067562880] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463083.4858658, \"EndTime\": 1631463084.016063, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 529.6804904937744, \"count\": 1, \"min\": 529.6804904937744, \"max\": 529.6804904937744}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:24 INFO 140084067562880] #throughput_metric: host=algo-1, train throughput=1194.782023600482 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:24 INFO 140084067562880] #progress_metric: host=algo-1, completed 5.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:24 INFO 140084067562880] #quality_metric: host=algo-1, epoch=10, train loss <loss>=10.880215930938721\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:24 INFO 140084067562880] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:24 INFO 140084067562880] Epoch[11] Batch[0] avg_epoch_loss=11.001858\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:24 INFO 140084067562880] #quality_metric: host=algo-1, epoch=11, batch=0 train loss <loss>=11.00185775756836\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:24 INFO 140084067562880] Epoch[11] Batch[5] avg_epoch_loss=10.528944\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:24 INFO 140084067562880] #quality_metric: host=algo-1, epoch=11, batch=5 train loss <loss>=10.52894401550293\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:24 INFO 140084067562880] Epoch[11] Batch [5]#011Speed: 2029.70 samples/sec#011loss=10.528944\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:24 INFO 140084067562880] processed a total of 602 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463084.0161495, \"EndTime\": 1631463084.6207235, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 604.0515899658203, \"count\": 1, \"min\": 604.0515899658203, \"max\": 604.0515899658203}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:24 INFO 140084067562880] #throughput_metric: host=algo-1, train throughput=996.3802404283261 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:24 INFO 140084067562880] #progress_metric: host=algo-1, completed 6.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:24 INFO 140084067562880] #quality_metric: host=algo-1, epoch=11, train loss <loss>=10.345161819458008\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:24 INFO 140084067562880] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:24 INFO 140084067562880] Saved checkpoint to \"/opt/ml/model/state_cf91f4bc-eb90-4c21-9ade-d40b5f324a78-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463084.6208181, \"EndTime\": 1631463084.6399086, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 18.542766571044922, \"count\": 1, \"min\": 18.542766571044922, \"max\": 18.542766571044922}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:24 INFO 140084067562880] Epoch[12] Batch[0] avg_epoch_loss=10.127295\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:24 INFO 140084067562880] #quality_metric: host=algo-1, epoch=12, batch=0 train loss <loss>=10.127294540405273\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:25 INFO 140084067562880] Epoch[12] Batch[5] avg_epoch_loss=10.310310\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:25 INFO 140084067562880] #quality_metric: host=algo-1, epoch=12, batch=5 train loss <loss>=10.310309727986654\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:25 INFO 140084067562880] Epoch[12] Batch [5]#011Speed: 2068.49 samples/sec#011loss=10.310310\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:25 INFO 140084067562880] processed a total of 588 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463084.6399758, \"EndTime\": 1631463085.158952, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 518.918514251709, \"count\": 1, \"min\": 518.918514251709, \"max\": 518.918514251709}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:25 INFO 140084067562880] #throughput_metric: host=algo-1, train throughput=1132.8500233345214 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:25 INFO 140084067562880] #progress_metric: host=algo-1, completed 6.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:25 INFO 140084067562880] #quality_metric: host=algo-1, epoch=12, train loss <loss>=10.08472762107849\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:25 INFO 140084067562880] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:25 INFO 140084067562880] Saved checkpoint to \"/opt/ml/model/state_7ac61104-eca1-43a9-8491-62542c3e8a42-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463085.1590428, \"EndTime\": 1631463085.1784, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 18.828153610229492, \"count\": 1, \"min\": 18.828153610229492, \"max\": 18.828153610229492}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:25 INFO 140084067562880] Epoch[13] Batch[0] avg_epoch_loss=11.303983\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:25 INFO 140084067562880] #quality_metric: host=algo-1, epoch=13, batch=0 train loss <loss>=11.303982734680176\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:25 INFO 140084067562880] Epoch[13] Batch[5] avg_epoch_loss=10.815550\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:25 INFO 140084067562880] #quality_metric: host=algo-1, epoch=13, batch=5 train loss <loss>=10.815549532572428\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:25 INFO 140084067562880] Epoch[13] Batch [5]#011Speed: 1867.80 samples/sec#011loss=10.815550\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:25 INFO 140084067562880] processed a total of 630 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463085.1784737, \"EndTime\": 1631463085.7392507, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 560.7180595397949, \"count\": 1, \"min\": 560.7180595397949, \"max\": 560.7180595397949}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:25 INFO 140084067562880] #throughput_metric: host=algo-1, train throughput=1123.309992075989 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:25 INFO 140084067562880] #progress_metric: host=algo-1, completed 7.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:25 INFO 140084067562880] #quality_metric: host=algo-1, epoch=13, train loss <loss>=10.729355239868164\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:25 INFO 140084067562880] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:26 INFO 140084067562880] Epoch[14] Batch[0] avg_epoch_loss=11.038615\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:26 INFO 140084067562880] #quality_metric: host=algo-1, epoch=14, batch=0 train loss <loss>=11.038615226745605\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:26 INFO 140084067562880] Epoch[14] Batch[5] avg_epoch_loss=10.709318\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:26 INFO 140084067562880] #quality_metric: host=algo-1, epoch=14, batch=5 train loss <loss>=10.709318319956461\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:26 INFO 140084067562880] Epoch[14] Batch [5]#011Speed: 2022.05 samples/sec#011loss=10.709318\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:26 INFO 140084067562880] processed a total of 616 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463085.73934, \"EndTime\": 1631463086.2992082, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 559.3602657318115, \"count\": 1, \"min\": 559.3602657318115, \"max\": 559.3602657318115}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:26 INFO 140084067562880] #throughput_metric: host=algo-1, train throughput=1101.001359370752 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:26 INFO 140084067562880] #progress_metric: host=algo-1, completed 7.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:26 INFO 140084067562880] #quality_metric: host=algo-1, epoch=14, train loss <loss>=10.681951999664307\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:26 INFO 140084067562880] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:26 INFO 140084067562880] Epoch[15] Batch[0] avg_epoch_loss=9.275947\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:26 INFO 140084067562880] #quality_metric: host=algo-1, epoch=15, batch=0 train loss <loss>=9.275946617126465\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:26 INFO 140084067562880] Epoch[15] Batch[5] avg_epoch_loss=10.565060\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:26 INFO 140084067562880] #quality_metric: host=algo-1, epoch=15, batch=5 train loss <loss>=10.565059502919516\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:26 INFO 140084067562880] Epoch[15] Batch [5]#011Speed: 1813.28 samples/sec#011loss=10.565060\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:26 INFO 140084067562880] Epoch[15] Batch[10] avg_epoch_loss=11.012275\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:26 INFO 140084067562880] #quality_metric: host=algo-1, epoch=15, batch=10 train loss <loss>=11.548933219909667\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:26 INFO 140084067562880] Epoch[15] Batch [10]#011Speed: 1926.33 samples/sec#011loss=11.548933\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:26 INFO 140084067562880] processed a total of 666 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463086.2992985, \"EndTime\": 1631463086.8978882, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 598.0644226074219, \"count\": 1, \"min\": 598.0644226074219, \"max\": 598.0644226074219}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:26 INFO 140084067562880] #throughput_metric: host=algo-1, train throughput=1113.372703180881 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:26 INFO 140084067562880] #progress_metric: host=algo-1, completed 8.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:26 INFO 140084067562880] #quality_metric: host=algo-1, epoch=15, train loss <loss>=11.01227482882413\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:26 INFO 140084067562880] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:27 INFO 140084067562880] Epoch[16] Batch[0] avg_epoch_loss=10.815910\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:27 INFO 140084067562880] #quality_metric: host=algo-1, epoch=16, batch=0 train loss <loss>=10.815910339355469\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:27 INFO 140084067562880] Epoch[16] Batch[5] avg_epoch_loss=10.392040\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:27 INFO 140084067562880] #quality_metric: host=algo-1, epoch=16, batch=5 train loss <loss>=10.392040252685547\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:27 INFO 140084067562880] Epoch[16] Batch [5]#011Speed: 2088.24 samples/sec#011loss=10.392040\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:27 INFO 140084067562880] processed a total of 611 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463086.8979707, \"EndTime\": 1631463087.432776, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 534.3387126922607, \"count\": 1, \"min\": 534.3387126922607, \"max\": 534.3387126922607}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:27 INFO 140084067562880] #throughput_metric: host=algo-1, train throughput=1143.195034877284 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:27 INFO 140084067562880] #progress_metric: host=algo-1, completed 8.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:27 INFO 140084067562880] #quality_metric: host=algo-1, epoch=16, train loss <loss>=10.099097490310669\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:27 INFO 140084067562880] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:27 INFO 140084067562880] Epoch[17] Batch[0] avg_epoch_loss=9.793971\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:27 INFO 140084067562880] #quality_metric: host=algo-1, epoch=17, batch=0 train loss <loss>=9.793971061706543\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:27 INFO 140084067562880] Epoch[17] Batch[5] avg_epoch_loss=10.307894\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:27 INFO 140084067562880] #quality_metric: host=algo-1, epoch=17, batch=5 train loss <loss>=10.307893594106039\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:27 INFO 140084067562880] Epoch[17] Batch [5]#011Speed: 1870.20 samples/sec#011loss=10.307894\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:28 INFO 140084067562880] Epoch[17] Batch[10] avg_epoch_loss=10.331551\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:28 INFO 140084067562880] #quality_metric: host=algo-1, epoch=17, batch=10 train loss <loss>=10.359940338134766\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:28 INFO 140084067562880] Epoch[17] Batch [10]#011Speed: 2059.65 samples/sec#011loss=10.359940\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:28 INFO 140084067562880] processed a total of 644 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463087.4328675, \"EndTime\": 1631463088.0626054, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 629.2321681976318, \"count\": 1, \"min\": 629.2321681976318, \"max\": 629.2321681976318}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:28 INFO 140084067562880] #throughput_metric: host=algo-1, train throughput=1023.2811726586245 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:28 INFO 140084067562880] #progress_metric: host=algo-1, completed 9.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:28 INFO 140084067562880] #quality_metric: host=algo-1, epoch=17, train loss <loss>=10.331551205028187\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:28 INFO 140084067562880] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:28 INFO 140084067562880] Epoch[18] Batch[0] avg_epoch_loss=11.354058\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:28 INFO 140084067562880] #quality_metric: host=algo-1, epoch=18, batch=0 train loss <loss>=11.354058265686035\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:28 INFO 140084067562880] Epoch[18] Batch[5] avg_epoch_loss=10.911254\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:28 INFO 140084067562880] #quality_metric: host=algo-1, epoch=18, batch=5 train loss <loss>=10.911254405975342\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:28 INFO 140084067562880] Epoch[18] Batch [5]#011Speed: 1940.81 samples/sec#011loss=10.911254\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:28 INFO 140084067562880] Epoch[18] Batch[10] avg_epoch_loss=10.617240\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:28 INFO 140084067562880] #quality_metric: host=algo-1, epoch=18, batch=10 train loss <loss>=10.26442356109619\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:28 INFO 140084067562880] Epoch[18] Batch [10]#011Speed: 1701.94 samples/sec#011loss=10.264424\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:28 INFO 140084067562880] processed a total of 654 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463088.0626857, \"EndTime\": 1631463088.6618736, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 598.7226963043213, \"count\": 1, \"min\": 598.7226963043213, \"max\": 598.7226963043213}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:28 INFO 140084067562880] #throughput_metric: host=algo-1, train throughput=1092.0718526256785 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:28 INFO 140084067562880] #progress_metric: host=algo-1, completed 9.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:28 INFO 140084067562880] #quality_metric: host=algo-1, epoch=18, train loss <loss>=10.617240385575728\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:28 INFO 140084067562880] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:28 INFO 140084067562880] Epoch[19] Batch[0] avg_epoch_loss=10.525333\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:28 INFO 140084067562880] #quality_metric: host=algo-1, epoch=19, batch=0 train loss <loss>=10.525333404541016\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:29 INFO 140084067562880] Epoch[19] Batch[5] avg_epoch_loss=10.584698\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:29 INFO 140084067562880] #quality_metric: host=algo-1, epoch=19, batch=5 train loss <loss>=10.58469788233439\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:29 INFO 140084067562880] Epoch[19] Batch [5]#011Speed: 2119.39 samples/sec#011loss=10.584698\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:29 INFO 140084067562880] Epoch[19] Batch[10] avg_epoch_loss=10.579251\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:29 INFO 140084067562880] #quality_metric: host=algo-1, epoch=19, batch=10 train loss <loss>=10.572713851928711\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:29 INFO 140084067562880] Epoch[19] Batch [10]#011Speed: 1599.39 samples/sec#011loss=10.572714\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:29 INFO 140084067562880] processed a total of 652 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463088.661976, \"EndTime\": 1631463089.2762809, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 613.8076782226562, \"count\": 1, \"min\": 613.8076782226562, \"max\": 613.8076782226562}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:29 INFO 140084067562880] #throughput_metric: host=algo-1, train throughput=1062.024372275459 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:29 INFO 140084067562880] #progress_metric: host=algo-1, completed 10.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:29 INFO 140084067562880] #quality_metric: host=algo-1, epoch=19, train loss <loss>=10.579250595786355\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:29 INFO 140084067562880] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:29 INFO 140084067562880] Epoch[20] Batch[0] avg_epoch_loss=9.665957\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:29 INFO 140084067562880] #quality_metric: host=algo-1, epoch=20, batch=0 train loss <loss>=9.6659574508667\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:29 INFO 140084067562880] Epoch[20] Batch[5] avg_epoch_loss=10.636593\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:29 INFO 140084067562880] #quality_metric: host=algo-1, epoch=20, batch=5 train loss <loss>=10.636592706044516\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:29 INFO 140084067562880] Epoch[20] Batch [5]#011Speed: 2083.26 samples/sec#011loss=10.636593\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:29 INFO 140084067562880] processed a total of 628 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463089.2763553, \"EndTime\": 1631463089.8020277, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 525.1429080963135, \"count\": 1, \"min\": 525.1429080963135, \"max\": 525.1429080963135}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:29 INFO 140084067562880] #throughput_metric: host=algo-1, train throughput=1195.594093435274 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:29 INFO 140084067562880] #progress_metric: host=algo-1, completed 10.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:29 INFO 140084067562880] #quality_metric: host=algo-1, epoch=20, train loss <loss>=10.3328218460083\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:29 INFO 140084067562880] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:30 INFO 140084067562880] Epoch[21] Batch[0] avg_epoch_loss=10.494526\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:30 INFO 140084067562880] #quality_metric: host=algo-1, epoch=21, batch=0 train loss <loss>=10.494525909423828\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:30 INFO 140084067562880] Epoch[21] Batch[5] avg_epoch_loss=10.464685\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:30 INFO 140084067562880] #quality_metric: host=algo-1, epoch=21, batch=5 train loss <loss>=10.464684963226318\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:30 INFO 140084067562880] Epoch[21] Batch [5]#011Speed: 2061.09 samples/sec#011loss=10.464685\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:30 INFO 140084067562880] processed a total of 621 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463089.802113, \"EndTime\": 1631463090.3306878, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 528.0256271362305, \"count\": 1, \"min\": 528.0256271362305, \"max\": 528.0256271362305}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:30 INFO 140084067562880] #throughput_metric: host=algo-1, train throughput=1175.7814469717111 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:30 INFO 140084067562880] #progress_metric: host=algo-1, completed 11.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:30 INFO 140084067562880] #quality_metric: host=algo-1, epoch=21, train loss <loss>=10.429028606414795\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:30 INFO 140084067562880] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:30 INFO 140084067562880] Epoch[22] Batch[0] avg_epoch_loss=9.916746\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:30 INFO 140084067562880] #quality_metric: host=algo-1, epoch=22, batch=0 train loss <loss>=9.916746139526367\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:30 INFO 140084067562880] Epoch[22] Batch[5] avg_epoch_loss=10.304668\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:30 INFO 140084067562880] #quality_metric: host=algo-1, epoch=22, batch=5 train loss <loss>=10.304667631785074\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:30 INFO 140084067562880] Epoch[22] Batch [5]#011Speed: 2092.89 samples/sec#011loss=10.304668\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:30 INFO 140084067562880] processed a total of 640 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463090.3307817, \"EndTime\": 1631463090.9016442, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 570.3465938568115, \"count\": 1, \"min\": 570.3465938568115, \"max\": 570.3465938568115}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:30 INFO 140084067562880] #throughput_metric: host=algo-1, train throughput=1121.9056837663634 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:30 INFO 140084067562880] #progress_metric: host=algo-1, completed 11.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:30 INFO 140084067562880] #quality_metric: host=algo-1, epoch=22, train loss <loss>=10.359090328216553\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:30 INFO 140084067562880] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:31 INFO 140084067562880] Epoch[23] Batch[0] avg_epoch_loss=10.626103\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:31 INFO 140084067562880] #quality_metric: host=algo-1, epoch=23, batch=0 train loss <loss>=10.626103401184082\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:31 INFO 140084067562880] Epoch[23] Batch[5] avg_epoch_loss=10.461663\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:31 INFO 140084067562880] #quality_metric: host=algo-1, epoch=23, batch=5 train loss <loss>=10.461663246154785\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:31 INFO 140084067562880] Epoch[23] Batch [5]#011Speed: 2102.56 samples/sec#011loss=10.461663\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:31 INFO 140084067562880] Epoch[23] Batch[10] avg_epoch_loss=10.322329\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:31 INFO 140084067562880] #quality_metric: host=algo-1, epoch=23, batch=10 train loss <loss>=10.15512752532959\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:31 INFO 140084067562880] Epoch[23] Batch [10]#011Speed: 1884.80 samples/sec#011loss=10.155128\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:31 INFO 140084067562880] processed a total of 686 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463090.9017234, \"EndTime\": 1631463091.4671783, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 564.9333000183105, \"count\": 1, \"min\": 564.9333000183105, \"max\": 564.9333000183105}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:31 INFO 140084067562880] #throughput_metric: host=algo-1, train throughput=1214.0193134758476 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:31 INFO 140084067562880] #progress_metric: host=algo-1, completed 12.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:31 INFO 140084067562880] #quality_metric: host=algo-1, epoch=23, train loss <loss>=10.322328827597879\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:31 INFO 140084067562880] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:31 INFO 140084067562880] Epoch[24] Batch[0] avg_epoch_loss=10.793716\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:31 INFO 140084067562880] #quality_metric: host=algo-1, epoch=24, batch=0 train loss <loss>=10.793716430664062\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:31 INFO 140084067562880] Epoch[24] Batch[5] avg_epoch_loss=10.362733\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:31 INFO 140084067562880] #quality_metric: host=algo-1, epoch=24, batch=5 train loss <loss>=10.362733205159506\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:31 INFO 140084067562880] Epoch[24] Batch [5]#011Speed: 2095.41 samples/sec#011loss=10.362733\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:32 INFO 140084067562880] Epoch[24] Batch[10] avg_epoch_loss=10.314490\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:32 INFO 140084067562880] #quality_metric: host=algo-1, epoch=24, batch=10 train loss <loss>=10.256597137451172\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:32 INFO 140084067562880] Epoch[24] Batch [10]#011Speed: 1766.59 samples/sec#011loss=10.256597\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:32 INFO 140084067562880] processed a total of 682 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463091.467269, \"EndTime\": 1631463092.0372093, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 569.4265365600586, \"count\": 1, \"min\": 569.4265365600586, \"max\": 569.4265365600586}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:32 INFO 140084067562880] #throughput_metric: host=algo-1, train throughput=1197.4680741777368 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:32 INFO 140084067562880] #progress_metric: host=algo-1, completed 12.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:32 INFO 140084067562880] #quality_metric: host=algo-1, epoch=24, train loss <loss>=10.314489538019354\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:32 INFO 140084067562880] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:32 INFO 140084067562880] Epoch[25] Batch[0] avg_epoch_loss=11.005616\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:32 INFO 140084067562880] #quality_metric: host=algo-1, epoch=25, batch=0 train loss <loss>=11.005616188049316\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:32 INFO 140084067562880] Epoch[25] Batch[5] avg_epoch_loss=10.104767\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:32 INFO 140084067562880] #quality_metric: host=algo-1, epoch=25, batch=5 train loss <loss>=10.104767481486002\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:32 INFO 140084067562880] Epoch[25] Batch [5]#011Speed: 1968.47 samples/sec#011loss=10.104767\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:32 INFO 140084067562880] processed a total of 624 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463092.0372872, \"EndTime\": 1631463092.5766473, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 538.9189720153809, \"count\": 1, \"min\": 538.9189720153809, \"max\": 538.9189720153809}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:32 INFO 140084067562880] #throughput_metric: host=algo-1, train throughput=1157.592864554446 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:32 INFO 140084067562880] #progress_metric: host=algo-1, completed 13.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:32 INFO 140084067562880] #quality_metric: host=algo-1, epoch=25, train loss <loss>=10.278859996795655\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:32 INFO 140084067562880] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:32 INFO 140084067562880] Epoch[26] Batch[0] avg_epoch_loss=10.416624\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:32 INFO 140084067562880] #quality_metric: host=algo-1, epoch=26, batch=0 train loss <loss>=10.416624069213867\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:33 INFO 140084067562880] Epoch[26] Batch[5] avg_epoch_loss=10.481122\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:33 INFO 140084067562880] #quality_metric: host=algo-1, epoch=26, batch=5 train loss <loss>=10.481122493743896\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:33 INFO 140084067562880] Epoch[26] Batch [5]#011Speed: 1880.17 samples/sec#011loss=10.481122\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:33 INFO 140084067562880] Epoch[26] Batch[10] avg_epoch_loss=9.999921\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:33 INFO 140084067562880] #quality_metric: host=algo-1, epoch=26, batch=10 train loss <loss>=9.422478199005127\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:33 INFO 140084067562880] Epoch[26] Batch [10]#011Speed: 1785.82 samples/sec#011loss=9.422478\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:33 INFO 140084067562880] processed a total of 657 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463092.5767386, \"EndTime\": 1631463093.2029767, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 625.7152557373047, \"count\": 1, \"min\": 625.7152557373047, \"max\": 625.7152557373047}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:33 INFO 140084067562880] #throughput_metric: host=algo-1, train throughput=1049.7979680305195 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:33 INFO 140084067562880] #progress_metric: host=algo-1, completed 13.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:33 INFO 140084067562880] #quality_metric: host=algo-1, epoch=26, train loss <loss>=9.99992054158991\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:33 INFO 140084067562880] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:33 INFO 140084067562880] Saved checkpoint to \"/opt/ml/model/state_c52645e7-115b-4d7c-93f7-1a731fb4619b-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463093.2030592, \"EndTime\": 1631463093.2144926, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 10.900735855102539, \"count\": 1, \"min\": 10.900735855102539, \"max\": 10.900735855102539}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:33 INFO 140084067562880] Epoch[27] Batch[0] avg_epoch_loss=9.591137\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:33 INFO 140084067562880] #quality_metric: host=algo-1, epoch=27, batch=0 train loss <loss>=9.591136932373047\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:33 INFO 140084067562880] Epoch[27] Batch[5] avg_epoch_loss=10.073158\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:33 INFO 140084067562880] #quality_metric: host=algo-1, epoch=27, batch=5 train loss <loss>=10.073158105214437\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:33 INFO 140084067562880] Epoch[27] Batch [5]#011Speed: 1953.45 samples/sec#011loss=10.073158\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:33 INFO 140084067562880] Epoch[27] Batch[10] avg_epoch_loss=10.224428\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:33 INFO 140084067562880] #quality_metric: host=algo-1, epoch=27, batch=10 train loss <loss>=10.405952644348144\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:33 INFO 140084067562880] Epoch[27] Batch [10]#011Speed: 1927.53 samples/sec#011loss=10.405953\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:33 INFO 140084067562880] processed a total of 668 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463093.2145653, \"EndTime\": 1631463093.782675, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 568.0539608001709, \"count\": 1, \"min\": 568.0539608001709, \"max\": 568.0539608001709}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:33 INFO 140084067562880] #throughput_metric: host=algo-1, train throughput=1175.7107221162662 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:33 INFO 140084067562880] #progress_metric: host=algo-1, completed 14.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:33 INFO 140084067562880] #quality_metric: host=algo-1, epoch=27, train loss <loss>=10.224428350275213\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:33 INFO 140084067562880] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:34 INFO 140084067562880] Epoch[28] Batch[0] avg_epoch_loss=10.276213\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:34 INFO 140084067562880] #quality_metric: host=algo-1, epoch=28, batch=0 train loss <loss>=10.276212692260742\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:34 INFO 140084067562880] Epoch[28] Batch[5] avg_epoch_loss=10.359002\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:34 INFO 140084067562880] #quality_metric: host=algo-1, epoch=28, batch=5 train loss <loss>=10.359002113342285\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:34 INFO 140084067562880] Epoch[28] Batch [5]#011Speed: 2108.61 samples/sec#011loss=10.359002\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:34 INFO 140084067562880] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463093.7827556, \"EndTime\": 1631463094.4005013, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 617.2795295715332, \"count\": 1, \"min\": 617.2795295715332, \"max\": 617.2795295715332}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:34 INFO 140084067562880] #throughput_metric: host=algo-1, train throughput=1025.2610296886096 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:34 INFO 140084067562880] #progress_metric: host=algo-1, completed 14.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:34 INFO 140084067562880] #quality_metric: host=algo-1, epoch=28, train loss <loss>=10.343198680877686\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:34 INFO 140084067562880] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:34 INFO 140084067562880] Epoch[29] Batch[0] avg_epoch_loss=9.992613\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:34 INFO 140084067562880] #quality_metric: host=algo-1, epoch=29, batch=0 train loss <loss>=9.992612838745117\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:34 INFO 140084067562880] Epoch[29] Batch[5] avg_epoch_loss=10.392579\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:34 INFO 140084067562880] #quality_metric: host=algo-1, epoch=29, batch=5 train loss <loss>=10.392578760782877\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:34 INFO 140084067562880] Epoch[29] Batch [5]#011Speed: 2129.04 samples/sec#011loss=10.392579\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:34 INFO 140084067562880] processed a total of 609 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463094.4005876, \"EndTime\": 1631463094.949135, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 548.0165481567383, \"count\": 1, \"min\": 548.0165481567383, \"max\": 548.0165481567383}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:34 INFO 140084067562880] #throughput_metric: host=algo-1, train throughput=1111.032345814226 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:34 INFO 140084067562880] #progress_metric: host=algo-1, completed 15.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:34 INFO 140084067562880] #quality_metric: host=algo-1, epoch=29, train loss <loss>=10.4228759765625\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:34 INFO 140084067562880] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:35 INFO 140084067562880] Epoch[30] Batch[0] avg_epoch_loss=10.051394\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:35 INFO 140084067562880] #quality_metric: host=algo-1, epoch=30, batch=0 train loss <loss>=10.051393508911133\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:35 INFO 140084067562880] Epoch[30] Batch[5] avg_epoch_loss=9.997554\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:35 INFO 140084067562880] #quality_metric: host=algo-1, epoch=30, batch=5 train loss <loss>=9.997554143269857\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:35 INFO 140084067562880] Epoch[30] Batch [5]#011Speed: 1919.86 samples/sec#011loss=9.997554\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:35 INFO 140084067562880] processed a total of 617 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463094.9492211, \"EndTime\": 1631463095.4981737, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 548.459529876709, \"count\": 1, \"min\": 548.459529876709, \"max\": 548.459529876709}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:35 INFO 140084067562880] #throughput_metric: host=algo-1, train throughput=1124.7123742519875 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:35 INFO 140084067562880] #progress_metric: host=algo-1, completed 15.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:35 INFO 140084067562880] #quality_metric: host=algo-1, epoch=30, train loss <loss>=10.07140474319458\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:35 INFO 140084067562880] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:35 INFO 140084067562880] Epoch[31] Batch[0] avg_epoch_loss=9.581840\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:35 INFO 140084067562880] #quality_metric: host=algo-1, epoch=31, batch=0 train loss <loss>=9.581839561462402\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:35 INFO 140084067562880] Epoch[31] Batch[5] avg_epoch_loss=10.508728\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:35 INFO 140084067562880] #quality_metric: host=algo-1, epoch=31, batch=5 train loss <loss>=10.508727868398031\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:35 INFO 140084067562880] Epoch[31] Batch [5]#011Speed: 2061.08 samples/sec#011loss=10.508728\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:36 INFO 140084067562880] processed a total of 639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463095.498261, \"EndTime\": 1631463096.048469, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 549.7028827667236, \"count\": 1, \"min\": 549.7028827667236, \"max\": 549.7028827667236}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:36 INFO 140084067562880] #throughput_metric: host=algo-1, train throughput=1162.1784930047977 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:36 INFO 140084067562880] #progress_metric: host=algo-1, completed 16.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:36 INFO 140084067562880] #quality_metric: host=algo-1, epoch=31, train loss <loss>=10.377014255523681\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:36 INFO 140084067562880] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:36 INFO 140084067562880] Epoch[32] Batch[0] avg_epoch_loss=9.524277\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:36 INFO 140084067562880] #quality_metric: host=algo-1, epoch=32, batch=0 train loss <loss>=9.524276733398438\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:36 INFO 140084067562880] Epoch[32] Batch[5] avg_epoch_loss=10.356786\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:36 INFO 140084067562880] #quality_metric: host=algo-1, epoch=32, batch=5 train loss <loss>=10.356786092122396\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:36 INFO 140084067562880] Epoch[32] Batch [5]#011Speed: 2100.94 samples/sec#011loss=10.356786\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:36 INFO 140084067562880] Epoch[32] Batch[10] avg_epoch_loss=10.716322\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:36 INFO 140084067562880] #quality_metric: host=algo-1, epoch=32, batch=10 train loss <loss>=11.147764015197755\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:36 INFO 140084067562880] Epoch[32] Batch [10]#011Speed: 1982.35 samples/sec#011loss=11.147764\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:36 INFO 140084067562880] processed a total of 643 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463096.0485573, \"EndTime\": 1631463096.5994706, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 550.386905670166, \"count\": 1, \"min\": 550.386905670166, \"max\": 550.386905670166}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:36 INFO 140084067562880] #throughput_metric: host=algo-1, train throughput=1168.0049614293905 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:36 INFO 140084067562880] #progress_metric: host=algo-1, completed 16.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:36 INFO 140084067562880] #quality_metric: host=algo-1, epoch=32, train loss <loss>=10.716321511702104\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:36 INFO 140084067562880] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:36 INFO 140084067562880] Epoch[33] Batch[0] avg_epoch_loss=10.052544\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:36 INFO 140084067562880] #quality_metric: host=algo-1, epoch=33, batch=0 train loss <loss>=10.052543640136719\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:36 INFO 140084067562880] Epoch[33] Batch[5] avg_epoch_loss=10.239470\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:36 INFO 140084067562880] #quality_metric: host=algo-1, epoch=33, batch=5 train loss <loss>=10.239469846089682\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:36 INFO 140084067562880] Epoch[33] Batch [5]#011Speed: 2081.00 samples/sec#011loss=10.239470\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:37 INFO 140084067562880] processed a total of 616 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463096.5995548, \"EndTime\": 1631463097.1203113, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 520.2693939208984, \"count\": 1, \"min\": 520.2693939208984, \"max\": 520.2693939208984}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:37 INFO 140084067562880] #throughput_metric: host=algo-1, train throughput=1183.7155810978716 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:37 INFO 140084067562880] #progress_metric: host=algo-1, completed 17.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:37 INFO 140084067562880] #quality_metric: host=algo-1, epoch=33, train loss <loss>=9.918480157852173\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:37 INFO 140084067562880] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:37 INFO 140084067562880] Saved checkpoint to \"/opt/ml/model/state_1de4770f-8c7b-4ffb-95c3-5aa8593ee9b7-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463097.1203983, \"EndTime\": 1631463097.1396124, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 18.660545349121094, \"count\": 1, \"min\": 18.660545349121094, \"max\": 18.660545349121094}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:37 INFO 140084067562880] Epoch[34] Batch[0] avg_epoch_loss=10.840281\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:37 INFO 140084067562880] #quality_metric: host=algo-1, epoch=34, batch=0 train loss <loss>=10.840280532836914\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:37 INFO 140084067562880] Epoch[34] Batch[5] avg_epoch_loss=10.301876\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:37 INFO 140084067562880] #quality_metric: host=algo-1, epoch=34, batch=5 train loss <loss>=10.301875909169516\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:37 INFO 140084067562880] Epoch[34] Batch [5]#011Speed: 1957.95 samples/sec#011loss=10.301876\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:37 INFO 140084067562880] processed a total of 636 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463097.1396825, \"EndTime\": 1631463097.714195, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 574.4552612304688, \"count\": 1, \"min\": 574.4552612304688, \"max\": 574.4552612304688}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:37 INFO 140084067562880] #throughput_metric: host=algo-1, train throughput=1106.9010714325727 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:37 INFO 140084067562880] #progress_metric: host=algo-1, completed 17.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:37 INFO 140084067562880] #quality_metric: host=algo-1, epoch=34, train loss <loss>=10.347736358642578\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:37 INFO 140084067562880] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:37 INFO 140084067562880] Epoch[35] Batch[0] avg_epoch_loss=10.047257\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:37 INFO 140084067562880] #quality_metric: host=algo-1, epoch=35, batch=0 train loss <loss>=10.047257423400879\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:38 INFO 140084067562880] Epoch[35] Batch[5] avg_epoch_loss=10.219993\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:38 INFO 140084067562880] #quality_metric: host=algo-1, epoch=35, batch=5 train loss <loss>=10.219992955525717\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:38 INFO 140084067562880] Epoch[35] Batch [5]#011Speed: 2052.90 samples/sec#011loss=10.219993\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:38 INFO 140084067562880] processed a total of 622 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463097.7142825, \"EndTime\": 1631463098.2531438, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 538.3551120758057, \"count\": 1, \"min\": 538.3551120758057, \"max\": 538.3551120758057}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:38 INFO 140084067562880] #throughput_metric: host=algo-1, train throughput=1155.1041833532797 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:38 INFO 140084067562880] #progress_metric: host=algo-1, completed 18.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:38 INFO 140084067562880] #quality_metric: host=algo-1, epoch=35, train loss <loss>=10.28642692565918\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:38 INFO 140084067562880] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:38 INFO 140084067562880] Epoch[36] Batch[0] avg_epoch_loss=10.083684\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:38 INFO 140084067562880] #quality_metric: host=algo-1, epoch=36, batch=0 train loss <loss>=10.083683967590332\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:38 INFO 140084067562880] Epoch[36] Batch[5] avg_epoch_loss=10.459209\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:38 INFO 140084067562880] #quality_metric: host=algo-1, epoch=36, batch=5 train loss <loss>=10.459209442138672\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:38 INFO 140084067562880] Epoch[36] Batch [5]#011Speed: 1747.27 samples/sec#011loss=10.459209\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:38 INFO 140084067562880] Epoch[36] Batch[10] avg_epoch_loss=10.407566\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:38 INFO 140084067562880] #quality_metric: host=algo-1, epoch=36, batch=10 train loss <loss>=10.345593643188476\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:38 INFO 140084067562880] Epoch[36] Batch [10]#011Speed: 1955.71 samples/sec#011loss=10.345594\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:38 INFO 140084067562880] processed a total of 664 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463098.2532306, \"EndTime\": 1631463098.861072, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 607.3310375213623, \"count\": 1, \"min\": 607.3310375213623, \"max\": 607.3310375213623}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:38 INFO 140084067562880] #throughput_metric: host=algo-1, train throughput=1093.0841991276577 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:38 INFO 140084067562880] #progress_metric: host=algo-1, completed 18.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:38 INFO 140084067562880] #quality_metric: host=algo-1, epoch=36, train loss <loss>=10.40756589716131\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:38 INFO 140084067562880] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:39 INFO 140084067562880] Epoch[37] Batch[0] avg_epoch_loss=10.312931\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:39 INFO 140084067562880] #quality_metric: host=algo-1, epoch=37, batch=0 train loss <loss>=10.312931060791016\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:39 INFO 140084067562880] Epoch[37] Batch[5] avg_epoch_loss=10.247623\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:39 INFO 140084067562880] #quality_metric: host=algo-1, epoch=37, batch=5 train loss <loss>=10.247622648874918\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:39 INFO 140084067562880] Epoch[37] Batch [5]#011Speed: 2009.38 samples/sec#011loss=10.247623\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:39 INFO 140084067562880] processed a total of 627 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463098.8611577, \"EndTime\": 1631463099.3968966, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 535.2215766906738, \"count\": 1, \"min\": 535.2215766906738, \"max\": 535.2215766906738}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:39 INFO 140084067562880] #throughput_metric: host=algo-1, train throughput=1171.2071826845997 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:39 INFO 140084067562880] #progress_metric: host=algo-1, completed 19.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:39 INFO 140084067562880] #quality_metric: host=algo-1, epoch=37, train loss <loss>=10.521413993835449\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:39 INFO 140084067562880] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:39 INFO 140084067562880] Epoch[38] Batch[0] avg_epoch_loss=10.392224\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:39 INFO 140084067562880] #quality_metric: host=algo-1, epoch=38, batch=0 train loss <loss>=10.392224311828613\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:39 INFO 140084067562880] Epoch[38] Batch[5] avg_epoch_loss=10.282839\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:39 INFO 140084067562880] #quality_metric: host=algo-1, epoch=38, batch=5 train loss <loss>=10.28283945719401\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:39 INFO 140084067562880] Epoch[38] Batch [5]#011Speed: 2113.52 samples/sec#011loss=10.282839\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:39 INFO 140084067562880] processed a total of 586 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463099.3969808, \"EndTime\": 1631463099.9780252, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 580.5284976959229, \"count\": 1, \"min\": 580.5284976959229, \"max\": 580.5284976959229}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:39 INFO 140084067562880] #throughput_metric: host=algo-1, train throughput=1009.202465252828 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:39 INFO 140084067562880] #progress_metric: host=algo-1, completed 19.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:39 INFO 140084067562880] #quality_metric: host=algo-1, epoch=38, train loss <loss>=10.69128189086914\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:39 INFO 140084067562880] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:40 INFO 140084067562880] Epoch[39] Batch[0] avg_epoch_loss=10.390980\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:40 INFO 140084067562880] #quality_metric: host=algo-1, epoch=39, batch=0 train loss <loss>=10.390979766845703\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:40 INFO 140084067562880] Epoch[39] Batch[5] avg_epoch_loss=10.222381\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:40 INFO 140084067562880] #quality_metric: host=algo-1, epoch=39, batch=5 train loss <loss>=10.222380956013998\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:40 INFO 140084067562880] Epoch[39] Batch [5]#011Speed: 1629.66 samples/sec#011loss=10.222381\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:40 INFO 140084067562880] Epoch[39] Batch[10] avg_epoch_loss=10.089303\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:40 INFO 140084067562880] #quality_metric: host=algo-1, epoch=39, batch=10 train loss <loss>=9.929608917236328\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:40 INFO 140084067562880] Epoch[39] Batch [10]#011Speed: 1765.34 samples/sec#011loss=9.929609\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:40 INFO 140084067562880] processed a total of 679 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463099.9781146, \"EndTime\": 1631463100.603036, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 624.41086769104, \"count\": 1, \"min\": 624.41086769104, \"max\": 624.41086769104}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:40 INFO 140084067562880] #throughput_metric: host=algo-1, train throughput=1087.214548253162 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:40 INFO 140084067562880] #progress_metric: host=algo-1, completed 20.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:40 INFO 140084067562880] #quality_metric: host=algo-1, epoch=39, train loss <loss>=10.089302756569602\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:40 INFO 140084067562880] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:40 INFO 140084067562880] Epoch[40] Batch[0] avg_epoch_loss=9.679522\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:40 INFO 140084067562880] #quality_metric: host=algo-1, epoch=40, batch=0 train loss <loss>=9.679521560668945\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:41 INFO 140084067562880] Epoch[40] Batch[5] avg_epoch_loss=9.975029\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:41 INFO 140084067562880] #quality_metric: host=algo-1, epoch=40, batch=5 train loss <loss>=9.975029309590658\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:41 INFO 140084067562880] Epoch[40] Batch [5]#011Speed: 2127.45 samples/sec#011loss=9.975029\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:41 INFO 140084067562880] processed a total of 638 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463100.6031203, \"EndTime\": 1631463101.144692, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 541.0592555999756, \"count\": 1, \"min\": 541.0592555999756, \"max\": 541.0592555999756}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:41 INFO 140084067562880] #throughput_metric: host=algo-1, train throughput=1178.8879244832497 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:41 INFO 140084067562880] #progress_metric: host=algo-1, completed 20.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:41 INFO 140084067562880] #quality_metric: host=algo-1, epoch=40, train loss <loss>=10.110918998718262\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:41 INFO 140084067562880] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:41 INFO 140084067562880] Epoch[41] Batch[0] avg_epoch_loss=10.135797\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:41 INFO 140084067562880] #quality_metric: host=algo-1, epoch=41, batch=0 train loss <loss>=10.135796546936035\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:41 INFO 140084067562880] Epoch[41] Batch[5] avg_epoch_loss=10.290863\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:41 INFO 140084067562880] #quality_metric: host=algo-1, epoch=41, batch=5 train loss <loss>=10.290862878163656\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:41 INFO 140084067562880] Epoch[41] Batch [5]#011Speed: 1934.10 samples/sec#011loss=10.290863\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:41 INFO 140084067562880] Epoch[41] Batch[10] avg_epoch_loss=10.076662\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:41 INFO 140084067562880] #quality_metric: host=algo-1, epoch=41, batch=10 train loss <loss>=9.81962184906006\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:41 INFO 140084067562880] Epoch[41] Batch [10]#011Speed: 1665.54 samples/sec#011loss=9.819622\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:41 INFO 140084067562880] processed a total of 702 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463101.1447809, \"EndTime\": 1631463101.7601976, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 614.9146556854248, \"count\": 1, \"min\": 614.9146556854248, \"max\": 614.9146556854248}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:41 INFO 140084067562880] #throughput_metric: host=algo-1, train throughput=1141.4138375234386 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:41 INFO 140084067562880] #progress_metric: host=algo-1, completed 21.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:41 INFO 140084067562880] #quality_metric: host=algo-1, epoch=41, train loss <loss>=10.076662410389293\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:41 INFO 140084067562880] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:42 INFO 140084067562880] Epoch[42] Batch[0] avg_epoch_loss=9.432406\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:42 INFO 140084067562880] #quality_metric: host=algo-1, epoch=42, batch=0 train loss <loss>=9.432406425476074\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:42 INFO 140084067562880] Epoch[42] Batch[5] avg_epoch_loss=9.969980\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:42 INFO 140084067562880] #quality_metric: host=algo-1, epoch=42, batch=5 train loss <loss>=9.969980398813883\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:42 INFO 140084067562880] Epoch[42] Batch [5]#011Speed: 2114.20 samples/sec#011loss=9.969980\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:42 INFO 140084067562880] Epoch[42] Batch[10] avg_epoch_loss=9.770725\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:42 INFO 140084067562880] #quality_metric: host=algo-1, epoch=42, batch=10 train loss <loss>=9.531618404388428\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:42 INFO 140084067562880] Epoch[42] Batch [10]#011Speed: 1954.47 samples/sec#011loss=9.531618\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:42 INFO 140084067562880] processed a total of 668 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463101.760275, \"EndTime\": 1631463102.3475044, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 586.7505073547363, \"count\": 1, \"min\": 586.7505073547363, \"max\": 586.7505073547363}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:42 INFO 140084067562880] #throughput_metric: host=algo-1, train throughput=1138.2465706899977 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:42 INFO 140084067562880] #progress_metric: host=algo-1, completed 21.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:42 INFO 140084067562880] #quality_metric: host=algo-1, epoch=42, train loss <loss>=9.770724946802313\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:42 INFO 140084067562880] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:42 INFO 140084067562880] Saved checkpoint to \"/opt/ml/model/state_2db82746-e5e1-4f67-acd0-978c9ffff429-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463102.3475866, \"EndTime\": 1631463102.3663008, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 18.229246139526367, \"count\": 1, \"min\": 18.229246139526367, \"max\": 18.229246139526367}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:42 INFO 140084067562880] Epoch[43] Batch[0] avg_epoch_loss=9.732444\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:42 INFO 140084067562880] #quality_metric: host=algo-1, epoch=43, batch=0 train loss <loss>=9.732443809509277\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:42 INFO 140084067562880] Epoch[43] Batch[5] avg_epoch_loss=9.903827\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:42 INFO 140084067562880] #quality_metric: host=algo-1, epoch=43, batch=5 train loss <loss>=9.90382687250773\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:42 INFO 140084067562880] Epoch[43] Batch [5]#011Speed: 2051.33 samples/sec#011loss=9.903827\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:42 INFO 140084067562880] Epoch[43] Batch[10] avg_epoch_loss=10.544357\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:42 INFO 140084067562880] #quality_metric: host=algo-1, epoch=43, batch=10 train loss <loss>=11.312992477416993\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:42 INFO 140084067562880] Epoch[43] Batch [10]#011Speed: 2018.50 samples/sec#011loss=11.312992\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:42 INFO 140084067562880] processed a total of 648 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463102.3663652, \"EndTime\": 1631463102.9329054, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 566.4844512939453, \"count\": 1, \"min\": 566.4844512939453, \"max\": 566.4844512939453}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:42 INFO 140084067562880] #throughput_metric: host=algo-1, train throughput=1143.658496682522 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:42 INFO 140084067562880] #progress_metric: host=algo-1, completed 22.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:42 INFO 140084067562880] #quality_metric: host=algo-1, epoch=43, train loss <loss>=10.544356692921031\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:42 INFO 140084067562880] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:43 INFO 140084067562880] Epoch[44] Batch[0] avg_epoch_loss=9.445262\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:43 INFO 140084067562880] #quality_metric: host=algo-1, epoch=44, batch=0 train loss <loss>=9.44526195526123\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:43 INFO 140084067562880] Epoch[44] Batch[5] avg_epoch_loss=10.561589\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:43 INFO 140084067562880] #quality_metric: host=algo-1, epoch=44, batch=5 train loss <loss>=10.561588605244955\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:43 INFO 140084067562880] Epoch[44] Batch [5]#011Speed: 2044.36 samples/sec#011loss=10.561589\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:43 INFO 140084067562880] processed a total of 640 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463102.9329894, \"EndTime\": 1631463103.4768655, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 543.4005260467529, \"count\": 1, \"min\": 543.4005260467529, \"max\": 543.4005260467529}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:43 INFO 140084067562880] #throughput_metric: host=algo-1, train throughput=1177.5230538562296 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:43 INFO 140084067562880] #progress_metric: host=algo-1, completed 22.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:43 INFO 140084067562880] #quality_metric: host=algo-1, epoch=44, train loss <loss>=10.392494583129883\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:43 INFO 140084067562880] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:43 INFO 140084067562880] Epoch[45] Batch[0] avg_epoch_loss=10.111364\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:43 INFO 140084067562880] #quality_metric: host=algo-1, epoch=45, batch=0 train loss <loss>=10.111364364624023\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:43 INFO 140084067562880] Epoch[45] Batch[5] avg_epoch_loss=9.724079\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:43 INFO 140084067562880] #quality_metric: host=algo-1, epoch=45, batch=5 train loss <loss>=9.72407865524292\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:43 INFO 140084067562880] Epoch[45] Batch [5]#011Speed: 1962.39 samples/sec#011loss=9.724079\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:44 INFO 140084067562880] Epoch[45] Batch[10] avg_epoch_loss=9.387790\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:44 INFO 140084067562880] #quality_metric: host=algo-1, epoch=45, batch=10 train loss <loss>=8.984242916107178\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:44 INFO 140084067562880] Epoch[45] Batch [10]#011Speed: 1813.23 samples/sec#011loss=8.984243\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:44 INFO 140084067562880] processed a total of 665 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463103.4769413, \"EndTime\": 1631463104.072232, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 594.8350429534912, \"count\": 1, \"min\": 594.8350429534912, \"max\": 594.8350429534912}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:44 INFO 140084067562880] #throughput_metric: host=algo-1, train throughput=1117.7576337560372 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:44 INFO 140084067562880] #progress_metric: host=algo-1, completed 23.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:44 INFO 140084067562880] #quality_metric: host=algo-1, epoch=45, train loss <loss>=9.387789682908492\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:44 INFO 140084067562880] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:44 INFO 140084067562880] Saved checkpoint to \"/opt/ml/model/state_098e6d65-39a0-469a-ac18-dc58846b4136-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463104.0723, \"EndTime\": 1631463104.083895, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 11.087656021118164, \"count\": 1, \"min\": 11.087656021118164, \"max\": 11.087656021118164}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:44 INFO 140084067562880] Epoch[46] Batch[0] avg_epoch_loss=10.214209\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:44 INFO 140084067562880] #quality_metric: host=algo-1, epoch=46, batch=0 train loss <loss>=10.214208602905273\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:44 INFO 140084067562880] Epoch[46] Batch[5] avg_epoch_loss=10.247302\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:44 INFO 140084067562880] #quality_metric: host=algo-1, epoch=46, batch=5 train loss <loss>=10.247302055358887\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:44 INFO 140084067562880] Epoch[46] Batch [5]#011Speed: 1960.61 samples/sec#011loss=10.247302\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:44 INFO 140084067562880] processed a total of 619 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463104.0839353, \"EndTime\": 1631463104.6195095, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 535.529375076294, \"count\": 1, \"min\": 535.529375076294, \"max\": 535.529375076294}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:44 INFO 140084067562880] #throughput_metric: host=algo-1, train throughput=1155.6063359352502 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:44 INFO 140084067562880] #progress_metric: host=algo-1, completed 23.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:44 INFO 140084067562880] #quality_metric: host=algo-1, epoch=46, train loss <loss>=9.87007179260254\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:44 INFO 140084067562880] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:44 INFO 140084067562880] Epoch[47] Batch[0] avg_epoch_loss=10.585101\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:44 INFO 140084067562880] #quality_metric: host=algo-1, epoch=47, batch=0 train loss <loss>=10.585101127624512\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:45 INFO 140084067562880] Epoch[47] Batch[5] avg_epoch_loss=9.907725\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:45 INFO 140084067562880] #quality_metric: host=algo-1, epoch=47, batch=5 train loss <loss>=9.907724857330322\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:45 INFO 140084067562880] Epoch[47] Batch [5]#011Speed: 1837.15 samples/sec#011loss=9.907725\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:45 INFO 140084067562880] processed a total of 622 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463104.619597, \"EndTime\": 1631463105.2303224, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 610.1975440979004, \"count\": 1, \"min\": 610.1975440979004, \"max\": 610.1975440979004}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:45 INFO 140084067562880] #throughput_metric: host=algo-1, train throughput=1019.1254057778865 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:45 INFO 140084067562880] #progress_metric: host=algo-1, completed 24.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:45 INFO 140084067562880] #quality_metric: host=algo-1, epoch=47, train loss <loss>=10.151851177215576\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:45 INFO 140084067562880] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:45 INFO 140084067562880] Epoch[48] Batch[0] avg_epoch_loss=10.009384\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:45 INFO 140084067562880] #quality_metric: host=algo-1, epoch=48, batch=0 train loss <loss>=10.009384155273438\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:45 INFO 140084067562880] Epoch[48] Batch[5] avg_epoch_loss=10.169802\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:45 INFO 140084067562880] #quality_metric: host=algo-1, epoch=48, batch=5 train loss <loss>=10.169802029927572\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:45 INFO 140084067562880] Epoch[48] Batch [5]#011Speed: 1912.43 samples/sec#011loss=10.169802\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:45 INFO 140084067562880] processed a total of 572 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463105.230414, \"EndTime\": 1631463105.7873142, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 556.4255714416504, \"count\": 1, \"min\": 556.4255714416504, \"max\": 556.4255714416504}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:45 INFO 140084067562880] #throughput_metric: host=algo-1, train throughput=1027.7422100334093 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:45 INFO 140084067562880] #progress_metric: host=algo-1, completed 24.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:45 INFO 140084067562880] #quality_metric: host=algo-1, epoch=48, train loss <loss>=10.31730047861735\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:45 INFO 140084067562880] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:46 INFO 140084067562880] Epoch[49] Batch[0] avg_epoch_loss=10.253188\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:46 INFO 140084067562880] #quality_metric: host=algo-1, epoch=49, batch=0 train loss <loss>=10.253188133239746\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:46 INFO 140084067562880] Epoch[49] Batch[5] avg_epoch_loss=10.243611\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:46 INFO 140084067562880] #quality_metric: host=algo-1, epoch=49, batch=5 train loss <loss>=10.243610699971518\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:46 INFO 140084067562880] Epoch[49] Batch [5]#011Speed: 1888.15 samples/sec#011loss=10.243611\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:46 INFO 140084067562880] processed a total of 623 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463105.7874084, \"EndTime\": 1631463106.3160636, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 528.0873775482178, \"count\": 1, \"min\": 528.0873775482178, \"max\": 528.0873775482178}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:46 INFO 140084067562880] #throughput_metric: host=algo-1, train throughput=1179.2924258240555 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:46 INFO 140084067562880] #progress_metric: host=algo-1, completed 25.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:46 INFO 140084067562880] #quality_metric: host=algo-1, epoch=49, train loss <loss>=10.14325828552246\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:46 INFO 140084067562880] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:46 INFO 140084067562880] Epoch[50] Batch[0] avg_epoch_loss=9.264230\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:46 INFO 140084067562880] #quality_metric: host=algo-1, epoch=50, batch=0 train loss <loss>=9.264229774475098\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:46 INFO 140084067562880] Epoch[50] Batch[5] avg_epoch_loss=9.769839\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:46 INFO 140084067562880] #quality_metric: host=algo-1, epoch=50, batch=5 train loss <loss>=9.769838651021322\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:46 INFO 140084067562880] Epoch[50] Batch [5]#011Speed: 2108.45 samples/sec#011loss=9.769839\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:46 INFO 140084067562880] processed a total of 638 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463106.3162153, \"EndTime\": 1631463106.833844, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 517.0614719390869, \"count\": 1, \"min\": 517.0614719390869, \"max\": 517.0614719390869}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:46 INFO 140084067562880] #throughput_metric: host=algo-1, train throughput=1233.573898055529 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:46 INFO 140084067562880] #progress_metric: host=algo-1, completed 25.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:46 INFO 140084067562880] #quality_metric: host=algo-1, epoch=50, train loss <loss>=9.95110330581665\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:46 INFO 140084067562880] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:47 INFO 140084067562880] Epoch[51] Batch[0] avg_epoch_loss=10.807998\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:47 INFO 140084067562880] #quality_metric: host=algo-1, epoch=51, batch=0 train loss <loss>=10.807997703552246\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:47 INFO 140084067562880] Epoch[51] Batch[5] avg_epoch_loss=9.991883\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:47 INFO 140084067562880] #quality_metric: host=algo-1, epoch=51, batch=5 train loss <loss>=9.991882801055908\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:47 INFO 140084067562880] Epoch[51] Batch [5]#011Speed: 1979.53 samples/sec#011loss=9.991883\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:47 INFO 140084067562880] Epoch[51] Batch[10] avg_epoch_loss=9.218741\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:47 INFO 140084067562880] #quality_metric: host=algo-1, epoch=51, batch=10 train loss <loss>=8.290970277786254\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:47 INFO 140084067562880] Epoch[51] Batch [10]#011Speed: 2038.10 samples/sec#011loss=8.290970\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:47 INFO 140084067562880] processed a total of 643 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463106.8339417, \"EndTime\": 1631463107.4040816, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 569.6213245391846, \"count\": 1, \"min\": 569.6213245391846, \"max\": 569.6213245391846}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:47 INFO 140084067562880] #throughput_metric: host=algo-1, train throughput=1128.5876893629504 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:47 INFO 140084067562880] #progress_metric: host=algo-1, completed 26.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:47 INFO 140084067562880] #quality_metric: host=algo-1, epoch=51, train loss <loss>=9.218740745024247\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:47 INFO 140084067562880] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:47 INFO 140084067562880] Saved checkpoint to \"/opt/ml/model/state_93857a48-ca5d-4cc3-aa6d-7b752b1d5762-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463107.404164, \"EndTime\": 1631463107.4173052, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 12.573719024658203, \"count\": 1, \"min\": 12.573719024658203, \"max\": 12.573719024658203}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:47 INFO 140084067562880] Epoch[52] Batch[0] avg_epoch_loss=9.206234\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:47 INFO 140084067562880] #quality_metric: host=algo-1, epoch=52, batch=0 train loss <loss>=9.206233978271484\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:47 INFO 140084067562880] Epoch[52] Batch[5] avg_epoch_loss=9.696846\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:47 INFO 140084067562880] #quality_metric: host=algo-1, epoch=52, batch=5 train loss <loss>=9.696846008300781\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:47 INFO 140084067562880] Epoch[52] Batch [5]#011Speed: 2010.31 samples/sec#011loss=9.696846\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:47 INFO 140084067562880] Epoch[52] Batch[10] avg_epoch_loss=10.114166\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:47 INFO 140084067562880] #quality_metric: host=algo-1, epoch=52, batch=10 train loss <loss>=10.614949226379395\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:47 INFO 140084067562880] Epoch[52] Batch [10]#011Speed: 2017.68 samples/sec#011loss=10.614949\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:47 INFO 140084067562880] processed a total of 644 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463107.4173672, \"EndTime\": 1631463107.9748614, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 557.4359893798828, \"count\": 1, \"min\": 557.4359893798828, \"max\": 557.4359893798828}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:47 INFO 140084067562880] #throughput_metric: host=algo-1, train throughput=1155.045058243331 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:47 INFO 140084067562880] #progress_metric: host=algo-1, completed 26.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:47 INFO 140084067562880] #quality_metric: host=algo-1, epoch=52, train loss <loss>=10.114165652881969\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:47 INFO 140084067562880] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:48 INFO 140084067562880] Epoch[53] Batch[0] avg_epoch_loss=10.058509\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:48 INFO 140084067562880] #quality_metric: host=algo-1, epoch=53, batch=0 train loss <loss>=10.05850887298584\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:48 INFO 140084067562880] Epoch[53] Batch[5] avg_epoch_loss=9.984749\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:48 INFO 140084067562880] #quality_metric: host=algo-1, epoch=53, batch=5 train loss <loss>=9.984748522440592\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:48 INFO 140084067562880] Epoch[53] Batch [5]#011Speed: 1861.37 samples/sec#011loss=9.984749\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:48 INFO 140084067562880] processed a total of 639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463107.9749448, \"EndTime\": 1631463108.5209608, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 545.5224514007568, \"count\": 1, \"min\": 545.5224514007568, \"max\": 545.5224514007568}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:48 INFO 140084067562880] #throughput_metric: host=algo-1, train throughput=1171.060907255774 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:48 INFO 140084067562880] #progress_metric: host=algo-1, completed 27.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:48 INFO 140084067562880] #quality_metric: host=algo-1, epoch=53, train loss <loss>=10.006114768981934\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:48 INFO 140084067562880] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:48 INFO 140084067562880] Epoch[54] Batch[0] avg_epoch_loss=9.339911\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:48 INFO 140084067562880] #quality_metric: host=algo-1, epoch=54, batch=0 train loss <loss>=9.339910507202148\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:48 INFO 140084067562880] Epoch[54] Batch[5] avg_epoch_loss=10.225254\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:48 INFO 140084067562880] #quality_metric: host=algo-1, epoch=54, batch=5 train loss <loss>=10.225253899892172\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:48 INFO 140084067562880] Epoch[54] Batch [5]#011Speed: 1940.12 samples/sec#011loss=10.225254\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:49 INFO 140084067562880] processed a total of 636 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463108.5210478, \"EndTime\": 1631463109.0762575, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 554.7058582305908, \"count\": 1, \"min\": 554.7058582305908, \"max\": 554.7058582305908}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:49 INFO 140084067562880] #throughput_metric: host=algo-1, train throughput=1146.2988807634495 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:49 INFO 140084067562880] #progress_metric: host=algo-1, completed 27.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:49 INFO 140084067562880] #quality_metric: host=algo-1, epoch=54, train loss <loss>=10.040835571289062\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:49 INFO 140084067562880] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:49 INFO 140084067562880] Epoch[55] Batch[0] avg_epoch_loss=9.301762\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:49 INFO 140084067562880] #quality_metric: host=algo-1, epoch=55, batch=0 train loss <loss>=9.301761627197266\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:49 INFO 140084067562880] Epoch[55] Batch[5] avg_epoch_loss=9.862420\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:49 INFO 140084067562880] #quality_metric: host=algo-1, epoch=55, batch=5 train loss <loss>=9.862419605255127\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:49 INFO 140084067562880] Epoch[55] Batch [5]#011Speed: 1977.19 samples/sec#011loss=9.862420\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:49 INFO 140084067562880] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463109.076344, \"EndTime\": 1631463109.67211, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 595.2227115631104, \"count\": 1, \"min\": 595.2227115631104, \"max\": 595.2227115631104}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:49 INFO 140084067562880] #throughput_metric: host=algo-1, train throughput=1064.9372320325122 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:49 INFO 140084067562880] #progress_metric: host=algo-1, completed 28.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:49 INFO 140084067562880] #quality_metric: host=algo-1, epoch=55, train loss <loss>=9.783275508880616\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:49 INFO 140084067562880] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:49 INFO 140084067562880] Epoch[56] Batch[0] avg_epoch_loss=9.956962\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:49 INFO 140084067562880] #quality_metric: host=algo-1, epoch=56, batch=0 train loss <loss>=9.956961631774902\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:50 INFO 140084067562880] Epoch[56] Batch[5] avg_epoch_loss=9.821539\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:50 INFO 140084067562880] #quality_metric: host=algo-1, epoch=56, batch=5 train loss <loss>=9.821538925170898\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:50 INFO 140084067562880] Epoch[56] Batch [5]#011Speed: 1754.83 samples/sec#011loss=9.821539\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:50 INFO 140084067562880] Epoch[56] Batch[10] avg_epoch_loss=9.518063\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:50 INFO 140084067562880] #quality_metric: host=algo-1, epoch=56, batch=10 train loss <loss>=9.15389289855957\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:50 INFO 140084067562880] Epoch[56] Batch [10]#011Speed: 1932.37 samples/sec#011loss=9.153893\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:50 INFO 140084067562880] processed a total of 654 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463109.672179, \"EndTime\": 1631463110.2819958, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 609.2438697814941, \"count\": 1, \"min\": 609.2438697814941, \"max\": 609.2438697814941}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:50 INFO 140084067562880] #throughput_metric: host=algo-1, train throughput=1073.191719708591 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:50 INFO 140084067562880] #progress_metric: host=algo-1, completed 28.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:50 INFO 140084067562880] #quality_metric: host=algo-1, epoch=56, train loss <loss>=9.518063458529385\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:50 INFO 140084067562880] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:50 INFO 140084067562880] Epoch[57] Batch[0] avg_epoch_loss=9.963855\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:50 INFO 140084067562880] #quality_metric: host=algo-1, epoch=57, batch=0 train loss <loss>=9.963854789733887\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:50 INFO 140084067562880] Epoch[57] Batch[5] avg_epoch_loss=10.045785\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:50 INFO 140084067562880] #quality_metric: host=algo-1, epoch=57, batch=5 train loss <loss>=10.045785427093506\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:50 INFO 140084067562880] Epoch[57] Batch [5]#011Speed: 2077.71 samples/sec#011loss=10.045785\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:50 INFO 140084067562880] Epoch[57] Batch[10] avg_epoch_loss=10.142340\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:50 INFO 140084067562880] #quality_metric: host=algo-1, epoch=57, batch=10 train loss <loss>=10.258205223083497\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:50 INFO 140084067562880] Epoch[57] Batch [10]#011Speed: 1902.70 samples/sec#011loss=10.258205\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:50 INFO 140084067562880] processed a total of 659 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463110.2820802, \"EndTime\": 1631463110.8699453, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 587.4004364013672, \"count\": 1, \"min\": 587.4004364013672, \"max\": 587.4004364013672}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:50 INFO 140084067562880] #throughput_metric: host=algo-1, train throughput=1121.6696267589202 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:50 INFO 140084067562880] #progress_metric: host=algo-1, completed 29.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:50 INFO 140084067562880] #quality_metric: host=algo-1, epoch=57, train loss <loss>=10.142339879816229\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:50 INFO 140084067562880] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:51 INFO 140084067562880] Epoch[58] Batch[0] avg_epoch_loss=10.867654\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:51 INFO 140084067562880] #quality_metric: host=algo-1, epoch=58, batch=0 train loss <loss>=10.867653846740723\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:51 INFO 140084067562880] Epoch[58] Batch[5] avg_epoch_loss=10.093336\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:51 INFO 140084067562880] #quality_metric: host=algo-1, epoch=58, batch=5 train loss <loss>=10.09333610534668\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:51 INFO 140084067562880] Epoch[58] Batch [5]#011Speed: 2019.54 samples/sec#011loss=10.093336\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:51 INFO 140084067562880] processed a total of 630 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463110.8700273, \"EndTime\": 1631463111.4344153, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 563.9183521270752, \"count\": 1, \"min\": 563.9183521270752, \"max\": 563.9183521270752}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:51 INFO 140084067562880] #throughput_metric: host=algo-1, train throughput=1116.9629426417682 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:51 INFO 140084067562880] #progress_metric: host=algo-1, completed 29.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:51 INFO 140084067562880] #quality_metric: host=algo-1, epoch=58, train loss <loss>=10.008490943908692\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:51 INFO 140084067562880] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:51 INFO 140084067562880] Epoch[59] Batch[0] avg_epoch_loss=10.803217\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:51 INFO 140084067562880] #quality_metric: host=algo-1, epoch=59, batch=0 train loss <loss>=10.803216934204102\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:51 INFO 140084067562880] Epoch[59] Batch[5] avg_epoch_loss=10.072838\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:51 INFO 140084067562880] #quality_metric: host=algo-1, epoch=59, batch=5 train loss <loss>=10.072837988535563\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:51 INFO 140084067562880] Epoch[59] Batch [5]#011Speed: 2046.00 samples/sec#011loss=10.072838\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:52 INFO 140084067562880] Epoch[59] Batch[10] avg_epoch_loss=9.971077\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:52 INFO 140084067562880] #quality_metric: host=algo-1, epoch=59, batch=10 train loss <loss>=9.848964881896972\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:52 INFO 140084067562880] Epoch[59] Batch [10]#011Speed: 1787.76 samples/sec#011loss=9.848965\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:52 INFO 140084067562880] processed a total of 667 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463111.434494, \"EndTime\": 1631463112.020947, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 585.914134979248, \"count\": 1, \"min\": 585.914134979248, \"max\": 585.914134979248}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:52 INFO 140084067562880] #throughput_metric: host=algo-1, train throughput=1138.1651234015408 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:52 INFO 140084067562880] #progress_metric: host=algo-1, completed 30.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:52 INFO 140084067562880] #quality_metric: host=algo-1, epoch=59, train loss <loss>=9.971077485518022\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:52 INFO 140084067562880] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:52 INFO 140084067562880] Epoch[60] Batch[0] avg_epoch_loss=10.697781\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:52 INFO 140084067562880] #quality_metric: host=algo-1, epoch=60, batch=0 train loss <loss>=10.69778060913086\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:52 INFO 140084067562880] Epoch[60] Batch[5] avg_epoch_loss=10.079966\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:52 INFO 140084067562880] #quality_metric: host=algo-1, epoch=60, batch=5 train loss <loss>=10.079965591430664\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:52 INFO 140084067562880] Epoch[60] Batch [5]#011Speed: 2103.60 samples/sec#011loss=10.079966\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:52 INFO 140084067562880] Epoch[60] Batch[10] avg_epoch_loss=10.298783\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:52 INFO 140084067562880] #quality_metric: host=algo-1, epoch=60, batch=10 train loss <loss>=10.561363410949706\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:52 INFO 140084067562880] Epoch[60] Batch [10]#011Speed: 1994.23 samples/sec#011loss=10.561363\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:52 INFO 140084067562880] processed a total of 653 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463112.0210295, \"EndTime\": 1631463112.588677, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 567.1613216400146, \"count\": 1, \"min\": 567.1613216400146, \"max\": 567.1613216400146}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:52 INFO 140084067562880] #throughput_metric: host=algo-1, train throughput=1151.1035407959814 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:52 INFO 140084067562880] #progress_metric: host=algo-1, completed 30.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:52 INFO 140084067562880] #quality_metric: host=algo-1, epoch=60, train loss <loss>=10.298782782121139\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:52 INFO 140084067562880] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:52 INFO 140084067562880] Epoch[61] Batch[0] avg_epoch_loss=10.510118\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:52 INFO 140084067562880] #quality_metric: host=algo-1, epoch=61, batch=0 train loss <loss>=10.51011848449707\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:53 INFO 140084067562880] Epoch[61] Batch[5] avg_epoch_loss=10.121020\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:53 INFO 140084067562880] #quality_metric: host=algo-1, epoch=61, batch=5 train loss <loss>=10.121019840240479\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:53 INFO 140084067562880] Epoch[61] Batch [5]#011Speed: 2014.65 samples/sec#011loss=10.121020\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:53 INFO 140084067562880] Epoch[61] Batch[10] avg_epoch_loss=10.129816\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:53 INFO 140084067562880] #quality_metric: host=algo-1, epoch=61, batch=10 train loss <loss>=10.140371131896973\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:53 INFO 140084067562880] Epoch[61] Batch [10]#011Speed: 1701.07 samples/sec#011loss=10.140371\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:53 INFO 140084067562880] processed a total of 698 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463112.588761, \"EndTime\": 1631463113.193416, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 604.1593551635742, \"count\": 1, \"min\": 604.1593551635742, \"max\": 604.1593551635742}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:53 INFO 140084067562880] #throughput_metric: host=algo-1, train throughput=1155.1014224801017 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:53 INFO 140084067562880] #progress_metric: host=algo-1, completed 31.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:53 INFO 140084067562880] #quality_metric: host=algo-1, epoch=61, train loss <loss>=10.129815881902521\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:53 INFO 140084067562880] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:53 INFO 140084067562880] Epoch[62] Batch[0] avg_epoch_loss=10.154725\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:53 INFO 140084067562880] #quality_metric: host=algo-1, epoch=62, batch=0 train loss <loss>=10.154725074768066\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:53 INFO 140084067562880] Epoch[62] Batch[5] avg_epoch_loss=10.222333\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:53 INFO 140084067562880] #quality_metric: host=algo-1, epoch=62, batch=5 train loss <loss>=10.22233279546102\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:53 INFO 140084067562880] Epoch[62] Batch [5]#011Speed: 2043.21 samples/sec#011loss=10.222333\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:53 INFO 140084067562880] Epoch[62] Batch[10] avg_epoch_loss=9.467252\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:53 INFO 140084067562880] #quality_metric: host=algo-1, epoch=62, batch=10 train loss <loss>=8.56115403175354\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:53 INFO 140084067562880] Epoch[62] Batch [10]#011Speed: 1888.16 samples/sec#011loss=8.561154\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:53 INFO 140084067562880] processed a total of 647 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463113.193498, \"EndTime\": 1631463113.7891626, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 595.1986312866211, \"count\": 1, \"min\": 595.1986312866211, \"max\": 595.1986312866211}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:53 INFO 140084067562880] #throughput_metric: host=algo-1, train throughput=1086.8178791927387 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:53 INFO 140084067562880] #progress_metric: host=algo-1, completed 31.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:53 INFO 140084067562880] #quality_metric: host=algo-1, epoch=62, train loss <loss>=9.467251539230347\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:53 INFO 140084067562880] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:54 INFO 140084067562880] Epoch[63] Batch[0] avg_epoch_loss=10.363356\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:54 INFO 140084067562880] #quality_metric: host=algo-1, epoch=63, batch=0 train loss <loss>=10.36335563659668\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:54 INFO 140084067562880] Epoch[63] Batch[5] avg_epoch_loss=10.207850\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:54 INFO 140084067562880] #quality_metric: host=algo-1, epoch=63, batch=5 train loss <loss>=10.207849661509195\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:54 INFO 140084067562880] Epoch[63] Batch [5]#011Speed: 2055.17 samples/sec#011loss=10.207850\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:54 INFO 140084067562880] Epoch[63] Batch[10] avg_epoch_loss=10.118407\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:54 INFO 140084067562880] #quality_metric: host=algo-1, epoch=63, batch=10 train loss <loss>=10.011075592041015\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:54 INFO 140084067562880] Epoch[63] Batch [10]#011Speed: 1799.43 samples/sec#011loss=10.011076\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:54 INFO 140084067562880] processed a total of 677 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463113.7892451, \"EndTime\": 1631463114.4034731, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 613.7528419494629, \"count\": 1, \"min\": 613.7528419494629, \"max\": 613.7528419494629}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:54 INFO 140084067562880] #throughput_metric: host=algo-1, train throughput=1102.824970259722 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:54 INFO 140084067562880] #progress_metric: host=algo-1, completed 32.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:54 INFO 140084067562880] #quality_metric: host=algo-1, epoch=63, train loss <loss>=10.118406902660023\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:54 INFO 140084067562880] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:54 INFO 140084067562880] Epoch[64] Batch[0] avg_epoch_loss=10.768948\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:54 INFO 140084067562880] #quality_metric: host=algo-1, epoch=64, batch=0 train loss <loss>=10.76894760131836\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:54 INFO 140084067562880] Epoch[64] Batch[5] avg_epoch_loss=10.279722\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:54 INFO 140084067562880] #quality_metric: host=algo-1, epoch=64, batch=5 train loss <loss>=10.279722054799398\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:54 INFO 140084067562880] Epoch[64] Batch [5]#011Speed: 1839.33 samples/sec#011loss=10.279722\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:55 INFO 140084067562880] Epoch[64] Batch[10] avg_epoch_loss=10.856661\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:55 INFO 140084067562880] #quality_metric: host=algo-1, epoch=64, batch=10 train loss <loss>=11.548987770080567\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:55 INFO 140084067562880] Epoch[64] Batch [10]#011Speed: 1922.42 samples/sec#011loss=11.548988\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:55 INFO 140084067562880] processed a total of 644 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463114.4035604, \"EndTime\": 1631463115.0047443, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 600.7096767425537, \"count\": 1, \"min\": 600.7096767425537, \"max\": 600.7096767425537}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:55 INFO 140084067562880] #throughput_metric: host=algo-1, train throughput=1071.823672378416 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:55 INFO 140084067562880] #progress_metric: host=algo-1, completed 32.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:55 INFO 140084067562880] #quality_metric: host=algo-1, epoch=64, train loss <loss>=10.856661016290838\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:55 INFO 140084067562880] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:55 INFO 140084067562880] Epoch[65] Batch[0] avg_epoch_loss=9.401445\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:55 INFO 140084067562880] #quality_metric: host=algo-1, epoch=65, batch=0 train loss <loss>=9.401445388793945\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:55 INFO 140084067562880] Epoch[65] Batch[5] avg_epoch_loss=10.248881\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:55 INFO 140084067562880] #quality_metric: host=algo-1, epoch=65, batch=5 train loss <loss>=10.248881022135416\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:55 INFO 140084067562880] Epoch[65] Batch [5]#011Speed: 2022.06 samples/sec#011loss=10.248881\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:55 INFO 140084067562880] processed a total of 627 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463115.0048385, \"EndTime\": 1631463115.5501778, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 544.8007583618164, \"count\": 1, \"min\": 544.8007583618164, \"max\": 544.8007583618164}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:55 INFO 140084067562880] #throughput_metric: host=algo-1, train throughput=1150.6140908221826 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:55 INFO 140084067562880] #progress_metric: host=algo-1, completed 33.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:55 INFO 140084067562880] #quality_metric: host=algo-1, epoch=65, train loss <loss>=10.17149248123169\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:55 INFO 140084067562880] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:55 INFO 140084067562880] Epoch[66] Batch[0] avg_epoch_loss=10.718224\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:55 INFO 140084067562880] #quality_metric: host=algo-1, epoch=66, batch=0 train loss <loss>=10.718223571777344\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:55 INFO 140084067562880] Epoch[66] Batch[5] avg_epoch_loss=10.399411\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:55 INFO 140084067562880] #quality_metric: host=algo-1, epoch=66, batch=5 train loss <loss>=10.399410565694174\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:55 INFO 140084067562880] Epoch[66] Batch [5]#011Speed: 1861.21 samples/sec#011loss=10.399411\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:56 INFO 140084067562880] processed a total of 623 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463115.5502644, \"EndTime\": 1631463116.100999, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 550.2324104309082, \"count\": 1, \"min\": 550.2324104309082, \"max\": 550.2324104309082}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:56 INFO 140084067562880] #throughput_metric: host=algo-1, train throughput=1131.9903325681466 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:56 INFO 140084067562880] #progress_metric: host=algo-1, completed 33.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:56 INFO 140084067562880] #quality_metric: host=algo-1, epoch=66, train loss <loss>=10.283963775634765\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:56 INFO 140084067562880] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:56 INFO 140084067562880] Epoch[67] Batch[0] avg_epoch_loss=10.500369\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:56 INFO 140084067562880] #quality_metric: host=algo-1, epoch=67, batch=0 train loss <loss>=10.50036907196045\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:56 INFO 140084067562880] Epoch[67] Batch[5] avg_epoch_loss=10.199635\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:56 INFO 140084067562880] #quality_metric: host=algo-1, epoch=67, batch=5 train loss <loss>=10.199634552001953\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:56 INFO 140084067562880] Epoch[67] Batch [5]#011Speed: 1856.35 samples/sec#011loss=10.199635\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:56 INFO 140084067562880] Epoch[67] Batch[10] avg_epoch_loss=9.428287\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:56 INFO 140084067562880] #quality_metric: host=algo-1, epoch=67, batch=10 train loss <loss>=8.502669095993042\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:56 INFO 140084067562880] Epoch[67] Batch [10]#011Speed: 1789.31 samples/sec#011loss=8.502669\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:56 INFO 140084067562880] processed a total of 644 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463116.1010864, \"EndTime\": 1631463116.7253304, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 623.7268447875977, \"count\": 1, \"min\": 623.7268447875977, \"max\": 623.7268447875977}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:56 INFO 140084067562880] #throughput_metric: host=algo-1, train throughput=1032.3189797929883 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:56 INFO 140084067562880] #progress_metric: host=algo-1, completed 34.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:56 INFO 140084067562880] #quality_metric: host=algo-1, epoch=67, train loss <loss>=9.428286617452448\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:56 INFO 140084067562880] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:57 INFO 140084067562880] Epoch[68] Batch[0] avg_epoch_loss=10.597234\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:57 INFO 140084067562880] #quality_metric: host=algo-1, epoch=68, batch=0 train loss <loss>=10.597233772277832\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:57 INFO 140084067562880] Epoch[68] Batch[5] avg_epoch_loss=10.324670\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:57 INFO 140084067562880] #quality_metric: host=algo-1, epoch=68, batch=5 train loss <loss>=10.32466983795166\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:57 INFO 140084067562880] Epoch[68] Batch [5]#011Speed: 2096.61 samples/sec#011loss=10.324670\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:57 INFO 140084067562880] Epoch[68] Batch[10] avg_epoch_loss=9.739823\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:57 INFO 140084067562880] #quality_metric: host=algo-1, epoch=68, batch=10 train loss <loss>=9.038007259368896\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:57 INFO 140084067562880] Epoch[68] Batch [10]#011Speed: 1912.23 samples/sec#011loss=9.038007\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:57 INFO 140084067562880] processed a total of 670 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463116.7254047, \"EndTime\": 1631463117.3407085, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 614.8476600646973, \"count\": 1, \"min\": 614.8476600646973, \"max\": 614.8476600646973}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:57 INFO 140084067562880] #throughput_metric: host=algo-1, train throughput=1089.4971892034396 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:57 INFO 140084067562880] #progress_metric: host=algo-1, completed 34.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:57 INFO 140084067562880] #quality_metric: host=algo-1, epoch=68, train loss <loss>=9.739823211323131\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:57 INFO 140084067562880] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:57 INFO 140084067562880] Epoch[69] Batch[0] avg_epoch_loss=10.977514\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:57 INFO 140084067562880] #quality_metric: host=algo-1, epoch=69, batch=0 train loss <loss>=10.977514266967773\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:57 INFO 140084067562880] Epoch[69] Batch[5] avg_epoch_loss=10.289947\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:57 INFO 140084067562880] #quality_metric: host=algo-1, epoch=69, batch=5 train loss <loss>=10.289946873982748\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:57 INFO 140084067562880] Epoch[69] Batch [5]#011Speed: 2084.24 samples/sec#011loss=10.289947\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:57 INFO 140084067562880] Epoch[69] Batch[10] avg_epoch_loss=9.854525\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:57 INFO 140084067562880] #quality_metric: host=algo-1, epoch=69, batch=10 train loss <loss>=9.332019233703614\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:57 INFO 140084067562880] Epoch[69] Batch [10]#011Speed: 1874.89 samples/sec#011loss=9.332019\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:57 INFO 140084067562880] processed a total of 641 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463117.3407872, \"EndTime\": 1631463117.9104633, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 569.1568851470947, \"count\": 1, \"min\": 569.1568851470947, \"max\": 569.1568851470947}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:57 INFO 140084067562880] #throughput_metric: host=algo-1, train throughput=1125.98576471618 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:57 INFO 140084067562880] #progress_metric: host=algo-1, completed 35.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:57 INFO 140084067562880] #quality_metric: host=algo-1, epoch=69, train loss <loss>=9.854525219310414\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:57 INFO 140084067562880] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:58 INFO 140084067562880] Epoch[70] Batch[0] avg_epoch_loss=10.003061\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:58 INFO 140084067562880] #quality_metric: host=algo-1, epoch=70, batch=0 train loss <loss>=10.003061294555664\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:58 INFO 140084067562880] Epoch[70] Batch[5] avg_epoch_loss=9.884987\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:58 INFO 140084067562880] #quality_metric: host=algo-1, epoch=70, batch=5 train loss <loss>=9.884987354278564\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:58 INFO 140084067562880] Epoch[70] Batch [5]#011Speed: 1551.09 samples/sec#011loss=9.884987\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:58 INFO 140084067562880] Epoch[70] Batch[10] avg_epoch_loss=9.828398\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:58 INFO 140084067562880] #quality_metric: host=algo-1, epoch=70, batch=10 train loss <loss>=9.760490226745606\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:58 INFO 140084067562880] Epoch[70] Batch [10]#011Speed: 1917.67 samples/sec#011loss=9.760490\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:58 INFO 140084067562880] processed a total of 643 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463117.9105337, \"EndTime\": 1631463118.5445533, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 633.3625316619873, \"count\": 1, \"min\": 633.3625316619873, \"max\": 633.3625316619873}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:58 INFO 140084067562880] #throughput_metric: host=algo-1, train throughput=1014.9958947601363 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:58 INFO 140084067562880] #progress_metric: host=algo-1, completed 35.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:58 INFO 140084067562880] #quality_metric: host=algo-1, epoch=70, train loss <loss>=9.828397750854492\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:58 INFO 140084067562880] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:58 INFO 140084067562880] Epoch[71] Batch[0] avg_epoch_loss=10.508357\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:58 INFO 140084067562880] #quality_metric: host=algo-1, epoch=71, batch=0 train loss <loss>=10.508357048034668\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:58 INFO 140084067562880] Epoch[71] Batch[5] avg_epoch_loss=10.001870\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:58 INFO 140084067562880] #quality_metric: host=algo-1, epoch=71, batch=5 train loss <loss>=10.001869996388754\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:58 INFO 140084067562880] Epoch[71] Batch [5]#011Speed: 2043.01 samples/sec#011loss=10.001870\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:59 INFO 140084067562880] Epoch[71] Batch[10] avg_epoch_loss=10.112775\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:59 INFO 140084067562880] #quality_metric: host=algo-1, epoch=71, batch=10 train loss <loss>=10.245860290527343\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:59 INFO 140084067562880] Epoch[71] Batch [10]#011Speed: 1881.69 samples/sec#011loss=10.245860\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:59 INFO 140084067562880] processed a total of 683 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463118.5446572, \"EndTime\": 1631463119.1120107, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 566.8179988861084, \"count\": 1, \"min\": 566.8179988861084, \"max\": 566.8179988861084}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:59 INFO 140084067562880] #throughput_metric: host=algo-1, train throughput=1204.7306058663921 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:59 INFO 140084067562880] #progress_metric: host=algo-1, completed 36.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:59 INFO 140084067562880] #quality_metric: host=algo-1, epoch=71, train loss <loss>=10.112774675542658\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:59 INFO 140084067562880] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:59 INFO 140084067562880] Epoch[72] Batch[0] avg_epoch_loss=9.939613\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:59 INFO 140084067562880] #quality_metric: host=algo-1, epoch=72, batch=0 train loss <loss>=9.939613342285156\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:59 INFO 140084067562880] Epoch[72] Batch[5] avg_epoch_loss=9.540846\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:59 INFO 140084067562880] #quality_metric: host=algo-1, epoch=72, batch=5 train loss <loss>=9.54084587097168\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:59 INFO 140084067562880] Epoch[72] Batch [5]#011Speed: 2099.50 samples/sec#011loss=9.540846\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:59 INFO 140084067562880] Epoch[72] Batch[10] avg_epoch_loss=9.632021\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:59 INFO 140084067562880] #quality_metric: host=algo-1, epoch=72, batch=10 train loss <loss>=9.74143009185791\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:59 INFO 140084067562880] Epoch[72] Batch [10]#011Speed: 1825.81 samples/sec#011loss=9.741430\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:59 INFO 140084067562880] processed a total of 661 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463119.1120896, \"EndTime\": 1631463119.6908863, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 578.3522129058838, \"count\": 1, \"min\": 578.3522129058838, \"max\": 578.3522129058838}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:59 INFO 140084067562880] #throughput_metric: host=algo-1, train throughput=1142.651576030667 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:59 INFO 140084067562880] #progress_metric: host=algo-1, completed 36.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:59 INFO 140084067562880] #quality_metric: host=algo-1, epoch=72, train loss <loss>=9.632020516829057\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:59 INFO 140084067562880] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:59 INFO 140084067562880] Epoch[73] Batch[0] avg_epoch_loss=9.425430\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:11:59 INFO 140084067562880] #quality_metric: host=algo-1, epoch=73, batch=0 train loss <loss>=9.425430297851562\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:00 INFO 140084067562880] Epoch[73] Batch[5] avg_epoch_loss=9.648906\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:00 INFO 140084067562880] #quality_metric: host=algo-1, epoch=73, batch=5 train loss <loss>=9.648905595143637\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:00 INFO 140084067562880] Epoch[73] Batch [5]#011Speed: 1997.65 samples/sec#011loss=9.648906\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:00 INFO 140084067562880] Epoch[73] Batch[10] avg_epoch_loss=9.558689\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:00 INFO 140084067562880] #quality_metric: host=algo-1, epoch=73, batch=10 train loss <loss>=9.450429916381836\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:00 INFO 140084067562880] Epoch[73] Batch [10]#011Speed: 1617.58 samples/sec#011loss=9.450430\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:00 INFO 140084067562880] processed a total of 660 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463119.6909757, \"EndTime\": 1631463120.3574646, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 666.0146713256836, \"count\": 1, \"min\": 666.0146713256836, \"max\": 666.0146713256836}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:00 INFO 140084067562880] #throughput_metric: host=algo-1, train throughput=990.7914654743843 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:00 INFO 140084067562880] #progress_metric: host=algo-1, completed 37.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:00 INFO 140084067562880] #quality_metric: host=algo-1, epoch=73, train loss <loss>=9.558689377524637\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:00 INFO 140084067562880] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:00 INFO 140084067562880] Epoch[74] Batch[0] avg_epoch_loss=10.601187\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:00 INFO 140084067562880] #quality_metric: host=algo-1, epoch=74, batch=0 train loss <loss>=10.601186752319336\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:00 INFO 140084067562880] Epoch[74] Batch[5] avg_epoch_loss=10.006716\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:00 INFO 140084067562880] #quality_metric: host=algo-1, epoch=74, batch=5 train loss <loss>=10.006715933481852\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:00 INFO 140084067562880] Epoch[74] Batch [5]#011Speed: 2027.33 samples/sec#011loss=10.006716\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:00 INFO 140084067562880] Epoch[74] Batch[10] avg_epoch_loss=9.559621\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:00 INFO 140084067562880] #quality_metric: host=algo-1, epoch=74, batch=10 train loss <loss>=9.023106288909911\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:00 INFO 140084067562880] Epoch[74] Batch [10]#011Speed: 1921.00 samples/sec#011loss=9.023106\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:00 INFO 140084067562880] processed a total of 653 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463120.3575482, \"EndTime\": 1631463120.9314444, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 573.4231472015381, \"count\": 1, \"min\": 573.4231472015381, \"max\": 573.4231472015381}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:00 INFO 140084067562880] #throughput_metric: host=algo-1, train throughput=1138.5346191562337 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:00 INFO 140084067562880] #progress_metric: host=algo-1, completed 37.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:00 INFO 140084067562880] #quality_metric: host=algo-1, epoch=74, train loss <loss>=9.559620640494607\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:00 INFO 140084067562880] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:01 INFO 140084067562880] Epoch[75] Batch[0] avg_epoch_loss=9.890569\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:01 INFO 140084067562880] #quality_metric: host=algo-1, epoch=75, batch=0 train loss <loss>=9.890568733215332\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:01 INFO 140084067562880] Epoch[75] Batch[5] avg_epoch_loss=9.900024\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:01 INFO 140084067562880] #quality_metric: host=algo-1, epoch=75, batch=5 train loss <loss>=9.900024255116781\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:01 INFO 140084067562880] Epoch[75] Batch [5]#011Speed: 2052.59 samples/sec#011loss=9.900024\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:01 INFO 140084067562880] Epoch[75] Batch[10] avg_epoch_loss=10.202392\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:01 INFO 140084067562880] #quality_metric: host=algo-1, epoch=75, batch=10 train loss <loss>=10.565233039855958\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:01 INFO 140084067562880] Epoch[75] Batch [10]#011Speed: 1846.77 samples/sec#011loss=10.565233\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:01 INFO 140084067562880] processed a total of 651 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463120.9315295, \"EndTime\": 1631463121.5215213, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 589.5037651062012, \"count\": 1, \"min\": 589.5037651062012, \"max\": 589.5037651062012}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:01 INFO 140084067562880] #throughput_metric: host=algo-1, train throughput=1103.9931491978084 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:01 INFO 140084067562880] #progress_metric: host=algo-1, completed 38.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:01 INFO 140084067562880] #quality_metric: host=algo-1, epoch=75, train loss <loss>=10.20239188454368\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:01 INFO 140084067562880] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:01 INFO 140084067562880] Epoch[76] Batch[0] avg_epoch_loss=10.523081\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:01 INFO 140084067562880] #quality_metric: host=algo-1, epoch=76, batch=0 train loss <loss>=10.523080825805664\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:01 INFO 140084067562880] Epoch[76] Batch[5] avg_epoch_loss=10.067000\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:01 INFO 140084067562880] #quality_metric: host=algo-1, epoch=76, batch=5 train loss <loss>=10.066999912261963\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:01 INFO 140084067562880] Epoch[76] Batch [5]#011Speed: 1769.15 samples/sec#011loss=10.067000\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:02 INFO 140084067562880] Epoch[76] Batch[10] avg_epoch_loss=9.902514\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:02 INFO 140084067562880] #quality_metric: host=algo-1, epoch=76, batch=10 train loss <loss>=9.705131530761719\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:02 INFO 140084067562880] Epoch[76] Batch [10]#011Speed: 1610.74 samples/sec#011loss=9.705132\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:02 INFO 140084067562880] processed a total of 678 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463121.5216055, \"EndTime\": 1631463122.1577976, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 635.4551315307617, \"count\": 1, \"min\": 635.4551315307617, \"max\": 635.4551315307617}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:02 INFO 140084067562880] #throughput_metric: host=algo-1, train throughput=1066.760090045244 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:02 INFO 140084067562880] #progress_metric: host=algo-1, completed 38.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:02 INFO 140084067562880] #quality_metric: host=algo-1, epoch=76, train loss <loss>=9.902514284307307\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:02 INFO 140084067562880] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:02 INFO 140084067562880] Epoch[77] Batch[0] avg_epoch_loss=9.156588\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:02 INFO 140084067562880] #quality_metric: host=algo-1, epoch=77, batch=0 train loss <loss>=9.156587600708008\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:02 INFO 140084067562880] Epoch[77] Batch[5] avg_epoch_loss=9.916486\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:02 INFO 140084067562880] #quality_metric: host=algo-1, epoch=77, batch=5 train loss <loss>=9.916486263275146\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:02 INFO 140084067562880] Epoch[77] Batch [5]#011Speed: 2088.79 samples/sec#011loss=9.916486\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:02 INFO 140084067562880] processed a total of 571 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463122.157876, \"EndTime\": 1631463122.638239, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 479.85124588012695, \"count\": 1, \"min\": 479.85124588012695, \"max\": 479.85124588012695}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:02 INFO 140084067562880] #throughput_metric: host=algo-1, train throughput=1189.6406034466058 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:02 INFO 140084067562880] #progress_metric: host=algo-1, completed 39.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:02 INFO 140084067562880] #quality_metric: host=algo-1, epoch=77, train loss <loss>=9.899040328131782\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:02 INFO 140084067562880] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:02 INFO 140084067562880] Epoch[78] Batch[0] avg_epoch_loss=10.124865\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:02 INFO 140084067562880] #quality_metric: host=algo-1, epoch=78, batch=0 train loss <loss>=10.12486457824707\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:03 INFO 140084067562880] Epoch[78] Batch[5] avg_epoch_loss=9.728891\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:03 INFO 140084067562880] #quality_metric: host=algo-1, epoch=78, batch=5 train loss <loss>=9.728891054789225\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:03 INFO 140084067562880] Epoch[78] Batch [5]#011Speed: 1561.93 samples/sec#011loss=9.728891\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:03 INFO 140084067562880] Epoch[78] Batch[10] avg_epoch_loss=9.625238\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:03 INFO 140084067562880] #quality_metric: host=algo-1, epoch=78, batch=10 train loss <loss>=9.500854396820069\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:03 INFO 140084067562880] Epoch[78] Batch [10]#011Speed: 1596.56 samples/sec#011loss=9.500854\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:03 INFO 140084067562880] processed a total of 657 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463122.6383257, \"EndTime\": 1631463123.3192806, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 680.4366111755371, \"count\": 1, \"min\": 680.4366111755371, \"max\": 680.4366111755371}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:03 INFO 140084067562880] #throughput_metric: host=algo-1, train throughput=965.3931288122614 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:03 INFO 140084067562880] #progress_metric: host=algo-1, completed 39.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:03 INFO 140084067562880] #quality_metric: host=algo-1, epoch=78, train loss <loss>=9.62523802843961\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:03 INFO 140084067562880] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:03 INFO 140084067562880] Epoch[79] Batch[0] avg_epoch_loss=8.454821\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:03 INFO 140084067562880] #quality_metric: host=algo-1, epoch=79, batch=0 train loss <loss>=8.45482063293457\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:03 INFO 140084067562880] Epoch[79] Batch[5] avg_epoch_loss=9.390797\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:03 INFO 140084067562880] #quality_metric: host=algo-1, epoch=79, batch=5 train loss <loss>=9.390796502431234\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:03 INFO 140084067562880] Epoch[79] Batch [5]#011Speed: 1921.16 samples/sec#011loss=9.390797\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:03 INFO 140084067562880] Epoch[79] Batch[10] avg_epoch_loss=10.025626\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:03 INFO 140084067562880] #quality_metric: host=algo-1, epoch=79, batch=10 train loss <loss>=10.787421417236327\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:03 INFO 140084067562880] Epoch[79] Batch [10]#011Speed: 1971.95 samples/sec#011loss=10.787421\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:03 INFO 140084067562880] processed a total of 657 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463123.3193653, \"EndTime\": 1631463123.9332113, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 613.4152412414551, \"count\": 1, \"min\": 613.4152412414551, \"max\": 613.4152412414551}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:03 INFO 140084067562880] #throughput_metric: host=algo-1, train throughput=1070.8449293119588 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:03 INFO 140084067562880] #progress_metric: host=algo-1, completed 40.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:03 INFO 140084067562880] #quality_metric: host=algo-1, epoch=79, train loss <loss>=10.025626009160822\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:03 INFO 140084067562880] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:04 INFO 140084067562880] Epoch[80] Batch[0] avg_epoch_loss=9.695729\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:04 INFO 140084067562880] #quality_metric: host=algo-1, epoch=80, batch=0 train loss <loss>=9.69572925567627\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:04 INFO 140084067562880] Epoch[80] Batch[5] avg_epoch_loss=10.136809\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:04 INFO 140084067562880] #quality_metric: host=algo-1, epoch=80, batch=5 train loss <loss>=10.136809349060059\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:04 INFO 140084067562880] Epoch[80] Batch [5]#011Speed: 1578.35 samples/sec#011loss=10.136809\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:04 INFO 140084067562880] Epoch[80] Batch[10] avg_epoch_loss=9.973932\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:04 INFO 140084067562880] #quality_metric: host=algo-1, epoch=80, batch=10 train loss <loss>=9.778478240966797\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:04 INFO 140084067562880] Epoch[80] Batch [10]#011Speed: 1576.32 samples/sec#011loss=9.778478\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:04 INFO 140084067562880] processed a total of 656 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463123.933296, \"EndTime\": 1631463124.6384292, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 704.653263092041, \"count\": 1, \"min\": 704.653263092041, \"max\": 704.653263092041}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:04 INFO 140084067562880] #throughput_metric: host=algo-1, train throughput=930.7817398593677 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:04 INFO 140084067562880] #progress_metric: host=algo-1, completed 40.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:04 INFO 140084067562880] #quality_metric: host=algo-1, epoch=80, train loss <loss>=9.973931572654031\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:04 INFO 140084067562880] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:04 INFO 140084067562880] Epoch[81] Batch[0] avg_epoch_loss=10.302171\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:04 INFO 140084067562880] #quality_metric: host=algo-1, epoch=81, batch=0 train loss <loss>=10.302170753479004\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:05 INFO 140084067562880] Epoch[81] Batch[5] avg_epoch_loss=10.668362\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:05 INFO 140084067562880] #quality_metric: host=algo-1, epoch=81, batch=5 train loss <loss>=10.668361981709799\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:05 INFO 140084067562880] Epoch[81] Batch [5]#011Speed: 1529.56 samples/sec#011loss=10.668362\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:05 INFO 140084067562880] Epoch[81] Batch[10] avg_epoch_loss=10.683085\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:05 INFO 140084067562880] #quality_metric: host=algo-1, epoch=81, batch=10 train loss <loss>=10.700751686096192\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:05 INFO 140084067562880] Epoch[81] Batch [10]#011Speed: 1496.89 samples/sec#011loss=10.700752\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:05 INFO 140084067562880] processed a total of 659 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463124.6385221, \"EndTime\": 1631463125.371662, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 732.586145401001, \"count\": 1, \"min\": 732.586145401001, \"max\": 732.586145401001}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:05 INFO 140084067562880] #throughput_metric: host=algo-1, train throughput=899.4214825367426 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:05 INFO 140084067562880] #progress_metric: host=algo-1, completed 41.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:05 INFO 140084067562880] #quality_metric: host=algo-1, epoch=81, train loss <loss>=10.683084574612705\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:05 INFO 140084067562880] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:05 INFO 140084067562880] Epoch[82] Batch[0] avg_epoch_loss=9.745263\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:05 INFO 140084067562880] #quality_metric: host=algo-1, epoch=82, batch=0 train loss <loss>=9.74526309967041\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:05 INFO 140084067562880] Epoch[82] Batch[5] avg_epoch_loss=9.629353\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:05 INFO 140084067562880] #quality_metric: host=algo-1, epoch=82, batch=5 train loss <loss>=9.629352887471518\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:05 INFO 140084067562880] Epoch[82] Batch [5]#011Speed: 2040.29 samples/sec#011loss=9.629353\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:05 INFO 140084067562880] Epoch[82] Batch[10] avg_epoch_loss=9.767502\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:05 INFO 140084067562880] #quality_metric: host=algo-1, epoch=82, batch=10 train loss <loss>=9.933280944824219\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:05 INFO 140084067562880] Epoch[82] Batch [10]#011Speed: 1712.60 samples/sec#011loss=9.933281\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:06 INFO 140084067562880] processed a total of 664 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463125.3717368, \"EndTime\": 1631463126.0001929, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 627.9473304748535, \"count\": 1, \"min\": 627.9473304748535, \"max\": 627.9473304748535}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:06 INFO 140084067562880] #throughput_metric: host=algo-1, train throughput=1057.2100144667918 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:06 INFO 140084067562880] #progress_metric: host=algo-1, completed 41.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:06 INFO 140084067562880] #quality_metric: host=algo-1, epoch=82, train loss <loss>=9.767502004450018\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:06 INFO 140084067562880] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:06 INFO 140084067562880] Epoch[83] Batch[0] avg_epoch_loss=9.702692\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:06 INFO 140084067562880] #quality_metric: host=algo-1, epoch=83, batch=0 train loss <loss>=9.702692031860352\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:06 INFO 140084067562880] Epoch[83] Batch[5] avg_epoch_loss=10.119668\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:06 INFO 140084067562880] #quality_metric: host=algo-1, epoch=83, batch=5 train loss <loss>=10.11966848373413\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:06 INFO 140084067562880] Epoch[83] Batch [5]#011Speed: 2143.60 samples/sec#011loss=10.119668\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:06 INFO 140084067562880] Epoch[83] Batch[10] avg_epoch_loss=9.394601\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:06 INFO 140084067562880] #quality_metric: host=algo-1, epoch=83, batch=10 train loss <loss>=8.524521112442017\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:06 INFO 140084067562880] Epoch[83] Batch [10]#011Speed: 2063.96 samples/sec#011loss=8.524521\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:06 INFO 140084067562880] processed a total of 645 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463126.0002766, \"EndTime\": 1631463126.5641334, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 563.3406639099121, \"count\": 1, \"min\": 563.3406639099121, \"max\": 563.3406639099121}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:06 INFO 140084067562880] #throughput_metric: host=algo-1, train throughput=1144.7147338129132 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:06 INFO 140084067562880] #progress_metric: host=algo-1, completed 42.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:06 INFO 140084067562880] #quality_metric: host=algo-1, epoch=83, train loss <loss>=9.394601496783169\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:06 INFO 140084067562880] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:06 INFO 140084067562880] Epoch[84] Batch[0] avg_epoch_loss=10.107089\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:06 INFO 140084067562880] #quality_metric: host=algo-1, epoch=84, batch=0 train loss <loss>=10.107089042663574\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:06 INFO 140084067562880] Epoch[84] Batch[5] avg_epoch_loss=10.000966\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:06 INFO 140084067562880] #quality_metric: host=algo-1, epoch=84, batch=5 train loss <loss>=10.00096575419108\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:06 INFO 140084067562880] Epoch[84] Batch [5]#011Speed: 2040.07 samples/sec#011loss=10.000966\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:07 INFO 140084067562880] Epoch[84] Batch[10] avg_epoch_loss=9.690601\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:07 INFO 140084067562880] #quality_metric: host=algo-1, epoch=84, batch=10 train loss <loss>=9.318163394927979\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:07 INFO 140084067562880] Epoch[84] Batch [10]#011Speed: 1949.40 samples/sec#011loss=9.318163\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:07 INFO 140084067562880] processed a total of 650 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463126.5642169, \"EndTime\": 1631463127.1327322, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 568.0110454559326, \"count\": 1, \"min\": 568.0110454559326, \"max\": 568.0110454559326}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:07 INFO 140084067562880] #throughput_metric: host=algo-1, train throughput=1144.1067100870575 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:07 INFO 140084067562880] #progress_metric: host=algo-1, completed 42.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:07 INFO 140084067562880] #quality_metric: host=algo-1, epoch=84, train loss <loss>=9.690601045435125\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:07 INFO 140084067562880] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:07 INFO 140084067562880] Epoch[85] Batch[0] avg_epoch_loss=10.383479\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:07 INFO 140084067562880] #quality_metric: host=algo-1, epoch=85, batch=0 train loss <loss>=10.383479118347168\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:07 INFO 140084067562880] Epoch[85] Batch[5] avg_epoch_loss=9.938577\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:07 INFO 140084067562880] #quality_metric: host=algo-1, epoch=85, batch=5 train loss <loss>=9.93857749303182\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:07 INFO 140084067562880] Epoch[85] Batch [5]#011Speed: 1662.17 samples/sec#011loss=9.938577\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:07 INFO 140084067562880] Epoch[85] Batch[10] avg_epoch_loss=10.331029\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:07 INFO 140084067562880] #quality_metric: host=algo-1, epoch=85, batch=10 train loss <loss>=10.801969718933105\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:07 INFO 140084067562880] Epoch[85] Batch [10]#011Speed: 1560.62 samples/sec#011loss=10.801970\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:07 INFO 140084067562880] processed a total of 657 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463127.1328135, \"EndTime\": 1631463127.8126783, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 679.3990135192871, \"count\": 1, \"min\": 679.3990135192871, \"max\": 679.3990135192871}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:07 INFO 140084067562880] #throughput_metric: host=algo-1, train throughput=966.8682727468574 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:07 INFO 140084067562880] #progress_metric: host=algo-1, completed 43.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:07 INFO 140084067562880] #quality_metric: host=algo-1, epoch=85, train loss <loss>=10.331028504805131\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:07 INFO 140084067562880] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:08 INFO 140084067562880] Epoch[86] Batch[0] avg_epoch_loss=9.542660\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:08 INFO 140084067562880] #quality_metric: host=algo-1, epoch=86, batch=0 train loss <loss>=9.542659759521484\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:08 INFO 140084067562880] Epoch[86] Batch[5] avg_epoch_loss=9.798619\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:08 INFO 140084067562880] #quality_metric: host=algo-1, epoch=86, batch=5 train loss <loss>=9.798618952433268\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:08 INFO 140084067562880] Epoch[86] Batch [5]#011Speed: 2060.36 samples/sec#011loss=9.798619\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:08 INFO 140084067562880] processed a total of 636 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463127.8127606, \"EndTime\": 1631463128.3458915, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 532.6235294342041, \"count\": 1, \"min\": 532.6235294342041, \"max\": 532.6235294342041}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:08 INFO 140084067562880] #throughput_metric: host=algo-1, train throughput=1193.7739140552453 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:08 INFO 140084067562880] #progress_metric: host=algo-1, completed 43.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:08 INFO 140084067562880] #quality_metric: host=algo-1, epoch=86, train loss <loss>=9.950597858428955\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:08 INFO 140084067562880] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:08 INFO 140084067562880] Epoch[87] Batch[0] avg_epoch_loss=9.717201\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:08 INFO 140084067562880] #quality_metric: host=algo-1, epoch=87, batch=0 train loss <loss>=9.717201232910156\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:08 INFO 140084067562880] Epoch[87] Batch[5] avg_epoch_loss=9.840651\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:08 INFO 140084067562880] #quality_metric: host=algo-1, epoch=87, batch=5 train loss <loss>=9.84065055847168\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:08 INFO 140084067562880] Epoch[87] Batch [5]#011Speed: 2080.04 samples/sec#011loss=9.840651\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:08 INFO 140084067562880] processed a total of 636 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463128.3459816, \"EndTime\": 1631463128.868305, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 521.7363834381104, \"count\": 1, \"min\": 521.7363834381104, \"max\": 521.7363834381104}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:08 INFO 140084067562880] #throughput_metric: host=algo-1, train throughput=1218.707355159631 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:08 INFO 140084067562880] #progress_metric: host=algo-1, completed 44.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:08 INFO 140084067562880] #quality_metric: host=algo-1, epoch=87, train loss <loss>=10.080802822113037\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:08 INFO 140084067562880] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:09 INFO 140084067562880] Epoch[88] Batch[0] avg_epoch_loss=10.099185\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:09 INFO 140084067562880] #quality_metric: host=algo-1, epoch=88, batch=0 train loss <loss>=10.0991849899292\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:09 INFO 140084067562880] Epoch[88] Batch[5] avg_epoch_loss=9.712122\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:09 INFO 140084067562880] #quality_metric: host=algo-1, epoch=88, batch=5 train loss <loss>=9.712122440338135\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:09 INFO 140084067562880] Epoch[88] Batch [5]#011Speed: 2094.61 samples/sec#011loss=9.712122\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:09 INFO 140084067562880] Epoch[88] Batch[10] avg_epoch_loss=9.418721\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:09 INFO 140084067562880] #quality_metric: host=algo-1, epoch=88, batch=10 train loss <loss>=9.066639804840088\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:09 INFO 140084067562880] Epoch[88] Batch [10]#011Speed: 2006.88 samples/sec#011loss=9.066640\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:09 INFO 140084067562880] processed a total of 644 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463128.8683941, \"EndTime\": 1631463129.4562922, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 587.3737335205078, \"count\": 1, \"min\": 587.3737335205078, \"max\": 587.3737335205078}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:09 INFO 140084067562880] #throughput_metric: host=algo-1, train throughput=1096.219415002092 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:09 INFO 140084067562880] #progress_metric: host=algo-1, completed 44.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:09 INFO 140084067562880] #quality_metric: host=algo-1, epoch=88, train loss <loss>=9.418721242384477\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:09 INFO 140084067562880] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:09 INFO 140084067562880] Epoch[89] Batch[0] avg_epoch_loss=10.079455\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:09 INFO 140084067562880] #quality_metric: host=algo-1, epoch=89, batch=0 train loss <loss>=10.079455375671387\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:09 INFO 140084067562880] Epoch[89] Batch[5] avg_epoch_loss=10.008047\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:09 INFO 140084067562880] #quality_metric: host=algo-1, epoch=89, batch=5 train loss <loss>=10.008047262827555\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:09 INFO 140084067562880] Epoch[89] Batch [5]#011Speed: 1853.72 samples/sec#011loss=10.008047\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:10 INFO 140084067562880] Epoch[89] Batch[10] avg_epoch_loss=9.693700\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:10 INFO 140084067562880] #quality_metric: host=algo-1, epoch=89, batch=10 train loss <loss>=9.316483688354491\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:10 INFO 140084067562880] Epoch[89] Batch [10]#011Speed: 1913.22 samples/sec#011loss=9.316484\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:10 INFO 140084067562880] processed a total of 647 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463129.456363, \"EndTime\": 1631463130.0489688, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 592.2234058380127, \"count\": 1, \"min\": 592.2234058380127, \"max\": 592.2234058380127}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:10 INFO 140084067562880] #throughput_metric: host=algo-1, train throughput=1092.2820541408644 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:10 INFO 140084067562880] #progress_metric: host=algo-1, completed 45.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:10 INFO 140084067562880] #quality_metric: host=algo-1, epoch=89, train loss <loss>=9.693700183521617\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:10 INFO 140084067562880] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:10 INFO 140084067562880] Epoch[90] Batch[0] avg_epoch_loss=9.932060\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:10 INFO 140084067562880] #quality_metric: host=algo-1, epoch=90, batch=0 train loss <loss>=9.932060241699219\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:10 INFO 140084067562880] Epoch[90] Batch[5] avg_epoch_loss=10.076890\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:10 INFO 140084067562880] #quality_metric: host=algo-1, epoch=90, batch=5 train loss <loss>=10.076889832814535\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:10 INFO 140084067562880] Epoch[90] Batch [5]#011Speed: 1981.80 samples/sec#011loss=10.076890\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:10 INFO 140084067562880] processed a total of 574 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463130.049049, \"EndTime\": 1631463130.6104507, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 560.931921005249, \"count\": 1, \"min\": 560.931921005249, \"max\": 560.931921005249}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:10 INFO 140084067562880] #throughput_metric: host=algo-1, train throughput=1023.0340131660579 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:10 INFO 140084067562880] #progress_metric: host=algo-1, completed 45.5 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:10 INFO 140084067562880] #quality_metric: host=algo-1, epoch=90, train loss <loss>=10.144149356418186\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:10 INFO 140084067562880] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:10 INFO 140084067562880] Epoch[91] Batch[0] avg_epoch_loss=10.231083\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:10 INFO 140084067562880] #quality_metric: host=algo-1, epoch=91, batch=0 train loss <loss>=10.231082916259766\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:11 INFO 140084067562880] Epoch[91] Batch[5] avg_epoch_loss=9.823478\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:11 INFO 140084067562880] #quality_metric: host=algo-1, epoch=91, batch=5 train loss <loss>=9.82347838083903\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:11 INFO 140084067562880] Epoch[91] Batch [5]#011Speed: 2049.17 samples/sec#011loss=9.823478\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:11 INFO 140084067562880] Epoch[91] Batch[10] avg_epoch_loss=9.716581\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:11 INFO 140084067562880] #quality_metric: host=algo-1, epoch=91, batch=10 train loss <loss>=9.588304138183593\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:11 INFO 140084067562880] Epoch[91] Batch [10]#011Speed: 1805.70 samples/sec#011loss=9.588304\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:11 INFO 140084067562880] processed a total of 663 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463130.6105437, \"EndTime\": 1631463131.1825902, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 571.5103149414062, \"count\": 1, \"min\": 571.5103149414062, \"max\": 571.5103149414062}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:11 INFO 140084067562880] #throughput_metric: host=algo-1, train throughput=1159.8421213319386 records/second\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:11 INFO 140084067562880] #progress_metric: host=algo-1, completed 46.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:11 INFO 140084067562880] #quality_metric: host=algo-1, epoch=91, train loss <loss>=9.716580997813832\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:11 INFO 140084067562880] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:11 INFO 140084067562880] Loading parameters from best epoch (51)\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463131.182673, \"EndTime\": 1631463131.1921244, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.deserialize.time\": {\"sum\": 8.903026580810547, \"count\": 1, \"min\": 8.903026580810547, \"max\": 8.903026580810547}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:11 INFO 140084067562880] stopping training now\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:11 INFO 140084067562880] #progress_metric: host=algo-1, completed 100 % of epochs\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:11 INFO 140084067562880] Final loss: 9.218740745024247 (occurred at epoch 51)\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:11 INFO 140084067562880] #quality_metric: host=algo-1, train final_loss <loss>=9.218740745024247\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:11 INFO 140084067562880] Worker algo-1 finished training.\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:11 WARNING 140084067562880] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:11 INFO 140084067562880] All workers finished. Serializing model for prediction.\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463131.1921966, \"EndTime\": 1631463131.3299859, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"get_graph.time\": {\"sum\": 137.04800605773926, \"count\": 1, \"min\": 137.04800605773926, \"max\": 137.04800605773926}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:11 INFO 140084067562880] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463131.3300474, \"EndTime\": 1631463131.3634403, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"finalize.time\": {\"sum\": 170.54247856140137, \"count\": 1, \"min\": 170.54247856140137, \"max\": 170.54247856140137}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:11 INFO 140084067562880] Serializing to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:11 INFO 140084067562880] Saved checkpoint to \"/opt/ml/model/model_algo-1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463131.3635187, \"EndTime\": 1631463131.3688753, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"model.serialize.time\": {\"sum\": 5.321264266967773, \"count\": 1, \"min\": 5.321264266967773, \"max\": 5.321264266967773}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:11 INFO 140084067562880] Successfully serialized the model for prediction.\u001b[0m\n",
      "\u001b[34m[09/12/2021 16:12:11 INFO 140084067562880] No test data passed, skipping evaluation.\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1631463131.3689227, \"EndTime\": 1631463131.3741293, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"setuptime\": {\"sum\": 6.501436233520508, \"count\": 1, \"min\": 6.501436233520508, \"max\": 6.501436233520508}, \"totaltime\": {\"sum\": 54597.09882736206, \"count\": 1, \"min\": 54597.09882736206, \"max\": 54597.09882736206}}}\n",
      "\u001b[0m\n",
      "\n",
      "2021-09-12 16:12:23 Uploading - Uploading generated training model\n",
      "2021-09-12 16:12:23 Completed - Training job completed\n",
      "Training seconds: 114\n",
      "Billable seconds: 114\n",
      "-------------![                   0.5\n",
      "2021-08-04  561.709595\n",
      "2021-08-05  513.951843\n",
      "2021-08-06  537.013367\n",
      "2021-08-07  434.110565\n",
      "2021-08-08  372.875366\n",
      "2021-08-09  498.317017\n",
      "2021-08-10  557.136047\n",
      "2021-08-11  633.599121\n",
      "2021-08-12  488.114746\n",
      "2021-08-13  601.183716\n",
      "2021-08-14  474.510864\n",
      "2021-08-15  410.358643\n",
      "2021-08-16  516.391785\n",
      "2021-08-17  583.040710,                    0.5\n",
      "2021-08-05  568.508728\n",
      "2021-08-06  571.410828\n",
      "2021-08-07  476.871002\n",
      "2021-08-08  414.217621\n",
      "2021-08-09  490.170471\n",
      "2021-08-10  616.815369\n",
      "2021-08-11  663.888977\n",
      "2021-08-12  696.506653\n",
      "2021-08-13  671.322021\n",
      "2021-08-14  515.558533\n",
      "2021-08-15  494.771545\n",
      "2021-08-16  526.059265\n",
      "2021-08-17  623.474854\n",
      "2021-08-18  629.334229,                    0.5\n",
      "2021-08-06  566.714478\n",
      "2021-08-07  435.188812\n",
      "2021-08-08  389.907623\n",
      "2021-08-09  457.138428\n",
      "2021-08-10  605.398926\n",
      "2021-08-11  705.253540\n",
      "2021-08-12  598.025391\n",
      "2021-08-13  674.648987\n",
      "2021-08-14  353.254791\n",
      "2021-08-15  475.566254\n",
      "2021-08-16  551.578369\n",
      "2021-08-17  580.077271\n",
      "2021-08-18  629.882446\n",
      "2021-08-19  607.658081,                    0.5\n",
      "2021-08-07  583.520569\n",
      "2021-08-08  487.186951\n",
      "2021-08-09  563.763428\n",
      "2021-08-10  733.793396\n",
      "2021-08-11  772.045044\n",
      "2021-08-12  628.404175\n",
      "2021-08-13  702.357544\n",
      "2021-08-14  646.249939\n",
      "2021-08-15  450.671234\n",
      "2021-08-16  636.467407\n",
      "2021-08-17  634.497253\n",
      "2021-08-18  638.832397\n",
      "2021-08-19  643.917725\n",
      "2021-08-20  648.924988,                    0.5\n",
      "2021-08-08  496.176056\n",
      "2021-08-09  520.376221\n",
      "2021-08-10  682.477234\n",
      "2021-08-11  741.413147\n",
      "2021-08-12  716.326721\n",
      "2021-08-13  659.664795\n",
      "2021-08-14  608.176208\n",
      "2021-08-15  585.034851\n",
      "2021-08-16  537.482300\n",
      "2021-08-17  737.579834\n",
      "2021-08-18  695.730225\n",
      "2021-08-19  689.150208\n",
      "2021-08-20  703.001892\n",
      "2021-08-21  566.689575,                    0.5\n",
      "2021-08-09  538.864685\n",
      "2021-08-10  613.441895\n",
      "2021-08-11  644.804932\n",
      "2021-08-12  662.422852\n",
      "2021-08-13  690.977356\n",
      "2021-08-14  562.782349\n",
      "2021-08-15  496.595856\n",
      "2021-08-16  643.761841\n",
      "2021-08-17  685.998535\n",
      "2021-08-18  744.435791\n",
      "2021-08-19  774.581848\n",
      "2021-08-20  731.381409\n",
      "2021-08-21  623.760315\n",
      "2021-08-22  562.669128,                    0.5\n",
      "2021-08-10  668.646973\n",
      "2021-08-11  690.602051\n",
      "2021-08-12  632.751404\n",
      "2021-08-13  688.510925\n",
      "2021-08-14  520.158813\n",
      "2021-08-15  471.398651\n",
      "2021-08-16  577.083862\n",
      "2021-08-17  735.846680\n",
      "2021-08-18  680.430054\n",
      "2021-08-19  735.700378\n",
      "2021-08-20  677.937012\n",
      "2021-08-21  493.704498\n",
      "2021-08-22  460.107422\n",
      "2021-08-23  477.405823,                    0.5\n",
      "2021-08-11  718.353455\n",
      "2021-08-12  670.806885\n",
      "2021-08-13  711.904541\n",
      "2021-08-14  583.427063\n",
      "2021-08-15  541.522400\n",
      "2021-08-16  725.105164\n",
      "2021-08-17  752.945007\n",
      "2021-08-18  865.547058\n",
      "2021-08-19  694.318359\n",
      "2021-08-20  827.674805\n",
      "2021-08-21  639.395386\n",
      "2021-08-22  575.389221\n",
      "2021-08-23  672.082031\n",
      "2021-08-24  758.859314,                    0.5\n",
      "2021-08-12  622.221741\n",
      "2021-08-13  617.814026\n",
      "2021-08-14  520.523193\n",
      "2021-08-15  474.672455\n",
      "2021-08-16  554.328979\n",
      "2021-08-17  561.837036\n",
      "2021-08-18  650.345215\n",
      "2021-08-19  686.536072\n",
      "2021-08-20  591.008850\n",
      "2021-08-21  530.552734\n",
      "2021-08-22  479.969208\n",
      "2021-08-23  493.181488\n",
      "2021-08-24  685.915649\n",
      "2021-08-25  601.316772,                    0.5\n",
      "2021-08-13  707.660583\n",
      "2021-08-14  567.064087\n",
      "2021-08-15  500.577057\n",
      "2021-08-16  635.247986\n",
      "2021-08-17  769.052185\n",
      "2021-08-18  738.669189\n",
      "2021-08-19  757.198547\n",
      "2021-08-20  850.695129\n",
      "2021-08-21  643.368591\n",
      "2021-08-22  548.843506\n",
      "2021-08-23  641.030029\n",
      "2021-08-24  687.938477\n",
      "2021-08-25  673.098633\n",
      "2021-08-26  594.191467]\n"
     ]
    }
   ],
   "source": [
    "#k should start from 45, end at 198\n",
    "forecast_outcome = []\n",
    "for k in range(300,310):\n",
    "#create an empty time series\n",
    "    timeseries = []\n",
    "    for i in range(num_timeseries):\n",
    "        timeseries.append(np.trim_zeros(data_new_use.iloc[k:k+222, i], trim=\"f\"))\n",
    "        training_data = [\n",
    "            {\n",
    "            \"start\" : str(start_date_use[k]),\n",
    "            \"target\" : ts[:].tolist()\n",
    "            }\n",
    "            for ts in timeseries  \n",
    "        ]\n",
    "            \n",
    "    write_dicts_to_file(\"train.json\", training_data)\n",
    "    copy_to_s3(\"train.json\", s3_data_path + \"/train/train.json\")\n",
    "    \n",
    "    s3filesystem = s3fs.S3FileSystem()\n",
    "    estimator.set_hyperparameters(**hyperparameters)\n",
    "    \n",
    "    data_channels = {\"train\": \"{}/train/\".format(s3_data_path)}\n",
    "    estimator.fit(inputs=data_channels, wait=True)\n",
    "    \n",
    "    predictor = estimator.deploy(\n",
    "    initial_instance_count=1, instance_type=\"ml.m5.large\", predictor_cls=DeepARPredictor\n",
    "    )\n",
    "    \n",
    "    forecast_outcome.append(predictor.predict(ts=timeseries[3], quantiles=[0.5]))\n",
    "    \n",
    "    predictor.delete_endpoint()\n",
    "    \n",
    "print(forecast_outcome)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
