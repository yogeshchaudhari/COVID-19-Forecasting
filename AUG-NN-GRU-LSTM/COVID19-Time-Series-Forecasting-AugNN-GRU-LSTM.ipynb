{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\r\n",
    "import gc\r\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\r\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = \"0\"\r\n",
    "\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import matplotlib.figure as fig\r\n",
    "import statsmodels.api as sm\r\n",
    "import statsmodels.tsa.api as tsa\r\n",
    "# print(\"statsmodels version: \"+sm.__version__)\r\n",
    "%matplotlib inline\r\n",
    "\r\n",
    "import tensorflow as tf\r\n",
    "from tensorflow.keras.models import Sequential\r\n",
    "from tensorflow.keras.layers import LSTM, GRU, Dense, Dropout, LeakyReLU\r\n",
    "from tensorflow.keras import layers\r\n",
    "from tensorflow.keras import optimizers\r\n",
    "from tensorflow.keras import initializers\r\n",
    "from tensorflow.keras import callbacks\r\n",
    "from tensorflow.keras.callbacks import EarlyStopping\r\n",
    "from tensorflow.keras import losses\r\n",
    "import sys\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tf.keras.backend.clear_session()\r\n",
    "tf.executing_eagerly()\r\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\r\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\r\n",
    "if gpus:\r\n",
    "  # Restrict TensorFlow to only use the desired GPU\r\n",
    "    try:\r\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\r\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\r\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\r\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\r\n",
    "        print(\"Memory growth set to: \"+str(tf.config.experimental.get_memory_growth(gpus[0])))\r\n",
    "    except RuntimeError as e:\r\n",
    "        # Visible devices must be set before GPUs have been initialized\r\n",
    "        print(e)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Helper Functions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Generate lagged Y to use as input matrix(X) and future lagged Y as output matrix(Y) for the NeuralNet\r\n",
    "def ts2IF(ts, s, h):\r\n",
    "    m = ts.shape[0] - s - h + 1\r\n",
    "    x = pd.DataFrame(np.random.randint(10, size=(m, s)))\r\n",
    "    y = pd.DataFrame(np.random.randint(10, size=(m, h)))\r\n",
    "    for i in range(m):\r\n",
    "        for j in range(x.shape[1]): x.iloc[i, j] = ts[i + j]\r\n",
    "        for j in range(y.shape[1]): y.iloc[i, j] = ts[i + j + s]\r\n",
    "    #x['ones'] = np.ones(x.shape[0])                        # add first column of all ones\r\n",
    "    x.columns = [\"lag\"+str(x) if(x!=0) else \"origY\" for x in range(1,s+1)]\r\n",
    "#     x.insert(0,\"ones\",np.ones(x.shape[0]).astype(int),True)                # add first column of all ones\r\n",
    "    return (x, y)\r\n",
    "\r\n",
    "def chopr(x,y,te,te_size,tr_size):\r\n",
    "    te2 = te + te_size\r\n",
    "    tr = te - tr_size\r\n",
    "#     print(\"te=\"+str(te))\r\n",
    "#     print(\"te_size=\"+str(te_size))\r\n",
    "#     print(\"tr_size=\"+str(tr_size))\r\n",
    "    x_e = x[te:te2]\r\n",
    "    y_e = y[te:te2]\r\n",
    "    x_r = x[tr:te]\r\n",
    "    y_r = y[tr:te]\r\n",
    "    \r\n",
    "#     print(\"test: x_e(\"+str(te)+\" .. \"+str(te2-1)+\")\")\r\n",
    "#     print(\"test: y_e(\"+str(te)+\" .. \"+str(te2-1)+\")\")\r\n",
    "#     print(\"train: x_r(\"+str(tr)+\" .. \"+str(te-1)+\")\")\r\n",
    "#     print(\"train: y_r(\"+str(tr)+\" .. \"+str(te-1)+\")\")\r\n",
    "    return(x_e, y_e, x_r, y_r)\r\n",
    "\r\n",
    "def shift_rm(xy1,xy2, x_test):\r\n",
    "    d1 = xy1[1].shape[0]\r\n",
    "    d2 = xy2[1].shape[0]\r\n",
    "    print(d1)\r\n",
    "    print(d2)\r\n",
    "    gap = d1 - d2\r\n",
    "    x = pd.DataFrame(np.random.randint(10, size=(d1, xy1[0].shape[1])))\r\n",
    "    y = pd.DataFrame(np.random.randint(10, size=(d1, xy1[1].shape[1])))\r\n",
    "    new_x_test = pd.DataFrame(np.random.randint(10, size=(x_test.shape[0] - d2, x_test.shape[1])))\r\n",
    "    for i in range(y.shape[0]):\r\n",
    "        if i<gap:\r\n",
    "            for j in range(x.shape[1]): x.iloc[i,j] = xy1[0].iloc[i+d2,j].copy()\r\n",
    "            y.iloc[i,:] = xy1[1].iloc[i+d2].copy()\r\n",
    "        else:\r\n",
    "            for j in range(x.shape[1]): x.iloc[i,j] = xy2[0].iloc[i-gap,j].copy()\r\n",
    "            y.iloc[i,:] = xy2[1].iloc[i-gap].copy()\r\n",
    "    for i in range(new_x_test.shape[0]):\r\n",
    "        for j in range(new_x_test.shape[1]): \r\n",
    "            new_x_test.iloc[i,j] = x_test.iloc[i+d2,j].copy()\r\n",
    "#     print(\"old x_r=\"+str(xy1[0].shape))\r\n",
    "#     print(\"old y_r=\"+str(xy1[1].shape))\r\n",
    "#     print(\"x_r=\"+str(x.shape))\r\n",
    "#     print(\"y_r=\"+str(y.shape))\r\n",
    "#     print(\"old x_e=\"+str(xy2[0].shape))\r\n",
    "#     print(\"old y_e=\"+str(xy2[1].shape))\r\n",
    "#     print(\"x_e=\"+str(xy2[0].shape))\r\n",
    "#     print(\"y_e=\"+str(xy2[1].shape))\r\n",
    "    x.columns = xy1[0].columns\r\n",
    "    y.columns = xy1[1].columns\r\n",
    "    new_x_test.columns = x_test.columns\r\n",
    "    return(x,y, new_x_test)\r\n",
    "\r\n",
    "def new_shift_rm(x_r, y_r, x_e, y_e):\r\n",
    "    if(isinstance(x_r, np.ndarray)):\r\n",
    "        x_r = np.delete(x_r, [0], axis=0)\r\n",
    "        x_r = np.concatenate([x_r, [x_e[0]]])\r\n",
    "        x_e = np.delete(x_e, [0], axis=0)\r\n",
    "    else:\r\n",
    "        x_r = x_r.iloc[1:, :]\r\n",
    "        x_r = x_r.append(pd.DataFrame([x_e.iloc[0].to_numpy()], columns = x_e.columns))\r\n",
    "        x_e = x_e.iloc[1:, :]\r\n",
    "        x_r.reset_index(inplace=True, drop= True)\r\n",
    "        x_e.reset_index(inplace=True, drop= True)\r\n",
    "    \r\n",
    "    y_r = y_r.iloc[1:, :]\r\n",
    "    y_r = y_r.append(pd.DataFrame([y_e.iloc[0].to_numpy()], columns = y_e.columns))\r\n",
    "    y_e = y_e.iloc[1:, :]\r\n",
    "    y_r.reset_index(inplace=True, drop= True)\r\n",
    "    y_e.reset_index(inplace=True, drop= True)\r\n",
    "    return (x_r, y_r, x_e, y_e)\r\n",
    "\r\n",
    "#Generic Rescale Method\r\n",
    "from sklearn.preprocessing import minmax_scale, scale\r\n",
    "def rescale(inp,act_func='tanh'):\r\n",
    "    if(act_func=='tanh'): return  minmax_scale(inp,feature_range=(-1,1))\r\n",
    "    if(act_func=='sigmoid'): return  minmax_scale(inp,feature_range=(0,1))\r\n",
    "    if(act_func=='linear' or act_func=='elu'): return  inp\r\n",
    "    else: return inp\r\n",
    "\r\n",
    "# Reverse scaling\r\n",
    "# from sklearn.preprocessing import MinMaxScaler\r\n",
    "# scaler = MinMaxScaler((-1,1))\r\n",
    "# scaler.fit(lagY)\r\n",
    "# yp_unscaled = scaler.inverse_transform(yp)\r\n",
    "# yp_unscaled.shape\r\n",
    "\r\n",
    "def eval(y,yp):\r\n",
    "    import math\r\n",
    "    roundTo = 5\r\n",
    "    m = y.shape[0]\r\n",
    "    e = y - yp\r\n",
    "    yt = y - y.mean()\r\n",
    "    sse = e.dot(e)\r\n",
    "    sst = yt.dot(yt)\r\n",
    "    rSq = round((1 - ((sse)/(sst))),roundTo)\r\n",
    "    mape = round(((np.absolute(e)/np.absolute(y)).sum())*100/m,roundTo)\r\n",
    "    mae = round(((np.absolute(e)).sum())/m,roundTo)\r\n",
    "    t = (np.absolute(e)/(np.absolute(y)+np.absolute(yp)))\r\n",
    "#     print(t.sum()*200/m)\r\n",
    "    smape = round(((np.absolute(e)/(np.absolute(y)+np.absolute(yp))).sum())*200/m,roundTo)\r\n",
    "    # smape  = 200 * (e.abs / (yy.abs + yp.abs)).sum / m\r\n",
    "    mse = round((sse/m),roundTo)\r\n",
    "    rmse = round(math.sqrt(mse),roundTo)\r\n",
    "#     return(rSq,mape,mse,rmse,sse,sst)\r\n",
    "    return(format(rSq,'.4f'),format(mape,'.2f'),format(smape,'.2f'),format(mse,'.2f'),format(rmse,'.2f'),format(sse,'.2f'),format(sst,'.2f'), format(mae,'.2f'))\r\n",
    "\r\n",
    "def plotHistory(fit_history,ls=\"MAPE\"):\r\n",
    "    #Get training loss\r\n",
    "    loss = fit_history.history['loss']\r\n",
    "    # val_loss = fit_history.history['val_loss']\r\n",
    "\r\n",
    "    # Create count of the number of epochs\r\n",
    "    epoch_count = range(1, len(loss) + 1)\r\n",
    "\r\n",
    "#     # Visualize loss history - Plotting Loss vs Number of Epochs during training\r\n",
    "    plt.figure(num=0,figsize=(6,3))\r\n",
    "    plt.plot(epoch_count, loss)\r\n",
    "    # plt.plot(val_loss)\r\n",
    "    plt.legend(['Training Loss - '+str(ls), 'Validation Loss - '+str(ls)])\r\n",
    "    plt.xlabel('Epochs')\r\n",
    "    plt.ylabel(ls)\r\n",
    "    plt.title(\"Loss vs Epochs\")\r\n",
    "    plt.show()\r\n",
    "\r\n",
    "#     print(\"_________________________________________________________________________________________\")\r\n",
    "    print(\"Min Training Loss = \" + str(np.min(loss)) + \" \",end='')\r\n",
    "    print(\"at Epoch \" + str(np.argmin(loss) + 1))\r\n",
    "    # print(\"Min Validation Loss = \" + str(np.min(val_loss)) + \" \",end='')\r\n",
    "    # print(\"at Epoch \" + str(np.argmin(val_loss) + 1))\r\n",
    "#     print(\"__________________________________________________________________________________________\")\r\n",
    "\r\n",
    "#Generic Model Build\r\n",
    "def buildModel(modelName,nfeat,nhidden,act_func='tanh',optimzer=optimizers.Adam(),loss_func='mean_squared_error',dropout=None,horizons=14):\r\n",
    "#     print(\"Create a \" + modelName + \" with \"+ str(nfeat) +\" input, \"+str(nhidden)+\" hidden nodes\")\r\n",
    "    model = Sequential()\r\n",
    "    if modelName == 'perceptron':\r\n",
    "        model.add(Dense(1,input_dim=nfeat,activation=act_func[0]))                  #1st Hidden Layer - 1 neuron\r\n",
    "    if modelName == 'NeuralNet_3L':\r\n",
    "        model.add(Dense(nhidden,input_dim=nfeat,activation=act_func[0]))         #1st Hidden Layer\r\n",
    "        model.add(Dense(1,activation=act_func[1]))                               #Output Layer\r\n",
    "    if modelName == 'NeuralNet_3L_MH':\r\n",
    "        model.add(Dense(nhidden,input_dim=nfeat,activation=act_func[0]))\r\n",
    "        model.add(Dropout(0.1))\r\n",
    "        model.add(Dense(14,activation=act_func[1]))                              #Output Layer - MultiHorizon(10)\r\n",
    "    if modelName == 'NeuralNet_4L':\r\n",
    "        model.add(Dense(nhidden,input_dim=nfeat,activation=act_func[0]))         #1st Hidden Layer\r\n",
    "        model.add(Dense(nhidden,activation=act_func[1]))                         #2nd Hidden Layer\r\n",
    "        if dropout: \r\n",
    "            model.add(Dropout(dropout))\r\n",
    "        model.add(Dense(1,activation=act_func[2]))                               #Output Layer\r\n",
    "    if modelName == 'NeuralNet_4L_MH':\r\n",
    "        model.add(Dense(nhidden,input_dim=nfeat,activation=act_func[0]))         #1st Hidden Layer\r\n",
    "        model.add(Dense(nhidden,activation=act_func[1]))                         #2nd Hidden Layer\r\n",
    "#         if not dropout: model.add(Dropout(dropout))\r\n",
    "        model.add(Dense(Y.shape[1],activation=act_func[2]))                              #Output Layer - MultiHorizon(10)\r\n",
    "    # model.compile(loss='mean_absolute_percentage_error', optimizer=optimzer, metrics=['mean_absolute_percentage_error'])\r\n",
    "    model.compile(loss=loss_func, optimizer=optimzer, metrics=['mean_absolute_percentage_error'])\r\n",
    "    return model\r\n",
    "#     model.compile(loss='mean_squared_error', optimizer=optimzer, metrics=['mean_absolute_percentage_error'])\r\n",
    "#model.add(Dense(nhidden,activation=act_func[0]))   \r\n",
    "#model.add(LeakyReLU())\r\n",
    "#model.add(Dropout(0.18))\r\n",
    "def rollingValidate(model,train,x,y,kt,horizons,callbk,nEpoch,nBatch,plot,p1=False,p2=False,shuffle=False, tr_ratio=0.40808823529):\r\n",
    "    TR_RATIO = tr_ratio\r\n",
    "    m = y.shape[0]\r\n",
    "    tr_size = int(m * TR_RATIO)\r\n",
    "    te_size = int(m - tr_size)\r\n",
    "    te = int(tr_size)\r\n",
    "    if kt < 0: kt = te_size\r\n",
    "    print(\"m = \"+str(m)+\", tr_size = \"+str(tr_size)+\", te_size = \"+str(te_size)+\", kt = \"+str(kt)+\", horizons = \"+str(horizons))\r\n",
    "    \r\n",
    "    (x_e, y_e, x_r, y_r) = chopr (x, y, te, te_size, tr_size)\r\n",
    "#     x_r = x_r[1000:]\r\n",
    "#     y_r = y_r[1000:]\r\n",
    "    print(\"Training shape\"+str(x_r.shape))\r\n",
    "    print(\"Training shape\"+str(y_r.shape))\r\n",
    "    print(\"Testing shape\"+str(x_e.shape))\r\n",
    "    print(\"Testing shape\"+str(y_e.shape))\r\n",
    "    smapearr = [x for x in range(10,(10+horizons))]\r\n",
    "#     for h in range(1,horizons+1):\r\n",
    "# #         print(type(y_r))\r\n",
    "#         yr =  y_r[h-1]\r\n",
    "#         inp = y_e[h-1] #.values.tolist()\r\n",
    "#         yrw = inp[:-h]\r\n",
    "#         for i in range(h):\r\n",
    "#             yrw.insert(i,inp[i])\r\n",
    "#         yrw = np.array(yrw)\r\n",
    "#         rSqr, maper, smaper, mser, rmser, sser, sstr = eval(y_e[h-1],yrw)\r\n",
    "#         smapearr[h-1] = smaper\r\n",
    "\r\n",
    "    rt = 0\r\n",
    "#     print(\"number of re-trainings required = \"+str(int((te_size/kt)+1)))\r\n",
    "    forecast_matrix = pd.DataFrame(columns = y_e.columns)\r\n",
    "    new_x_e = x_e.copy()\r\n",
    "    new_y_e = y_e.copy()\r\n",
    "    fit_history = model.fit(x_r, y_r,validation_split=0.0,shuffle=shuffle,epochs=nEpoch, \\\r\n",
    "                                        batch_size=nBatch,verbose=0, callbacks=None)#callbacks=[tfa.callbacks.TQDMProgressBar(leave_epoch_progress=False,show_epoch_progress=False)]  workers=10, use_multiprocessing=True,\r\n",
    "    for i in range(y_e.shape[0]):\r\n",
    "        print(f\"i: {i} of {y_e.shape[0]}\")\r\n",
    "        if i%kt == 0:\r\n",
    "            rt = rt + 1\r\n",
    "            if train:\r\n",
    "                print(\"training set: \"+str(x_r.shape))\r\n",
    "                if callbk==1:\r\n",
    "                    callb = callbacks.EarlyStopping(monitor='loss', restore_best_weights=True, patience=40,verbose=1)\r\n",
    "                else: callb = None   #batch_input_shape=(10, 5, 1)\r\n",
    "#                 fit_history = model.fit(x_r, y_r,validation_split=0.0,shuffle=True,epochs=nEpoch, \\\r\n",
    "#                                         batch_input_shape=(64, 44, 1),verbose=0, callbacks=callb) #callbacks=[tfa.callbacks.TQDMProgressBar(leave_epoch_progress=False,show_epoch_progress=False)]  workers=10, use_multiprocessing=True,\r\n",
    "                fit_history = model.fit(x_r, y_r,validation_split=0.0,shuffle=shuffle,epochs=nEpoch, \\\r\n",
    "                                        batch_size=nBatch,verbose=0, callbacks=callb) #callbacks=[tfa.callbacks.TQDMProgressBar(leave_epoch_progress=False,show_epoch_progress=False)]  workers=10, use_multiprocessing=True,\r\n",
    "            if plot: plotHistory(fit_history)\r\n",
    "            \r\n",
    "        prediction = model.predict(np.array([new_x_e.iloc[0].to_numpy()]))\r\n",
    "        forecast_matrix = forecast_matrix.append(pd.DataFrame([prediction[0]]))\r\n",
    "        x_r, y_r, new_x_e, new_y_e = new_shift_rm(x_r, y_r, new_x_e, new_y_e)\r\n",
    "#     print(\"\\nOut-Sample Results\")\r\n",
    "#     rw = np.array([2.47,3.79,4.6,5.23,5.75,6.18,6.61,6.94,7.29,7.70]) #RW results - mape for 50% TR\r\n",
    "    rw = np.array([float(x) for x in smapearr]) #calculated using python\r\n",
    "    beats = []\r\n",
    "    prediction = model.predict(x_e,batch_size=x_e.shape[0])\r\n",
    "#     print(type(prediction))\r\n",
    "#     print(prediction.shape)\r\n",
    "#     print(type(y_e))\r\n",
    "#     print(y_e.shape)\r\n",
    "#     print(y_e.columns)\r\n",
    "    rSq  = [x for x in range(y.shape[1])]\r\n",
    "    mse  = [x for x in range(y.shape[1])]\r\n",
    "    sse  = [x for x in range(y.shape[1])]\r\n",
    "    sst  = [x for x in range(y.shape[1])]\r\n",
    "    rmse = [x for x in range(y.shape[1])]\r\n",
    "    mape = [x for x in range(y.shape[1])]\r\n",
    "    smape = [x for x in range(y.shape[1])]\r\n",
    "    mae = [x for x in range(y.shape[1])]\r\n",
    "    maxdif = 0\r\n",
    "    maxdifh = 0\r\n",
    "    for h in range(1,y.shape[1]+1):\r\n",
    "        yf = prediction[:,h-1]\r\n",
    "        rSq[h-1], mape[h-1], smape[h-1], mse[h-1], rmse[h-1], sse[h-1], sst[h-1], mae[h-1] = eval(y_e.values[:,h-1],forecast_matrix.values[:,h-1])\r\n",
    "        if p1:\r\n",
    "            print(str(smape[h-1]))  #,str(rw[h-1])\r\n",
    "#     for h in range(1,y.shape[1]+1):\r\n",
    "#         yf = prediction[:,h-1]\r\n",
    "#         rSq[h-1], mape[h-1], smape[h-1], mse[h-1], rmse[h-1], sse[h-1], sst[h-1], mae = eval(y_e.values[:,h-1],yf)\r\n",
    "#         if p1:\r\n",
    "#             print(str(smape[h-1]))  #,str(rw[h-1])\r\n",
    "\r\n",
    "    return (rSq, mape, smape, mse, rmse, sse, sst, mae, beats, prediction, forecast_matrix, y_e)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "all_data = pd.read_csv(\"2021-08-26-21-42-06-OWID.csv\")\r\n",
    "data = all_data[['new_deaths']]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Augmentation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sarima_forecasts = pd.read_csv('SARIMAX_IN+OUT_FORECASTS_new_dataset.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sarima_forecasts = sarima_forecasts[[\"h1\"]]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "orig = data.iloc[38:-1]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "half = pd.DataFrame((data.iloc[38:-1].values.squeeze() + sarima_forecasts.iloc[1:].values.squeeze())/2)\n",
    "# half"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "quater = pd.DataFrame((orig.values.squeeze() + half.values.squeeze())/2)\n",
    "# quater"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "third_quater = pd.DataFrame((half.values.squeeze() + sarima_forecasts.iloc[1:].values.squeeze())/2)\n",
    "# third_quater"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "augmented_ts = np.arange(2*half.shape[0]).tolist()\n",
    "for i in range(0,half.shape[0]):\n",
    "    augmented_ts[2*i] = orig.iloc[i][0]\n",
    "    augmented_ts[2*i+1] = half.iloc[i][0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Visualization"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dates = all_data[\"date\"].values"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def reformat_date(date):\n",
    "    spl = date.split(\"-\")\n",
    "    return spl[1]+\"-\"+spl[0]+\"-\"+spl[2]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dates = [reformat_date(date) for date in dates]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ma = all_data.new_deaths.rolling(window=7, center=True).mean()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "xtick = np.arange(0, 582, 14)\n",
    "ytick = np.arange(0, 5000, 500)\n",
    "plt.figure(num=0, figsize=(27,7),dpi=100)\n",
    "# plt.plot(dat.values.squeeze())\n",
    "plt.plot(dates, data.values)\n",
    "plt.plot(ma.values, \"r\")\n",
    "plt.legend(['Original Time Series', '7-day Moving Average'])\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Deaths')\n",
    "# x1,x2,y1,y2 = plt.axis()\n",
    "# plt.axis((x1,x2,0,101))\n",
    "plt.yticks(ytick)\n",
    "plt.xticks(xtick, rotation=25)\n",
    "plt.title(\"Time Series Plot - Daily Deaths\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.figure(num=0, figsize=(27,7),dpi=100)\n",
    "plt.plot(augmented_ts)\n",
    "plt.legend(['augmented time series'])\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Deaths/Hosp')\n",
    "# x1,x2,y1,y2 = plt.axis()\n",
    "# plt.axis((x1,x2,0,101))\n",
    "# plt.yticks(np.arange(0, 101, 10.0))\n",
    "plt.title(\"Time Series Plot - Response variable\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ACF Plot"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels.tsa.api as tsa\n",
    "fig = sm.graphics.tsa.plot_acf(orig.values.squeeze())\n",
    "# fig = sm.graphics.tsa.plot_acf(x=np.array(augmented_ts))\n",
    "# fig.suptitle(\"ACF Plot\")\n",
    "axes = fig.axes\n",
    "axes[0].set_title(\"Auto Correlation Function Plot - New Deaths\")\n",
    "axes[0].set_xlabel(\"Lags\")\n",
    "axes[0].set_ylabel(\"Correlation Coefficient\")\n",
    "axes[0].set_xticks(np.arange(0, 31, 3))\n",
    "fig._set_dpi(120)\n",
    "fig.set_size_inches(10,5)\n",
    "fig.show()\n",
    "axes[0].set_yticks(np.arange(0, 1.1, 0.1))\n",
    "fig.savefig(\"./COVID/ACFPlot.png\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import statsmodels\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.tsa.api as tsa\n",
    "fig = sm.graphics.tsa.plot_pacf(orig.values.squeeze())\n",
    "# fig = sm.graphics.tsa.plot_acf(x=np.array(augmented_ts))\n",
    "# fig.suptitle(\"ACF Plot\")\n",
    "axes = fig.axes\n",
    "axes[0].set_title(\"Partial Auto Correlation Function - New Deaths\")\n",
    "axes[0].set_xlabel(\"Lags\")\n",
    "axes[0].set_ylabel(\"Correlation Coefficient\")\n",
    "axes[0].set_xticks(np.arange(0, 31, 3))\n",
    "fig._set_dpi(120)\n",
    "fig.set_size_inches(10,5)\n",
    "fig.show()\n",
    "axes[0].set_yticks(np.arange(-0.4, 1.1, 0.1))\n",
    "fig.savefig(\"./COVID/PACFPlot.png\")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels.tsa.api as tsa\n",
    "fig = sm.graphics.tsa.plot_acf(np.array(augmented_ts))\n",
    "# fig = sm.graphics.tsa.plot_acf(x=np.array(augmented_ts))\n",
    "# fig.suptitle(\"ACF Plot\")\n",
    "axes = fig.axes\n",
    "axes[0].set_title(\"Autocorrelation Plot - Augmented Series\")\n",
    "axes[0].set_xlabel(\"Lags\")\n",
    "axes[0].set_ylabel(\"Correlation Coefficient\")\n",
    "axes[0].set_xticks(np.arange(0, 31, 3))\n",
    "fig._set_dpi(120)\n",
    "fig.set_size_inches(10,5)\n",
    "fig.show()\n",
    "# axes[0].set_yticks(np.arange(0, 1, 0.1))\n",
    "# fig.savefig(\"./COVID/Test.png\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels.tsa.api as tsa\n",
    "fig = sm.graphics.tsa.plot_pacf(np.array(augmented_ts))\n",
    "# fig = sm.graphics.tsa.plot_acf(x=np.array(augmented_ts))\n",
    "# fig.suptitle(\"ACF Plot\")\n",
    "axes = fig.axes\n",
    "axes[0].set_title(\"Autocorrelation Plot - Augmented Series\")\n",
    "axes[0].set_xlabel(\"Lags\")\n",
    "axes[0].set_ylabel(\"Correlation Coefficient\")\n",
    "axes[0].set_xticks(np.arange(0, 31, 3))\n",
    "fig._set_dpi(120)\n",
    "fig.set_size_inches(10,5)\n",
    "fig.show()\n",
    "# axes[0].set_yticks(np.arange(0, 1, 0.1))\n",
    "# fig.savefig(\"./COVID/Test.png\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<hr style=\"border:5px solid white\"> </hr>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Neural Nets"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "augmented_ts = np.array(augmented_ts)\n",
    "augmented_ts = np.reshape(augmented_ts,(-1,2))\n",
    "augmented_ts = np.reshape(augmented_ts,(-1,1))\n",
    "augmented_ts.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Generating X and Y matrices"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "elim_samples = 0\n",
    "lags = 52\n",
    "horizons = 28"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(augmented_ts)\n",
    "sc = MinMaxScaler(feature_range = (-1, 1))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "lagX, lagY = ts2IF(np.array(orig),lags,horizons)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "lagX, lagY = ts2IF(np.array(augmented_ts),lags,horizons)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X = pd.DataFrame(lagX)\n",
    "X.columns = lagX.columns\n",
    "feat = X.shape[1]\n",
    "print(X.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "time = [float(x) for x in range(1,X.shape[0]+1)]\n",
    "X['time'] = time"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "Y = pd.DataFrame(rescale(lagY,\"linear\"))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Network Architecture Search - RNN"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\n",
    "import autokeras as ak\n",
    "rsX = pd.DataFrame(rescale(X,\"linear\"))\n",
    "lX = rsX.values.reshape(rsX.shape[0],rsX.shape[1],1) #(batch_size, timestep/window, no. features/lags)\n",
    "lY = Y.values.reshape(Y.shape[0],Y.shape[1],1)\n",
    "input_layer = ak.Input()\n",
    "rnn_layer = ak.RNNBlock(layer_type=\"lstm\")(input_layer)\n",
    "dense_layer = ak.DenseBlock()(rnn_layer)\n",
    "output_layer = ak.RegressionHead()(dense_layer)\n",
    "\n",
    "automodel = ak.AutoModel(input_layer, output_layer, max_trials=10, loss='mean_absolute_percentage_error',metrics='mean_absolute_percentage_error',overwrite=True,objective='val_loss', project_name='NAS')\n",
    "automodel.fit(lX, Y.values, validation_split=0.4, epochs=500, batch_size=32, verbose=0)\n",
    "model = automodel.export_model()\n",
    "print(model.summary())\n",
    "rSq1, mape1, smape1, mse1, rmse1, sse1, sst1, beats, prediction, y_e = rollingValidate(model,True,lX,Y,-1,horizons,callbk=1,nEpoch=1\n",
    "                                                                                       ,nBatch=32,plot=True,p1=True,p2=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "true_pred = [x for ind,x in enumerate(prediction[:,1].squeeze().tolist()) if (ind+1)%2==0]\n",
    "true_data = [x for ind,x in enumerate(y_e.values.squeeze()[:,1].tolist()) if (ind+1)%2==0]\n",
    "dates = range(1, len(true_data) + 1)\n",
    "# ytick = np.arange(0, 5000, 500)\n",
    "# xtick = np.arange(0, 133, 7)\n",
    "plt.figure(num=0, figsize=(20,8))\n",
    "plt.plot(dates,true_data)\n",
    "plt.plot(dates,true_pred)\n",
    "plt.legend(['True Value','Forecasted Value'])\n",
    "plt.xlabel('Days')\n",
    "plt.ylabel('Daily Deaths')\n",
    "# plt.axis((x1,x2,0,101))\n",
    "# plt.xticks(xtick)\n",
    "# plt.yticks(ytick)\n",
    "plt.title(\"Time Series Plot - Response vs Out-Of-Sample Forecasts(h=1)\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<hr style=\"border:2px solid white\"> </hr>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Network Architecture Search - Neural Nets"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "feat = X.shape[1]\n",
    "l1 = X.columns.tolist()\n",
    "l2 = ['numerical' for x in l1]\n",
    "# print(l1)\n",
    "# print(l2)\n",
    "col_typ = dict(zip(l1,l2))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import autokeras as ak\n",
    "reg = ak.StructuredDataRegressor(max_trials=500,column_names=l1,column_types=col_typ, output_dim=Y.shape[1], loss='mean_absolute_percentage_error',metrics='mean_absolute_percentage_error',overwrite=True,objective='val_loss', project_name='NAS')\n",
    "reg.fit(x=X, y=Y, epochs=200, validation_split=0.6, validation_data=None, verbose=0)\n",
    "model = reg.export_model()\n",
    "print(model.summary())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "rSq1, mape1, smape1, mse1, rmse1, sse1, sst1, beats, prediction, y_e = rollingValidate(model,True,X,Y, 5,horizons,callbk=0,nEpoch=200\n",
    "                                                                        ,nBatch=32,plot=False,p1=True,p2=True, shuffle=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32,input_dim=feat, activation='linear', kernel_initializer='he_normal'))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.ReLU())\n",
    "# model.add(Dropout(0.1))\n",
    "model.add(Dense(1024,activation='linear', kernel_initializer='he_normal'))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.ReLU())\n",
    "# model.add(Dropout(0.1))\n",
    "model.add(Dense(Y.shape[1],activation='linear'))\n",
    "model.compile(loss='mean_absolute_percentage_error', optimizer='Adam', metrics=['mean_absolute_percentage_error'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\n",
    "rSq1, mape1, smape1, mse1, rmse1, sse1, sst1, mae1, beats, prediction,fm, y_e = rollingValidate(model,True,X,Y,5,horizons,callbk=0,nEpoch=200\n",
    "                                                                        ,nBatch=32,plot=False,p1=True,p2=True, shuffle=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "true_pred = [x for ind,x in enumerate(prediction[:,0].squeeze().tolist()) if (ind+1)%2==0]\n",
    "true_data = [x for ind,x in enumerate(y_e.values.squeeze()[:,0].tolist()) if (ind+1)%2==0]\n",
    "dates = range(1, len(true_data) + 1)\n",
    "# ytick = np.arange(0, 5000, 500)\n",
    "# xtick = np.arange(0, 133, 7)\n",
    "plt.figure(num=0, figsize=(20,8))\n",
    "plt.plot(dates,true_data)\n",
    "plt.plot(dates,true_pred)\n",
    "plt.legend(['True Value','Forecasted Value'])\n",
    "plt.xlabel('Days')\n",
    "plt.ylabel('Daily Deaths')\n",
    "# plt.axis((x1,x2,0,101))\n",
    "# plt.xticks(xtick)\n",
    "# plt.yticks(ytick)\n",
    "plt.title(\"Time Series Plot - Response vs Out-Of-Sample Forecasts(h=1)\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Data Pre-Processing for LSTM\n",
    "import copy\n",
    "X_np = copy.deepcopy(X.values.tolist())\n",
    "for i in range(X.shape[0]):\n",
    "    new_dim = copy.copy(X_np[i])\n",
    "#     print(new_dim)\n",
    "#     if i==5: break\n",
    "    for j in range(X.shape[1]):\n",
    "        X_np[i][j] = new_dim\n",
    "#         print(X_np[i][j])\n",
    "X_np1 = np.array(X_np)\n",
    "print(X_np1.shape)\n",
    "lX = rescale(X_np1,\"linear\")\n",
    "# lX = rsX.values.reshape(rsX.shape[0],rsX.shape[1],1) #(batch_size, timestep/window, no. features/lags)\n",
    "print(lX.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Build RNN - Generic\n",
    "from tensorflow.keras.layers import TimeDistributed, Bidirectional, RepeatVector\n",
    "from tensorflow.keras.optimizers import Adamax\n",
    "\n",
    "def buildRNN(modelName,nUnits,act_func='linear',optimzer=Adamax(learning_rate=0.005, beta_1=0.9, beta_2=0.999, epsilon=1e-07)):\n",
    "#     print(\"Create a \" + modelName + \" with \"+ str(nUnits) +\" units and 10 output nodes\")\n",
    "    model = Sequential()\n",
    "    if modelName == 'GRU':\n",
    "        model.add(GRU(nUnits, input_shape=(lX.shape[1], lX.shape[2])))\n",
    "    if modelName == 'LSTM':\n",
    "        model.add(GRU(256, activation='relu', return_sequences=True, input_shape=(lX.shape[1], lX.shape[2])))\n",
    "#         model.add(RepeatVector(28))\n",
    "        model.add(GRU(92, activation='relu', return_sequences=True))\n",
    "#         model.add(RepeatVector(28))\n",
    "        model.add(GRU(128, activation='relu'))\n",
    "#         model.add(RepeatVector(28))\n",
    "    model.add(Dense(28))  \n",
    "    model.compile(loss='mean_absolute_percentage_error', optimizer='adamax', metrics=['mean_absolute_percentage_error'])\n",
    "#     model.compile(loss='mean_squared_error', optimizer=optimzer, metrics=['mean_squared_error'])\n",
    "    model.build((None,lX.shape[1], lX.shape[2]))\n",
    "    print(model.summary())\n",
    "    return model"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}